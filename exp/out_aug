(0.24300000000005184, array([-1.000000e+00, -1.000000e+00,  1.189783e+03,  1.790000e-01,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 4, array([  0.   ,   0.   , -15.851,   0.091,   0.   ,   0.   ,   0.   ,
         0.   ,   0.   ,   0.   ,   0.   ,   0.   ,   0.   ,   0.   ]))
((0.236, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.364685e+03, -3.300000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), array([0., 0., 0.])), (0.24, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.543573e+03, -3.290000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), array([0., 0., 0.])))
(0.236, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.364685e+03, -3.300000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([0.   , 0.   , 0.   , 0.   , 5.823, 3.129, 0.   , 0.   , 0.   ,
       0.   , 0.   , 0.   , 0.   , 0.   ]))
(0.236, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.364685e+03, -3.300000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([ 0.   ,  0.   ,  0.   ,  0.   , -8.411, -0.011,  0.   ,  0.   ,
        0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ]))
(0.236, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.364685e+03, -3.300000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  6.2433e+01,
       -3.0000e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
        0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]))
(0.236, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.364685e+03, -3.300000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([ 0.00000e+00,  0.00000e+00,  0.00000e+00,  0.00000e+00,
       -1.19043e+02, -4.00000e-03,  0.00000e+00,  0.00000e+00,
        0.00000e+00,  0.00000e+00,  0.00000e+00,  0.00000e+00,
        0.00000e+00,  0.00000e+00]))
(0.236, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.364685e+03, -3.300000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  5.0464e+01,
       -1.3000e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
        0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]))
(0.236, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.364685e+03, -3.300000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([ 0.00000e+00,  0.00000e+00,  0.00000e+00,  0.00000e+00,
       -1.53656e+02, -1.00000e-02,  0.00000e+00,  0.00000e+00,
        0.00000e+00,  0.00000e+00,  0.00000e+00,  0.00000e+00,
        0.00000e+00,  0.00000e+00]))
(0.236, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.364685e+03, -3.300000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([ 0.000000e+00,  0.000000e+00,  2.399328e+03,  1.498000e+00,
       -1.455700e+01, -6.000000e-03,  0.000000e+00,  0.000000e+00,
        0.000000e+00,  0.000000e+00,  0.000000e+00,  0.000000e+00,
        0.000000e+00,  0.000000e+00]))
(0.236, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.364685e+03, -3.300000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  1.9086e+01,
       -9.0000e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
        0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]))
(0.236, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.364685e+03, -3.300000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([ 0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  7.052e+01,
       -2.000e-02,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,
        0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00]))
(0.236, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.364685e+03, -3.300000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  4.1406e+01,
       -1.7000e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
        0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]))
14 1 14

 ===== Epoch 1	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484  2.7624774   1.9472378
 -0.3536378  -0.35852543  2.3152876   0.92958707 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071  0.02132734  0.03107306 -0.18920934  0.29762688
 -0.03495734 -0.02773637  0.19301265  0.24765371 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 3 1
train:	 Loss = 1.3717,	 Acc = 0.4788
57318 0.295
110965 0.525
60326 0.559
6176 0.576
903 0.496
88 0.33
0 0.0
0 0.0
0.5377231617523451
0.0
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.4275,	 Acc = 0.4592
6218 0.41
33592 0.5
17796 0.405
1318 0.393
29 0.0
0 0.0
0 0.0
0 0.0
0.46506115483075755
0.46506115483075755
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 0
Testing:	 Loss = 1.4823,	 Acc = 0.4426

 ===== Epoch 2	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071  0.02132734  0.03107306  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 3 1
train:	 Loss = 1.2942,	 Acc = 0.5015
57324 0.3
110966 0.554
60319 0.585
6176 0.607
903 0.464
88 0.239
0 0.0
0 0.0
0.5660513751597068
0.46506115483075755
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3753,	 Acc = 0.4758
6218 0.437
33592 0.502
17796 0.442
1318 0.45
29 0.0
0 0.0
0 0.0
0 0.0
0.48040201005025124
0.48040201005025124
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.4281,	 Acc = 0.4587

 ===== Epoch 3	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103  1.7318157   1.1934364
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071  0.02132734  0.03107306  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436  0.2253556   0.23141134
 -0.02464492 -0.01491357] 2 2
train:	 Loss = 1.2814,	 Acc = 0.5045
57320 0.301
110964 0.557
60326 0.592
6175 0.608
903 0.464
88 0.295
0 0.0
0 0.0
0.5697819070247008
0.48040201005025124
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3200,	 Acc = 0.4668
6218 0.41
33592 0.47
17796 0.48
1318 0.482
29 0.0
0 0.0
0 0.0
0 0.0
0.4734426851237319
0.48040201005025124
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.3806,	 Acc = 0.4485

 ===== Epoch 4	 =====
[ 0.8321287   3.0839717  -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.03917291 -0.04090034  0.02132734  0.03107306  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 0 0
train:	 Loss = 1.2750,	 Acc = 0.5051
57323 0.302
110964 0.556
60322 0.594
6176 0.606
903 0.468
88 0.307
0 0.0
0 0.0
0.570155727278331
0.48040201005025124
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3966,	 Acc = 0.4636
6218 0.41
33592 0.495
17796 0.425
1318 0.439
29 0.0
0 0.0
0 0.0
0 0.0
0.46989665307670425
0.48040201005025124
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 0
Testing:	 Loss = 1.4707,	 Acc = 0.4432

 ===== Epoch 5	 =====
[ 0.19318686  1.5319136  -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02653641 -0.03388147  0.02132734  0.03107306  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 0 0
train:	 Loss = 1.2717,	 Acc = 0.5071
57321 0.303
110967 0.559
60320 0.596
6177 0.614
903 0.513
88 0.307
0 0.0
0 0.0
0.572676585133507
0.48040201005025124
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3348,	 Acc = 0.4895
6218 0.43
33592 0.505
17796 0.484
1318 0.461
29 0.0
0 0.0
0 0.0
0 0.0
0.4964824120603015
0.4964824120603015
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.3988,	 Acc = 0.4699

 ===== Epoch 6	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.2429922   2.9709227  -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-2.6300831e-02 -3.8560715e-02  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -1.7614663e+00 -3.4121888e+00 -2.7775567e-02 -1.1532969e-02
 -2.4644915e-02 -1.4913575e-02] 1 1
train:	 Loss = 1.2698,	 Acc = 0.5083
57325 0.304
110962 0.56
60323 0.595
6175 0.623
903 0.565
88 0.398
0 0.0
0 0.0
0.5740455363096872
0.4964824120603015
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3165,	 Acc = 0.4867
6218 0.434
33592 0.497
17796 0.487
1318 0.464
29 0.0
0 0.0
0 0.0
0 0.0
0.4929174172750545
0.4964824120603015
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 0
Testing:	 Loss = 1.3831,	 Acc = 0.4665

 ===== Epoch 7	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  0.9799096   2.2178547  -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071  0.02132734  0.03107306  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.09997535  1.0310395  -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 6 6
train:	 Loss = 1.2684,	 Acc = 0.5081
57318 0.302
110964 0.559
60326 0.597
6177 0.626
903 0.569
88 0.33
0 0.0
0 0.0
0.5742583689159354
0.4964824120603015
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3702,	 Acc = 0.4824
6218 0.457
33592 0.525
17796 0.418
1318 0.382
29 0.0
0 0.0
0 0.0
0 0.0
0.48538921020195314
0.4964824120603015
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 0
Testing:	 Loss = 1.4407,	 Acc = 0.4621

 ===== Epoch 8	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.0145571   2.6583285  -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-2.6300831e-02 -3.8560715e-02  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -1.5135795e+00 -3.0920436e+00 -2.7775567e-02 -1.1532969e-02
 -2.4644915e-02 -1.4913575e-02] 1 6
train:	 Loss = 1.2684,	 Acc = 0.5087
57318 0.302
110967 0.559
60325 0.599
6176 0.629
902 0.57
88 0.33
0 0.0
0 0.0
0.574975624516693
0.4964824120603015
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.4303,	 Acc = 0.4577
6218 0.429
33592 0.483
17796 0.426
1318 0.398
29 0.0
0 0.0
0 0.0
0 0.0
0.461135868019342
0.4964824120603015
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.4803,	 Acc = 0.4396

 ===== Epoch 9	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
  1.8144374   1.1832851  -0.42422777 -0.43446103  1.959729    3.1801689
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071  0.02132734  0.03107306  0.00132258 -0.00350269
 -0.12294013 -0.03328341  0.04772128  0.07545436 -0.01602104 -0.02198219
 -0.02464492 -0.01491357] 0 0
train:	 Loss = 1.2673,	 Acc = 0.5088
57321 0.302
110962 0.559
60326 0.599
6177 0.632
902 0.573
88 0.341
0 0.0
0 0.0
0.575198229245468
0.4964824120603015
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3332,	 Acc = 0.4875
6218 0.428
33592 0.519
17796 0.452
1318 0.449
29 0.0
0 0.0
0 0.0
0 0.0
0.49456717550014223
0.4964824120603015
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.3917,	 Acc = 0.4679

 ===== Epoch 10	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484  1.7873005   1.0464745
 -0.3536378  -0.35852543  5.445262    0.88696057 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071  0.02132734  0.03107306 -0.00844875  0.04770983
 -0.03495734 -0.02773637 -6.321569   -1.277887   -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 3 3
train:	 Loss = 1.2662,	 Acc = 0.5098
57322 0.303
110965 0.561
60321 0.6
6177 0.629
903 0.542
88 0.341
0 0.0
0 0.0
0.576378226321629
0.4964824120603015
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3952,	 Acc = 0.4554
6218 0.426
33592 0.471
17796 0.436
1318 0.476
29 0.0
0 0.0
0 0.0
0 0.0
0.4589741158623305
0.4964824120603015
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.4531,	 Acc = 0.4390

 ===== Epoch 11	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-2.6300831e-02 -3.8560715e-02  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
  1.2419709e+00  3.1823184e+00  3.8678856e+00  1.2789453e+00
  2.8681676e+00  3.2684479e+00] 6 5
train:	 Loss = 1.2654,	 Acc = 0.5095
57322 0.303
110967 0.561
60320 0.6
6177 0.623
902 0.498
88 0.33
0 0.0
0 0.0
0.5758514799332041
0.4964824120603015
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3978,	 Acc = 0.4641
6218 0.407
33592 0.487
17796 0.443
1318 0.44
29 0.0
0 0.0
0 0.0
0 0.0
0.47078790177301605
0.4964824120603015
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.4678,	 Acc = 0.4448

 ===== Epoch 12	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 2.9538233e+00  2.6800807e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03  1.7509509e+00  3.5223703e+00
  4.7721278e-02  7.5454362e-02 -2.7775567e-02 -1.1532969e-02
 -2.4644915e-02 -1.4913575e-02] 4 5
train:	 Loss = 1.2651,	 Acc = 0.5083
57323 0.303
110969 0.559
60318 0.599
6175 0.614
903 0.474
88 0.364
0 0.0
0 0.0
0.5741735919261655
0.4964824120603015
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3652,	 Acc = 0.4766
6218 0.409
33592 0.499
17796 0.463
1318 0.435
29 0.0
0 0.0
0 0.0
0 0.0
0.4845548497202996
0.4964824120603015
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.4257,	 Acc = 0.4573

 ===== Epoch 13	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103  1.8132983   2.8389363
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071  0.02132734  0.03107306  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436  0.01252308  0.4586818
 -0.02464492 -0.01491357] 4 4
train:	 Loss = 1.2649,	 Acc = 0.5085
57320 0.302
110964 0.56
60324 0.599
6177 0.617
903 0.48
88 0.352
0 0.0
0 0.0
0.5748475814766665
0.4964824120603015
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3586,	 Acc = 0.4854
6218 0.394
33592 0.525
17796 0.451
1318 0.395
29 0.0
0 0.0
0 0.0
0 0.0
0.49617900824879113
0.4964824120603015
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.4166,	 Acc = 0.4680

 ===== Epoch 14	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484  2.2797706   2.1165543
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071  0.02132734  0.03107306  0.04218311 -0.0485697
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 2 0
train:	 Loss = 1.2656,	 Acc = 0.5092
57323 0.302
110961 0.561
60324 0.6
6177 0.613
903 0.481
88 0.352
0 0.0
0 0.0
0.5756025396042656
0.4964824120603015
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.4031,	 Acc = 0.4621
6218 0.279
33592 0.513
17796 0.434
1318 0.414
29 0.0
0 0.0
0 0.0
0 0.0
0.48375841471508485
0.4964824120603015
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.4528,	 Acc = 0.4455

 ===== Epoch 15	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.8437368   1.7750126  -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071  0.02132734  0.03107306  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.6562864   0.04149956 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 2 2
train:	 Loss = 1.2649,	 Acc = 0.5090
57320 0.301
110965 0.561
60324 0.601
6176 0.613
903 0.488
88 0.364
0 0.0
0 0.0
0.5756881248038732
0.4964824120603015
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.2931,	 Acc = 0.4974
6218 0.444
33592 0.513
17796 0.491
1318 0.461
29 0.0
0 0.0
0 0.0
0 0.0
0.5037261780601119
0.5037261780601119
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 0
Testing:	 Loss = 1.3597,	 Acc = 0.4759

 ===== Epoch 16	 =====
[ 2.75161     3.295848   -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
  2.3456154   3.4278994 ] [-0.02895639 -0.03154185  0.02132734  0.03107306  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 0 0
train:	 Loss = 1.2657,	 Acc = 0.5086
57318 0.302
110965 0.56
60326 0.599
6176 0.607
903 0.496
88 0.364
0 0.0
0 0.0
0.5748019141758846
0.5037261780601119
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3206,	 Acc = 0.4915
6218 0.418
33592 0.526
17796 0.459
1318 0.417
29 0.0
0 0.0
0 0.0
0 0.0
0.5001611832748649
0.5037261780601119
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 0
Testing:	 Loss = 1.3839,	 Acc = 0.4734

 ===== Epoch 17	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071  0.02132734  0.03107306  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 1 1
train:	 Loss = 1.2664,	 Acc = 0.5082
57324 0.302
110959 0.561
60326 0.598
6176 0.611
903 0.483
88 0.33
0 0.0
0 0.0
0.5745802792907897
0.5037261780601119
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3415,	 Acc = 0.4764
6218 0.412
33592 0.495
17796 0.461
1318 0.517
29 0.0
0 0.0
0 0.0
0 0.0
0.4840238930501564
0.5037261780601119
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.3956,	 Acc = 0.4584

 ===== Epoch 18	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-2.6300831e-02 -3.8560715e-02  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
  1.0890324e+00  2.9713137e+00 -2.7775567e-02 -1.1532969e-02
 -2.4644915e-02 -1.4913575e-02] 6 5
train:	 Loss = 1.2656,	 Acc = 0.5083
57323 0.302
110964 0.56
60322 0.599
6176 0.617
903 0.481
88 0.409
0 0.0
0 0.0
0.5745042111928631
0.5037261780601119
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3711,	 Acc = 0.4723
6218 0.386
33592 0.487
17796 0.476
1318 0.452
29 0.0
0 0.0
0 0.0
0 0.0
0.48239309756328813
0.5037261780601119
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.4268,	 Acc = 0.4540

 ===== Epoch 19	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  2.8226602   1.9786726  -0.4190337  -0.41667622
  4.0626187   1.8008624 ] [-0.02630083 -0.03856071  0.02132734  0.03107306  0.00132258 -0.00350269
 -0.03495734 -0.02773637 -0.0070799   0.09000642 -0.02777557 -0.01153297
 -0.27020553  0.18026996] 3 3
train:	 Loss = 1.2656,	 Acc = 0.5089
57317 0.303
110971 0.561
60323 0.599
6174 0.614
903 0.474
88 0.375
0 0.0
0 0.0
0.5751853366879788
0.5037261780601119
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3309,	 Acc = 0.4846
6218 0.449
33592 0.504
17796 0.465
1318 0.438
29 0.0
0 0.0
0 0.0
0 0.0
0.4887835403432256
0.5037261780601119
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.3818,	 Acc = 0.4686

 ===== Epoch 20	 =====
[-0.378686   -0.39794773  3.0030165   2.7815344   2.214161    0.88167304
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071  0.96787006 -6.250083    0.21361722  0.3283544
 -0.03495734 -0.02773637  1.8620069   1.7077101  -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 2 2
train:	 Loss = 1.2656,	 Acc = 0.5091
57322 0.302
110963 0.561
60324 0.6
6176 0.613
903 0.485
88 0.284
0 0.0
0 0.0
0.5756049177939413
0.5037261780601119
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3249,	 Acc = 0.4933
6218 0.428
33592 0.533
17796 0.438
1318 0.542
29 0.0
0 0.0
0 0.0
0 0.0
0.5009955437565184
0.5037261780601119
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.3787,	 Acc = 0.4757

 ===== Epoch 21	 =====
[ 3.2697153   3.4668806  -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-3.3148196e+00 -3.5807498e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
  4.7721278e-02  7.5454362e-02 -2.7775567e-02 -1.1532969e-02
 -2.4644915e-02 -1.4913575e-02] 3 1
train:	 Loss = 1.2652,	 Acc = 0.5102
57318 0.304
110967 0.562
60324 0.599
6177 0.618
902 0.471
88 0.364
0 0.0
0 0.0
0.5762980645305898
0.5037261780601119
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3686,	 Acc = 0.4767
6218 0.417
33592 0.509
17796 0.438
1318 0.453
29 0.0
0 0.0
0 0.0
0 0.0
0.48373945197686546
0.5037261780601119
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.4212,	 Acc = 0.4600

 ===== Epoch 22	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-2.6300831e-02 -3.8560715e-02  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
  4.7721278e-02  7.5454362e-02  2.3717642e+00  1.7360986e+00
 -2.4644915e-02 -1.4913575e-02] 6 5
train:	 Loss = 1.2642,	 Acc = 0.5088
57320 0.303
110972 0.56
60318 0.6
6175 0.618
903 0.466
88 0.341
0 0.0
0 0.0
0.5749652575424754
0.5037261780601119
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3862,	 Acc = 0.4808
6218 0.414
33592 0.512
17796 0.45
1318 0.432
29 0.0
0 0.0
0 0.0
0 0.0
0.488612875699251
0.5037261780601119
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.4405,	 Acc = 0.4623

 ===== Epoch 23	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484  0.14898692  0.8997335
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071  2.3784204   1.312355   -0.65381336 -1.1404204
  3.7506423   3.918984    0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 5 5
train:	 Loss = 1.2628,	 Acc = 0.5095
57319 0.302
110967 0.562
60322 0.599
6177 0.624
903 0.481
88 0.409
0 0.0
0 0.0
0.5759706820130339
0.5037261780601119
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.2725,	 Acc = 0.5044
6218 0.386
33592 0.516
17796 0.526
1318 0.491
29 0.0
0 0.0
0 0.0
0 0.0
0.5184033374419266
0.5184033374419266
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.3436,	 Acc = 0.4822

 ===== Epoch 24	 =====
[-0.378686   -0.39794773  2.5179577   3.2023258   2.4293482   1.159352
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071  0.12184511 -0.15167387  0.11324318 -0.13050972
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 2 3
train:	 Loss = 1.2625,	 Acc = 0.5110
57322 0.302
110962 0.564
60324 0.601
6177 0.618
903 0.478
88 0.409
0 0.0
0 0.0
0.5780649355015859
0.5184033374419266
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3941,	 Acc = 0.4610
6218 0.39
33592 0.495
17796 0.423
1318 0.434
29 0.0
0 0.0
0 0.0
0 0.0
0.46927088271546413
0.5184033374419266
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.4493,	 Acc = 0.4426

 ===== Epoch 25	 =====
[ 3.2957335   2.1164882  -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
  3.0856216   2.2166038 ] [-4.1385878e-02 -4.8074940e-01  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
  3.4130542e+00  3.7134686e+00 -2.7775567e-02 -1.1532969e-02
  5.0458027e-04 -2.6624578e-01] 5 5
train:	 Loss = 1.2609,	 Acc = 0.5123
57326 0.303
110960 0.566
60323 0.6
6176 0.629
903 0.474
88 0.375
0 0.0
0 0.0
0.5794059960773326
0.5184033374419266
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.2903,	 Acc = 0.5095
6218 0.432
33592 0.545
17796 0.472
1318 0.491
29 0.0
0 0.0
0 0.0
0 0.0
0.5186877785152176
0.5186877785152176
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.3595,	 Acc = 0.4883

 ===== Epoch 26	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071  0.02132734  0.03107306  1.0255324   1.6926558
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 6 6
train:	 Loss = 1.2615,	 Acc = 0.5129
57323 0.303
110964 0.567
60322 0.602
6176 0.625
903 0.465
88 0.364
0 0.0
0 0.0
0.580326472516573
0.5186877785152176
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3552,	 Acc = 0.4910
6218 0.421
33592 0.532
17796 0.44
1318 0.488
29 0.0
0 0.0
0 0.0
0 0.0
0.49925097184033373
0.5186877785152176
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.4120,	 Acc = 0.4717

 ===== Epoch 27	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103  1.5070508   1.752047
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071  0.02132734  0.03107306  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.01476227 -0.12647435
 -0.02464492 -0.01491357] 3 3
train:	 Loss = 1.2603,	 Acc = 0.5140
57322 0.303
110965 0.57
60322 0.601
6176 0.619
903 0.482
88 0.352
0 0.0
0 0.0
0.5817185381106615
0.5186877785152176
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 2
val:	 Loss = 1.3592,	 Acc = 0.4895
6218 0.408
33592 0.53
17796 0.447
1318 0.426
29 0.0
0 0.0
0 0.0
0 0.0
0.49913719541101736
0.5186877785152176
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.4139,	 Acc = 0.4700

 ===== Epoch 28	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484  1.7915959   2.2384622
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071  0.02132734  0.03107306  0.0093173   0.40619743
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 6 4
train:	 Loss = 1.2602,	 Acc = 0.5138
57322 0.304
110963 0.569
60325 0.601
6176 0.617
902 0.467
88 0.375
0 0.0
0 0.0
0.5811581696123371
0.5186877785152176
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 2
val:	 Loss = 1.4404,	 Acc = 0.4657
6218 0.4
33592 0.515
17796 0.404
1318 0.35
29 0.0
0 0.0
0 0.0
0 0.0
0.47342372238551245
0.5186877785152176
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.4875,	 Acc = 0.4492

 ===== Epoch 29	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
  1.1559484   1.3847717  -0.42422777 -0.43446103  2.3730445   2.8161874
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071  0.02132734  0.03107306  0.00132258 -0.00350269
  0.00174914  0.47704443  0.04772128  0.07545436 -0.1646254   0.5135402
 -0.02464492 -0.01491357] 6 4
train:	 Loss = 1.2603,	 Acc = 0.5128
57319 0.302
110967 0.569
60324 0.599
6175 0.624
903 0.482
88 0.341
0 0.0
0 0.0
0.5805936444073362
0.5186877785152176
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3197,	 Acc = 0.5046
6218 0.428
33592 0.543
17796 0.466
1318 0.433
29 0.0
0 0.0
0 0.0
0 0.0
0.5136247274106381
0.5186877785152176
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.3836,	 Acc = 0.4842

 ===== Epoch 30	 =====
[ 1.7642821   2.8414626  -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
  1.2243414   3.0532188 ] [-0.29884893  0.22113742  0.02132734  0.03107306  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.01321166  0.27385113] 4 1
train:	 Loss = 1.2598,	 Acc = 0.5140
57323 0.302
110959 0.57
60326 0.602
6177 0.615
903 0.484
88 0.432
0 0.0
0 0.0
0.5821084543269096
0.5186877785152176
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3355,	 Acc = 0.4886
6218 0.428
33592 0.51
17796 0.468
1318 0.517
29 0.0
0 0.0
0 0.0
0 0.0
0.49572390253152554
0.5186877785152176
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.3896,	 Acc = 0.4718

 ===== Epoch 31	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
  1.2512045   1.2260103 ] [-0.02630083 -0.03856071  0.02132734  0.03107306  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
  0.11894086 -0.23148708] 3 5
train:	 Loss = 1.2601,	 Acc = 0.5140
57321 0.304
110971 0.571
60318 0.599
6175 0.622
903 0.483
88 0.352
0 0.0
0 0.0
0.5815023395253706
0.5186877785152176
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3392,	 Acc = 0.4967
6218 0.433
33592 0.54
17796 0.438
1318 0.489
29 0.0
0 0.0
0 0.0
0 0.0
0.5042381719920357
0.5186877785152176
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.3869,	 Acc = 0.4795

 ===== Epoch 32	 =====
[ 2.3079195   2.1266992  -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
  2.1211874   2.2730625 ] [-0.06212113 -0.03856071  0.02132734  0.03107306  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02738873 -0.01491357] 0 0
train:	 Loss = 1.2605,	 Acc = 0.5132
57322 0.303
110967 0.568
60321 0.601
6176 0.617
902 0.491
88 0.443
0 0.0
0 0.0
0.5805753863740796
0.5186877785152176
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3212,	 Acc = 0.4884
6218 0.406
33592 0.508
17796 0.482
1318 0.486
29 0.0
0 0.0
0 0.0
0 0.0
0.4981511330236086
0.5186877785152176
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.3838,	 Acc = 0.4684

 ===== Epoch 33	 =====
[-0.378686   -0.39794773  4.058114   -5.105366    4.0391517   0.86135507
  3.9307518   3.4726403  -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071  0.4294476   6.2793756  -5.0778155  -1.105596
 -0.50574553 -0.31340903  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 5 0
train:	 Loss = 1.2591,	 Acc = 0.5145
57321 0.303
110970 0.571
60318 0.602
6176 0.617
903 0.495
88 0.398
0 0.0
0 0.0
0.5824045277520944
0.5186877785152176
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3653,	 Acc = 0.4791
6218 0.426
33592 0.513
17796 0.434
1318 0.502
29 0.0
0 0.0
0 0.0
0 0.0
0.48546506115483073
0.5186877785152176
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 0
Testing:	 Loss = 1.4268,	 Acc = 0.4595

 ===== Epoch 34	 =====
[-0.378686   -0.39794773  2.133865    1.655506   -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-2.6300831e-02 -3.8560715e-02 -2.7983329e+00 -1.7122095e+00
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
  4.7721278e-02  7.5454362e-02 -2.7775567e-02 -1.1532969e-02
 -2.4644915e-02 -1.4913575e-02] 2 2
train:	 Loss = 1.2602,	 Acc = 0.5141
57324 0.302
110964 0.57
60320 0.601
6177 0.619
903 0.485
88 0.386
0 0.0
0 0.0
0.5820500750902203
0.5186877785152176
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3210,	 Acc = 0.4959
6218 0.419
33592 0.512
17796 0.49
1318 0.536
29 0.0
0 0.0
0 0.0
0 0.0
0.5049966815208116
0.5186877785152176
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.3851,	 Acc = 0.4758

 ===== Epoch 35	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-2.6300831e-02 -3.8560715e-02  9.1160434e-01  2.5895302e+00
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
  4.7721278e-02  7.5454362e-02 -2.7775567e-02 -1.1532969e-02
 -2.4644915e-02 -1.4913575e-02] 2 5
train:	 Loss = 1.2600,	 Acc = 0.5145
57320 0.303
110963 0.571
60325 0.601
6177 0.617
903 0.486
88 0.42
0 0.0
0 0.0
0.5823844533106199
0.5186877785152176
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3963,	 Acc = 0.4754
6218 0.396
33592 0.511
17796 0.437
1318 0.456
29 0.0
0 0.0
0 0.0
0 0.0
0.4847255143642742
0.5186877785152176
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.4531,	 Acc = 0.4582

 ===== Epoch 36	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484  3.1690302   2.2158866
 -0.3536378  -0.35852543  3.168544    1.1829779  -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071  0.02132734  0.03107306 -0.00534038 -0.07929721
 -0.03495734 -0.02773637  0.3892836   0.06332765 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 2 2
train:	 Loss = 1.2595,	 Acc = 0.5146
57320 0.303
110961 0.572
60327 0.6
6177 0.618
903 0.505
88 0.409
0 0.0
0 0.0
0.5825245438651544
0.5186877785152176
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3605,	 Acc = 0.4843
6218 0.427
33592 0.528
17796 0.424
1318 0.467
29 0.0
0 0.0
0 0.0
0 0.0
0.49102114345311465
0.5186877785152176
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.4212,	 Acc = 0.4643

 ===== Epoch 37	 =====
[-0.378686   -0.39794773  2.0101817   1.6225948  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-2.6300831e-02 -3.8560715e-02  1.3280597e+00  8.6267430e-01
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
  4.7721278e-02  7.5454362e-02 -2.7775567e-02 -1.1532969e-02
 -2.4644915e-02 -1.4913575e-02] 2 2
train:	 Loss = 1.2598,	 Acc = 0.5151
57322 0.302
110966 0.572
60322 0.602
6175 0.624
903 0.492
88 0.375
0 0.0
0 0.0
0.583489302565367
0.5186877785152176
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 2
val:	 Loss = 1.3660,	 Acc = 0.4921
6218 0.448
33592 0.526
17796 0.448
1318 0.448
29 0.0
0 0.0
0 0.0
0 0.0
0.49725988432729684
0.5186877785152176
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 0
Testing:	 Loss = 1.4362,	 Acc = 0.4698

 ===== Epoch 38	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.6199981   2.002354   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071  0.02132734  0.03107306  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.05090861  0.05847696 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 0 0
train:	 Loss = 1.2592,	 Acc = 0.5156
57322 0.304
110966 0.572
60321 0.602
6176 0.621
903 0.506
88 0.398
0 0.0
0 0.0
0.5835061136203167
0.5186877785152176
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.2583,	 Acc = 0.5160
6218 0.423
33592 0.55
17796 0.484
1318 0.539
29 0.0
0 0.0
0 0.0
0 0.0
0.5270313833317531
0.5270313833317531
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.3194,	 Acc = 0.4962

 ===== Epoch 39	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103  1.5905008   1.9492036
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071  0.02132734  0.03107306  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.12768388  0.14259298
 -0.02464492 -0.01491357] 0 1
train:	 Loss = 1.2600,	 Acc = 0.5139
57319 0.302
110967 0.57
60322 0.602
6177 0.615
903 0.501
88 0.375
0 0.0
0 0.0
0.5820449744196081
0.5270313833317531
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3076,	 Acc = 0.5119
6218 0.402
33592 0.545
17796 0.487
1318 0.533
29 0.0
0 0.0
0 0.0
0 0.0
0.5247748174836446
0.5270313833317531
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.3655,	 Acc = 0.4915

 ===== Epoch 40	 =====
[ 0.87146246  3.127368   -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-1.1531333e+00 -3.2695799e+00  2.2546334e+00  1.0947013e+00
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
  4.7721278e-02  7.5454362e-02 -2.7775567e-02 -1.1532969e-02
 -2.4644915e-02 -1.4913575e-02] 6 6
train:	 Loss = 1.2588,	 Acc = 0.5147
57320 0.304
110970 0.571
60320 0.601
6175 0.623
903 0.485
88 0.409
0 0.0
0 0.0
0.5824629040211593
0.5270313833317531
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 2
val:	 Loss = 1.4372,	 Acc = 0.4717
6218 0.365
33592 0.521
17796 0.422
1318 0.407
29 0.0
0 0.0
0 0.0
0 0.0
0.48428937138522804
0.5270313833317531
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 1
Testing:	 Loss = 1.4839,	 Acc = 0.4566

 ===== Epoch 41	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103  2.6611843   1.4007038
  3.3278134   3.450996  ] [-2.6300831e-02 -3.8560715e-02  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
  2.9281135e+00  3.7280207e+00 -3.3126466e+00 -1.8897797e+00
 -2.8438258e-01 -1.1191710e+00] 5 5
train:	 Loss = 1.2586,	 Acc = 0.5153
57318 0.304
110966 0.572
60324 0.602
6177 0.622
903 0.498
88 0.33
0 0.0
0 0.0
0.5832352710441673
0.5270313833317531
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3645,	 Acc = 0.4771
6218 0.418
33592 0.499
17796 0.46
1318 0.438
29 0.0
0 0.0
0 0.0
0 0.0
0.48404285578837586
0.5270313833317531
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 0
Testing:	 Loss = 1.4267,	 Acc = 0.4606

 ===== Epoch 42	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
  3.4362922   1.8636676  -0.42422777 -0.43446103  3.2988706   2.7150815
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071  0.02132734  0.03107306  0.00132258 -0.00350269
  0.5721756   0.39106527  0.04772128  0.07545436  0.06037952  0.19222677
 -0.02464492 -0.01491357] 2 2
train:	 Loss = 1.2586,	 Acc = 0.5142
57321 0.302
110967 0.571
60321 0.602
6176 0.613
903 0.491
88 0.364
0 0.0
0 0.0
0.5824381496735872
0.5270313833317531
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.4130,	 Acc = 0.4664
6218 0.401
33592 0.482
17796 0.46
1318 0.472
29 0.0
0 0.0
0 0.0
0 0.0
0.47408741822319145
0.5270313833317531
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 0
Testing:	 Loss = 1.4832,	 Acc = 0.4501

 ===== Epoch 43	 =====
[-0.378686   -0.39794773  1.1448172   3.1059434  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-2.6300831e-02 -3.8560715e-02 -1.7042714e+00 -2.9791181e+00
  1.3225782e-03 -3.5026856e-03  2.6896181e+00  4.0632071e+00
  4.7721278e-02  7.5454362e-02 -2.7775567e-02 -1.1532969e-02
 -2.4644915e-02 -1.4913575e-02] 5 5
train:	 Loss = 1.2589,	 Acc = 0.5152
57323 0.303
110968 0.571
60320 0.604
6174 0.616
903 0.502
88 0.398
0 0.0
0 0.0
0.5833748942298532
0.5270313833317531
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3096,	 Acc = 0.5042
6218 0.405
33592 0.534
17796 0.484
1318 0.492
29 0.0
0 0.0
0 0.0
0 0.0
0.5159571442116242
0.5270313833317531
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.3733,	 Acc = 0.4839

 ===== Epoch 44	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-2.6300831e-02 -3.8560715e-02  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
  4.7721278e-02  7.5454362e-02  1.3458058e+00  3.8625143e+00
 -2.4644915e-02 -1.4913575e-02] 5 5
train:	 Loss = 1.2584,	 Acc = 0.5148
57324 0.302
110965 0.571
60319 0.603
6177 0.62
903 0.505
88 0.386
0 0.0
0 0.0
0.5830363347006478
0.5270313833317531
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.2899,	 Acc = 0.5123
6218 0.39
33592 0.545
17796 0.495
1318 0.505
29 0.0
0 0.0
0 0.0
0 0.0
0.5267469422584621
0.5270313833317531
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.3476,	 Acc = 0.4921

 ===== Epoch 45	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484  3.6037004   2.2316895
 -0.3536378  -0.35852543  2.9994197   1.6708145  -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071  0.02132734  0.03107306 -0.31045783  0.07024333
 -0.03495734 -0.02773637 -0.10967724  0.1336626  -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 3 3
train:	 Loss = 1.2599,	 Acc = 0.5141
57317 0.302
110968 0.57
60324 0.602
6177 0.618
902 0.512
88 0.443
0 0.0
0 0.0
0.5822121607764248
0.5270313833317531
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.4048,	 Acc = 0.4525
6218 0.235
33592 0.5
17796 0.439
1318 0.462
29 0.0
0 0.0
0 0.0
0 0.0
0.4781454442021428
0.5270313833317531
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.4640,	 Acc = 0.4347

 ===== Epoch 46	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103  1.5535002   3.3798532
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071  0.02132734  0.03107306  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436  0.08304767  0.00152855
 -0.02464492 -0.01491357] 0 0
train:	 Loss = 1.2577,	 Acc = 0.5155
57325 0.304
110964 0.571
60319 0.604
6177 0.619
903 0.508
88 0.42
0 0.0
0 0.0
0.5835551495928855
0.5270313833317531
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3050,	 Acc = 0.5070
6218 0.423
33592 0.541
17796 0.47
1318 0.556
29 0.0
0 0.0
0 0.0
0 0.0
0.5169052811225942
0.5270313833317531
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.3629,	 Acc = 0.4863

 ===== Epoch 47	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071  0.02132734  0.03107306  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 5 1
train:	 Loss = 1.2582,	 Acc = 0.5146
57324 0.303
110962 0.571
60323 0.602
6176 0.618
903 0.492
88 0.386
0 0.0
0 0.0
0.5823750924618385
0.5270313833317531
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3464,	 Acc = 0.4746
6218 0.412
33592 0.501
17796 0.451
1318 0.423
29 0.0
0 0.0
0 0.0
0 0.0
0.48193799184602254
0.5270313833317531
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.4025,	 Acc = 0.4590

 ===== Epoch 48	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.5007902   1.1308789  -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-2.6300831e-02 -3.8560715e-02  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
  1.5031664e-01  1.7537916e+00 -2.7775567e-02 -1.1532969e-02
 -2.4644915e-02 -1.4913575e-02] 4 4
train:	 Loss = 1.2589,	 Acc = 0.5149
57317 0.303
110964 0.571
60329 0.603
6175 0.613
903 0.517
88 0.42
0 0.0
0 0.0
0.5829238088300394
0.5270313833317531
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3512,	 Acc = 0.4899
6218 0.396
33592 0.516
17796 0.473
1318 0.506
29 0.0
0 0.0
0 0.0
0 0.0
0.500976581018299
0.5270313833317531
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.4038,	 Acc = 0.4717

 ===== Epoch 49	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
  1.947018    3.0301223 ] [-2.6300831e-02 -3.8560715e-02  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
  4.7721278e-02  7.5454362e-02  3.0073223e+00  1.3808252e+00
  2.1176967e-01  4.4497091e-01] 4 4
train:	 Loss = 1.2579,	 Acc = 0.5158
57319 0.304
110964 0.571
60325 0.605
6177 0.615
903 0.515
88 0.42
0 0.0
0 0.0
0.5838045019248335
0.5270313833317531
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.2935,	 Acc = 0.5069
6218 0.42
33592 0.516
17796 0.521
1318 0.513
29 0.0
0 0.0
0 0.0
0 0.0
0.5171897221958851
0.5270313833317531
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.3494,	 Acc = 0.4887

 ===== Epoch 50	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071  0.02132734  0.03107306  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 1 1
train:	 Loss = 1.2583,	 Acc = 0.5148
57321 0.304
110970 0.571
60318 0.603
6176 0.619
903 0.492
88 0.409
0 0.0
0 0.0
0.5826622958168726
0.5270313833317531
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 2
val:	 Loss = 1.3283,	 Acc = 0.5007
6218 0.414
33592 0.544
17796 0.45
1318 0.502
29 0.0
0 0.0
0 0.0
0 0.0
0.5109320185834835
0.5270313833317531
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.3815,	 Acc = 0.4814

 ===== Epoch 51	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
  1.169617    2.3425632  -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071  0.02132734  0.03107306  0.00132258 -0.00350269
 -0.0285487   0.23852167  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
train:	 Loss = 1.2587,	 Acc = 0.5150
57322 0.303
110963 0.57
60323 0.604
6177 0.618
903 0.501
88 0.386
0 0.0
0 0.0
0.5830241967117576
0.5270313833317531
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3217,	 Acc = 0.4947
6218 0.402
33592 0.51
17796 0.493
1318 0.585
29 0.0
0 0.0
0 0.0
0 0.0
0.5056034891438324
0.5270313833317531
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.3995,	 Acc = 0.4733

 ===== Epoch 52	 =====
[ 0.81178534  3.0839717  -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02128715 -0.04090034  0.02132734  0.03107306  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 0 0
train:	 Loss = 1.2587,	 Acc = 0.5156
57322 0.305
110967 0.571
60321 0.604
6176 0.622
902 0.497
88 0.398
0 0.0
0 0.0
0.5834332657155346
0.5270313833317531
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.2997,	 Acc = 0.5020
6218 0.434
33592 0.526
17796 0.481
1318 0.511
29 0.0
0 0.0
0 0.0
0 0.0
0.5100597326253911
0.5270313833317531
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.3614,	 Acc = 0.4813

 ===== Epoch 53	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.5501183   1.2327088  -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071  0.02132734  0.03107306  2.5857687   1.2501796
 -0.03495734 -0.02773637 -2.094745   -1.631987   -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 5 6
train:	 Loss = 1.2584,	 Acc = 0.5146
57323 0.303
110962 0.57
60325 0.603
6176 0.619
902 0.5
88 0.466
0 0.0
0 0.0
0.5823942438625296
0.5270313833317531
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3425,	 Acc = 0.4890
6218 0.396
33592 0.529
17796 0.45
1318 0.436
29 0.0
0 0.0
0 0.0
0 0.0
0.5000474068455485
0.5270313833317531
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.4027,	 Acc = 0.4702

 ===== Epoch 54	 =====
[ 2.274931    2.1266992  -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
  2.1190214   2.2730625 ] [-0.04842454 -0.03856071  0.02132734  0.03107306  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02510292 -0.01491357] 0 0
train:	 Loss = 1.2584,	 Acc = 0.5165
57320 0.304
110967 0.574
60323 0.603
6175 0.619
903 0.506
88 0.375
0 0.0
0 0.0
0.5845866768279016
0.5270313833317531
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3048,	 Acc = 0.5062
6218 0.418
33592 0.539
17796 0.473
1318 0.536
29 0.0
0 0.0
0 0.0
0 0.0
0.5166398027875225
0.5270313833317531
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.3705,	 Acc = 0.4845

 ===== Epoch 55	 =====
[-0.378686   -0.39794773  2.1662593   1.7025218  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071  1.3499304  -5.0776277   3.8297846   7.895515
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 2 2
train:	 Loss = 1.2577,	 Acc = 0.5159
57322 0.304
110967 0.572
60319 0.604
6177 0.622
903 0.516
88 0.398
0 0.0
0 0.0
0.5840776894886077
0.5270313833317531
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3336,	 Acc = 0.4860
6218 0.383
33592 0.523
17796 0.459
1318 0.398
29 0.0
0 0.0
0 0.0
0 0.0
0.49807528207073104
0.5270313833317531
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.3974,	 Acc = 0.4660

 ===== Epoch 56	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
  2.026737    2.6785386 ] [ 2.6819329e+00  2.6215901e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
  4.7721278e-02  7.5454362e-02 -2.7775567e-02 -1.1532969e-02
 -1.7326175e-01 -1.7587323e-02] 3 3
train:	 Loss = 1.2586,	 Acc = 0.5152
57324 0.303
110964 0.571
60321 0.604
6176 0.619
903 0.506
88 0.386
0 0.0
0 0.0
0.5834117858023447
0.5270313833317531
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3525,	 Acc = 0.4978
6218 0.445
33592 0.542
17796 0.435
1318 0.471
29 0.0
0 0.0
0 0.0
0 0.0
0.5039726936569641
0.5270313833317531
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 0
Testing:	 Loss = 1.4142,	 Acc = 0.4777

 ===== Epoch 57	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484  2.5758002   1.4596064
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071  0.02132734  0.03107306 -0.00045403  0.13989235
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 3 3
train:	 Loss = 1.2573,	 Acc = 0.5170
57318 0.305
110964 0.573
60326 0.604
6177 0.625
903 0.516
88 0.42
0 0.0
0 0.0
0.5849331495365856
0.5270313833317531
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.4546,	 Acc = 0.4752
6218 0.448
33592 0.531
17796 0.391
1318 0.346
29 0.0
0 0.0
0 0.0
0 0.0
0.4785057362283114
0.5270313833317531
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.5002,	 Acc = 0.4581

 ===== Epoch 58	 =====
[ 2.4924881   3.0278118  -0.41514966 -0.34031484 -0.42709485 -0.35320815
  1.7764021   3.1747906  -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-2.6142590e+00 -3.1783345e+00  3.4710863e+00  1.0474746e+00
  1.3225782e-03 -3.5026856e-03  1.8994841e-01  4.9922224e-02
  4.7721278e-02  7.5454362e-02 -2.7775567e-02 -1.1532969e-02
 -2.4644915e-02 -1.4913575e-02] 2 2
train:	 Loss = 1.2578,	 Acc = 0.5151
57320 0.303
110970 0.571
60319 0.604
6176 0.621
903 0.508
88 0.432
0 0.0
0 0.0
0.5831969785269198
0.5270313833317531
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3265,	 Acc = 0.5054
6218 0.436
33592 0.546
17796 0.455
1318 0.473
29 0.0
0 0.0
0 0.0
0 0.0
0.5134919882431023
0.5270313833317531
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.3822,	 Acc = 0.4856

 ===== Epoch 59	 =====
[ 3.2014704   3.1452372  -0.41514966 -0.34031484 -0.42709485 -0.35320815
  1.2765927   3.4697201  -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-3.2533064e+00 -3.2859573e+00  2.3225760e+00  2.0474494e+00
  1.3225782e-03 -3.5026856e-03 -1.6332499e+00 -3.6638227e+00
  4.7721278e-02  7.5454362e-02 -2.7775567e-02 -1.1532969e-02
 -2.4644915e-02 -1.4913575e-02] 4 6
train:	 Loss = 1.2572,	 Acc = 0.5157
57323 0.304
110962 0.571
60324 0.604
6176 0.617
903 0.527
88 0.409
0 0.0
0 0.0
0.5837839655259368
0.5270313833317531
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3958,	 Acc = 0.4675
6218 0.394
33592 0.491
17796 0.448
1318 0.478
29 0.0
0 0.0
0 0.0
0 0.0
0.4760785057362283
0.5270313833317531
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 0
Testing:	 Loss = 1.4604,	 Acc = 0.4484

 ===== Epoch 60	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071  0.02132734  0.03107306  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 1 1
train:	 Loss = 1.2586,	 Acc = 0.5157
57326 0.304
110963 0.57
60319 0.605
6177 0.623
903 0.514
88 0.42
0 0.0
0 0.0
0.5836424768842813
0.5270313833317531
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3177,	 Acc = 0.5086
6218 0.406
33592 0.542
17796 0.479
1318 0.539
29 0.0
0 0.0
0 0.0
0 0.0
0.5206599032900351
0.5270313833317531
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.3904,	 Acc = 0.4867

 ===== Epoch 61	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484  0.9066332   2.9992573
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071  0.02132734  0.03107306 -1.5154296  -3.0455258
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 1
train:	 Loss = 1.2585,	 Acc = 0.5145
57317 0.302
110967 0.571
60325 0.603
6176 0.615
903 0.511
88 0.398
0 0.0
0 0.0
0.5826660465429034
0.5270313833317531
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3810,	 Acc = 0.4681
6218 0.383
33592 0.497
17796 0.438
1318 0.552
29 0.0
0 0.0
0 0.0
0 0.0
0.4781264814639234
0.5270313833317531
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.4328,	 Acc = 0.4522

 ===== Epoch 62	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071  0.02132734  0.03107306  3.4189622   1.5758913
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 2 2
train:	 Loss = 1.2580,	 Acc = 0.5158
57321 0.303
110970 0.572
60320 0.604
6174 0.618
903 0.512
88 0.409
0 0.0
0 0.0
0.5841528676697206
0.5270313833317531
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3607,	 Acc = 0.4959
6218 0.447
33592 0.539
17796 0.436
1318 0.439
29 0.0
0 0.0
0 0.0
0 0.0
0.5016023513795392
0.5270313833317531
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.4180,	 Acc = 0.4772

 ===== Epoch 63	 =====
[ 2.6716964   3.0584445  -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
  2.2104383   3.1994984 ] [-8.5051492e-02 -1.3206739e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
  4.7721278e-02  7.5454362e-02 -2.7775567e-02 -1.1532969e-02
 -6.1684262e-02 -1.4828008e+00] 5 5
train:	 Loss = 1.2574,	 Acc = 0.5157
57323 0.304
110967 0.572
60320 0.603
6175 0.612
903 0.515
88 0.455
0 0.0
0 0.0
0.5837951729587062
0.5270313833317531
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3376,	 Acc = 0.5027
6218 0.394
33592 0.535
17796 0.478
1318 0.547
29 0.0
0 0.0
0 0.0
0 0.0
0.5155778894472361
0.5270313833317531
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.3995,	 Acc = 0.4823

 ===== Epoch 64	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484  1.9716358   2.3016737
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071  0.02132734  0.03107306 -2.72658    -2.4125392
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 1 6
train:	 Loss = 1.2586,	 Acc = 0.5162
57322 0.305
110962 0.571
60324 0.605
6177 0.619
903 0.509
88 0.443
0 0.0
0 0.0
0.5838535420892779
0.5270313833317531
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3154,	 Acc = 0.5111
6218 0.426
33592 0.533
17796 0.499
1318 0.529
29 0.0
0 0.0
0 0.0
0 0.0
0.52113397174552
0.5270313833317531
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.3710,	 Acc = 0.4914

 ===== Epoch 65	 =====
[ 2.224509    2.2517827  -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
  1.9262207   2.3962452 ] [-0.03981182  0.6773638   0.02132734  0.03107306  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.06488467  0.82999057] 4 4
train:	 Loss = 1.2584,	 Acc = 0.5157
57321 0.304
110962 0.571
60327 0.604
6175 0.619
903 0.526
88 0.409
0 0.0
0 0.0
0.5837438009582248
0.5270313833317531
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3190,	 Acc = 0.5015
6218 0.418
33592 0.529
17796 0.479
1318 0.522
29 0.0
0 0.0
0 0.0
0 0.0
0.5113871243007491
0.5270313833317531
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.3729,	 Acc = 0.4820

 ===== Epoch 66	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3610272   2.807521   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 3.0970366e+00  2.4624958e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03  1.8435932e+00  2.0274427e+00
 -1.8895524e+00 -3.2448401e+00 -2.7775567e-02 -1.1532969e-02
 -2.4644915e-02 -1.4913575e-02] 6 6
train:	 Loss = 1.2589,	 Acc = 0.5159
57322 0.304
110964 0.571
60323 0.605
6176 0.622
903 0.509
88 0.477
0 0.0
0 0.0
0.5838255236643617
0.5270313833317531
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3317,	 Acc = 0.5005
6218 0.393
33592 0.531
17796 0.476
1318 0.564
29 0.0
0 0.0
0 0.0
0 0.0
0.5132075471698113
0.5270313833317531
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.4033,	 Acc = 0.4777

 ===== Epoch 67	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071  0.02132734  0.03107306  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 0 1
train:	 Loss = 1.2586,	 Acc = 0.5150
57322 0.303
110963 0.57
60323 0.606
6177 0.616
903 0.512
88 0.352
0 0.0
0 0.0
0.5829569524919587
0.5270313833317531
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3057,	 Acc = 0.5000
6218 0.394
33592 0.51
17796 0.516
1318 0.545
29 0.0
0 0.0
0 0.0
0 0.0
0.5124680003792548
0.5270313833317531
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 1
Testing:	 Loss = 1.3626,	 Acc = 0.4815

 ===== Epoch 68	 =====
[-0.378686   -0.39794773  1.9428706   0.9056039  -0.42709485 -0.35320815
  3.5605001   3.2273524  -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071  0.26378098  0.13168654  0.00132258 -0.00350269
 -0.00698844  0.1248073   0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 3 3
train:	 Loss = 1.2588,	 Acc = 0.5155
57320 0.304
110963 0.571
60325 0.604
6177 0.617
903 0.515
88 0.455
0 0.0
0 0.0
0.5834659523916259
0.5270313833317531
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3613,	 Acc = 0.5034
6218 0.385
33592 0.537
17796 0.479
1318 0.53
29 0.0
0 0.0
0 0.0
0 0.0
0.5173034986252015
0.5270313833317531
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.4201,	 Acc = 0.4828

 ===== Epoch 69	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.7621101   3.096434   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 1.3143445e+00  3.1246092e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -2.3247883e+00 -3.5407321e+00 -2.7775567e-02 -1.1532969e-02
  1.6732874e+00  3.7711122e+00] 6 6
train:	 Loss = 1.2582,	 Acc = 0.5160
57321 0.304
110964 0.572
60324 0.604
6176 0.619
903 0.504
88 0.466
0 0.0
0 0.0
0.584001569023003
0.5270313833317531
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3151,	 Acc = 0.5109
6218 0.453
33592 0.526
17796 0.501
1318 0.55
29 0.0
0 0.0
0 0.0
0 0.0
0.5177017161278089
0.5270313833317531
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.3896,	 Acc = 0.4891

 ===== Epoch 70	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484  2.0755186   1.1819276
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071  0.02132734  0.03107306 -2.8447182  -1.3964831
 -0.03495734 -0.02773637  1.201824    3.1701918  -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 6 5
train:	 Loss = 1.2580,	 Acc = 0.5150
57325 0.303
110958 0.571
60325 0.603
6177 0.62
903 0.513
88 0.5
0 0.0
0 0.0
0.5829723565572622
0.5270313833317531
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3537,	 Acc = 0.4920
6218 0.412
33592 0.529
17796 0.451
1318 0.477
29 0.0
0 0.0
0 0.0
0 0.0
0.5014127239973453
0.5270313833317531
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.4129,	 Acc = 0.4721

 ===== Epoch 71	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-2.6300831e-02 -3.8560715e-02  2.2639410e+00  1.1131814e+00
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
  4.7721278e-02  7.5454362e-02 -2.7775567e-02 -1.1532969e-02
 -2.4644915e-02 -1.4913575e-02] 3 2
train:	 Loss = 1.2590,	 Acc = 0.5151
57321 0.303
110962 0.571
60325 0.604
6177 0.617
903 0.536
88 0.409
0 0.0
0 0.0
0.5834075817432967
0.5270313833317531
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.2859,	 Acc = 0.4948
6218 0.414
33592 0.507
17796 0.5
1318 0.511
29 0.0
0 0.0
0 0.0
0 0.0
0.504351948421352
0.5270313833317531
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.3509,	 Acc = 0.4768

 ===== Epoch 72	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484  2.8663619   2.213629
 -0.3536378  -0.35852543  1.5659735   1.5381988  -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071  0.02132734  0.03107306 -3.7440865  -2.3326476
 -0.03495734 -0.02773637 -2.1119504  -1.9448562  -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 6 6
train:	 Loss = 1.2585,	 Acc = 0.5160
57324 0.305
110966 0.572
60318 0.604
6177 0.618
903 0.525
88 0.443
0 0.0
0 0.0
0.5838881043641988
0.5270313833317531
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3269,	 Acc = 0.5036
6218 0.429
33592 0.546
17796 0.454
1318 0.451
29 0.0
0 0.0
0 0.0
0 0.0
0.5123731866881578
0.5270313833317531
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.3821,	 Acc = 0.4837

 ===== Epoch 73	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
  0.96458185  2.7805774  -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.6813745e+00  3.0988731e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03  1.0831683e+00  3.0508617e-01
  4.7721278e-02  7.5454362e-02 -2.7775567e-02 -1.1532969e-02
 -2.4644915e-02 -1.4913575e-02] 2 2
train:	 Loss = 1.2583,	 Acc = 0.5157
57324 0.303
110965 0.572
60320 0.604
6176 0.612
903 0.512
88 0.364
0 0.0
0 0.0
0.584106650527873
0.5270313833317531
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3003,	 Acc = 0.5001
6218 0.366
33592 0.536
17796 0.483
1318 0.454
29 0.0
0 0.0
0 0.0
0 0.0
0.5159381814734048
0.5270313833317531
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.3646,	 Acc = 0.4791

 ===== Epoch 74	 =====
[-0.378686   -0.39794773  3.0644376   2.348989   -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.04242756  0.33907354  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 3
train:	 Loss = 1.2581,	 Acc = 0.5167
57323 0.305
110963 0.572
60322 0.606
6177 0.615
903 0.517
88 0.432
0 0.0
0 0.0
0.5846861638638745
0.5270313833317531
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.2922,	 Acc = 0.5061
6218 0.385
33592 0.54
17796 0.487
1318 0.479
29 0.0
0 0.0
0 0.0
0 0.0
0.5203564994785247
0.5270313833317531
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.3536,	 Acc = 0.4855

 ===== Epoch 75	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071  0.02132734  0.03107306  1.5620435   1.2829556
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 6 6
train:	 Loss = 1.2572,	 Acc = 0.5162
57324 0.303
110968 0.571
60319 0.607
6174 0.624
903 0.509
88 0.386
0 0.0
0 0.0
0.5847006477932442
0.5270313833317531
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3647,	 Acc = 0.4823
6218 0.408
33592 0.528
17796 0.426
1318 0.43
29 0.0
0 0.0
0 0.0
0 0.0
0.4909832179766758
0.5270313833317531
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.4257,	 Acc = 0.4641

 ===== Epoch 76	 =====
[-0.378686   -0.39794773  0.16165797  1.0466514  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.6167237  -1.1803954   0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 3 5
train:	 Loss = 1.2581,	 Acc = 0.5164
57320 0.304
110964 0.572
60325 0.605
6177 0.622
902 0.531
88 0.409
0 0.0
0 0.0
0.5846707311606222
0.5270313833317531
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3422,	 Acc = 0.5009
6218 0.417
33592 0.541
17796 0.455
1318 0.505
29 0.0
0 0.0
0 0.0
0 0.0
0.5107803166777283
0.5270313833317531
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.3972,	 Acc = 0.4822

 ===== Epoch 77	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484  1.9716358   2.3016737
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071  0.02132734  0.03107306 -2.72658    -2.4125392
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 1 6
train:	 Loss = 1.2585,	 Acc = 0.5161
57320 0.304
110963 0.572
60327 0.605
6176 0.618
902 0.509
88 0.432
0 0.0
0 0.0
0.5842560631192002
0.5270313833317531
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3804,	 Acc = 0.4723
6218 0.403
33592 0.499
17796 0.449
1318 0.443
29 0.0
0 0.0
0 0.0
0 0.0
0.48051578647956766
0.5270313833317531
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.4462,	 Acc = 0.4535

 ===== Epoch 78	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
  0.9425926   2.4360063  -0.42422777 -0.43446103  2.467517    2.1741643
 -0.41038254 -0.40359274] [-2.6300831e-02 -3.8560715e-02  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -1.3057933e+00 -2.6819961e+00
  4.7721278e-02  7.5454362e-02 -3.1061118e+00 -2.6891448e+00
 -2.4644915e-02 -1.4913575e-02] 6 6
train:	 Loss = 1.2583,	 Acc = 0.5160
57318 0.305
110969 0.57
60321 0.607
6177 0.615
903 0.519
88 0.386
0 0.0
0 0.0
0.5837956269822592
0.5270313833317531
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 2
val:	 Loss = 1.4051,	 Acc = 0.4848
6218 0.435
33592 0.537
17796 0.403
1318 0.498
29 0.0
0 0.0
0 0.0
0 0.0
0.49067981416516543
0.5270313833317531
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.4569,	 Acc = 0.4681

 ===== Epoch 79	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071  0.02132734  0.03107306  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 3 1
train:	 Loss = 1.2580,	 Acc = 0.5166
57323 0.304
110962 0.573
60324 0.605
6177 0.619
902 0.506
88 0.432
0 0.0
0 0.0
0.5847309935949522
0.5270313833317531
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 2
val:	 Loss = 1.3495,	 Acc = 0.4979
6218 0.374
33592 0.536
17796 0.471
1318 0.492
29 0.0
0 0.0
0 0.0
0 0.0
0.512524888593913
0.5270313833317531
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.4195,	 Acc = 0.4760

 ===== Epoch 80	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071  0.02132734  0.03107306  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 1
train:	 Loss = 1.2581,	 Acc = 0.5166
57323 0.304
110960 0.572
60325 0.606
6177 0.624
903 0.516
88 0.398
0 0.0
0 0.0
0.5849551422503404
0.5270313833317531
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.4228,	 Acc = 0.4625
6218 0.39
33592 0.489
17796 0.437
1318 0.489
29 0.0
0 0.0
0 0.0
0 0.0
0.47103441736986823
0.5270313833317531
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.4795,	 Acc = 0.4463

 ===== Epoch 81	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
  2.0141726   2.6657069 ] [ 2.6049778e+00  2.7549489e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
  4.7721278e-02  7.5454362e-02 -2.7775567e-02 -1.1532969e-02
  1.9711267e-02  1.6155373e-01] 3 4
train:	 Loss = 1.2581,	 Acc = 0.5161
57321 0.304
110961 0.572
60326 0.604
6177 0.623
903 0.516
88 0.42
0 0.0
0 0.0
0.5842257151662884
0.5270313833317531
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.4123,	 Acc = 0.4778
6218 0.41
33592 0.506
17796 0.452
1318 0.444
29 0.0
0 0.0
0 0.0
0 0.0
0.48569261401346353
0.5270313833317531
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.4542,	 Acc = 0.4631

 ===== Epoch 82	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103  1.523584    0.8951738
 -0.41038254 -0.40359274] [-2.6300831e-02 -3.8560715e-02  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
  4.7721278e-02  7.5454362e-02 -8.7805681e-02  2.0208397e+00
 -2.4644915e-02 -1.4913575e-02] 6 6
train:	 Loss = 1.2587,	 Acc = 0.5172
57319 0.306
110963 0.573
60329 0.605
6174 0.616
903 0.495
88 0.443
0 0.0
0 0.0
0.5849700488072757
0.5270313833317531
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 2
val:	 Loss = 1.3588,	 Acc = 0.4904
6218 0.419
33592 0.528
17796 0.446
1318 0.469
29 0.0
0 0.0
0 0.0
0 0.0
0.49873897790841
0.5270313833317531
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.4112,	 Acc = 0.4709

 ===== Epoch 83	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071  0.02132734  0.03107306  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 1 1
train:	 Loss = 1.2578,	 Acc = 0.5169
57321 0.305
110964 0.572
60323 0.606
6177 0.623
903 0.518
88 0.42
0 0.0
0 0.0
0.5850774705107731
0.5270313833317531
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3526,	 Acc = 0.4825
6218 0.402
33592 0.482
17796 0.511
1318 0.493
29 0.0
0 0.0
0 0.0
0 0.0
0.49204513131696215
0.5270313833317531
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 0
Testing:	 Loss = 1.4295,	 Acc = 0.4615

 ===== Epoch 84	 =====
[ 2.2317677   2.1266992  -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
  2.1190214   2.2730625 ] [-0.03817708 -0.03856071  0.02132734  0.03107306  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 0 0
train:	 Loss = 1.2588,	 Acc = 0.5162
57320 0.305
110968 0.57
60322 0.608
6175 0.616
903 0.505
88 0.386
0 0.0
0 0.0
0.5841215761868471
0.5270313833317531
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3399,	 Acc = 0.4908
6218 0.415
33592 0.523
17796 0.459
1318 0.467
29 0.0
0 0.0
0 0.0
0 0.0
0.4997440030340381
0.5270313833317531
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.4023,	 Acc = 0.4720

 ===== Epoch 85	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
  2.6672635   2.6579335  -0.42422777 -0.43446103  3.0040393   3.3874362
 -0.41038254 -0.40359274] [-2.6300831e-02 -3.8560715e-02  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -2.9966762e+00 -2.8927836e+00
  4.7721278e-02  7.5454362e-02 -3.6782813e+00 -3.9430511e+00
 -2.4644915e-02 -1.4913575e-02] 6 6
train:	 Loss = 1.2580,	 Acc = 0.5160
57321 0.304
110967 0.571
60322 0.606
6175 0.623
903 0.519
88 0.455
0 0.0
0 0.0
0.5841416603625564
0.5270313833317531
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3347,	 Acc = 0.4957
6218 0.417
33592 0.52
17796 0.478
1318 0.498
29 0.0
0 0.0
0 0.0
0 0.0
0.5049018678297146
0.5270313833317531
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.3946,	 Acc = 0.4748

 ===== Epoch 86	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071  0.02132734  0.03107306  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 1 1
train:	 Loss = 1.2593,	 Acc = 0.5161
57325 0.304
110960 0.572
60326 0.605
6174 0.615
903 0.512
88 0.364
0 0.0
0 0.0
0.5843340749001127
0.5270313833317531
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3201,	 Acc = 0.4841
6218 0.404
33592 0.498
17796 0.485
1318 0.502
29 0.0
0 0.0
0 0.0
0 0.0
0.4934673366834171
0.5270313833317531
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.3730,	 Acc = 0.4671

 ===== Epoch 87	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
  1.0601368   1.2927344 ] [ 1.8358607e+00  1.1476281e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
  4.7721278e-02  7.5454362e-02 -2.7775567e-02 -1.1532969e-02
  2.5018162e-01 -2.1277086e-01] 3 3
train:	 Loss = 1.2583,	 Acc = 0.5163
57322 0.305
110965 0.571
60323 0.606
6175 0.617
903 0.518
88 0.375
0 0.0
0 0.0
0.584380288477703
0.5270313833317531
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3728,	 Acc = 0.4756
6218 0.419
33592 0.51
17796 0.436
1318 0.417
29 0.0
0 0.0
0 0.0
0 0.0
0.4822603583957523
0.5270313833317531
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.4238,	 Acc = 0.4597

 ===== Epoch 88	 =====
[ 0.19626898  0.8707574  -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02901231  0.5697412   0.02132734  0.03107306  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 0 4
train:	 Loss = 1.2577,	 Acc = 0.5162
57324 0.304
110966 0.572
60321 0.605
6174 0.617
903 0.523
88 0.386
0 0.0
0 0.0
0.584235536726963
0.5270313833317531
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3111,	 Acc = 0.4867
6218 0.368
33592 0.493
17796 0.507
1318 0.615
29 0.0
0 0.0
0 0.0
0 0.0
0.5006731772067886
0.5270313833317531
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 0
Testing:	 Loss = 1.3825,	 Acc = 0.4661

 ===== Epoch 89	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.287036    2.632279   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071  0.02132734  0.03107306  0.00132258 -0.00350269
 -0.03495734 -0.02773637 -0.2575177   0.53626955 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 1 1
train:	 Loss = 1.2582,	 Acc = 0.5164
57323 0.305
110969 0.572
60316 0.605
6177 0.616
903 0.505
88 0.398
0 0.0
0 0.0
0.5844003743282545
0.5270313833317531
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3959,	 Acc = 0.4826
6218 0.4
33592 0.517
17796 0.447
1318 0.489
29 0.0
0 0.0
0 0.0
0 0.0
0.49227268417559494
0.5270313833317531
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.4446,	 Acc = 0.4646

 ===== Epoch 90	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
  3.5646613   1.1365635  -0.42422777 -0.43446103  2.8367448   2.2651598
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071  0.02132734  0.03107306  0.00132258 -0.00350269
  0.31988275 -0.04992453  0.04772128  0.07545436  0.2866445  -0.07684058
 -0.02464492 -0.01491357] 4 2
train:	 Loss = 1.2582,	 Acc = 0.5157
57322 0.304
110961 0.571
60325 0.606
6177 0.617
903 0.526
88 0.375
0 0.0
0 0.0
0.5838647494592444
0.5270313833317531
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3769,	 Acc = 0.4864
6218 0.408
33592 0.514
17796 0.46
1318 0.53
29 0.0
0 0.0
0 0.0
0 0.0
0.49570493979330615
0.5270313833317531
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 1
Testing:	 Loss = 1.4324,	 Acc = 0.4675

 ===== Epoch 91	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071  0.02132734  0.03107306  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 2 1
train:	 Loss = 1.2592,	 Acc = 0.5165
57323 0.305
110965 0.572
60322 0.605
6175 0.615
903 0.519
88 0.42
0 0.0
0 0.0
0.5843947706118698
0.5270313833317531
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3365,	 Acc = 0.4845
6218 0.41
33592 0.511
17796 0.461
1318 0.492
29 0.0
0 0.0
0 0.0
0 0.0
0.4933156347776619
0.5270313833317531
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.3872,	 Acc = 0.4675

 ===== Epoch 92	 =====
[ 1.2060999   1.2996156  -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
  1.1489551   1.4800744 ] [-1.4547616e+00 -1.5944098e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
  4.7721278e-02  7.5454362e-02 -2.7775567e-02 -1.1532969e-02
 -1.6704476e+00 -1.9774441e+00] 6 6
train:	 Loss = 1.2579,	 Acc = 0.5168
57324 0.305
110960 0.572
60324 0.606
6177 0.619
903 0.518
88 0.386
0 0.0
0 0.0
0.5847454777755363
0.5270313833317531
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3416,	 Acc = 0.4935
6218 0.339
33592 0.542
17796 0.459
1318 0.466
29 0.0
0 0.0
0 0.0
0 0.0
0.5117474163269176
0.5270313833317531
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.3999,	 Acc = 0.4730

 ===== Epoch 93	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484  3.130367    1.5341057
 -0.3536378  -0.35852543  2.6347454   0.7377678  -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071  0.02132734  0.03107306 -0.6937458   0.20954137
 -0.03495734 -0.02773637 -0.814467    0.10698383 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 1 1
train:	 Loss = 1.2588,	 Acc = 0.5158
57321 0.303
110962 0.572
60325 0.604
6177 0.613
903 0.501
88 0.443
0 0.0
0 0.0
0.5842481297806169
0.5270313833317531
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3673,	 Acc = 0.4814
6218 0.375
33592 0.521
17796 0.446
1318 0.467
29 0.0
0 0.0
0 0.0
0 0.0
0.49394140513890206
0.5270313833317531
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.4266,	 Acc = 0.4627

 ===== Epoch 94	 =====
[-0.378686   -0.39794773  3.068645    2.1491718  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -3.8323646  -2.1434102   2.8513603   2.9873083
 -0.03495734 -0.02773637  1.1820697   2.6705713  -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 6 5
train:	 Loss = 1.2584,	 Acc = 0.5159
57325 0.304
110960 0.572
60324 0.604
6176 0.616
903 0.515
88 0.398
0 0.0
0 0.0
0.583936206577716
0.5270313833317531
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3969,	 Acc = 0.4684
6218 0.391
33592 0.498
17796 0.436
1318 0.522
29 0.0
0 0.0
0 0.0
0 0.0
0.4775196738409026
0.5270313833317531
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 1
Testing:	 Loss = 1.4517,	 Acc = 0.4505

 ===== Epoch 95	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484  2.1973674   2.8615468
 -0.3536378  -0.35852543  1.1032293   1.2303407  -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071  0.02132734  0.03107306 -2.9832878  -2.9205673
 -0.03495734 -0.02773637 -1.6098022  -1.6295617  -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 6
train:	 Loss = 1.2585,	 Acc = 0.5168
57323 0.304
110964 0.573
60321 0.605
6177 0.613
903 0.483
88 0.42
0 0.0
0 0.0
0.5850840277271887
0.5270313833317531
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3547,	 Acc = 0.4878
6218 0.41
33592 0.523
17796 0.449
1318 0.476
29 0.0
0 0.0
0 0.0
0 0.0
0.4968995923011283
0.5270313833317531
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 0
Testing:	 Loss = 1.4157,	 Acc = 0.4701

 ===== Epoch 96	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
  2.7766144   3.1309888  -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-2.6300831e-02 -3.8560715e-02  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.1038849e+00 -3.3420942e+00
  4.7721278e-02  7.5454362e-02  3.5954435e+00  1.8353662e+00
  3.2897816e+00  3.9743168e+00] 5 5
train:	 Loss = 1.2600,	 Acc = 0.5161
57327 0.305
110963 0.572
60318 0.604
6177 0.619
903 0.509
88 0.409
0 0.0
0 0.0
0.5839203357822123
0.5270313833317531
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3509,	 Acc = 0.4850
6218 0.402
33592 0.496
17796 0.488
1318 0.573
29 0.0
0 0.0
0 0.0
0 0.0
0.4948136910969944
0.5270313833317531
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.4082,	 Acc = 0.4665

 ===== Epoch 97	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103  1.9424095   1.744464
 -0.41038254 -0.40359274] [-2.6300831e-02 -3.8560715e-02  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03  3.7483118e+00  2.0745926e+00
  4.7721278e-02  7.5454362e-02  1.6610278e+00  5.7362318e-01
 -2.4644915e-02 -1.4913575e-02] 2 2
train:	 Loss = 1.2596,	 Acc = 0.5164
57324 0.304
110965 0.572
60321 0.606
6176 0.617
903 0.508
87 0.414
0 0.0
0 0.0
0.5846390065675924
0.5270313833317531
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3061,	 Acc = 0.5113
6218 0.417
33592 0.546
17796 0.484
1318 0.464
29 0.0
0 0.0
0 0.0
0 0.0
0.5224044752062198
0.5270313833317531
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.3773,	 Acc = 0.4896

 ===== Epoch 98	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071  0.02132734  0.03107306  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 0 1
train:	 Loss = 1.2587,	 Acc = 0.5157
57322 0.305
110961 0.57
60326 0.605
6177 0.618
902 0.511
88 0.386
0 0.0
0 0.0
0.5834108509756015
0.5270313833317531
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.2821,	 Acc = 0.5151
6218 0.423
33592 0.536
17796 0.502
1318 0.618
29 0.0
0 0.0
0 0.0
0 0.0
0.526026358206125
0.5270313833317531
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 0
Testing:	 Loss = 1.3516,	 Acc = 0.4942

 ===== Epoch 99	 =====
[-0.378686   -0.39794773  1.7342072   1.150086   -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-2.6300831e-02 -3.8560715e-02 -2.3562407e+00 -1.2707422e+00
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
  4.7721278e-02  7.5454362e-02  2.9565282e+00  2.0077782e+00
 -2.4644915e-02 -1.4913575e-02] 5 5
train:	 Loss = 1.2592,	 Acc = 0.5157
57322 0.305
110965 0.571
60322 0.604
6176 0.612
903 0.509
88 0.364
0 0.0
0 0.0
0.5832539477960706
0.5270313833317531
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.4562,	 Acc = 0.4530
6218 0.396
33592 0.503
17796 0.381
1318 0.434
29 0.0
0 0.0
0 0.0
0 0.0
0.45977055086754526
0.5270313833317531
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.5046,	 Acc = 0.4385

 ===== Epoch 100	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484  2.4684012   1.1232312
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071  0.02132734  0.03107306 -0.0182187   0.00059432
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 0 0
train:	 Loss = 1.2581,	 Acc = 0.5167
57321 0.304
110965 0.572
60322 0.607
6177 0.626
903 0.498
88 0.364
0 0.0
0 0.0
0.5849710010927125
0.5270313833317531
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3029,	 Acc = 0.5078
6218 0.402
33592 0.536
17796 0.494
1318 0.486
29 0.0
0 0.0
0 0.0
0 0.0
0.5203375367403053
0.5270313833317531
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 1
Testing:	 Loss = 1.3725,	 Acc = 0.4863

 ===== Epoch 101	 =====
[ 2.311016    2.1266992  -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
  2.1185875   2.2730625 ] [-0.00904922 -0.04791921  0.02132734  0.03107306  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 0 0
train:	 Loss = 1.2580,	 Acc = 0.5164
57325 0.305
110961 0.571
60322 0.606
6177 0.619
903 0.532
88 0.386
0 0.0
0 0.0
0.5844181315879429
0.5270313833317531
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3709,	 Acc = 0.4874
6218 0.382
33592 0.528
17796 0.445
1318 0.53
29 0.0
0 0.0
0 0.0
0 0.0
0.4998767422015739
0.5270313833317531
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.4342,	 Acc = 0.4674

 ===== Epoch 102	 =====
[-0.378686   -0.39794773  1.3905017   3.030718   -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-2.6300831e-02 -3.8560715e-02 -1.9760418e+00 -2.9134114e+00
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
  2.0053875e+00  1.3220806e+00 -2.7775567e-02 -1.1532969e-02
 -2.4644915e-02 -1.4913575e-02] 6 6
train:	 Loss = 1.2582,	 Acc = 0.5168
57320 0.305
110962 0.572
60326 0.606
6177 0.619
903 0.515
88 0.375
0 0.0
0 0.0
0.5847828036042498
0.5270313833317531
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3805,	 Acc = 0.4696
6218 0.336
33592 0.524
17796 0.423
1318 0.353
29 0.0
0 0.0
0 0.0
0 0.0
0.48535128472551436
0.5270313833317531
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 1
Testing:	 Loss = 1.4366,	 Acc = 0.4510

 ===== Epoch 103	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484  1.4569044   1.4280008
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071  0.02132734  0.03107306 -0.16211818  0.20544437
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 6 1
train:	 Loss = 1.2588,	 Acc = 0.5161
57320 0.304
110972 0.572
60317 0.605
6176 0.617
903 0.501
88 0.409
0 0.0
0 0.0
0.5842784776079257
0.5270313833317531
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3296,	 Acc = 0.5070
6218 0.412
33592 0.547
17796 0.462
1318 0.561
29 0.0
0 0.0
0 0.0
0 0.0
0.5181757845832938
0.5270313833317531
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.3801,	 Acc = 0.4883

 ===== Epoch 104	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.1267197   1.1900823  -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071  0.02132734  0.03107306  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.03115229  1.2299176  -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 6
train:	 Loss = 1.2592,	 Acc = 0.5165
57324 0.303
110961 0.573
60324 0.606
6176 0.613
903 0.538
88 0.398
0 0.0
0 0.0
0.5851209288772331
0.5270313833317531
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.2894,	 Acc = 0.5029
6218 0.428
33592 0.516
17796 0.504
1318 0.531
29 0.0
0 0.0
0 0.0
0 0.0
0.5117474163269176
0.5270313833317531
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 1
Testing:	 Loss = 1.3529,	 Acc = 0.4834

 ===== Epoch 105	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484  1.7861295   1.8682235
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071  0.02132734  0.03107306  0.02974688  0.3631789
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 6 4
train:	 Loss = 1.2584,	 Acc = 0.5161
57321 0.304
110962 0.571
60326 0.605
6176 0.624
903 0.505
88 0.455
0 0.0
0 0.0
0.5841136420946457
0.5270313833317531
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3994,	 Acc = 0.4824
6218 0.385
33592 0.549
17796 0.396
1318 0.42
29 0.0
0 0.0
0 0.0
0 0.0
0.49386555418602446
0.5270313833317531
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.4415,	 Acc = 0.4659

 ===== Epoch 106	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103  1.0976716   1.0721092
 -0.41038254 -0.40359274] [ 2.0576284e+00  1.8541942e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03  1.6734564e+00  1.3839859e+00
  4.7721278e-02  7.5454362e-02 -1.6452525e+00 -1.5501801e+00
 -2.4644915e-02 -1.4913575e-02] 6 6
train:	 Loss = 1.2595,	 Acc = 0.5149
57325 0.304
110958 0.569
60325 0.606
6177 0.619
903 0.498
88 0.443
0 0.0
0 0.0
0.5826753562602619
0.5270313833317531
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.2779,	 Acc = 0.5180
6218 0.404
33592 0.551
17796 0.498
1318 0.49
29 0.0
0 0.0
0 0.0
0 0.0
0.5314307385986536
0.5314307385986536
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 0
Testing:	 Loss = 1.3447,	 Acc = 0.4976

 ===== Epoch 107	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
  1.0543228   1.542457   -0.42422777 -0.43446103  2.3084884   1.787434
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071  0.02132734  0.03107306  0.00132258 -0.00350269
  0.17305145  0.20801292  0.04772128  0.07545436  0.15986998  0.1478176
 -0.02464492 -0.01491357] 2 2
train:	 Loss = 1.2584,	 Acc = 0.5161
57320 0.302
110966 0.572
60322 0.606
6177 0.621
903 0.522
88 0.443
0 0.0
0 0.0
0.5848332362038822
0.5314307385986536
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3892,	 Acc = 0.4699
6218 0.409
33592 0.501
17796 0.437
1318 0.415
29 0.0
0 0.0
0 0.0
0 0.0
0.4771214563382952
0.5314307385986536
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.4468,	 Acc = 0.4528

 ===== Epoch 108	 =====
[-0.378686   -0.39794773  1.1435546   1.6037886  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071  0.01295194  0.03928641  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 0 0
train:	 Loss = 1.2589,	 Acc = 0.5163
57324 0.304
110965 0.572
60320 0.604
6176 0.628
903 0.515
88 0.398
0 0.0
0 0.0
0.5845325353596486
0.5314307385986536
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3077,	 Acc = 0.5028
6218 0.404
33592 0.506
17796 0.525
1318 0.594
29 0.0
0 0.0
0 0.0
0 0.0
0.5145349388451692
0.5314307385986536
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.3642,	 Acc = 0.4816

 ===== Epoch 109	 =====
[ 0.59788954  1.595732   -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
  0.47740406  1.8239594 ] [-5.0777812e-03  1.6132129e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
  4.7721278e-02  7.5454362e-02 -2.7775567e-02 -1.1532969e-02
 -9.6165907e-01 -2.3357260e+00] 6 6
train:	 Loss = 1.2579,	 Acc = 0.5163
57320 0.304
110965 0.572
60323 0.606
6177 0.617
903 0.523
88 0.398
0 0.0
0 0.0
0.5845530550948133
0.5314307385986536
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3442,	 Acc = 0.4934
6218 0.419
33592 0.522
17796 0.467
1318 0.464
29 0.0
0 0.0
0 0.0
0 0.0
0.5021522707879018
0.5314307385986536
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 0
Testing:	 Loss = 1.4062,	 Acc = 0.4738

 ===== Epoch 110	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071  0.02132734  0.03107306  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 1 1
train:	 Loss = 1.2581,	 Acc = 0.5164
57321 0.305
110965 0.572
60323 0.605
6176 0.616
903 0.516
88 0.443
0 0.0
0 0.0
0.5843826174665883
0.5314307385986536
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.2963,	 Acc = 0.5057
6218 0.396
33592 0.536
17796 0.485
1318 0.542
29 0.0
0 0.0
0 0.0
0 0.0
0.5185360766094624
0.5314307385986536
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 1
Testing:	 Loss = 1.3617,	 Acc = 0.4856

 ===== Epoch 111	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-2.6300831e-02 -3.8560715e-02  2.2895362e+00  1.0864880e+00
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
  4.7721278e-02  7.5454362e-02 -2.7775567e-02 -1.1532969e-02
 -2.4644915e-02 -1.4913575e-02] 3 2
train:	 Loss = 1.2584,	 Acc = 0.5166
57318 0.306
110964 0.571
60327 0.606
6176 0.622
903 0.514
88 0.42
0 0.0
0 0.0
0.5843447758015892
0.5314307385986536
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3693,	 Acc = 0.4840
6218 0.427
33592 0.533
17796 0.419
1318 0.397
29 0.0
0 0.0
0 0.0
0 0.0
0.49071773964160426
0.5314307385986536
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.4218,	 Acc = 0.4664

 ===== Epoch 112	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071  0.02132734  0.03107306  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 2 1
train:	 Loss = 1.2586,	 Acc = 0.5158
57320 0.305
110962 0.57
60326 0.606
6177 0.618
903 0.505
88 0.398
0 0.0
0 0.0
0.583393105303268
0.5314307385986536
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.4178,	 Acc = 0.4741
6218 0.353
33592 0.534
17796 0.414
1318 0.352
29 0.0
0 0.0
0 0.0
0 0.0
0.4884422110552764
0.5314307385986536
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.4680,	 Acc = 0.4555

 ===== Epoch 113	 =====
[ 0.19626898  0.8707574  -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.03158339  0.5674016   0.02132734  0.03107306  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 0 4
train:	 Loss = 1.2587,	 Acc = 0.5162
57321 0.305
110964 0.571
60325 0.606
6176 0.62
902 0.521
88 0.398
0 0.0
0 0.0
0.5839735507550923
0.5314307385986536
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3060,	 Acc = 0.5153
6218 0.419
33592 0.541
17796 0.504
1318 0.489
29 0.0
0 0.0
0 0.0
0 0.0
0.5266142030909263
0.5314307385986536
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 0
Testing:	 Loss = 1.3792,	 Acc = 0.4933

 ===== Epoch 114	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484  1.9278946   1.6650438
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071  0.02132734  0.03107306  0.25714266  0.961341
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
train:	 Loss = 1.2583,	 Acc = 0.5162
57325 0.304
110963 0.571
60320 0.605
6177 0.62
903 0.527
88 0.432
0 0.0
0 0.0
0.5842780371082258
0.5314307385986536
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3593,	 Acc = 0.4889
6218 0.379
33592 0.535
17796 0.442
1318 0.477
29 0.0
0 0.0
0 0.0
0 0.0
0.5018488669763914
0.5314307385986536
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.4288,	 Acc = 0.4675

 ===== Epoch 115	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071  0.02132734  0.03107306  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 2 1
train:	 Loss = 1.2583,	 Acc = 0.5158
57320 0.305
110965 0.572
60325 0.603
6175 0.617
903 0.514
88 0.398
0 0.0
0 0.0
0.5834603487694445
0.5314307385986536
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3554,	 Acc = 0.4790
6218 0.312
33592 0.527
17796 0.453
1318 0.416
29 0.0
0 0.0
0 0.0
0 0.0
0.49875794064662937
0.5314307385986536
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 0
Testing:	 Loss = 1.4156,	 Acc = 0.4622

 ===== Epoch 116	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103  2.301797    1.8607359
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071  0.02132734  0.03107306  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.24648407  0.04332542
 -0.02464492 -0.01491357] 3 1
train:	 Loss = 1.2592,	 Acc = 0.5155
57319 0.305
110964 0.57
60325 0.605
6177 0.619
903 0.505
88 0.375
0 0.0
0 0.0
0.5832497464375171
0.5314307385986536
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3783,	 Acc = 0.4832
6218 0.381
33592 0.523
17796 0.444
1318 0.48
29 0.0
0 0.0
0 0.0
0 0.0
0.4952119085996018
0.5314307385986536
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 0
Testing:	 Loss = 1.4416,	 Acc = 0.4639

 ===== Epoch 117	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-2.6300831e-02 -3.8560715e-02  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
  4.7721278e-02  7.5454362e-02  2.2000713e+00  2.0130029e+00
 -2.4644915e-02 -1.4913575e-02] 6 6
train:	 Loss = 1.2581,	 Acc = 0.5162
57322 0.304
110964 0.573
60323 0.604
6176 0.618
903 0.511
88 0.432
0 0.0
0 0.0
0.584335458997837
0.5314307385986536
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3236,	 Acc = 0.4945
6218 0.367
33592 0.53
17796 0.467
1318 0.577
29 0.0
0 0.0
0 0.0
0 0.0
0.5094908504788092
0.5314307385986536
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.3890,	 Acc = 0.4732

 ===== Epoch 118	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071  0.02132734  0.03107306  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 1 1
train:	 Loss = 1.2585,	 Acc = 0.5160
57322 0.303
110963 0.572
60325 0.605
6175 0.617
903 0.526
88 0.386
0 0.0
0 0.0
0.5842514037230883
0.5314307385986536
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3807,	 Acc = 0.4848
6218 0.419
33592 0.516
17796 0.448
1318 0.505
29 0.0
0 0.0
0 0.0
0 0.0
0.49250023703422774
0.5314307385986536
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.4387,	 Acc = 0.4681

 ===== Epoch 119	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484  2.2227519   1.1751548
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071  0.02132734  0.03107306 -3.0121558  -1.3903375
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 5 5
train:	 Loss = 1.2599,	 Acc = 0.5141
57326 0.303
110963 0.569
60321 0.603
6175 0.618
903 0.522
88 0.398
0 0.0
0 0.0
0.581994956570468
0.5314307385986536
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3252,	 Acc = 0.4975
6218 0.39
33592 0.518
17796 0.493
1318 0.555
29 0.0
0 0.0
0 0.0
0 0.0
0.5101355835782687
0.5314307385986536
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.3854,	 Acc = 0.4780

 ===== Epoch 120	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484  3.065927    2.5951555
 -0.3536378  -0.35852543  3.3182893   1.6495013  -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071  0.02132734  0.03107306 -3.9710376  -2.6788442
 -0.03495734 -0.02773637  0.14713128  0.8879442  -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
train:	 Loss = 1.2568,	 Acc = 0.5169
57321 0.303
110964 0.572
60324 0.608
6176 0.621
903 0.523
88 0.398
0 0.0
0 0.0
0.5855817993331652
0.5314307385986536
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3429,	 Acc = 0.4955
6218 0.413
33592 0.54
17796 0.45
1318 0.389
29 0.0
0 0.0
0 0.0
0 0.0
0.5053380108087608
0.5314307385986536
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.3940,	 Acc = 0.4764

 ===== Epoch 121	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.0826759   2.5493941  -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-2.6300831e-02 -3.8560715e-02  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -1.5874988e+00 -2.9804778e+00 -2.7775567e-02 -1.1532969e-02
 -2.4644915e-02 -1.4913575e-02] 1 1
train:	 Loss = 1.2588,	 Acc = 0.5155
57322 0.303
110962 0.571
60325 0.605
6176 0.615
903 0.514
88 0.386
0 0.0
0 0.0
0.5836181873199816
0.5314307385986536
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.2922,	 Acc = 0.5262
6218 0.425
33592 0.56
17796 0.496
1318 0.563
29 0.0
0 0.0
0 0.0
0 0.0
0.5381245851901014
0.5381245851901014
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.3548,	 Acc = 0.5039

 ===== Epoch 122	 =====
[ 2.6554186   1.8816373  -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
  2.47386     2.0036006 ] [ 0.5064455   0.08309968  0.02132734  0.03107306  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
  0.02108388  0.148185  ] 1 3
train:	 Loss = 1.2581,	 Acc = 0.5161
57325 0.305
110960 0.572
60325 0.604
6175 0.618
903 0.514
88 0.398
0 0.0
0 0.0
0.5840426783823011
0.5381245851901014
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 2
val:	 Loss = 1.3490,	 Acc = 0.4955
6218 0.336
33592 0.546
17796 0.46
1318 0.451
29 0.0
0 0.0
0 0.0
0 0.0
0.5142504977718783
0.5381245851901014
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.4106,	 Acc = 0.4738

 ===== Epoch 123	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071  0.02132734  0.03107306  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 1 1
train:	 Loss = 1.2579,	 Acc = 0.5160
57317 0.304
110968 0.571
60323 0.606
6177 0.62
903 0.503
88 0.409
0 0.0
0 0.0
0.5841846026258132
0.5381245851901014
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.4216,	 Acc = 0.4773
6218 0.407
33592 0.525
17796 0.416
1318 0.436
29 0.0
0 0.0
0 0.0
0 0.0
0.48565468853702476
0.5381245851901014
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.4827,	 Acc = 0.4579

 ===== Epoch 124	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071  0.02132734  0.03107306  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 3 1
train:	 Loss = 1.2596,	 Acc = 0.5147
57321 0.303
110970 0.572
60319 0.602
6175 0.612
903 0.517
88 0.375
0 0.0
0 0.0
0.5828920456137402
0.5381245851901014
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3247,	 Acc = 0.4853
6218 0.398
33592 0.508
17796 0.473
1318 0.504
29 0.0
0 0.0
0 0.0
0 0.0
0.4956670143168674
0.5381245851901014
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 0
Testing:	 Loss = 1.3946,	 Acc = 0.4644

 ===== Epoch 125	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071  0.02132734  0.03107306  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 1 1
train:	 Loss = 1.2577,	 Acc = 0.5170
57324 0.305
110962 0.573
60322 0.606
6177 0.618
903 0.516
88 0.443
0 0.0
0 0.0
0.5851657588595253
0.5381245851901014
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3363,	 Acc = 0.4941
6218 0.406
33592 0.535
17796 0.45
1318 0.467
29 0.0
0 0.0
0 0.0
0 0.0
0.5044088366360102
0.5381245851901014
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.3966,	 Acc = 0.4742

 ===== Epoch 126	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071  0.02132734  0.03107306  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 5 1
train:	 Loss = 1.2576,	 Acc = 0.5163
57321 0.305
110967 0.571
60320 0.606
6177 0.623
903 0.501
88 0.42
0 0.0
0 0.0
0.5840632092124065
0.5381245851901014
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.2698,	 Acc = 0.5054
6218 0.387
33592 0.513
17796 0.531
1318 0.547
29 0.0
0 0.0
0 0.0
0 0.0
0.5193514743528965
0.5381245851901014
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.3430,	 Acc = 0.4829

 ===== Epoch 127	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071  0.02132734  0.03107306  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 0 1
train:	 Loss = 1.2579,	 Acc = 0.5170
57320 0.303
110961 0.573
60327 0.607
6177 0.613
903 0.514
88 0.409
0 0.0
0 0.0
0.5857298157529027
0.5381245851901014
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3840,	 Acc = 0.4663
6218 0.408
33592 0.492
17796 0.436
1318 0.499
29 0.0
0 0.0
0 0.0
0 0.0
0.47325305774153786
0.5381245851901014
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.4410,	 Acc = 0.4487

 ===== Epoch 128	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071  0.02132734  0.03107306  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 1 1
train:	 Loss = 1.2582,	 Acc = 0.5164
57323 0.305
110962 0.572
60323 0.605
6177 0.619
903 0.52
88 0.398
0 0.0
0 0.0
0.5844508077757169
0.5381245851901014
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3391,	 Acc = 0.5097
6218 0.417
33592 0.548
17796 0.47
1318 0.515
29 0.0
0 0.0
0 0.0
0 0.0
0.5205650895989381
0.5381245851901014
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.4055,	 Acc = 0.4880

 ===== Epoch 129	 =====
[-0.378686   -0.39794773  2.553296    1.7072233  -0.42709485 -0.35320815
  1.178531    3.563163   -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00566326  0.02285971  0.00132258 -0.00350269
  0.13343243 -0.11648905  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 2 3
train:	 Loss = 1.2587,	 Acc = 0.5162
57321 0.303
110969 0.572
60318 0.606
6177 0.616
903 0.508
88 0.398
0 0.0
0 0.0
0.5846011599562915
0.5381245851901014
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3085,	 Acc = 0.5045
6218 0.421
33592 0.541
17796 0.469
1318 0.445
29 0.0
0 0.0
0 0.0
0 0.0
0.5143642742011947
0.5381245851901014
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.3618,	 Acc = 0.4850

 ===== Epoch 130	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071  0.02132734  0.03107306  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 6 1
train:	 Loss = 1.2582,	 Acc = 0.5164
57322 0.305
110967 0.572
60319 0.605
6177 0.617
903 0.513
88 0.432
0 0.0
0 0.0
0.584313044257904
0.5381245851901014
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3376,	 Acc = 0.4909
6218 0.42
33592 0.512
17796 0.476
1318 0.488
29 0.0
0 0.0
0 0.0
0 0.0
0.49930786005499195
0.5381245851901014
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.3912,	 Acc = 0.4732

 ===== Epoch 131	 =====
[ 0.73350316  0.87841564 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  2.977692   -4.595281   -0.4190337  -0.41667622
  1.0783345   1.061767  ] [ 3.6622450e-01  3.2876009e-01  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
  2.8158814e-01  7.8098726e+00 -2.7775567e-02 -1.1532969e-02
  6.1780233e-02  2.0968118e-01] 2 2
train:	 Loss = 1.2589,	 Acc = 0.5157
57319 0.304
110968 0.571
60321 0.605
6177 0.616
903 0.517
88 0.432
0 0.0
0 0.0
0.5835579439304707
0.5381245851901014
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.2943,	 Acc = 0.5120
6218 0.42
33592 0.534
17796 0.5
1318 0.544
29 0.0
0 0.0
0 0.0
0 0.0
0.5228216554470465
0.5381245851901014
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 1
Testing:	 Loss = 1.3534,	 Acc = 0.4937

 ===== Epoch 132	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 3.9806977e-01  2.1536658e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
  4.7721278e-02  7.5454362e-02 -2.7775567e-02 -1.1532969e-02
 -2.4644915e-02 -1.4913575e-02] 4 6
train:	 Loss = 1.2588,	 Acc = 0.5157
57322 0.305
110960 0.571
60326 0.604
6177 0.617
903 0.512
88 0.443
0 0.0
0 0.0
0.5834556804554675
0.5381245851901014
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.4241,	 Acc = 0.4673
6218 0.33
33592 0.525
17796 0.411
1318 0.417
29 0.0
0 0.0
0 0.0
0 0.0
0.48353086185645205
0.5381245851901014
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.4679,	 Acc = 0.4508

 ===== Epoch 133	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484  0.7699446   2.6041856
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071  0.02132734  0.03107306 -1.3599836  -2.6870382
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 5 1
train:	 Loss = 1.2589,	 Acc = 0.5152
57318 0.303
110965 0.571
60326 0.604
6177 0.615
902 0.508
88 0.443
0 0.0
0 0.0
0.5832184603660245
0.5381245851901014
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3398,	 Acc = 0.4918
6218 0.394
33592 0.52
17796 0.471
1318 0.542
29 0.0
0 0.0
0 0.0
0 0.0
0.5033658860339433
0.5381245851901014
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 0
Testing:	 Loss = 1.4217,	 Acc = 0.4715

 ===== Epoch 134	 =====
[ 1.9225135   2.6755354  -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
  2.3837423   2.8068535 ] [ 2.0322053e+00  5.1592988e-01  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
  4.7721278e-02  7.5454362e-02 -2.7775567e-02 -1.1532969e-02
 -7.0373707e-02  6.1074322e-01] 4 2
train:	 Loss = 1.2584,	 Acc = 0.5160
57323 0.304
110965 0.571
60322 0.606
6176 0.614
902 0.494
88 0.375
0 0.0
0 0.0
0.5840585476287874
0.5381245851901014
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3213,	 Acc = 0.5042
6218 0.404
33592 0.536
17796 0.478
1318 0.546
29 0.0
0 0.0
0 0.0
0 0.0
0.51608988337916
0.5381245851901014
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 0
Testing:	 Loss = 1.3962,	 Acc = 0.4837

 ===== Epoch 135	 =====
[ 1.1596818   3.0686553  -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
  0.78675044  3.3534765 ] [-1.4129223e+00 -3.2157686e+00  3.9601798e+00  1.4725152e+00
  1.3225782e-03 -3.5026856e-03  3.2157607e+00  3.4086559e+00
  4.7721278e-02  7.5454362e-02 -2.7775567e-02 -1.1532969e-02
 -1.2881587e+00 -3.9292793e+00] 6 6
train:	 Loss = 1.2583,	 Acc = 0.5162
57321 0.305
110964 0.572
60323 0.605
6177 0.621
903 0.504
88 0.375
0 0.0
0 0.0
0.5839679471015102
0.5381245851901014
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3943,	 Acc = 0.4750
6218 0.432
33592 0.515
17796 0.424
1318 0.363
29 0.0
0 0.0
0 0.0
0 0.0
0.48007964350052146
0.5381245851901014
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 0
Testing:	 Loss = 1.4653,	 Acc = 0.4554

 ===== Epoch 136	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071  0.02132734  0.03107306  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 6 1
train:	 Loss = 1.2589,	 Acc = 0.5160
57322 0.304
110966 0.571
60322 0.605
6175 0.618
903 0.509
88 0.375
0 0.0
0 0.0
0.5841617447633564
0.5381245851901014
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3524,	 Acc = 0.4788
6218 0.406
33592 0.504
17796 0.454
1318 0.512
29 0.0
0 0.0
0 0.0
0 0.0
0.48738029771499003
0.5381245851901014
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.4114,	 Acc = 0.4596

 ===== Epoch 137	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103  2.914291    1.4790609
  3.562206    3.332946  ] [-0.02630083 -0.03856071  0.02132734  0.03107306  1.3630735   2.983211
 -0.03495734 -0.02773637  0.04772128  0.07545436 -3.5825698  -1.9707612
 -4.2175126  -3.9078894 ] 5 5
train:	 Loss = 1.2582,	 Acc = 0.5155
57317 0.305
110967 0.57
60325 0.605
6176 0.618
903 0.515
88 0.409
0 0.0
0 0.0
0.5831927781731379
0.5381245851901014
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3527,	 Acc = 0.5033
6218 0.436
33592 0.54
17796 0.456
1318 0.539
29 0.0
0 0.0
0 0.0
0 0.0
0.5113112733478714
0.5381245851901014
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 1
Testing:	 Loss = 1.4169,	 Acc = 0.4832

 ===== Epoch 138	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
  2.2788935   2.1755428 ] [ 4.3836004e-01  1.2880054e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03  3.5991514e+00  1.4144945e+00
  4.7721278e-02  7.5454362e-02  4.3762479e+00  1.9424707e+00
 -2.8630407e+00 -2.7020295e+00] 2 2
train:	 Loss = 1.2583,	 Acc = 0.5160
57321 0.304
110966 0.572
60322 0.606
6177 0.614
902 0.516
88 0.42
0 0.0
0 0.0
0.5841528676697206
0.5381245851901014
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3113,	 Acc = 0.4967
6218 0.366
33592 0.541
17796 0.464
1318 0.442
29 0.0
0 0.0
0 0.0
0 0.0
0.5121645965677444
0.5381245851901014
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.3796,	 Acc = 0.4756

 ===== Epoch 139	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484  0.99997306  2.001419
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071  0.02132734  0.03107306 -0.96111244  0.80975205
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 1 1
train:	 Loss = 1.2584,	 Acc = 0.5158
57321 0.304
110967 0.57
60322 0.606
6176 0.622
902 0.501
88 0.386
0 0.0
0 0.0
0.5837269899974784
0.5381245851901014
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3382,	 Acc = 0.4966
6218 0.405
33592 0.536
17796 0.456
1318 0.484
29 0.0
0 0.0
0 0.0
0 0.0
0.5074808002275528
0.5381245851901014
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.4012,	 Acc = 0.4757

 ===== Epoch 140	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484  1.5518054   1.4912124
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071  0.02132734  0.03107306  0.11812954  0.05385533
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 1 3
train:	 Loss = 1.2584,	 Acc = 0.5160
57324 0.305
110963 0.571
60321 0.606
6177 0.616
903 0.518
88 0.432
0 0.0
0 0.0
0.5838881043641988
0.5381245851901014
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3850,	 Acc = 0.4702
6218 0.338
33592 0.529
17796 0.413
1318 0.376
29 0.0
0 0.0
0 0.0
0 0.0
0.48574950222812174
0.5381245851901014
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.4384,	 Acc = 0.4528

 ===== Epoch 141	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071  0.02132734  0.03107306  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 5 1
train:	 Loss = 1.2591,	 Acc = 0.5160
57324 0.306
110960 0.571
60325 0.604
6176 0.614
903 0.512
88 0.375
0 0.0
0 0.0
0.583394974558985
0.5381245851901014
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3944,	 Acc = 0.4835
6218 0.419
33592 0.531
17796 0.421
1318 0.433
29 0.0
0 0.0
0 0.0
0 0.0
0.49119180809708923
0.5381245851901014
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.4424,	 Acc = 0.4658

 ===== Epoch 142	 =====
[-0.378686   -0.39794773  2.6214485   1.6390504  -0.42709485 -0.35320815
  0.94615865  3.5106015  -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.08105194  0.08240647  0.00132258 -0.00350269
  0.36125252 -0.06656566  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 2 0
train:	 Loss = 1.2593,	 Acc = 0.5149
57324 0.302
110964 0.572
60320 0.603
6177 0.612
903 0.517
88 0.318
0 0.0
0 0.0
0.5831988433864569
0.5381245851901014
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.4279,	 Acc = 0.4686
6218 0.376
33592 0.513
17796 0.42
1318 0.442
29 0.0
0 0.0
0 0.0
0 0.0
0.4796055750450365
0.5381245851901014
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.4761,	 Acc = 0.4516

 ===== Epoch 143	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103  1.2629986   1.9770079
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071  0.02132734  0.03107306  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436  0.07507203  0.25753438
 -0.02464492 -0.01491357] 4 4
train:	 Loss = 1.2598,	 Acc = 0.5148
57320 0.304
110966 0.57
60323 0.605
6176 0.614
903 0.505
88 0.364
0 0.0
0 0.0
0.5826310126866007
0.5381245851901014
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.2904,	 Acc = 0.5219
6218 0.423
33592 0.548
17796 0.508
1318 0.519
29 0.0
0 0.0
0 0.0
0 0.0
0.533535602541007
0.5381245851901014
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.3580,	 Acc = 0.4993

 ===== Epoch 144	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071  0.02132734  0.03107306  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 0 1
train:	 Loss = 1.2592,	 Acc = 0.5156
57319 0.304
110969 0.571
60320 0.605
6177 0.615
903 0.515
88 0.432
0 0.0
0 0.0
0.5837036372907759
0.5381245851901014
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 2
val:	 Loss = 1.3722,	 Acc = 0.4924
6218 0.393
33592 0.541
17796 0.436
1318 0.477
29 0.0
0 0.0
0 0.0
0 0.0
0.5040485446098416
0.5381245851901014
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 0
Testing:	 Loss = 1.4279,	 Acc = 0.4737

 ===== Epoch 145	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071  0.02132734  0.03107306  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 5 1
train:	 Loss = 1.2592,	 Acc = 0.5154
57326 0.303
110960 0.57
60322 0.606
6177 0.617
903 0.525
88 0.409
0 0.0
0 0.0
0.5835135892406836
0.5381245851901014
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3939,	 Acc = 0.4880
6218 0.381
33592 0.532
17796 0.441
1318 0.511
29 0.0
0 0.0
0 0.0
0 0.0
0.5005973262539111
0.5381245851901014
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.4478,	 Acc = 0.4707

 ===== Epoch 146	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 5.0081557e-01  1.8846093e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
  4.7721278e-02  7.5454362e-02 -2.7775567e-02 -1.1532969e-02
 -2.4644915e-02 -1.4913575e-02] 6 6
train:	 Loss = 1.2591,	 Acc = 0.5162
57317 0.304
110966 0.572
60327 0.605
6175 0.616
903 0.508
88 0.432
0 0.0
0 0.0
0.5842574484895691
0.5381245851901014
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.2929,	 Acc = 0.5098
6218 0.39
33592 0.52
17796 0.532
1318 0.527
29 0.0
0 0.0
0 0.0
0 0.0
0.5239783824784299
0.5381245851901014
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.3577,	 Acc = 0.4908

 ===== Epoch 147	 =====
[-0.378686   -0.39794773  3.4472685  -4.4518466   2.2715697   1.0848528
 -0.3536378  -0.35852543  0.98343325  0.87038356 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071  0.53834224  0.05160642  0.06039262  6.506632
 -0.03495734 -0.02773637  0.15414025  0.28403386 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 2 2
train:	 Loss = 1.2595,	 Acc = 0.5160
57323 0.304
110963 0.572
60322 0.603
6177 0.618
903 0.508
88 0.409
0 0.0
0 0.0
0.5841201885090191
0.5381245851901014
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3042,	 Acc = 0.5126
6218 0.377
33592 0.553
17796 0.484
1318 0.53
29 0.0
0 0.0
0 0.0
0 0.0
0.5286811415568408
0.5381245851901014
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.3651,	 Acc = 0.4922

 ===== Epoch 148	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  2.0886152   2.1041842  -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071  0.02132734  0.03107306  0.00132258 -0.00350269
 -0.03495734 -0.02773637 -0.00262198  0.44653183 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 6 4
train:	 Loss = 1.2588,	 Acc = 0.5163
57323 0.305
110965 0.572
60321 0.605
6176 0.616
903 0.511
88 0.386
0 0.0
0 0.0
0.5841762256728662
0.5381245851901014
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3414,	 Acc = 0.5083
6218 0.398
33592 0.555
17796 0.458
1318 0.52
29 0.0
0 0.0
0 0.0
0 0.0
0.5213046363894946
0.5381245851901014
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.4008,	 Acc = 0.4864

 ===== Epoch 149	 =====
[-0.378686   -0.39794773  0.54364747  2.7227647  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-2.6300831e-02 -3.8560715e-02 -1.0392715e+00 -2.6444242e+00
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
  4.7721278e-02  7.5454362e-02 -2.7775567e-02 -1.1532969e-02
 -2.4644915e-02 -1.4913575e-02] 6 5
train:	 Loss = 1.2590,	 Acc = 0.5158
57322 0.303
110956 0.572
60330 0.605
6177 0.618
903 0.516
88 0.295
0 0.0
0 0.0
0.5841281226534569
0.5381245851901014
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.4151,	 Acc = 0.4677
6218 0.407
33592 0.508
17796 0.418
1318 0.416
29 0.0
0 0.0
0 0.0
0 0.0
0.4749028159666256
0.5381245851901014
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.4728,	 Acc = 0.4504

 ===== Epoch 150	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103  1.4944551   2.0882244
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071  0.02132734  0.03107306  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436  0.09102331  0.01197777
 -0.02464492 -0.01491357] 0 2
train:	 Loss = 1.2584,	 Acc = 0.5154
57325 0.304
110959 0.57
60325 0.605
6176 0.621
903 0.515
88 0.386
0 0.0
0 0.0
0.5833982437756022
0.5381245851901014
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3092,	 Acc = 0.5172
6218 0.402
33592 0.559
17796 0.479
1318 0.527
29 0.0
0 0.0
0 0.0
0 0.0
0.5308428937138523
0.5381245851901014
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.3619,	 Acc = 0.4986

 ===== Epoch 151	 =====
[-0.378686   -0.39794773  2.985768    3.221132    2.6421924   1.3602742
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -3.7406883  -3.0797317  -0.03953796  0.4102944
 -0.03495734 -0.02773637  2.520277    1.3148046  -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 1 6
train:	 Loss = 1.2579,	 Acc = 0.5161
57323 0.304
110968 0.572
60319 0.604
6175 0.619
903 0.516
88 0.398
0 0.0
0 0.0
0.5841369996581733
0.5381245851901014
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3943,	 Acc = 0.4882
6218 0.411
33592 0.534
17796 0.437
1318 0.389
29 0.0
0 0.0
0 0.0
0 0.0
0.49722195885085807
0.5381245851901014
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.4422,	 Acc = 0.4712

 ===== Epoch 152	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103  2.3218725   2.4496782
 -0.41038254 -0.40359274] [-2.6300831e-02 -3.8560715e-02  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
  4.7721278e-02  7.5454362e-02 -2.9507906e+00 -2.9738860e+00
 -2.4644915e-02 -1.4913575e-02] 5 6
train:	 Loss = 1.2578,	 Acc = 0.5165
57320 0.306
110967 0.571
60323 0.607
6176 0.618
902 0.508
88 0.341
0 0.0
0 0.0
0.5842280450082934
0.5381245851901014
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3252,	 Acc = 0.5011
6218 0.419
33592 0.542
17796 0.46
1318 0.414
29 0.0
0 0.0
0 0.0
0 0.0
0.5107803166777283
0.5381245851901014
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.3870,	 Acc = 0.4812

 ===== Epoch 153	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-2.6300831e-02 -3.8560715e-02  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
  1.7511262e+00  1.9114391e+00 -2.7775567e-02 -1.1532969e-02
 -2.4644915e-02 -1.4913575e-02] 4 6
train:	 Loss = 1.2597,	 Acc = 0.5154
57326 0.305
110956 0.57
60327 0.604
6177 0.614
902 0.519
88 0.432
0 0.0
0 0.0
0.5829195853180162
0.5381245851901014
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3611,	 Acc = 0.4766
6218 0.374
33592 0.513
17796 0.443
1318 0.491
29 0.0
0 0.0
0 0.0
0 0.0
0.488707689390348
0.5381245851901014
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 0
Testing:	 Loss = 1.4306,	 Acc = 0.4568

 ===== Epoch 154	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484  1.2362493   2.8231683
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071  0.02132734  0.03107306  0.20784394  0.15423185
 -0.03495734 -0.02773637  3.350604    1.3269312  -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 2 2
train:	 Loss = 1.2582,	 Acc = 0.5166
57319 0.304
110968 0.572
60322 0.605
6176 0.621
903 0.512
88 0.398
0 0.0
0 0.0
0.5847851303115036
0.5381245851901014
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3836,	 Acc = 0.4882
6218 0.425
33592 0.534
17796 0.432
1318 0.379
29 0.0
0 0.0
0 0.0
0 0.0
0.49562908884042856
0.5381245851901014
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.4282,	 Acc = 0.4703

 ===== Epoch 155	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484  1.7392638   1.633438
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071  0.02132734  0.03107306  0.03507669 -0.00759969
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 6 3
train:	 Loss = 1.2582,	 Acc = 0.5159
57321 0.304
110963 0.571
60325 0.606
6176 0.62
903 0.512
88 0.375
0 0.0
0 0.0
0.5839959653694209
0.5381245851901014
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3425,	 Acc = 0.4980
6218 0.369
33592 0.536
17796 0.473
1318 0.476
29 0.0
0 0.0
0 0.0
0 0.0
0.5131506589551531
0.5381245851901014
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.3990,	 Acc = 0.4779

 ===== Epoch 156	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
  2.6672635   2.6579335  -0.42422777 -0.43446103  3.0040393   3.3874362
 -0.41038254 -0.40359274] [-2.6300831e-02 -3.8560715e-02  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -2.9966762e+00 -2.8927836e+00
  4.7721278e-02  7.5454362e-02 -3.6782813e+00 -3.9430511e+00
 -2.4644915e-02 -1.4913575e-02] 6 6
train:	 Loss = 1.2583,	 Acc = 0.5171
57316 0.306
110967 0.572
60326 0.607
6176 0.625
903 0.512
88 0.42
0 0.0
0 0.0
0.584864955732377
0.5381245851901014
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3676,	 Acc = 0.4793
6218 0.386
33592 0.5
17796 0.473
1318 0.483
29 0.0
0 0.0
0 0.0
0 0.0
0.49028159666255805
0.5381245851901014
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.4282,	 Acc = 0.4599

 ===== Epoch 157	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071  0.02132734  0.03107306  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 1 1
train:	 Loss = 1.2584,	 Acc = 0.5164
57320 0.306
110962 0.572
60327 0.605
6176 0.619
903 0.503
88 0.341
0 0.0
0 0.0
0.5841159725646659
0.5381245851901014
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3283,	 Acc = 0.4904
6218 0.406
33592 0.5
17796 0.505
1318 0.477
29 0.0
0 0.0
0 0.0
0 0.0
0.5003887361334977
0.5381245851901014
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.3958,	 Acc = 0.4724

 ===== Epoch 158	 =====
[-0.378686   -0.39794773  1.6824619   2.4124606  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-2.6300831e-02 -3.8560715e-02 -2.2990012e+00 -2.3733840e+00
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
  4.7721278e-02  7.5454362e-02 -2.7775567e-02 -1.1532969e-02
 -2.4644915e-02 -1.4913575e-02] 4 1
train:	 Loss = 1.2581,	 Acc = 0.5159
57322 0.304
110962 0.571
60326 0.607
6175 0.616
903 0.516
88 0.432
0 0.0
0 0.0
0.5841001042285406
0.5381245851901014
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.3634,	 Acc = 0.4894
6218 0.409
33592 0.53
17796 0.445
1318 0.447
29 0.0
0 0.0
0 0.0
0 0.0
0.49890964255238457
0.5381245851901014
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.4189,	 Acc = 0.4722

 ===== Epoch 159	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071  0.02132734  0.03107306  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 1 1
train:	 Loss = 1.2584,	 Acc = 0.5156
57318 0.304
110968 0.572
60322 0.603
6177 0.617
903 0.512
88 0.364
0 0.0
0 0.0
0.5836219166414507
0.5381245851901014
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.2747,	 Acc = 0.5218
6218 0.42
33592 0.548
17796 0.508
1318 0.531
29 0.0
0 0.0
0 0.0
0 0.0
0.5337821181378591
0.5381245851901014
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.3355,	 Acc = 0.5013

 ===== Epoch 160	 =====
[ 1.8045357   1.3864083  -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-1.9941671e+00 -1.6739570e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
  4.7721278e-02  7.5454362e-02 -2.7775567e-02 -1.1532969e-02
 -2.4644915e-02 -1.4913575e-02] 0 6
train:	 Loss = 1.2595,	 Acc = 0.5155
57318 0.305
110963 0.571
60327 0.604
6177 0.616
903 0.504
88 0.386
0 0.0
0 0.0
0.5831456140940725
0.5381245851901014
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00] 3 5
val:	 Loss = 1.4157,	 Acc = 0.4620
6218 0.372
33592 0.499
17796 0.425
1318 0.454
29 0.0
0 0.0
0 0.0
0 0.0
0.4725893619038589
0.5381245851901014
[-0.378686   -0.39794773  1.1334587   2.4312665  -0.42709485 -0.35320815
 -0.3536378  -0.35852543 -0.42422777 -0.43446103 -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-0.02630083 -0.03856071 -0.00147556  0.21792667  0.00132258 -0.00350269
 -0.03495734 -0.02773637  0.04772128  0.07545436 -0.02777557 -0.01153297
 -0.02464492 -0.01491357] 4 4
Testing:	 Loss = 1.4562,	 Acc = 0.4473

 ===== Epoch 161	 =====
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  2.4756038   1.424528   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [-2.6300831e-02 -3.8560715e-02  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -3.0990376e+00 -1.8284398e+00 -2.7775567e-02 -1.1532969e-02
 -2.4644915e-02 -1.4913575e-02] 6 4
train:	 Loss = 1.2584,	 Acc = 0.5172
57322 0.307
110964 0.572
60323 0.607
6176 0.623
903 0.503
88 0.42
0 0.0
0 0.0
0.5848285832763626
0.5381245851901014
[-0.378686   -0.39794773 -0.41514966 -0.34031484 -0.42709485 -0.35320815
 -0.3536378  -0.35852543  1.3287288   2.395465   -0.4190337  -0.41667622
 -0.41038254 -0.40359274] [ 4.2823305e+00  1.6389488e+00  2.1327343e-02  3.1073060e-02
  1.3225782e-03 -3.5026856e-03 -3.4957342e-02 -2.7736368e-02
 -6.3157588e-02 -6.5942454e-01 -2.7775567e-02 -1.1532969e-02
  3.5216246e+00  1.9823754e+00]