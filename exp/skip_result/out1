(0.24300000000005184, array([-1.000000e+00, -1.000000e+00,  1.189783e+03,  1.790000e-01,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 4, array([  0.   ,   0.   , -15.851,   0.091,   0.   ,   0.   ,   0.   ,
         0.   ,   0.   ,   0.   ,   0.   ,   0.   ,   0.   ,   0.   ]), 1)
((0.236, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.364685e+03, -3.300000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), array([0., 0., 0.])), (0.24, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.543573e+03, -3.290000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), array([0., 0., 0.])))
(0.24, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.543573e+03, -3.290000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([   0.   ,    0.   ,    0.   ,    0.   , -173.065,    3.128,
          0.   ,    0.   ,    0.   ,    0.   ,    0.   ,    0.   ,
          0.   ,    0.   ]))
(0.238, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.370508e+03,  2.799000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([  0.   ,   0.   ,   0.   ,   0.   , -14.234,  -3.14 ,   0.   ,
         0.   ,   0.   ,   0.   ,   0.   ,   0.   ,   0.   ,   0.   ]))
(0.236, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.356274e+03, -3.410000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 7.0844e+01,
       8.0000e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
       0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]))
(0.2350000000000001, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.427118e+03, -3.330000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([ 0.00000e+00,  0.00000e+00,  0.00000e+00,  0.00000e+00,
       -1.81476e+02, -1.00000e-03,  0.00000e+00,  0.00000e+00,
        0.00000e+00,  0.00000e+00,  0.00000e+00,  0.00000e+00,
        0.00000e+00,  0.00000e+00]))
(0.248, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.245642e+03, -3.340000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([ 0.00000e+00,  0.00000e+00,  0.00000e+00,  0.00000e+00,
        1.69507e+02, -9.00000e-03,  0.00000e+00,  0.00000e+00,
        0.00000e+00,  0.00000e+00,  0.00000e+00,  0.00000e+00,
        0.00000e+00,  0.00000e+00]))
(0.256, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.415149e+03, -3.430000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -2.0412e+02,
        3.0000e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
        0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]))
(0.256, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.211029e+03, -3.400000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([0.000000e+00, 0.000000e+00, 2.399328e+03, 1.498000e+00,
       1.390990e+02, 4.000000e-03, 0.000000e+00, 0.000000e+00,
       0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,
       0.000000e+00, 0.000000e+00]))
(0.2670000000000001, array([-1.000000e+00, -1.000000e+00,  2.398328e+03,  4.980000e-01,
        2.350128e+03, -3.360000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([ 0.000000e+00,  0.000000e+00, -2.399328e+03, -1.498000e+00,
        3.364300e+01, -3.000000e-03,  0.000000e+00,  0.000000e+00,
        0.000000e+00,  0.000000e+00,  0.000000e+00,  0.000000e+00,
        0.000000e+00,  0.000000e+00]))
(0.2609999999999997, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.383771e+03, -3.390000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  5.1434e+01,
       -1.1000e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
        0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]))
(0.28500000000000014, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.435205e+03, -3.500000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -2.9114e+01,
        3.0000e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
        0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]))
14 1 14

 ===== Epoch 1	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.00014811 -0.00011583] 0 3
train:	 Loss = 1.9261,	 Acc = 0.2137
2821 0.177
5491 0.212
2838 0.239
365 0.288
63 0.46
6 0.333
0 0.0
0 0.0
0.22560766860664155
0.0
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.7785,	 Acc = 0.3178
365 0.205
1671 0.35
791 0.306
82 0.268
2 0.5
0 0.0
0 0.0
0 0.0
0.33385703063629224
0.33385703063629224
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.8038,	 Acc1 = 0.2674,	 Acc2 = 0.3062

 ===== Epoch 2	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829   3.3346007   1.4573783  -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.00014811 -0.00011583] 0 2
train:	 Loss = 1.7011,	 Acc = 0.3527
2819 0.235
5505 0.364
2829 0.437
363 0.427
62 0.468
6 0.0
0 0.0
0 0.0
0.39053051911009695
0.33385703063629224
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 5
val:	 Loss = 1.6569,	 Acc = 0.3610
365 0.323
1671 0.387
791 0.329
82 0.317
2 0.0
0 0.0
0 0.0
0 0.0
0.36645718774548314
0.36645718774548314
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.6845,	 Acc1 = 0.3293,	 Acc2 = 0.3407

 ===== Epoch 3	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913  0.7482764   2.8324356
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275 -0.03525094 -0.113786
 -0.00014811 -0.00011583] 5 5
train:	 Loss = 1.6653,	 Acc = 0.3680
2822 0.234
5494 0.382
2834 0.464
364 0.448
64 0.359
6 0.167
0 0.0
0 0.0
0.41120748687514264
0.36645718774548314
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 0
val:	 Loss = 1.6073,	 Acc = 0.4294
365 0.321
1671 0.434
791 0.475
82 0.378
2 0.0
0 0.0
0 0.0
0 0.0
0.44501178318931656
0.44501178318931656
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.6685,	 Acc1 = 0.3318,	 Acc2 = 0.3437

 ===== Epoch 4	 =====
[ 3.2611687   3.047116   -0.42037553 -0.33554247 -0.4410947  -0.36475152
  1.50194     2.3041751  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [-4.0272226e+00 -4.2046204e+00  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -1.4569114e-01 -4.4758078e-01
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 5 5
train:	 Loss = 1.6552,	 Acc = 0.3712
2821 0.236
5494 0.386
2835 0.462
365 0.488
63 0.381
6 0.167
0 0.0
0 0.0
0.4146981627296588
0.44501178318931656
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 0
val:	 Loss = 1.6213,	 Acc = 0.3981
365 0.332
1671 0.412
791 0.411
82 0.305
2 0.0
0 0.0
0 0.0
0 0.0
0.40769835035349566
0.44501178318931656
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.6861,	 Acc1 = 0.3218,	 Acc2 = 0.3318

 ===== Epoch 5	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913  2.5359247   1.3487209
  3.4141016   3.563067  ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  1.2436514e-01 -4.3551436e-01
  1.8246561e-01 -4.8558870e-01] 5 5
train:	 Loss = 1.6519,	 Acc = 0.3754
2821 0.24
5489 0.386
2838 0.475
366 0.489
64 0.406
6 0.167
0 0.0
0 0.0
0.4190345771995892
0.44501178318931656
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.6375,	 Acc = 0.3930
365 0.337
1671 0.411
791 0.383
82 0.378
2 0.0
0 0.0
0 0.0
0 0.0
0.4010212097407698
0.44501178318931656
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.6662,	 Acc1 = 0.3414,	 Acc2 = 0.3554

 ===== Epoch 6	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.00014811 -0.00011583] 1 1
train:	 Loss = 1.6450,	 Acc = 0.3741
2823 0.242
5500 0.384
2827 0.475
364 0.459
64 0.422
6 0.167
0 0.0
0 0.0
0.4167332496290378
0.44501178318931656
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.6182,	 Acc = 0.3762
365 0.323
1671 0.382
791 0.391
82 0.354
2 0.0
0 0.0
0 0.0
0 0.0
0.38373919874312645
0.44501178318931656
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.6656,	 Acc1 = 0.3086,	 Acc2 = 0.3159

 ===== Epoch 7	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.00014811 -0.00011583] 4 1
train:	 Loss = 1.6431,	 Acc = 0.3744
2818 0.246
5497 0.383
2836 0.476
363 0.452
64 0.375
6 0.167
0 0.0
0 0.0
0.4156970111795574
0.44501178318931656
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.6232,	 Acc = 0.3614
365 0.34
1671 0.376
791 0.349
82 0.28
2 0.5
0 0.0
0 0.0
0 0.0
0.3644933228593873
0.44501178318931656
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.6784,	 Acc1 = 0.3070,	 Acc2 = 0.3139

 ===== Epoch 8	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247  1.8862274   2.1367307
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346  0.00035502  0.00176777  0.08641689  0.11348094
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.00014811 -0.00011583] 1 0
train:	 Loss = 1.6493,	 Acc = 0.3729
2821 0.239
5493 0.381
2837 0.475
363 0.488
64 0.437
6 0.167
0 0.0
0 0.0
0.416181672943056
0.44501178318931656
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 5
val:	 Loss = 1.6455,	 Acc = 0.3580
365 0.332
1671 0.376
791 0.338
82 0.305
2 0.0
0 0.0
0 0.0
0 0.0
0.3617439120188531
0.44501178318931656
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.6764,	 Acc1 = 0.3229,	 Acc2 = 0.3330

 ===== Epoch 9	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  1.6070484   2.1311274  -0.4012819  -0.40874913  2.6074154   2.1533225
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04  1.6591813e-01  1.7338851e-01
 -2.4058432e-03  2.0827476e-03  9.8368716e-01  1.6772634e-01
 -1.4811044e-04 -1.1583284e-04] 2 2
train:	 Loss = 1.6386,	 Acc = 0.3775
2826 0.251
5496 0.384
2828 0.48
364 0.481
64 0.312
6 0.167
0 0.0
0 0.0
0.41824617492578214
0.44501178318931656
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 0
val:	 Loss = 1.6285,	 Acc = 0.3717
365 0.345
1671 0.391
791 0.354
82 0.28
2 0.0
0 0.0
0 0.0
0 0.0
0.37549096622152395
0.44501178318931656
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.6735,	 Acc1 = 0.3200,	 Acc2 = 0.3295

 ===== Epoch 10	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.00014811 -0.00011583] 4 1
train:	 Loss = 1.6335,	 Acc = 0.3846
2823 0.252
5495 0.388
2834 0.496
363 0.501
63 0.381
6 0.167
0 0.0
0 0.0
0.42734847620134686
0.44501178318931656
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.6129,	 Acc = 0.3748
365 0.332
1671 0.382
791 0.387
82 0.305
2 0.5
0 0.0
0 0.0
0 0.0
0.3809897879025923
0.44501178318931656
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.6838,	 Acc1 = 0.3066,	 Acc2 = 0.3134

 ===== Epoch 11	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.00014811 -0.00011583] 1 1
train:	 Loss = 1.6343,	 Acc = 0.3857
2823 0.253
5492 0.392
2834 0.488
365 0.504
64 0.469
6 0.167
0 0.0
0 0.0
0.4284898984134231
0.44501178318931656
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.6064,	 Acc = 0.3957
365 0.337
1671 0.423
791 0.375
82 0.305
2 0.0
0 0.0
0 0.0
0 0.0
0.40416339355852315
0.44501178318931656
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.6614,	 Acc1 = 0.3340,	 Acc2 = 0.3464

 ===== Epoch 12	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04  4.5021558e+00  4.7385368e+00
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 1 5
train:	 Loss = 1.6272,	 Acc = 0.3830
2818 0.248
5496 0.395
2834 0.481
366 0.492
64 0.375
6 0.167
0 0.0
0 0.0
0.4265343372119553
0.44501178318931656
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 0
val:	 Loss = 1.6052,	 Acc = 0.3799
365 0.334
1671 0.387
791 0.396
82 0.293
2 0.0
0 0.0
0 0.0
0 0.0
0.38648860958366066
0.44501178318931656
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.6676,	 Acc1 = 0.3157,	 Acc2 = 0.3243

 ===== Epoch 13	 =====
[ 2.0041118   2.1394992  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  0.66781694  1.307755   -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 4.0553176e-01 -4.4595387e-02  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -2.3056979e+00 -4.1051583e+00
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 3 2
train:	 Loss = 1.6221,	 Acc = 0.3862
2815 0.246
5500 0.398
2835 0.487
365 0.51
64 0.344
5 0.0
0 0.0
0 0.0
0.4311780134564945
0.44501178318931656
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 5
val:	 Loss = 1.6328,	 Acc = 0.3813
365 0.353
1671 0.399
791 0.364
82 0.305
2 0.5
0 0.0
0 0.0
0 0.0
0.3853102906520031
0.44501178318931656
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.6618,	 Acc1 = 0.3338,	 Acc2 = 0.3462

 ===== Epoch 14	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913  1.2036986   1.7670152
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03 -1.6690258e-02  2.3307741e-01
 -1.4811044e-04 -1.1583284e-04] 2 6
train:	 Loss = 1.6166,	 Acc = 0.3902
2817 0.244
5500 0.407
2836 0.495
363 0.477
62 0.306
6 0.167
0 0.0
0 0.0
0.43732177483745865
0.44501178318931656
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 5
val:	 Loss = 1.6140,	 Acc = 0.3834
365 0.34
1671 0.394
791 0.392
82 0.268
2 0.5
0 0.0
0 0.0
0 0.0
0.389630793401414
0.44501178318931656
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.6424,	 Acc1 = 0.3410,	 Acc2 = 0.3549

 ===== Epoch 15	 =====
[-0.36678174 -0.3791378   1.6306622   3.0508351  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346  0.04927219  0.01951826 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.00014811 -0.00011583] 5 0
train:	 Loss = 1.5970,	 Acc = 0.4005
2823 0.25
5491 0.421
2837 0.501
364 0.5
63 0.349
6 0.0
0 0.0
0 0.0
0.4490354982307956
0.44501178318931656
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.6060,	 Acc = 0.3872
365 0.337
1671 0.401
791 0.393
82 0.28
2 0.0
0 0.0
0 0.0
0 0.0
0.394344069128044
0.44501178318931656
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.6333,	 Acc1 = 0.3478,	 Acc2 = 0.3631

 ===== Epoch 16	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  5.6433706e+00  3.8510888e+00
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 2 1
train:	 Loss = 1.5890,	 Acc = 0.4005
2819 0.255
5493 0.415
2841 0.506
363 0.493
62 0.387
6 0.0
0 0.0
0 0.0
0.4472333143183115
0.44501178318931656
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.5952,	 Acc = 0.4208
365 0.337
1671 0.445
791 0.424
82 0.293
2 0.0
0 0.0
0 0.0
0 0.0
0.43283582089552236
0.44501178318931656
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.6254,	 Acc1 = 0.3526,	 Acc2 = 0.3688

 ===== Epoch 17	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.00014811 -0.00011583] 5 1
train:	 Loss = 1.5874,	 Acc = 0.3961
2829 0.236
5487 0.421
2832 0.499
366 0.481
64 0.328
6 0.0
0 0.0
0 0.0
0.4477441462021702
0.44501178318931656
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.6048,	 Acc = 0.4047
365 0.345
1671 0.421
791 0.416
82 0.232
2 0.0
0 0.0
0 0.0
0 0.0
0.413197172034564
0.44501178318931656
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.6273,	 Acc1 = 0.3792,	 Acc2 = 0.4008

 ===== Epoch 18	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913  1.9500028   1.862977
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  4.8256901e-01  1.4259130e-01
 -1.4811044e-04 -1.1583284e-04] 2 2
train:	 Loss = 1.5789,	 Acc = 0.4032
2822 0.244
5495 0.424
2831 0.512
366 0.484
64 0.391
6 0.333
0 0.0
0 0.0
0.4545765806893403
0.44501178318931656
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.5672,	 Acc = 0.4212
365 0.345
1671 0.433
791 0.445
82 0.28
2 0.5
0 0.0
0 0.0
0 0.0
0.43205027494108406
0.44501178318931656
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.6134,	 Acc1 = 0.3606,	 Acc2 = 0.3785

 ===== Epoch 19	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.00014811 -0.00011583] 6 1
train:	 Loss = 1.5761,	 Acc = 0.4091
2818 0.25
5500 0.433
2832 0.51
365 0.501
63 0.349
6 0.0
0 0.0
0 0.0
0.46018708647045403
0.44501178318931656
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5954,	 Acc = 0.4098
365 0.337
1671 0.43
791 0.413
82 0.28
2 0.5
0 0.0
0 0.0
0 0.0
0.42026708562450904
0.44501178318931656
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.6249,	 Acc1 = 0.3710,	 Acc2 = 0.3909

 ===== Epoch 20	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  3.219086    2.1562474  -0.4012819  -0.40874913  3.0386293   3.1867557
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -3.1352203e-02 -3.8616624e-01
 -2.4058432e-03  2.0827476e-03 -7.7857304e-01 -2.6459616e-01
 -1.4811044e-04 -1.1583284e-04] 5 5
train:	 Loss = 1.5745,	 Acc = 0.4049
2824 0.248
5493 0.426
2832 0.51
366 0.511
63 0.286
6 0.167
0 0.0
0 0.0
0.45536529680365295
0.44501178318931656
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.5682,	 Acc = 0.4074
365 0.34
1671 0.406
791 0.453
82 0.293
2 0.5
0 0.0
0 0.0
0 0.0
0.4171249018067557
0.44501178318931656
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.6059,	 Acc1 = 0.3765,	 Acc2 = 0.3976

 ===== Epoch 21	 =====
[ 0.60713017  2.0058777  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
  0.46460068  2.2553277 ] [-1.2592822e-01  2.4635875e-01  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
  1.8144697e-01  3.7332487e-01] 2 6
train:	 Loss = 1.5755,	 Acc = 0.4080
2824 0.249
5498 0.431
2830 0.513
362 0.489
64 0.375
6 0.167
0 0.0
0 0.0
0.4591324200913242
0.44501178318931656
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5568,	 Acc = 0.4026
365 0.195
1671 0.437
791 0.436
82 0.305
2 0.0
0 0.0
0 0.0
0 0.0
0.43244304791830324
0.44501178318931656
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.6151,	 Acc1 = 0.3425,	 Acc2 = 0.3904

 ===== Epoch 22	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913  1.6288615   1.1789426
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03 -3.4891489e-01  2.1296938e-01
 -1.4811044e-04 -1.1583284e-04] 1 6
train:	 Loss = 1.5639,	 Acc = 0.4119
2825 0.25
5490 0.437
2834 0.513
365 0.515
64 0.375
6 0.333
0 0.0
0 0.0
0.4640940746660578
0.44501178318931656
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5493,	 Acc = 0.4126
365 0.334
1671 0.427
791 0.431
82 0.28
2 0.5
0 0.0
0 0.0
0 0.0
0.42380204241948155
0.44501178318931656
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.6119,	 Acc1 = 0.3710,	 Acc2 = 0.3909

 ===== Epoch 23	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  3.5580182   1.7682854  -0.4012819  -0.40874913  2.8990526   2.847199
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -7.9529113e-01  7.1247178e-01
 -2.4058432e-03  2.0827476e-03  1.0394841e-01  4.4923866e-01
 -1.4811044e-04 -1.1583284e-04] 6 6
train:	 Loss = 1.5654,	 Acc = 0.4126
2820 0.251
5495 0.437
2835 0.515
365 0.512
64 0.359
5 0.2
0 0.0
0 0.0
0.464628023733455
0.44501178318931656
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5682,	 Acc = 0.4205
365 0.353
1671 0.446
791 0.41
82 0.293
2 0.5
0 0.0
0 0.0
0 0.0
0.4300864100549882
0.44501178318931656
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.6250,	 Acc1 = 0.3668,	 Acc2 = 0.3859

 ===== Epoch 24	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  1.2934101   2.03623    -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04  8.8569466e-03 -5.2264303e-01
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 5 5
train:	 Loss = 1.5647,	 Acc = 0.4086
2817 0.244
5497 0.432
2835 0.516
365 0.501
64 0.391
6 0.167
0 0.0
0 0.0
0.4616174289950952
0.44501178318931656
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 5
val:	 Loss = 1.5709,	 Acc = 0.4167
365 0.337
1671 0.436
791 0.427
82 0.28
2 0.5
0 0.0
0 0.0
0 0.0
0.42812254516889237
0.44501178318931656
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5957,	 Acc1 = 0.3994,	 Acc2 = 0.4252

 ===== Epoch 25	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829   2.0832064   1.2160258  -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
  3.6558011e-01 -1.1289993e-01  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 6 5
train:	 Loss = 1.5595,	 Acc = 0.4162
2822 0.249
5494 0.444
2835 0.521
364 0.492
63 0.381
6 0.0
0 0.0
0 0.0
0.4698698927185574
0.44501178318931656
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5594,	 Acc = 0.3954
365 0.348
1671 0.412
791 0.393
82 0.293
2 0.5
0 0.0
0 0.0
0 0.0
0.40219952867242736
0.44501178318931656
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.6095,	 Acc1 = 0.3592,	 Acc2 = 0.3767

 ===== Epoch 26	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913  1.4249803   1.5923159
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03 -2.6725078e-01  1.5767232e-01
 -1.4811044e-04 -1.1583284e-04] 2 6
train:	 Loss = 1.5648,	 Acc = 0.4110
2822 0.241
5494 0.44
2835 0.517
363 0.493
64 0.297
6 0.167
0 0.0
0 0.0
0.4656471125313855
0.44501178318931656
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 5
val:	 Loss = 1.5470,	 Acc = 0.4414
365 0.334
1671 0.454
791 0.48
82 0.28
2 0.5
0 0.0
0 0.0
0 0.0
0.45679497250589157
0.45679497250589157
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5825,	 Acc1 = 0.3980,	 Acc2 = 0.4235

 ===== Epoch 27	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913  1.6795479   2.1459408
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  3.2481188e-01 -4.5562238e-01
 -1.4811044e-04 -1.1583284e-04] 5 5
train:	 Loss = 1.5585,	 Acc = 0.4093
2823 0.248
5500 0.431
2832 0.519
360 0.508
63 0.302
6 0.0
0 0.0
0 0.0
0.46124871590001143
0.45679497250589157
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5738,	 Acc = 0.4098
365 0.342
1671 0.43
791 0.41
82 0.293
2 0.5
0 0.0
0 0.0
0 0.0
0.4194815396700707
0.45679497250589157
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.6148,	 Acc1 = 0.3658,	 Acc2 = 0.3847

 ===== Epoch 28	 =====
[ 2.2967072   2.1142876  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
  2.1847017   2.357741  ] [-0.02020486  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
  0.0008737  -0.00011583] 0 0
train:	 Loss = 1.5613,	 Acc = 0.4123
2820 0.244
5497 0.442
2833 0.513
364 0.505
64 0.359
6 0.0
0 0.0
0 0.0
0.46645367412140576
0.45679497250589157
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 5
val:	 Loss = 1.5368,	 Acc = 0.4346
365 0.351
1671 0.446
791 0.466
82 0.268
2 0.5
0 0.0
0 0.0
0 0.0
0.4465828750981932
0.45679497250589157
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5923,	 Acc1 = 0.3734,	 Acc2 = 0.3939

 ===== Epoch 29	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.00014811 -0.00011583] 1 1
train:	 Loss = 1.5606,	 Acc = 0.4110
2818 0.241
5493 0.437
2837 0.52
366 0.497
64 0.375
6 0.167
0 0.0
0 0.0
0.4657768651608487
0.45679497250589157
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 5
val:	 Loss = 1.5852,	 Acc = 0.3978
365 0.332
1671 0.415
791 0.408
82 0.244
2 0.5
0 0.0
0 0.0
0 0.0
0.40730557737627654
0.45679497250589157
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.6068,	 Acc1 = 0.3858,	 Acc2 = 0.4088

 ===== Epoch 30	 =====
[-0.36678174 -0.3791378   0.6458243   2.2841938  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -1.2703690e-01 -2.5181070e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 5 5
train:	 Loss = 1.5543,	 Acc = 0.4104
2821 0.246
5493 0.437
2836 0.514
365 0.488
63 0.349
6 0.167
0 0.0
0 0.0
0.46319753509072237
0.45679497250589157
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.5202,	 Acc = 0.4452
365 0.345
1671 0.452
791 0.489
82 0.317
2 0.5
0 0.0
0 0.0
0 0.0
0.4595443833464258
0.4595443833464258
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5836,	 Acc1 = 0.3891,	 Acc2 = 0.4128

 ===== Epoch 31	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.00014811 -0.00011583] 1 1
train:	 Loss = 1.5545,	 Acc = 0.4140
2814 0.244
5500 0.441
2835 0.521
365 0.493
64 0.312
6 0.333
0 0.0
0 0.0
0.46841505131128847
0.4595443833464258
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5673,	 Acc = 0.4047
365 0.189
1671 0.445
791 0.437
82 0.244
2 0.0
0 0.0
0 0.0
0 0.0
0.43558523173605657
0.4595443833464258
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.6137,	 Acc1 = 0.3450,	 Acc2 = 0.3934

 ===== Epoch 32	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.00014811 -0.00011583] 2 6
train:	 Loss = 1.5529,	 Acc = 0.4139
2822 0.241
5500 0.441
2829 0.52
364 0.538
64 0.344
5 0.4
0 0.0
0 0.0
0.4696416343300616
0.4595443833464258
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 5
val:	 Loss = 1.5464,	 Acc = 0.4150
365 0.345
1671 0.422
791 0.448
82 0.268
2 0.5
0 0.0
0 0.0
0 0.0
0.42498036135113904
0.4595443833464258
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.6047,	 Acc1 = 0.3703,	 Acc2 = 0.3902

 ===== Epoch 33	 =====
[-0.36678174 -0.3791378   1.6270019   2.2274895  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.00014811 -0.00011583] 0 0
train:	 Loss = 1.5549,	 Acc = 0.4127
2816 0.246
5501 0.439
2832 0.516
365 0.512
64 0.391
6 0.333
0 0.0
0 0.0
0.46624087591240876
0.4595443833464258
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.5199,	 Acc = 0.4359
365 0.34
1671 0.45
791 0.461
82 0.329
2 0.5
0 0.0
0 0.0
0 0.0
0.4497250589159466
0.4595443833464258
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5824,	 Acc1 = 0.3846,	 Acc2 = 0.4073

 ===== Epoch 34	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247  3.0091517   1.4718924
 -0.36420733 -0.3724829   2.461722    0.81294227 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346  0.00035502  0.00176777  0.01280009  0.06316236
  0.00131773  0.00279255 -0.01840585  0.02603747  0.00094095  0.00183513
 -0.00014811 -0.00011583] 1 3
train:	 Loss = 1.5557,	 Acc = 0.4132
2824 0.25
5492 0.438
2834 0.518
365 0.49
63 0.349
6 0.167
0 0.0
0 0.0
0.4656392694063927
0.4595443833464258
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5690,	 Acc = 0.3858
365 0.323
1671 0.391
791 0.41
82 0.317
2 0.5
0 0.0
0 0.0
0 0.0
0.39473684210526316
0.4595443833464258
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.6140,	 Acc1 = 0.3691,	 Acc2 = 0.3887

 ===== Epoch 35	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.00014811 -0.00011583] 0 1
train:	 Loss = 1.5555,	 Acc = 0.4119
2824 0.248
5495 0.441
2832 0.508
363 0.504
64 0.359
6 0.0
0 0.0
0 0.0
0.46472602739726027
0.4595443833464258
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 5
val:	 Loss = 1.5466,	 Acc = 0.4249
365 0.34
1671 0.433
791 0.458
82 0.329
2 0.5
0 0.0
0 0.0
0 0.0
0.43715632364493323
0.4595443833464258
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5902,	 Acc1 = 0.3914,	 Acc2 = 0.4155

 ===== Epoch 36	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913  1.3186895   1.1715609
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.05476404 -0.108759
 -0.00014811 -0.00011583] 5 5
train:	 Loss = 1.5484,	 Acc = 0.4133
2820 0.242
5494 0.436
2836 0.528
364 0.516
64 0.391
6 0.167
0 0.0
0 0.0
0.4685075308078503
0.4595443833464258
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.5439,	 Acc = 0.4397
365 0.348
1671 0.449
791 0.478
82 0.293
2 0.5
0 0.0
0 0.0
0 0.0
0.45286724273369994
0.4595443833464258
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5837,	 Acc1 = 0.3988,	 Acc2 = 0.4245

 ===== Epoch 37	 =====
[ 2.3573527   3.4303322  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913  2.2178094   1.5726314
 -0.38645998 -0.3811587 ] [-3.0237541e+00 -4.6750998e+00  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  1.0221289e-02 -7.3569953e-02
 -1.4811044e-04 -1.1583284e-04] 2 3
train:	 Loss = 1.5581,	 Acc = 0.4130
2815 0.25
5496 0.436
2837 0.52
366 0.508
64 0.359
6 0.167
0 0.0
0 0.0
0.46527540198426276
0.4595443833464258
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5768,	 Acc = 0.4098
365 0.334
1671 0.417
791 0.436
82 0.341
2 0.5
0 0.0
0 0.0
0 0.0
0.4206598586017282
0.4595443833464258
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.6099,	 Acc1 = 0.3786,	 Acc2 = 0.4001

 ===== Epoch 38	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.00014811 -0.00011583] 5 1
train:	 Loss = 1.5461,	 Acc = 0.4154
2827 0.244
5489 0.442
2835 0.524
364 0.516
63 0.365
6 0.333
0 0.0
0 0.0
0.47070914696813976
0.4595443833464258
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5460,	 Acc = 0.4150
365 0.342
1671 0.428
791 0.431
82 0.305
2 0.5
0 0.0
0 0.0
0 0.0
0.4253731343283582
0.4595443833464258
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5874,	 Acc1 = 0.3848,	 Acc2 = 0.4076

 ===== Epoch 39	 =====
[ 1.0276322   2.7244081  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [-2.0131488e-01  3.8976546e-02  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 0 0
train:	 Loss = 1.5517,	 Acc = 0.4170
2822 0.245
5494 0.444
2833 0.522
365 0.529
64 0.375
6 0.167
0 0.0
0 0.0
0.4722666057977631
0.4595443833464258
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5329,	 Acc = 0.4184
365 0.332
1671 0.426
791 0.455
82 0.293
2 0.5
0 0.0
0 0.0
0 0.0
0.43087195600942657
0.4595443833464258
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5866,	 Acc1 = 0.3809,	 Acc2 = 0.4028

 ===== Epoch 40	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  3.190983    1.9971551  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -5.1384264e-01  2.9621762e-01
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 2 6
train:	 Loss = 1.5458,	 Acc = 0.4169
2826 0.246
5494 0.446
2829 0.519
365 0.523
64 0.391
6 0.0
0 0.0
0 0.0
0.4720255766156657
0.4595443833464258
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.5434,	 Acc = 0.4284
365 0.334
1671 0.438
791 0.46
82 0.341
2 0.5
0 0.0
0 0.0
0 0.0
0.4418695993715632
0.4595443833464258
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5817,	 Acc1 = 0.3914,	 Acc2 = 0.4155

 ===== Epoch 41	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.00014811 -0.00011583] 3 1
train:	 Loss = 1.5411,	 Acc = 0.4198
2824 0.246
5491 0.446
2836 0.528
363 0.543
64 0.375
6 0.0
0 0.0
0 0.0
0.4757990867579909
0.4595443833464258
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5333,	 Acc = 0.4301
365 0.337
1671 0.442
791 0.456
82 0.341
2 0.5
0 0.0
0 0.0
0 0.0
0.4434406912804399
0.4595443833464258
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5783,	 Acc1 = 0.3862,	 Acc2 = 0.4093

 ===== Epoch 42	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.00014811 -0.00011583] 3 1
train:	 Loss = 1.5410,	 Acc = 0.4221
2819 0.247
5495 0.451
2835 0.527
366 0.544
63 0.349
6 0.167
0 0.0
0 0.0
0.47837992013690817
0.4595443833464258
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5889,	 Acc = 0.4184
365 0.348
1671 0.446
791 0.413
82 0.22
2 0.5
0 0.0
0 0.0
0 0.0
0.42851531814611155
0.4595443833464258
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.6264,	 Acc1 = 0.3842,	 Acc2 = 0.4068

 ===== Epoch 43	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913  1.9643761   0.8393859
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  2.4129167e-01  3.1997167e-02
 -1.4811044e-04 -1.1583284e-04] 4 2
train:	 Loss = 1.5467,	 Acc = 0.4186
2819 0.247
5498 0.445
2832 0.525
365 0.526
64 0.375
6 0.333
0 0.0
0 0.0
0.47370222475755847
0.4595443833464258
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.5307,	 Acc = 0.4421
365 0.34
1671 0.445
791 0.491
82 0.378
2 0.5
0 0.0
0 0.0
0 0.0
0.45679497250589157
0.4595443833464258
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5888,	 Acc1 = 0.3788,	 Acc2 = 0.4003

 ===== Epoch 44	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.330442    3.2475646  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -3.6306214e-01 -1.3368422e-01
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 5 3
train:	 Loss = 1.5365,	 Acc = 0.4224
2819 0.247
5500 0.447
2831 0.54
365 0.493
63 0.397
6 0.333
0 0.0
0 0.0
0.4787221905305191
0.4595443833464258
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.5497,	 Acc = 0.4270
365 0.337
1671 0.437
791 0.46
82 0.293
2 0.5
0 0.0
0 0.0
0 0.0
0.4399057344854674
0.4595443833464258
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5761,	 Acc1 = 0.3978,	 Acc2 = 0.4232

 ===== Epoch 45	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
  1.5865213   1.3677456 ] [ 0.00073671  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.00218858  0.06923743] 1 4
train:	 Loss = 1.5387,	 Acc = 0.4206
2823 0.239
5499 0.448
2829 0.538
363 0.518
64 0.375
6 0.167
0 0.0
0 0.0
0.4789407601871932
0.4595443833464258
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5408,	 Acc = 0.4332
365 0.34
1671 0.455
791 0.442
82 0.317
2 0.5
0 0.0
0 0.0
0 0.0
0.4465828750981932
0.4595443833464258
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5937,	 Acc1 = 0.3846,	 Acc2 = 0.4073

 ===== Epoch 46	 =====
[ 1.6905895   1.7840159  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
  1.381695    2.2894657 ] [ 1.2096537e+00  3.2064492e-01  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -2.7662009e-01  6.1339390e-01] 6 6
train:	 Loss = 1.5380,	 Acc = 0.4217
2814 0.242
5499 0.449
2838 0.535
363 0.529
64 0.328
6 0.167
0 0.0
0 0.0
0.4792474344355758
0.4595443833464258
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.5535,	 Acc = 0.4325
365 0.337
1671 0.448
791 0.456
82 0.317
2 0.5
0 0.0
0 0.0
0 0.0
0.4461901021209741
0.4595443833464258
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5884,	 Acc1 = 0.3936,	 Acc2 = 0.4182

 ===== Epoch 47	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.00014811 -0.00011583] 0 1
train:	 Loss = 1.5443,	 Acc = 0.4191
2821 0.251
5495 0.446
2833 0.525
365 0.504
64 0.406
6 0.167
0 0.0
0 0.0
0.4733538742439804
0.4595443833464258
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.5432,	 Acc = 0.4297
365 0.334
1671 0.435
791 0.472
82 0.341
2 0.5
0 0.0
0 0.0
0 0.0
0.4434406912804399
0.4595443833464258
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5903,	 Acc1 = 0.3930,	 Acc2 = 0.4175

 ===== Epoch 48	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
  2.0525553   2.6045833 ] [ 3.2633584e+00  3.3447111e+00  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
  3.9466766e-01 -5.4506999e-03] 3 0
train:	 Loss = 1.5406,	 Acc = 0.4181
2824 0.248
5501 0.442
2829 0.53
361 0.524
63 0.365
6 0.167
0 0.0
0 0.0
0.4730593607305936
0.4595443833464258
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.5942,	 Acc = 0.4256
365 0.323
1671 0.455
791 0.426
82 0.268
2 0.5
0 0.0
0 0.0
0 0.0
0.44029850746268656
0.4595443833464258
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.6296,	 Acc1 = 0.3718,	 Acc2 = 0.3919

 ===== Epoch 49	 =====
[-0.36678174 -0.3791378   1.6038241   2.0142815  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  6.9658801e-02 -2.2898862e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 5 5
train:	 Loss = 1.5391,	 Acc = 0.4197
2827 0.233
5496 0.448
2826 0.539
365 0.54
64 0.328
6 0.0
0 0.0
0 0.0
0.4799588900308325
0.4595443833464258
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.5516,	 Acc = 0.4212
365 0.334
1671 0.43
791 0.456
82 0.293
2 0.5
0 0.0
0 0.0
0 0.0
0.4336213668499607
0.4595443833464258
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5947,	 Acc1 = 0.3864,	 Acc2 = 0.4095

 ===== Epoch 50	 =====
[ 1.7232237   1.0806128  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
  1.6931181   0.96334434] [-2.3197074e+00 -1.7903203e+00  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.6337584e-01  1.0124663e-01] 4 4
train:	 Loss = 1.5371,	 Acc = 0.4201
2818 0.244
5490 0.445
2843 0.535
364 0.525
63 0.333
6 0.333
0 0.0
0 0.0
0.47672826830937715
0.4595443833464258
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5517,	 Acc = 0.4157
365 0.337
1671 0.437
791 0.421
82 0.28
2 0.5
0 0.0
0 0.0
0 0.0
0.4269442262372349
0.4595443833464258
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.6010,	 Acc1 = 0.3802,	 Acc2 = 0.4021

 ===== Epoch 51	 =====
[-0.36678174 -0.3791378   2.3979568  -4.1438      2.259388    0.89377207
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  2.7552232e-01  7.8905935e+00
  9.8518543e-02 -1.7347362e-02  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 2 2
train:	 Loss = 1.5354,	 Acc = 0.4190
2819 0.237
5497 0.45
2832 0.528
366 0.514
64 0.422
6 0.167
0 0.0
0 0.0
0.4775812892184826
0.4595443833464258
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.5318,	 Acc = 0.4335
365 0.334
1671 0.454
791 0.455
82 0.244
2 0.5
0 0.0
0 0.0
0 0.0
0.44776119402985076
0.4595443833464258
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5750,	 Acc1 = 0.3947,	 Acc2 = 0.4195

 ===== Epoch 52	 =====
[ 2.0330188   2.197486   -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [-2.6636600e+00 -3.1615188e+00  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 3 1
train:	 Loss = 1.5401,	 Acc = 0.4192
2818 0.244
5500 0.449
2831 0.521
365 0.545
64 0.391
6 0.0
0 0.0
0 0.0
0.4753593429158111
0.4595443833464258
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.5214,	 Acc = 0.4260
365 0.337
1671 0.436
791 0.455
82 0.341
2 0.5
0 0.0
0 0.0
0 0.0
0.4387274155538099
0.4595443833464258
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5740,	 Acc1 = 0.3881,	 Acc2 = 0.4115

 ===== Epoch 53	 =====
[ 2.2951572   2.1142876  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
  2.1847017   2.357741  ] [ 0.00073671  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.00014811 -0.00011583] 0 0
train:	 Loss = 1.5363,	 Acc = 0.4202
2822 0.246
5496 0.449
2833 0.524
363 0.534
64 0.422
6 0.167
0 0.0
0 0.0
0.4762611275964392
0.4595443833464258
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.5205,	 Acc = 0.4414
365 0.334
1671 0.457
791 0.472
82 0.317
2 0.5
0 0.0
0 0.0
0 0.0
0.45679497250589157
0.4595443833464258
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5720,	 Acc1 = 0.3924,	 Acc2 = 0.4167

 ===== Epoch 54	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247  2.623373    2.9661112
 -0.36420733 -0.3724829   1.9340953   2.303356   -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
  6.6727114e-01  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 0 0
train:	 Loss = 1.5338,	 Acc = 0.4233
2826 0.247
5493 0.453
2831 0.526
364 0.547
64 0.375
6 0.333
0 0.0
0 0.0
0.4802466316510619
0.4595443833464258
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5587,	 Acc = 0.4177
365 0.329
1671 0.446
791 0.416
82 0.244
2 0.5
0 0.0
0 0.0
0 0.0
0.4304791830322074
0.4595443833464258
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5912,	 Acc1 = 0.3893,	 Acc2 = 0.4130

 ===== Epoch 55	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913  1.4208189   2.4018385
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  2.8861997e-01  2.2302340e-01
 -1.4811044e-04 -1.1583284e-04] 4 6
train:	 Loss = 1.5373,	 Acc = 0.4176
2817 0.237
5500 0.447
2831 0.528
366 0.522
64 0.406
6 0.167
0 0.0
0 0.0
0.4757613778943766
0.4595443833464258
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.5014,	 Acc = 0.4644
365 0.334
1671 0.483
791 0.497
82 0.354
2 0.5
0 0.0
0 0.0
0 0.0
0.4831107619795758
0.4831107619795758
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5785,	 Acc1 = 0.3910,	 Acc2 = 0.4150

 ===== Epoch 56	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247  1.3062216   0.78926563
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346  0.00035502  0.00176777 -0.04871196 -0.0123155
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.00014811 -0.00011583] 1 0
train:	 Loss = 1.5341,	 Acc = 0.4218
2823 0.244
5493 0.45
2834 0.532
364 0.53
64 0.359
6 0.167
0 0.0
0 0.0
0.4790549024084009
0.4831107619795758
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.5282,	 Acc = 0.4586
365 0.334
1671 0.484
791 0.475
82 0.329
2 0.5
0 0.0
0 0.0
0 0.0
0.47643362136684997
0.4831107619795758
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5779,	 Acc1 = 0.3860,	 Acc2 = 0.4090

 ===== Epoch 57	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.00014811 -0.00011583] 1 1
train:	 Loss = 1.5328,	 Acc = 0.4202
2818 0.248
5499 0.45
2833 0.52
364 0.536
64 0.359
6 0.167
0 0.0
0 0.0
0.4755874971480721
0.4831107619795758
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.5190,	 Acc = 0.4397
365 0.345
1671 0.46
791 0.453
82 0.317
2 0.5
0 0.0
0 0.0
0 0.0
0.45326001571091906
0.4831107619795758
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5754,	 Acc1 = 0.3914,	 Acc2 = 0.4155

 ===== Epoch 58	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247  1.8873739   1.7720703
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -7.2915256e-02 -1.6075532e-01  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 5 5
train:	 Loss = 1.5335,	 Acc = 0.4247
2824 0.25
5488 0.452
2836 0.532
366 0.538
64 0.391
6 0.333
0 0.0
0 0.0
0.48093607305936076
0.4831107619795758
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.5291,	 Acc = 0.4394
365 0.34
1671 0.464
791 0.445
82 0.329
2 0.5
0 0.0
0 0.0
0 0.0
0.45365278868813824
0.4831107619795758
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5720,	 Acc1 = 0.3949,	 Acc2 = 0.4197

 ===== Epoch 59	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913  1.1083783   1.0583754
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  1.3003956e-02  4.4421166e-01
 -1.4811044e-04 -1.1583284e-04] 6 6
train:	 Loss = 1.5379,	 Acc = 0.4211
2821 0.239
5488 0.448
2839 0.537
366 0.536
64 0.344
6 0.333
0 0.0
0 0.0
0.479858495948876
0.4831107619795758
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.5253,	 Acc = 0.4414
365 0.326
1671 0.464
791 0.458
82 0.329
2 0.5
0 0.0
0 0.0
0 0.0
0.4579732914375491
0.4831107619795758
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5728,	 Acc1 = 0.3864,	 Acc2 = 0.4095

 ===== Epoch 60	 =====
[-0.36678174 -0.3791378   0.45877787  1.3383669  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  8.0864981e-02  2.1477367e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 6 6
train:	 Loss = 1.5313,	 Acc = 0.4229
2821 0.241
5495 0.454
2832 0.53
366 0.541
64 0.375
6 0.0
0 0.0
0 0.0
0.48145612233253454
0.4831107619795758
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5708,	 Acc = 0.4212
365 0.34
1671 0.449
791 0.421
82 0.22
2 0.5
0 0.0
0 0.0
0 0.0
0.43283582089552236
0.4831107619795758
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.6030,	 Acc1 = 0.3901,	 Acc2 = 0.4140

 ===== Epoch 61	 =====
[ 1.8719699   1.0100204  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
  1.8565394   1.1996827 ] [ 0.07650346  0.01421449  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
  0.10085018  0.11725123] 1 4
train:	 Loss = 1.5355,	 Acc = 0.4235
2822 0.251
5501 0.454
2828 0.523
363 0.545
64 0.344
6 0.167
0 0.0
0 0.0
0.47911435745263636
0.4831107619795758
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5278,	 Acc = 0.4401
365 0.342
1671 0.464
791 0.449
82 0.305
2 0.5
0 0.0
0 0.0
0 0.0
0.4540455616653574
0.4831107619795758
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5960,	 Acc1 = 0.3749,	 Acc2 = 0.3956

 ===== Epoch 62	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913  1.6428571   1.1469553
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03 -3.5250936e-02  1.6269933e-01
 -1.4811044e-04 -1.1583284e-04] 2 6
train:	 Loss = 1.5302,	 Acc = 0.4258
2819 0.244
5496 0.452
2835 0.539
364 0.563
64 0.406
6 0.167
0 0.0
0 0.0
0.484312606959498
0.4831107619795758
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5331,	 Acc = 0.4325
365 0.337
1671 0.455
791 0.442
82 0.305
2 0.5
0 0.0
0 0.0
0 0.0
0.4461901021209741
0.4831107619795758
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5835,	 Acc1 = 0.3901,	 Acc2 = 0.4140

 ===== Epoch 63	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247  3.093649    1.6297637
 -0.36420733 -0.3724829   2.835649    0.9348626  -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -7.7578980e-01  5.5614576e-02  1.3177313e-03  2.7925514e-03
 -2.7782211e-01  1.0269259e-01  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 3 2
train:	 Loss = 1.5273,	 Acc = 0.4236
2823 0.249
5500 0.452
2829 0.528
364 0.538
62 0.403
6 0.333
0 0.0
0 0.0
0.47985389795685424
0.4831107619795758
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.5204,	 Acc = 0.4517
365 0.337
1671 0.478
791 0.465
82 0.293
2 0.5
0 0.0
0 0.0
0 0.0
0.46818538884524746
0.4831107619795758
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5725,	 Acc1 = 0.3928,	 Acc2 = 0.4172

 ===== Epoch 64	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829   2.3820055   2.1814358  -0.42398092 -0.42041838
  3.8105388   1.4281433 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -3.5667750e-01 -5.1054835e-01  9.4095006e-04  1.8351302e-03
 -9.1933817e-01 -4.5357952e-01] 5 5
train:	 Loss = 1.5300,	 Acc = 0.4276
2819 0.25
5493 0.458
2837 0.535
366 0.514
63 0.365
6 0.333
0 0.0
0 0.0
0.48465487735310897
0.4831107619795758
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5361,	 Acc = 0.4256
365 0.34
1671 0.449
791 0.431
82 0.28
2 0.5
0 0.0
0 0.0
0 0.0
0.43794186959937154
0.4831107619795758
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5825,	 Acc1 = 0.3780,	 Acc2 = 0.3994

 ===== Epoch 65	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.00014811 -0.00011583] 6 1
train:	 Loss = 1.5316,	 Acc = 0.4213
2825 0.248
5495 0.449
2833 0.528
362 0.522
64 0.359
5 0.0
0 0.0
0 0.0
0.4769950907637858
0.4831107619795758
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5329,	 Acc = 0.4349
365 0.334
1671 0.473
791 0.424
82 0.22
2 0.5
0 0.0
0 0.0
0 0.0
0.4493322859387274
0.4831107619795758
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5812,	 Acc1 = 0.3891,	 Acc2 = 0.4128

 ===== Epoch 66	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247  1.6342665   1.029408
 -0.36420733 -0.3724829   5.336717    0.9448153  -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
  2.4373209e-01 -1.7347362e-02  1.3177313e-03  2.7925514e-03
 -1.3834417e-02  6.9155976e-02  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 3 2
train:	 Loss = 1.5312,	 Acc = 0.4246
2822 0.25
5496 0.453
2831 0.53
365 0.545
64 0.391
6 0.167
0 0.0
0 0.0
0.4809404245606026
0.4831107619795758
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 5
val:	 Loss = 1.5606,	 Acc = 0.4431
365 0.345
1671 0.482
791 0.425
82 0.256
2 0.5
0 0.0
0 0.0
0 0.0
0.45718774548311075
0.4831107619795758
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.6117,	 Acc1 = 0.3771,	 Acc2 = 0.3984

 ===== Epoch 67	 =====
[ 2.294617    2.1142876  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
  2.1847017   2.357741  ] [ 0.05022888  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.00014811 -0.00011583] 0 0
train:	 Loss = 1.5304,	 Acc = 0.4234
2817 0.248
5505 0.45
2831 0.532
361 0.546
64 0.391
6 0.333
0 0.0
0 0.0
0.4797536215353028
0.4831107619795758
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5430,	 Acc = 0.4287
365 0.337
1671 0.456
791 0.429
82 0.28
2 0.5
0 0.0
0 0.0
0 0.0
0.4418695993715632
0.4831107619795758
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5796,	 Acc1 = 0.3953,	 Acc2 = 0.4202

 ===== Epoch 68	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.00014811 -0.00011583] 5 1
train:	 Loss = 1.5302,	 Acc = 0.4223
2822 0.251
5498 0.448
2829 0.528
365 0.556
64 0.391
6 0.333
0 0.0
0 0.0
0.4776306779274138
0.4831107619795758
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.5074,	 Acc = 0.4531
365 0.337
1671 0.474
791 0.475
82 0.329
2 0.5
0 0.0
0 0.0
0 0.0
0.46975648075412413
0.4831107619795758
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5533,	 Acc1 = 0.4062,	 Acc2 = 0.4334

 ===== Epoch 69	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.00014811 -0.00011583] 2 1
train:	 Loss = 1.5280,	 Acc = 0.4287
2829 0.252
5488 0.454
2834 0.541
363 0.54
64 0.437
6 0.167
0 0.0
0 0.0
0.4857795545402627
0.4831107619795758
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5621,	 Acc = 0.4318
365 0.326
1671 0.474
791 0.413
82 0.22
2 0.5
0 0.0
0 0.0
0 0.0
0.4469756480754124
0.4831107619795758
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.6052,	 Acc1 = 0.3844,	 Acc2 = 0.4071

 ===== Epoch 70	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913  1.3981234   1.9835441
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00836522 -0.00319188
 -0.00014811 -0.00011583] 4 0
train:	 Loss = 1.5278,	 Acc = 0.4271
2817 0.25
5499 0.454
2836 0.539
362 0.528
64 0.375
6 0.333
0 0.0
0 0.0
0.4840880574883084
0.4831107619795758
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5276,	 Acc = 0.4273
365 0.326
1671 0.454
791 0.432
82 0.293
2 0.5
0 0.0
0 0.0
0 0.0
0.4418695993715632
0.4831107619795758
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5768,	 Acc1 = 0.3831,	 Acc2 = 0.4056

 ===== Epoch 71	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247  1.4006586   2.2412372
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
  2.3465432e-01  3.2971218e-02  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 1 2
train:	 Loss = 1.5315,	 Acc = 0.4214
2821 0.246
5494 0.45
2834 0.527
365 0.529
64 0.391
6 0.167
0 0.0
0 0.0
0.4778044048841721
0.4831107619795758
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.5029,	 Acc = 0.4665
365 0.337
1671 0.484
791 0.499
82 0.366
2 0.5
0 0.0
0 0.0
0 0.0
0.48507462686567165
0.48507462686567165
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5793,	 Acc1 = 0.3872,	 Acc2 = 0.4105

 ===== Epoch 72	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.7177129   1.8743469  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -4.0578049e-01  4.1222286e-01
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 6 6
train:	 Loss = 1.5322,	 Acc = 0.4232
2818 0.252
5501 0.45
2832 0.529
363 0.521
64 0.391
6 0.333
0 0.0
0 0.0
0.4782112708190737
0.48507462686567165
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.5127,	 Acc = 0.4572
365 0.329
1671 0.483
791 0.477
82 0.317
2 0.5
0 0.0
0 0.0
0 0.0
0.4756480754124116
0.48507462686567165
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5589,	 Acc1 = 0.4002,	 Acc2 = 0.4262

 ===== Epoch 73	 =====
[ 1.7751826   1.3806306  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [-0.01209993  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.00014811 -0.00011583] 0 0
train:	 Loss = 1.5294,	 Acc = 0.4267
2816 0.251
5497 0.458
2835 0.527
366 0.541
64 0.375
6 0.333
0 0.0
0 0.0
0.4832344890510949
0.48507462686567165
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.5376,	 Acc = 0.4366
365 0.332
1671 0.459
791 0.453
82 0.293
2 0.5
0 0.0
0 0.0
0 0.0
0.4516889238020424
0.48507462686567165
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5797,	 Acc1 = 0.3953,	 Acc2 = 0.4202

 ===== Epoch 74	 =====
[-0.36678174 -0.3791378   1.6021988   1.9190184  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  4.9272187e-02 -1.9348764e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 5 5
train:	 Loss = 1.5283,	 Acc = 0.4227
2819 0.248
5498 0.451
2836 0.531
362 0.536
63 0.333
6 0.0
0 0.0
0 0.0
0.4790644609241301
0.48507462686567165
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.5037,	 Acc = 0.4562
365 0.332
1671 0.478
791 0.482
82 0.329
2 0.5
0 0.0
0 0.0
0 0.0
0.47407698350353494
0.48507462686567165
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5627,	 Acc1 = 0.3920,	 Acc2 = 0.4163

 ===== Epoch 75	 =====
[ 1.9382267   0.9469914  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
  1.8138125   1.1497891 ] [-1.9825049e-01  6.0643338e-02  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
  5.0382465e-01  8.5242040e-02] 1 4
train:	 Loss = 1.5348,	 Acc = 0.4217
2822 0.242
5497 0.452
2832 0.526
364 0.552
63 0.381
6 0.167
0 0.0
0 0.0
0.4796850034238758
0.48507462686567165
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5369,	 Acc = 0.4352
365 0.345
1671 0.463
791 0.436
82 0.268
2 0.5
0 0.0
0 0.0
0 0.0
0.4481539670070699
0.48507462686567165
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5778,	 Acc1 = 0.3934,	 Acc2 = 0.4180

 ===== Epoch 76	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  1.0836860e+01 -5.1382675e+00
  1.1532167e+01  1.3538338e+00  9.0594158e+00  8.9556694e+00
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 5 2
train:	 Loss = 1.5279,	 Acc = 0.4228
2820 0.246
5498 0.454
2833 0.523
363 0.548
64 0.406
6 0.333
0 0.0
0 0.0
0.47968963943404835
0.48507462686567165
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5208,	 Acc = 0.4373
365 0.337
1671 0.47
791 0.436
82 0.232
2 0.5
0 0.0
0 0.0
0 0.0
0.4516889238020424
0.48507462686567165
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5759,	 Acc1 = 0.3850,	 Acc2 = 0.4078

 ===== Epoch 77	 =====
[ 1.5500607   2.6563365  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
  1.2380956   3.0247405 ] [-0.23125666 -0.08483373  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.07462035 -0.25085455] 3 5
train:	 Loss = 1.5273,	 Acc = 0.4238
2827 0.242
5491 0.455
2834 0.532
362 0.536
64 0.375
6 0.167
0 0.0
0 0.0
0.48235697156560464
0.48507462686567165
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.4963,	 Acc = 0.4651
365 0.345
1671 0.484
791 0.491
82 0.366
2 0.5
0 0.0
0 0.0
0 0.0
0.48232521602513745
0.48507462686567165
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5652,	 Acc1 = 0.3872,	 Acc2 = 0.4105

 ===== Epoch 78	 =====
[-0.36678174 -0.3791378   1.7274373   0.85751486 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -5.4181993e-01  7.5305521e-02
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 3 4
train:	 Loss = 1.5282,	 Acc = 0.4243
2821 0.248
5505 0.453
2824 0.531
364 0.541
64 0.359
6 0.333
0 0.0
0 0.0
0.48088554148122786
0.48507462686567165
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5548,	 Acc = 0.4225
365 0.334
1671 0.454
791 0.413
82 0.268
2 0.5
0 0.0
0 0.0
0 0.0
0.4351924587588374
0.48507462686567165
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5964,	 Acc1 = 0.3782,	 Acc2 = 0.3996

 ===== Epoch 79	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  1.8093963   2.8289006  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04  1.5141569e-02  2.4162690e-01
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 6 6
train:	 Loss = 1.5269,	 Acc = 0.4237
2816 0.25
5497 0.452
2838 0.528
363 0.543
64 0.375
6 0.333
0 0.0
0 0.0
0.479470802919708
0.48507462686567165
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5446,	 Acc = 0.4270
365 0.337
1671 0.457
791 0.421
82 0.268
2 0.5
0 0.0
0 0.0
0 0.0
0.4399057344854674
0.48507462686567165
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5895,	 Acc1 = 0.3891,	 Acc2 = 0.4128

 ===== Epoch 80	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
  1.5019468   1.0736356 ] [ 2.5238533e+00  1.8466066e+00  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -2.0622562e-01 -6.9469109e-02] 3 2
train:	 Loss = 1.5286,	 Acc = 0.4215
2827 0.247
5498 0.451
2826 0.53
363 0.512
64 0.328
6 0.333
0 0.0
0 0.0
0.4779033915724563
0.48507462686567165
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.5353,	 Acc = 0.4373
365 0.34
1671 0.458
791 0.449
82 0.341
2 0.5
0 0.0
0 0.0
0 0.0
0.4512961508248233
0.48507462686567165
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5760,	 Acc1 = 0.3908,	 Acc2 = 0.4148

 ===== Epoch 81	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  1.6014274   1.098423   -0.4012819  -0.40874913  1.861868    3.0784912
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773 -0.00403129 -0.00240584  0.00208275  0.0269259  -0.01324589
 -0.00014811 -0.00011583] 0 0
train:	 Loss = 1.5316,	 Acc = 0.4245
2820 0.25
5497 0.452
2832 0.532
365 0.534
64 0.359
6 0.333
0 0.0
0 0.0
0.48060246462802375
0.48507462686567165
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.5100,	 Acc = 0.4541
365 0.334
1671 0.469
791 0.488
82 0.366
2 0.5
0 0.0
0 0.0
0 0.0
0.4713275726630008
0.48507462686567165
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5615,	 Acc1 = 0.3996,	 Acc2 = 0.4254

 ===== Epoch 82	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  3.298338    1.4417275  -0.4012819  -0.40874913  3.3707402   2.0475185
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -3.5803601e-01  4.3735586e-02
 -2.4058432e-03  2.0827476e-03  1.2119749e+00  2.1943154e-02
 -1.4811044e-04 -1.1583284e-04] 2 3
train:	 Loss = 1.5302,	 Acc = 0.4250
2823 0.249
5499 0.455
2830 0.53
363 0.523
63 0.397
6 0.333
0 0.0
0 0.0
0.48168017349617626
0.48507462686567165
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.5221,	 Acc = 0.4452
365 0.345
1671 0.473
791 0.455
82 0.232
2 0.5
0 0.0
0 0.0
0 0.0
0.4595443833464258
0.48507462686567165
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5757,	 Acc1 = 0.3912,	 Acc2 = 0.4153

 ===== Epoch 83	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.00014811 -0.00011583] 1 1
train:	 Loss = 1.5302,	 Acc = 0.4233
2819 0.248
5494 0.452
2837 0.527
366 0.538
62 0.387
6 0.333
0 0.0
0 0.0
0.47952082144894465
0.48507462686567165
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 5
val:	 Loss = 1.5638,	 Acc = 0.4325
365 0.332
1671 0.472
791 0.413
82 0.268
2 0.5
0 0.0
0 0.0
0 0.0
0.4469756480754124
0.48507462686567165
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5965,	 Acc1 = 0.3796,	 Acc2 = 0.4013

 ===== Epoch 84	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.00014811 -0.00011583] 0 1
train:	 Loss = 1.5269,	 Acc = 0.4234
2817 0.237
5497 0.453
2837 0.535
365 0.556
62 0.403
6 0.333
0 0.0
0 0.0
0.48328960876012317
0.48507462686567165
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5583,	 Acc = 0.4263
365 0.345
1671 0.455
791 0.422
82 0.232
2 0.5
0 0.0
0 0.0
0 0.0
0.43794186959937154
0.48507462686567165
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5996,	 Acc1 = 0.3786,	 Acc2 = 0.4001

 ===== Epoch 85	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247  2.3935885   1.091667
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346  0.00035502  0.00176777 -0.00030847 -0.00225179
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.00014811 -0.00011583] 0 0
train:	 Loss = 1.5335,	 Acc = 0.4226
2825 0.246
5503 0.456
2822 0.52
364 0.538
64 0.437
6 0.333
0 0.0
0 0.0
0.4796209612969517
0.48507462686567165
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.5014,	 Acc = 0.4614
365 0.351
1671 0.472
791 0.502
82 0.341
2 0.5
0 0.0
0 0.0
0 0.0
0.4772191673212883
0.48507462686567165
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5880,	 Acc1 = 0.3734,	 Acc2 = 0.3939

 ===== Epoch 86	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247  3.0997653   1.8587883
 -0.36420733 -0.3724829   2.9646893   1.1140108  -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
  1.5196595e-01  1.1851279e-01  1.3177313e-03  2.7925514e-03
  8.9019217e-02  1.4581110e-01  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 3 3
train:	 Loss = 1.5252,	 Acc = 0.4296
2821 0.249
5494 0.459
2835 0.539
364 0.547
64 0.328
6 0.333
0 0.0
0 0.0
0.48773251169690746
0.48507462686567165
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5459,	 Acc = 0.4256
365 0.332
1671 0.46
791 0.416
82 0.244
2 0.5
0 0.0
0 0.0
0 0.0
0.4391201885310291
0.48507462686567165
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5813,	 Acc1 = 0.3895,	 Acc2 = 0.4133

 ===== Epoch 87	 =====
[-0.36678174 -0.3791378   1.6623789   2.8149455  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346 -0.00066573  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.00014811 -0.00011583] 0 0
train:	 Loss = 1.5318,	 Acc = 0.4214
2820 0.247
5498 0.451
2834 0.524
365 0.542
61 0.361
6 0.333
0 0.0
0 0.0
0.4775216795983569
0.48507462686567165
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5406,	 Acc = 0.4435
365 0.334
1671 0.475
791 0.446
82 0.256
2 0.5
0 0.0
0 0.0
0 0.0
0.4591516103692066
0.48507462686567165
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5909,	 Acc1 = 0.3823,	 Acc2 = 0.4046

 ===== Epoch 88	 =====
[ 2.7047324   1.7461985  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829   1.9518738   3.355852   -0.42398092 -0.42041838
  2.6295931   1.9428358 ] [ 2.8450868e-01 -1.3642821e-02  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
  1.7020965e-02 -1.7081033e-02  9.4095006e-04  1.8351302e-03
  4.7322085e-01 -4.2794771e-02] 3 3
train:	 Loss = 1.5272,	 Acc = 0.4263
2823 0.248
5493 0.456
2834 0.534
365 0.529
63 0.397
6 0.167
0 0.0
0 0.0
0.4838488756991211
0.48507462686567165
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 5
val:	 Loss = 1.5449,	 Acc = 0.4352
365 0.345
1671 0.473
791 0.424
82 0.183
2 0.5
0 0.0
0 0.0
0 0.0
0.4481539670070699
0.48507462686567165
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5946,	 Acc1 = 0.3879,	 Acc2 = 0.4113

 ===== Epoch 89	 =====
[-0.36678174 -0.3791378   3.2339714   2.7083414  -0.4410947  -0.36475152
 -0.36420733 -0.3724829   2.739301    0.92739815 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  1.5596238e+00 -7.8185921e+00
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -6.2605343e+00 -2.5706549e+00  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 2 1
train:	 Loss = 1.5299,	 Acc = 0.4261
2819 0.248
5499 0.457
2830 0.531
366 0.536
64 0.375
6 0.167
0 0.0
0 0.0
0.4833998859098688
0.48507462686567165
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5882,	 Acc = 0.4188
365 0.332
1671 0.451
791 0.412
82 0.22
2 0.5
0 0.0
0 0.0
0 0.0
0.4312647289866457
0.48507462686567165
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.6361,	 Acc1 = 0.3740,	 Acc2 = 0.3946

 ===== Epoch 90	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.00014811 -0.00011583] 6 1
train:	 Loss = 1.5283,	 Acc = 0.4261
2815 0.238
5503 0.458
2832 0.538
364 0.552
64 0.375
6 0.167
0 0.0
0 0.0
0.48660052457520814
0.48507462686567165
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5392,	 Acc = 0.4260
365 0.337
1671 0.455
791 0.425
82 0.232
2 0.5
0 0.0
0 0.0
0 0.0
0.4387274155538099
0.48507462686567165
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5831,	 Acc1 = 0.3903,	 Acc2 = 0.4143

 ===== Epoch 91	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247  0.50407827  0.83151287
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
  2.1851878e-01  6.3162364e-02  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 6 4
train:	 Loss = 1.5279,	 Acc = 0.4252
2823 0.24
5493 0.457
2832 0.535
366 0.546
64 0.375
6 0.167
0 0.0
0 0.0
0.48499029791119735
0.48507462686567165
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5412,	 Acc = 0.4349
365 0.329
1671 0.467
791 0.436
82 0.244
2 0.5
0 0.0
0 0.0
0 0.0
0.45011783189316573
0.48507462686567165
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5968,	 Acc1 = 0.3831,	 Acc2 = 0.4056

 ===== Epoch 92	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247  1.7084408   2.8060162
 -0.36420733 -0.3724829   0.8105926   0.9448153  -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
  2.5784758e-01 -6.0118150e-02  1.3177313e-03  2.7925514e-03
 -1.5896848e-01 -3.1453867e-02  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 5 2
train:	 Loss = 1.5257,	 Acc = 0.4255
2822 0.243
5499 0.452
2832 0.538
362 0.561
63 0.429
6 0.167
0 0.0
0 0.0
0.48413604199954346
0.48507462686567165
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5252,	 Acc = 0.4356
365 0.332
1671 0.466
791 0.435
82 0.28
2 0.5
0 0.0
0 0.0
0 0.0
0.4505106048703849
0.48507462686567165
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5748,	 Acc1 = 0.3955,	 Acc2 = 0.4205

 ===== Epoch 93	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913  0.9646399   3.213822
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03 -2.5054619e-01  7.2213210e-02
 -1.4811044e-04 -1.1583284e-04] 1 4
train:	 Loss = 1.5251,	 Acc = 0.4266
2823 0.247
5497 0.454
2833 0.543
361 0.512
64 0.375
6 0.333
0 0.0
0 0.0
0.48464787124757447
0.48507462686567165
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.5301,	 Acc = 0.4449
365 0.329
1671 0.47
791 0.463
82 0.28
2 0.5
0 0.0
0 0.0
0 0.0
0.4615082482325216
0.48507462686567165
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5712,	 Acc1 = 0.3959,	 Acc2 = 0.4210

 ===== Epoch 94	 =====
[-0.36678174 -0.3791378   1.7201191   1.7194195  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -2.5137290e-01  1.3109277e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 4
train:	 Loss = 1.5255,	 Acc = 0.4252
2815 0.25
5502 0.458
2834 0.524
363 0.523
64 0.422
6 0.167
0 0.0
0 0.0
0.4814688105827346
0.48507462686567165
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.5078,	 Acc = 0.4538
365 0.337
1671 0.476
791 0.474
82 0.329
2 0.5
0 0.0
0 0.0
0 0.0
0.47054202670856243
0.48507462686567165
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5662,	 Acc1 = 0.3918,	 Acc2 = 0.4160

 ===== Epoch 95	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829   1.1357721   2.3083324  -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.0938309  -0.0027082   0.00094095  0.00183513
 -0.00014811 -0.00011583] 4 0
train:	 Loss = 1.5254,	 Acc = 0.4243
2826 0.242
5492 0.453
2831 0.537
365 0.537
64 0.422
6 0.333
0 0.0
0 0.0
0.4832153459693994
0.48507462686567165
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.5287,	 Acc = 0.4490
365 0.334
1671 0.47
791 0.473
82 0.305
2 0.5
0 0.0
0 0.0
0 0.0
0.46543597800471326
0.48507462686567165
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 4
Testing:	 Loss = 1.5706,	 Acc1 = 0.3994,	 Acc2 = 0.4252

 ===== Epoch 96	 =====
[-0.36678174 -0.3791378   1.8278728   2.0505722  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -5.7443345e-01  2.1223788e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
train:	 Loss = 1.5255,	 Acc = 0.4255
2824 0.245
5491 0.454
2835 0.538
364 0.533
64 0.406
6 0.167
0 0.0
0 0.0
0.48378995433789956
0.48507462686567165
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.5132,	 Acc = 0.4524
365 0.332
1671 0.479
791 0.466
82 0.317
2 0.5
0 0.0
0 0.0
0 0.0
0.46975648075412413
0.48507462686567165
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5612,	 Acc1 = 0.3980,	 Acc2 = 0.4235

 ===== Epoch 97	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.00014811 -0.00011583] 5 1
train:	 Loss = 1.5251,	 Acc = 0.4274
2820 0.246
5495 0.457
2835 0.535
364 0.56
64 0.359
6 0.333
0 0.0
0 0.0
0.48562300319488816
0.48507462686567165
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5169,	 Acc = 0.4383
365 0.337
1671 0.46
791 0.456
82 0.28
2 0.5
0 0.0
0 0.0
0 0.0
0.45286724273369994
0.48507462686567165
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5909,	 Acc1 = 0.3757,	 Acc2 = 0.3966

 ===== Epoch 98	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.00014811 -0.00011583] 2 1
train:	 Loss = 1.5254,	 Acc = 0.4247
2817 0.251
5498 0.454
2835 0.528
364 0.536
64 0.391
6 0.333
0 0.0
0 0.0
0.48043800615946164
0.48507462686567165
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.5280,	 Acc = 0.4469
365 0.342
1671 0.467
791 0.468
82 0.305
2 0.5
0 0.0
0 0.0
0 0.0
0.46190102120974075
0.48507462686567165
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5783,	 Acc1 = 0.3885,	 Acc2 = 0.4120

 ===== Epoch 99	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829   1.2693988   3.0000439  -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.00014811 -0.00011583] 0 0
train:	 Loss = 1.5294,	 Acc = 0.4257
2821 0.245
5492 0.454
2835 0.537
366 0.546
64 0.406
6 0.333
0 0.0
0 0.0
0.4839666780782837
0.48507462686567165
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 5
val:	 Loss = 1.5598,	 Acc = 0.4315
365 0.34
1671 0.467
791 0.42
82 0.22
2 0.5
0 0.0
0 0.0
0 0.0
0.44461901021209743
0.48507462686567165
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5999,	 Acc1 = 0.3916,	 Acc2 = 0.4158

 ===== Epoch 100	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829   0.9608527   2.8955405  -0.42398092 -0.42041838
  2.6282723   2.360367  ] [ 0.00073671  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.13497378 -0.0458267   0.00094095  0.00183513
  0.00087055 -0.14949211] 4 5
train:	 Loss = 1.5257,	 Acc = 0.4247
2824 0.243
5494 0.455
2834 0.536
363 0.532
63 0.381
6 0.333
0 0.0
0 0.0
0.4834474885844749
0.48507462686567165
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5361,	 Acc = 0.4311
365 0.334
1671 0.459
791 0.439
82 0.22
2 0.5
0 0.0
0 0.0
0 0.0
0.44501178318931656
0.48507462686567165
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5772,	 Acc1 = 0.3852,	 Acc2 = 0.4081

 ===== Epoch 101	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829   1.3301915   2.962721   -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -1.6582915e-01  2.1246528e-02  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 2 0
train:	 Loss = 1.5299,	 Acc = 0.4226
2818 0.248
5492 0.45
2841 0.53
364 0.519
63 0.413
6 0.333
0 0.0
0 0.0
0.4786675792835957
0.48507462686567165
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5533,	 Acc = 0.4421
365 0.34
1671 0.478
791 0.435
82 0.244
2 0.5
0 0.0
0 0.0
0 0.0
0.45679497250589157
0.48507462686567165
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5972,	 Acc1 = 0.3831,	 Acc2 = 0.4056

 ===== Epoch 102	 =====
[-0.36678174 -0.3791378   2.5069323   1.3338306  -0.4410947  -0.36475152
  1.7604959   3.0577703  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  1.2559229e+00  1.4884327e-01
 -3.0847333e-04  2.6414116e-04 -1.2307351e-01  4.4634208e-01
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 6 6
train:	 Loss = 1.5268,	 Acc = 0.4263
2818 0.244
5503 0.46
2835 0.528
361 0.535
61 0.41
6 0.333
0 0.0
0 0.0
0.48471366643851244
0.48507462686567165
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.5193,	 Acc = 0.4504
365 0.351
1671 0.475
791 0.465
82 0.256
2 0.5
0 0.0
0 0.0
0 0.0
0.46465043205027495
0.48507462686567165
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5938,	 Acc1 = 0.3763,	 Acc2 = 0.3974

 ===== Epoch 103	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.2865999   1.2212311  -0.4012819  -0.40874913  2.661507    2.057361
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04  2.8528324e-01 -4.3393311e-01
 -2.4058432e-03  2.0827476e-03  8.7511152e-01 -3.8021728e-01
 -1.4811044e-04 -1.1583284e-04] 0 5
train:	 Loss = 1.5234,	 Acc = 0.4261
2822 0.251
5492 0.452
2836 0.536
364 0.552
64 0.359
6 0.333
0 0.0
0 0.0
0.4826523624743209
0.48507462686567165
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 5
val:	 Loss = 1.5251,	 Acc = 0.4438
365 0.332
1671 0.482
791 0.434
82 0.256
2 0.5
0 0.0
0 0.0
0 0.0
0.45993715632364496
0.48507462686567165
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5798,	 Acc1 = 0.3930,	 Acc2 = 0.4175

 ===== Epoch 104	 =====
[ 0.21383716  1.5319002  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [-0.00054458 -0.00435705  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.00014811 -0.00011583] 0 0
train:	 Loss = 1.5239,	 Acc = 0.4269
2815 0.248
5502 0.457
2836 0.529
361 0.568
64 0.375
6 0.333
0 0.0
0 0.0
0.4842057247120538
0.48507462686567165
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 5
val:	 Loss = 1.5603,	 Acc = 0.4297
365 0.34
1671 0.461
791 0.426
82 0.22
2 0.5
0 0.0
0 0.0
0 0.0
0.4426551453260016
0.48507462686567165
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.6025,	 Acc1 = 0.3769,	 Acc2 = 0.3981

 ===== Epoch 105	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247  3.1311173   1.9944242
 -0.36420733 -0.3724829   3.1717246   1.2060732  -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -8.8167447e-01  7.3226079e-02  1.3177313e-03  2.7925514e-03
 -4.3667048e-01  1.2185638e-01  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 3 2
train:	 Loss = 1.5255,	 Acc = 0.4284
2819 0.246
5491 0.459
2839 0.536
365 0.545
64 0.391
6 0.333
0 0.0
0 0.0
0.4870507701083856
0.48507462686567165
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5907,	 Acc = 0.4160
365 0.34
1671 0.449
791 0.407
82 0.171
2 0.5
0 0.0
0 0.0
0 0.0
0.4269442262372349
0.48507462686567165
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.6183,	 Acc1 = 0.3906,	 Acc2 = 0.4145

 ===== Epoch 106	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.00014811 -0.00011583] 4 1
train:	 Loss = 1.5275,	 Acc = 0.4217
2820 0.234
5494 0.451
2837 0.537
364 0.536
63 0.397
6 0.333
0 0.0
0 0.0
0.48208580556823366
0.48507462686567165
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.5581,	 Acc = 0.4246
365 0.326
1671 0.449
791 0.436
82 0.256
2 0.5
0 0.0
0 0.0
0 0.0
0.4387274155538099
0.48507462686567165
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5893,	 Acc1 = 0.3945,	 Acc2 = 0.4192

 ===== Epoch 107	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913  0.9271925   0.8098593
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03 -3.3139412e+00 -2.5116677e+00
 -1.4811044e-04 -1.1583284e-04] 1 1
train:	 Loss = 1.5315,	 Acc = 0.4248
2824 0.248
5488 0.456
2837 0.529
365 0.521
64 0.406
6 0.333
0 0.0
0 0.0
0.48184931506849316
0.48507462686567165
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 5
val:	 Loss = 1.5535,	 Acc = 0.4260
365 0.332
1671 0.463
791 0.413
82 0.22
2 0.0
0 0.0
0 0.0
0 0.0
0.43951296150824826
0.48507462686567165
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.6007,	 Acc1 = 0.3823,	 Acc2 = 0.4046

 ===== Epoch 108	 =====
[ 2.2967072   2.1142876  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
  2.1847017   2.357741  ] [-0.02020486  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
  0.0008737  -0.00011583] 0 0
train:	 Loss = 1.5296,	 Acc = 0.4222
2822 0.247
5490 0.454
2839 0.523
363 0.523
64 0.422
6 0.167
0 0.0
0 0.0
0.47877196986989273
0.48507462686567165
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5576,	 Acc = 0.4222
365 0.342
1671 0.453
791 0.41
82 0.268
2 0.5
0 0.0
0 0.0
0 0.0
0.4336213668499607
0.48507462686567165
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5987,	 Acc1 = 0.3798,	 Acc2 = 0.4016

 ===== Epoch 109	 =====
[ 0.8491963   3.057201   -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 0.00639486  0.00492872  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.00014811 -0.00011583] 6 0
train:	 Loss = 1.5297,	 Acc = 0.4243
2821 0.25
5495 0.453
2833 0.53
365 0.521
64 0.437
6 0.167
0 0.0
0 0.0
0.48031496062992124
0.48507462686567165
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5581,	 Acc = 0.4239
365 0.342
1671 0.457
791 0.407
82 0.268
2 0.5
0 0.0
0 0.0
0 0.0
0.43558523173605657
0.48507462686567165
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5958,	 Acc1 = 0.3749,	 Acc2 = 0.3956

 ===== Epoch 110	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829   2.7467558   1.8928081  -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
  4.2500514e-01 -1.3685465e-01  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 5 2
train:	 Loss = 1.5227,	 Acc = 0.4283
2821 0.248
5495 0.456
2833 0.537
366 0.563
63 0.397
6 0.333
0 0.0
0 0.0
0.4862490014835102
0.48507462686567165
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.5125,	 Acc = 0.4497
365 0.329
1671 0.476
791 0.465
82 0.293
2 0.5
0 0.0
0 0.0
0 0.0
0.4670070699135899
0.48507462686567165
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5841,	 Acc1 = 0.3788,	 Acc2 = 0.4003

 ===== Epoch 111	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.00014811 -0.00011583] 2 1
train:	 Loss = 1.5222,	 Acc = 0.4282
2821 0.251
5497 0.456
2832 0.532
364 0.566
64 0.469
6 0.333
0 0.0
0 0.0
0.48510783978089694
0.48507462686567165
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5676,	 Acc = 0.4057
365 0.332
1671 0.429
791 0.408
82 0.232
2 0.5
0 0.0
0 0.0
0 0.0
0.41633935585231735
0.48507462686567165
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.6070,	 Acc1 = 0.3817,	 Acc2 = 0.4038

 ===== Epoch 112	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
  2.0556397   2.6387212 ] [ 0.00073671  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
  0.01107287  0.0745723 ] 3 4
train:	 Loss = 1.5233,	 Acc = 0.4274
2826 0.241
5493 0.458
2834 0.54
361 0.548
64 0.422
6 0.333
0 0.0
0 0.0
0.4876684174469057
0.48507462686567165
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5184,	 Acc = 0.4456
365 0.337
1671 0.47
791 0.461
82 0.268
2 0.5
0 0.0
0 0.0
0 0.0
0.46111547525530244
0.48507462686567165
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5709,	 Acc1 = 0.3941,	 Acc2 = 0.4187

 ===== Epoch 113	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247  2.0858076   2.0789187
 -0.36420733 -0.3724829   1.1954159   0.8801229  -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
  4.4339997e-01  8.5805722e-02  1.3177313e-03  2.7925514e-03
 -8.2402326e-02  1.6497488e-01  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 2 4
train:	 Loss = 1.5283,	 Acc = 0.4208
2823 0.238
5491 0.453
2834 0.527
366 0.525
64 0.406
6 0.333
0 0.0
0 0.0
0.47985389795685424
0.48507462686567165
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5503,	 Acc = 0.4150
365 0.181
1671 0.469
791 0.426
82 0.256
2 0.0
0 0.0
0 0.0
0 0.0
0.44854673998428907
0.48507462686567165
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.6052,	 Acc1 = 0.3396,	 Acc2 = 0.3869

 ===== Epoch 114	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829   1.8669946   2.9577448  -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -1.2240057e-01  3.3265793e-01  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
train:	 Loss = 1.5248,	 Acc = 0.4254
2821 0.248
5495 0.458
2834 0.527
365 0.518
63 0.413
6 0.333
0 0.0
0 0.0
0.4825972840351478
0.48507462686567165
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5504,	 Acc = 0.4277
365 0.334
1671 0.461
791 0.424
82 0.207
2 0.5
0 0.0
0 0.0
0 0.0
0.4410840534171249
0.48507462686567165
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.6042,	 Acc1 = 0.3784,	 Acc2 = 0.3999

 ===== Epoch 115	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.00014811 -0.00011583] 0 1
train:	 Loss = 1.5271,	 Acc = 0.4243
2822 0.247
5497 0.457
2830 0.525
365 0.529
64 0.422
6 0.167
0 0.0
0 0.0
0.48139694133759414
0.48507462686567165
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.5175,	 Acc = 0.4493
365 0.326
1671 0.479
791 0.46
82 0.293
2 0.5
0 0.0
0 0.0
0 0.0
0.4670070699135899
0.48507462686567165
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5720,	 Acc1 = 0.3887,	 Acc2 = 0.4123

 ===== Epoch 116	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829   1.028526    2.942816   -0.42398092 -0.42041838
  2.7886095   2.210686  ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
  1.3358888e-01  2.7037567e-01  9.4095006e-04  1.8351302e-03
 -3.2966903e-01  1.4392558e-01] 4 6
train:	 Loss = 1.5249,	 Acc = 0.4263
2824 0.242
5493 0.457
2835 0.537
362 0.539
64 0.422
6 0.333
0 0.0
0 0.0
0.48561643835616436
0.48507462686567165
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.5283,	 Acc = 0.4370
365 0.334
1671 0.464
791 0.442
82 0.28
2 0.5
0 0.0
0 0.0
0 0.0
0.4516889238020424
0.48507462686567165
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.6032,	 Acc1 = 0.3734,	 Acc2 = 0.3939

 ===== Epoch 117	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.00014811 -0.00011583] 3 1
train:	 Loss = 1.5244,	 Acc = 0.4244
2821 0.239
5492 0.456
2837 0.533
365 0.526
63 0.508
6 0.333
0 0.0
0 0.0
0.4839666780782837
0.48507462686567165
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 5
val:	 Loss = 1.5311,	 Acc = 0.4445
365 0.345
1671 0.478
791 0.437
82 0.28
2 0.5
0 0.0
0 0.0
0 0.0
0.4587588373919874
0.48507462686567165
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5834,	 Acc1 = 0.3953,	 Acc2 = 0.4202

 ===== Epoch 118	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913  1.3969892   1.3167337
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275 -0.09742777  0.04707818
 -0.00014811 -0.00011583] 1 4
train:	 Loss = 1.5217,	 Acc = 0.4265
2821 0.249
5493 0.458
2837 0.527
364 0.541
63 0.444
6 0.333
0 0.0
0 0.0
0.4835102133972384
0.48507462686567165
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5391,	 Acc = 0.4291
365 0.337
1671 0.461
791 0.422
82 0.256
2 0.5
0 0.0
0 0.0
0 0.0
0.4422623723487824
0.48507462686567165
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.6070,	 Acc1 = 0.3747,	 Acc2 = 0.3954

 ===== Epoch 119	 =====
[ 0.23105824  3.568996   -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 0.07695777  0.09778643  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.00014811 -0.00011583] 6 6
train:	 Loss = 1.5245,	 Acc = 0.4267
2820 0.249
5494 0.458
2834 0.53
366 0.527
64 0.437
6 0.333
0 0.0
0 0.0
0.4840255591054313
0.48507462686567165
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 5
val:	 Loss = 1.5540,	 Acc = 0.4366
365 0.332
1671 0.481
791 0.415
82 0.207
2 0.5
0 0.0
0 0.0
0 0.0
0.4516889238020424
0.48507462686567165
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.6069,	 Acc1 = 0.3908,	 Acc2 = 0.4148

 ===== Epoch 120	 =====
[ 1.6482602   2.391615   -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
  1.537627    2.7043707 ] [-0.24490394  0.05135757  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.02259323  0.01588877] 4 0
train:	 Loss = 1.5210,	 Acc = 0.4297
2819 0.248
5496 0.464
2834 0.53
365 0.532
64 0.484
6 0.333
0 0.0
0 0.0
0.4883057615516258
0.48507462686567165
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5684,	 Acc = 0.4263
365 0.329
1671 0.456
791 0.425
82 0.268
2 0.5
0 0.0
0 0.0
0 0.0
0.44029850746268656
0.48507462686567165
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.6124,	 Acc1 = 0.3771,	 Acc2 = 0.3984

 ===== Epoch 121	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247  1.6679125   1.8387765
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
  3.3549827e-01 -5.2570362e-02  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 6 2
train:	 Loss = 1.5284,	 Acc = 0.4230
2824 0.243
5490 0.454
2834 0.531
366 0.527
64 0.375
6 0.167
0 0.0
0 0.0
0.48093607305936076
0.48507462686567165
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5582,	 Acc = 0.4325
365 0.34
1671 0.466
791 0.425
82 0.244
2 0.5
0 0.0
0 0.0
0 0.0
0.4457973291437549
0.48507462686567165
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5978,	 Acc1 = 0.3943,	 Acc2 = 0.4190

 ===== Epoch 122	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247  0.55569303  1.2673266
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -4.3671161e-02  1.3109244e-01  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 6 6
train:	 Loss = 1.5213,	 Acc = 0.4302
2819 0.243
5498 0.463
2834 0.537
364 0.552
63 0.413
6 0.333
0 0.0
0 0.0
0.49047347404449515
0.48507462686567165
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 5
val:	 Loss = 1.5521,	 Acc = 0.4280
365 0.334
1671 0.462
791 0.421
82 0.22
2 0.5
0 0.0
0 0.0
0 0.0
0.44147682639434405
0.48507462686567165
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 4
Testing:	 Loss = 1.5755,	 Acc1 = 0.4050,	 Acc2 = 0.4319

 ===== Epoch 123	 =====
[ 0.21265237  1.5293789  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.00014811 -0.00011583] 0 0
train:	 Loss = 1.5192,	 Acc = 0.4264
2821 0.248
5499 0.456
2830 0.529
364 0.549
64 0.469
6 0.333
0 0.0
0 0.0
0.48362432956749973
0.48507462686567165
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.5199,	 Acc = 0.4541
365 0.334
1671 0.493
791 0.446
82 0.268
2 0.5
0 0.0
0 0.0
0 0.0
0.4713275726630008
0.48507462686567165
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5743,	 Acc1 = 0.4007,	 Acc2 = 0.4267

 ===== Epoch 124	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.9189365   2.449312   -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04  2.3251259e-01  7.5341481e-01
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 6 6
train:	 Loss = 1.5188,	 Acc = 0.4270
2823 0.245
5497 0.459
2832 0.532
362 0.539
64 0.422
6 0.333
0 0.0
0 0.0
0.4855610090172355
0.48507462686567165
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.5063,	 Acc = 0.4490
365 0.342
1671 0.473
791 0.466
82 0.256
2 0.5
0 0.0
0 0.0
0 0.0
0.4642576590730558
0.48507462686567165
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5730,	 Acc1 = 0.3831,	 Acc2 = 0.4056

 ===== Epoch 125	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829   1.279148    2.330726   -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -8.0116615e-02  6.0095084e-01  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 6 6
train:	 Loss = 1.5229,	 Acc = 0.4264
2816 0.247
5500 0.457
2834 0.534
364 0.522
64 0.375
6 0.333
0 0.0
0 0.0
0.48391879562043794
0.48507462686567165
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5474,	 Acc = 0.4414
365 0.34
1671 0.476
791 0.437
82 0.22
2 0.5
0 0.0
0 0.0
0 0.0
0.45600942655145327
0.48507462686567165
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5887,	 Acc1 = 0.3872,	 Acc2 = 0.4105

 ===== Epoch 126	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247  2.8978922   2.9661112
 -0.36420733 -0.3724829   1.9340953   2.3133087  -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.00014811 -0.00011583] 0 0
train:	 Loss = 1.5213,	 Acc = 0.4279
2817 0.242
5500 0.459
2833 0.539
364 0.527
64 0.406
6 0.333
0 0.0
0 0.0
0.48750998060910233
0.48507462686567165
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.4997,	 Acc = 0.4521
365 0.178
1671 0.49
791 0.511
82 0.341
2 0.0
0 0.0
0 0.0
0 0.0
0.4913589945011783
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5757,	 Acc1 = 0.3643,	 Acc2 = 0.4167

 ===== Epoch 127	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247  2.8921566   1.2095146
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  6.3058047e+00  3.8155878e+00
  2.2887914e-02 -1.0792080e-01  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 5 5
train:	 Loss = 1.5217,	 Acc = 0.4280
2827 0.245
5496 0.46
2827 0.534
364 0.558
64 0.391
6 0.333
0 0.0
0 0.0
0.48715313463514903
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 5
val:	 Loss = 1.5395,	 Acc = 0.4308
365 0.34
1671 0.463
791 0.424
82 0.256
2 0.5
0 0.0
0 0.0
0 0.0
0.44383346425765907
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5849,	 Acc1 = 0.3936,	 Acc2 = 0.4182

 ===== Epoch 128	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913  1.5509406   1.0829809
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00650915 -0.04340792
 -0.00014811 -0.00011583] 3 2
train:	 Loss = 1.5199,	 Acc = 0.4271
2823 0.241
5496 0.458
2832 0.539
363 0.543
64 0.375
6 0.333
0 0.0
0 0.0
0.4870448578929346
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5198,	 Acc = 0.4497
365 0.332
1671 0.489
791 0.44
82 0.268
2 0.5
0 0.0
0 0.0
0 0.0
0.4666142969363708
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5783,	 Acc1 = 0.3916,	 Acc2 = 0.4158

 ===== Epoch 129	 =====
[ 0.09407981  1.9554547  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 0.01862723  0.03588129  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.00014811 -0.00011583] 4 1
train:	 Loss = 1.5183,	 Acc = 0.4292
2820 0.244
5491 0.458
2839 0.542
365 0.542
63 0.429
6 0.5
0 0.0
0 0.0
0.48881789137380194
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5510,	 Acc = 0.4177
365 0.334
1671 0.445
791 0.415
82 0.256
2 0.5
0 0.0
0 0.0
0 0.0
0.42969363707776903
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 4
Testing:	 Loss = 1.5753,	 Acc1 = 0.3953,	 Acc2 = 0.4202

 ===== Epoch 130	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829   2.406092    1.1239635  -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.0353813e-01 -7.9363316e-02  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 2 2
train:	 Loss = 1.5179,	 Acc = 0.4301
2822 0.247
5495 0.464
2834 0.53
363 0.551
64 0.422
6 0.333
0 0.0
0 0.0
0.4889294681579548
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5261,	 Acc = 0.4545
365 0.34
1671 0.492
791 0.446
82 0.28
2 0.5
0 0.0
0 0.0
0 0.0
0.4709347996857816
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5846,	 Acc1 = 0.3862,	 Acc2 = 0.4093

 ===== Epoch 131	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829   1.2854577   1.7982577  -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
  1.0616031e-01  3.2786703e-01  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 6 6
train:	 Loss = 1.5172,	 Acc = 0.4265
2820 0.244
5495 0.459
2837 0.53
363 0.532
63 0.476
6 0.5
0 0.0
0 0.0
0.4850524874486536
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.5149,	 Acc = 0.4480
365 0.337
1671 0.482
791 0.446
82 0.268
2 0.5
0 0.0
0 0.0
0 0.0
0.4638648860958366
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5735,	 Acc1 = 0.3856,	 Acc2 = 0.4085

 ===== Epoch 132	 =====
[-0.36678174 -0.3791378   1.6619717   2.8149455  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346 -0.00473927  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.00014811 -0.00011583] 0 0
train:	 Loss = 1.5221,	 Acc = 0.4294
2824 0.25
5492 0.459
2834 0.535
364 0.555
64 0.406
6 0.333
0 0.0
0 0.0
0.4872146118721461
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5131,	 Acc = 0.4600
365 0.337
1671 0.498
791 0.453
82 0.305
2 0.5
0 0.0
0 0.0
0 0.0
0.47761194029850745
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5704,	 Acc1 = 0.3943,	 Acc2 = 0.4190

 ===== Epoch 133	 =====
[ 2.3147333   2.1142876  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
  2.1847017   2.357741  ] [-0.05503868  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
  0.0008737  -0.00011583] 0 0
train:	 Loss = 1.5182,	 Acc = 0.4287
2820 0.243
5492 0.461
2836 0.535
366 0.557
64 0.422
6 0.333
0 0.0
0 0.0
0.48847558192606116
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.5175,	 Acc = 0.4411
365 0.34
1671 0.462
791 0.461
82 0.268
2 0.5
0 0.0
0 0.0
0 0.0
0.4556166535742341
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5787,	 Acc1 = 0.3800,	 Acc2 = 0.4018

 ===== Epoch 134	 =====
[-0.36678174 -0.3791378   2.3792524   3.034958    2.4998782   0.92934865
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.4500634e-01 -3.8804788e-02
 -5.6301028e-01 -3.9990719e-02  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 2 3
train:	 Loss = 1.5203,	 Acc = 0.4273
2820 0.245
5502 0.457
2829 0.536
363 0.545
64 0.406
6 0.333
0 0.0
0 0.0
0.48596531264262893
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5236,	 Acc = 0.4483
365 0.334
1671 0.479
791 0.45
82 0.305
2 0.5
0 0.0
0 0.0
0 0.0
0.46465043205027495
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5768,	 Acc1 = 0.3934,	 Acc2 = 0.4180

 ===== Epoch 135	 =====
[ 2.145356    2.2025282  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
  2.0512347   2.4732842 ] [ 6.1287338e-01  1.1119233e-02  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -8.1763551e-02 -1.6120434e-02] 3 3
train:	 Loss = 1.5182,	 Acc = 0.4238
2823 0.244
5497 0.456
2830 0.528
364 0.525
64 0.437
6 0.333
0 0.0
0 0.0
0.4815660312749686
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5482,	 Acc = 0.4363
365 0.332
1671 0.475
791 0.421
82 0.256
2 0.5
0 0.0
0 0.0
0 0.0
0.4512961508248233
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5928,	 Acc1 = 0.3961,	 Acc2 = 0.4212

 ===== Epoch 136	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.00014811 -0.00011583] 6 1
train:	 Loss = 1.5227,	 Acc = 0.4260
2815 0.244
5500 0.454
2837 0.539
363 0.551
64 0.328
5 0.4
0 0.0
0 0.0
0.4844338008894971
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 5
val:	 Loss = 1.5702,	 Acc = 0.4370
365 0.334
1671 0.475
791 0.421
82 0.28
2 0.5
0 0.0
0 0.0
0 0.0
0.4516889238020424
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.6004,	 Acc1 = 0.3930,	 Acc2 = 0.4175

 ===== Epoch 137	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913  2.6932795   1.6685932
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  5.3837445e-02 -1.6908307e-01
 -1.4811044e-04 -1.1583284e-04] 5 5
train:	 Loss = 1.5221,	 Acc = 0.4283
2822 0.248
5496 0.456
2832 0.538
364 0.549
64 0.422
6 0.333
0 0.0
0 0.0
0.48619036749600547
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.4990,	 Acc = 0.4510
365 0.332
1671 0.473
791 0.473
82 0.317
2 0.5
0 0.0
0 0.0
0 0.0
0.46818538884524746
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5597,	 Acc1 = 0.3928,	 Acc2 = 0.4172

 ===== Epoch 138	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.00014811 -0.00011583] 3 1
train:	 Loss = 1.5153,	 Acc = 0.4277
2818 0.249
5500 0.46
2832 0.53
364 0.538
64 0.391
6 0.333
0 0.0
0 0.0
0.48505589778690394
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5448,	 Acc = 0.4194
365 0.326
1671 0.436
791 0.439
82 0.317
2 0.5
0 0.0
0 0.0
0 0.0
0.43283582089552236
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 4
Testing:	 Loss = 1.5766,	 Acc1 = 0.3893,	 Acc2 = 0.4130

 ===== Epoch 139	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  3.8143258   1.8883023  -0.4012819  -0.40874913  3.873824    2.360009
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -4.3216968e-01 -1.2003654e-01
 -2.4058432e-03  2.0827476e-03 -9.9943644e-01 -8.8650972e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
train:	 Loss = 1.5185,	 Acc = 0.4265
2820 0.245
5499 0.457
2829 0.533
366 0.555
64 0.359
6 0.333
0 0.0
0 0.0
0.48482428115015974
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.5016,	 Acc = 0.4548
365 0.332
1671 0.49
791 0.458
82 0.256
2 0.5
0 0.0
0 0.0
0 0.0
0.4725058915946583
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5556,	 Acc1 = 0.4031,	 Acc2 = 0.4297

 ===== Epoch 140	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.00014811 -0.00011583] 1 1
train:	 Loss = 1.5155,	 Acc = 0.4272
2824 0.237
5497 0.463
2829 0.533
364 0.549
64 0.422
6 0.167
0 0.0
0 0.0
0.4885844748858447
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.5097,	 Acc = 0.4535
365 0.334
1671 0.485
791 0.458
82 0.293
2 0.5
0 0.0
0 0.0
0 0.0
0.47054202670856243
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5816,	 Acc1 = 0.3990,	 Acc2 = 0.4247

 ===== Epoch 141	 =====
[-0.36678174 -0.3791378   1.6257826   0.9006101  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346  0.0676173  -0.01851851 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.00014811 -0.00011583] 4 2
train:	 Loss = 1.5184,	 Acc = 0.4298
2817 0.247
5497 0.458
2841 0.537
363 0.565
60 0.483
6 0.333
0 0.0
0 0.0
0.488422493441314
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.5353,	 Acc = 0.4414
365 0.337
1671 0.472
791 0.441
82 0.28
2 0.5
0 0.0
0 0.0
0 0.0
0.45640219952867245
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5753,	 Acc1 = 0.3965,	 Acc2 = 0.4217

 ===== Epoch 142	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829   1.3319111   2.7039514  -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -1.2582737e-01  2.1288432e-01  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 1 4
train:	 Loss = 1.5197,	 Acc = 0.4296
2818 0.251
5499 0.456
2831 0.538
366 0.563
64 0.437
6 0.333
0 0.0
0 0.0
0.486881131644992
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.5052,	 Acc = 0.4531
365 0.332
1671 0.485
791 0.458
82 0.293
2 0.5
0 0.0
0 0.0
0 0.0
0.47054202670856243
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5698,	 Acc1 = 0.3879,	 Acc2 = 0.4113

 ===== Epoch 143	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913  2.5147424   1.2847464
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03 -8.0270195e-01 -3.4502825e-01
 -1.4811044e-04 -1.1583284e-04] 5 5
train:	 Loss = 1.5226,	 Acc = 0.4262
2827 0.243
5497 0.462
2827 0.526
363 0.537
64 0.375
6 0.333
0 0.0
0 0.0
0.48521183053557154
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.4972,	 Acc = 0.4572
365 0.332
1671 0.482
791 0.477
82 0.317
2 0.5
0 0.0
0 0.0
0 0.0
0.4752553024351925
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5655,	 Acc1 = 0.3910,	 Acc2 = 0.4150

 ===== Epoch 144	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913  1.3368455   1.7104225
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275 -0.10206651  0.08226722
 -0.00014811 -0.00011583] 1 4
train:	 Loss = 1.5195,	 Acc = 0.4245
2819 0.239
5499 0.455
2831 0.533
366 0.552
64 0.391
5 0.4
0 0.0
0 0.0
0.4840844266970907
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5181,	 Acc = 0.4521
365 0.334
1671 0.497
791 0.432
82 0.256
2 0.0
0 0.0
0 0.0
0 0.0
0.46897093479968577
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5754,	 Acc1 = 0.3951,	 Acc2 = 0.4200

 ===== Epoch 145	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829   1.4001588   3.313553   -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -3.5920732e+00 -7.1651711e+00  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 6 1
train:	 Loss = 1.5181,	 Acc = 0.4291
2822 0.248
5498 0.46
2830 0.537
364 0.536
64 0.391
6 0.333
0 0.0
0 0.0
0.48755991782698016
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5069,	 Acc = 0.4497
365 0.332
1671 0.482
791 0.454
82 0.28
2 0.5
0 0.0
0 0.0
0 0.0
0.4666142969363708
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5572,	 Acc1 = 0.3996,	 Acc2 = 0.4254

 ===== Epoch 146	 =====
[-0.36678174 -0.3791378   1.5599096   3.0122764  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346  0.0401012   0.01698247 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.00014811 -0.00011583] 3 0
train:	 Loss = 1.5139,	 Acc = 0.4327
2816 0.248
5500 0.461
2836 0.546
363 0.562
63 0.413
6 0.333
0 0.0
0 0.0
0.4920164233576642
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5369,	 Acc = 0.4284
365 0.329
1671 0.464
791 0.42
82 0.232
2 0.5
0 0.0
0 0.0
0 0.0
0.4426551453260016
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5847,	 Acc1 = 0.3839,	 Acc2 = 0.4066

 ===== Epoch 147	 =====
[ 3.0365124   3.1151874  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
  2.5062573   3.3687444 ] [ 1.4585340e+00  4.8262317e-02  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
  1.2635756e-01  1.1191637e-01] 4 2
train:	 Loss = 1.5223,	 Acc = 0.4276
2820 0.248
5496 0.459
2833 0.533
366 0.527
63 0.46
6 0.167
0 0.0
0 0.0
0.48550890004564123
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.5004,	 Acc = 0.4569
365 0.332
1671 0.483
791 0.475
82 0.305
2 0.5
0 0.0
0 0.0
0 0.0
0.4748625294579733
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5782,	 Acc1 = 0.3819,	 Acc2 = 0.4041

 ===== Epoch 148	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913  2.7935183   1.5332624
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  1.4077829e+00  2.6323944e-01
 -1.4811044e-04 -1.1583284e-04] 2 6
train:	 Loss = 1.5172,	 Acc = 0.4300
2820 0.245
5495 0.464
2835 0.534
364 0.536
64 0.437
6 0.333
0 0.0
0 0.0
0.48950251026928343
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5427,	 Acc = 0.4346
365 0.326
1671 0.467
791 0.434
82 0.256
2 0.5
0 0.0
0 0.0
0 0.0
0.45011783189316573
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 4
Testing:	 Loss = 1.5727,	 Acc1 = 0.4027,	 Acc2 = 0.4292

 ===== Epoch 149	 =====
[-0.36678174 -0.3791378   2.2336822   1.6082791  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -2.9213667e-01 -1.4277196e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 5 5
train:	 Loss = 1.5178,	 Acc = 0.4250
2818 0.237
5499 0.453
2833 0.54
365 0.562
63 0.444
6 0.333
0 0.0
0 0.0
0.48528405201916497
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.5106,	 Acc = 0.4627
365 0.332
1671 0.503
791 0.461
82 0.256
2 0.0
0 0.0
0 0.0
0 0.0
0.48153967007069914
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5655,	 Acc1 = 0.3963,	 Acc2 = 0.4215

 ===== Epoch 150	 =====
[ 0.98459154  1.801664   -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829   4.0405893   3.313553   -0.42398092 -0.42041838
  1.601057    1.908698  ] [ 1.2321692e+00  6.0643338e-02  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -8.8535652e+00 -7.1651711e+00  9.4095006e-04  1.8351302e-03
  1.8144381e-01  2.6558502e-02] 6 4
train:	 Loss = 1.5209,	 Acc = 0.4267
2825 0.245
5494 0.456
2831 0.536
365 0.551
63 0.397
6 0.333
0 0.0
0 0.0
0.48521520721543554
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5277,	 Acc = 0.4297
365 0.337
1671 0.455
791 0.434
82 0.293
2 0.5
0 0.0
0 0.0
0 0.0
0.4430479183032207
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5777,	 Acc1 = 0.3870,	 Acc2 = 0.4103

 ===== Epoch 151	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247  1.7466737   2.0855894
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -6.3837461e-02 -2.5132874e-01  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 5 5
train:	 Loss = 1.5153,	 Acc = 0.4317
2823 0.247
5491 0.464
2836 0.538
364 0.555
64 0.422
6 0.333
0 0.0
0 0.0
0.4911539778564091
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.4882,	 Acc = 0.4617
365 0.34
1671 0.481
791 0.489
82 0.341
2 0.5
0 0.0
0 0.0
0 0.0
0.4791830322073841
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5576,	 Acc1 = 0.3899,	 Acc2 = 0.4138

 ===== Epoch 152	 =====
[-0.36678174 -0.3791378   1.7843643   3.0190809  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.00014811 -0.00011583] 0 0
train:	 Loss = 1.5198,	 Acc = 0.4303
2824 0.251
5496 0.461
2829 0.536
365 0.548
64 0.453
6 0.167
0 0.0
0 0.0
0.4882420091324201
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5166,	 Acc = 0.4373
365 0.332
1671 0.464
791 0.448
82 0.268
2 0.0
0 0.0
0 0.0
0 0.0
0.45247446975648076
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5717,	 Acc1 = 0.3854,	 Acc2 = 0.4083

 ===== Epoch 153	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913  1.4056891   2.222218
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  4.3627638e-02  2.7832046e-01
 -1.4811044e-04 -1.1583284e-04] 2 6
train:	 Loss = 1.5187,	 Acc = 0.4286
2824 0.249
5491 0.46
2834 0.533
365 0.542
64 0.406
6 0.167
0 0.0
0 0.0
0.48641552511415526
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.5167,	 Acc = 0.4438
365 0.326
1671 0.476
791 0.451
82 0.244
2 0.5
0 0.0
0 0.0
0 0.0
0.46072270227808326
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5670,	 Acc1 = 0.3908,	 Acc2 = 0.4148

 ===== Epoch 154	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247  0.48113826  0.79371274
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -2.4327307e+00 -1.3105350e+00  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 1 1
train:	 Loss = 1.5222,	 Acc = 0.4258
2817 0.246
5494 0.459
2838 0.524
365 0.553
64 0.422
6 0.333
0 0.0
0 0.0
0.48340367286414965
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5540,	 Acc = 0.4260
365 0.323
1671 0.462
791 0.416
82 0.244
2 0.5
0 0.0
0 0.0
0 0.0
0.44069128043990574
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5911,	 Acc1 = 0.3897,	 Acc2 = 0.4135

 ===== Epoch 155	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.954909    2.145083   -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.9130303e-01  6.1011422e-01
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 6 6
train:	 Loss = 1.5156,	 Acc = 0.4272
2821 0.247
5498 0.458
2829 0.533
366 0.538
64 0.437
6 0.333
0 0.0
0 0.0
0.48510783978089694
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5610,	 Acc = 0.4311
365 0.332
1671 0.466
791 0.422
82 0.256
2 0.5
0 0.0
0 0.0
0 0.0
0.44540455616653574
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5868,	 Acc1 = 0.3939,	 Acc2 = 0.4185

 ===== Epoch 156	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913  1.3818583   2.7906063
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03 -3.4705597e-01  2.9340148e-01
 -1.4811044e-04 -1.1583284e-04] 4 6
train:	 Loss = 1.5155,	 Acc = 0.4273
2823 0.241
5492 0.46
2835 0.538
365 0.54
63 0.365
6 0.167
0 0.0
0 0.0
0.4875014267777651
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5309,	 Acc = 0.4480
365 0.329
1671 0.49
791 0.432
82 0.268
2 0.5
0 0.0
0 0.0
0 0.0
0.46504320502749413
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5922,	 Acc1 = 0.3854,	 Acc2 = 0.4083

 ===== Epoch 157	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.00014811 -0.00011583] 1 1
train:	 Loss = 1.5218,	 Acc = 0.4263
2822 0.248
5495 0.456
2833 0.536
364 0.522
64 0.359
6 0.167
0 0.0
0 0.0
0.4836795252225519
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5416,	 Acc = 0.4174
365 0.337
1671 0.443
791 0.415
82 0.28
2 0.5
0 0.0
0 0.0
0 0.0
0.4289080911233307
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5871,	 Acc1 = 0.3831,	 Acc2 = 0.4056

 ===== Epoch 158	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.00014811 -0.00011583] 3 1
train:	 Loss = 1.5157,	 Acc = 0.4277
2817 0.246
5497 0.459
2835 0.535
366 0.527
63 0.46
6 0.0
0 0.0
0 0.0
0.48591308315273185
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.5062,	 Acc = 0.4490
365 0.326
1671 0.479
791 0.459
82 0.28
2 0.5
0 0.0
0 0.0
0 0.0
0.4666142969363708
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5740,	 Acc1 = 0.3839,	 Acc2 = 0.4066

 ===== Epoch 159	 =====
[-0.36678174 -0.3791378   2.951369   -4.3411307   2.043368    0.9538076
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  1.0571920e+00  1.9518258e-02
  1.2271873e-01  7.8119277e-03  1.3177313e-03  2.7925514e-03
  2.3153160e+00  2.5269105e+00  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 2 2
train:	 Loss = 1.5166,	 Acc = 0.4287
2816 0.246
5500 0.459
2834 0.537
365 0.551
63 0.365
6 0.167
0 0.0
0 0.0
0.48722627737226276
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5362,	 Acc = 0.4280
365 0.326
1671 0.466
791 0.412
82 0.268
2 0.5
0 0.0
0 0.0
0 0.0
0.4426551453260016
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5837,	 Acc1 = 0.3817,	 Acc2 = 0.4038

 ===== Epoch 160	 =====
[ 6.043801    2.101682   -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.819449    1.6650147  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [-7.1166611e+00 -3.0438993e+00  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -2.6882392e-01 -3.6569476e-01
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 5 5
train:	 Loss = 1.5123,	 Acc = 0.4321
2816 0.249
5498 0.464
2834 0.537
366 0.568
64 0.359
6 0.333
0 0.0
0 0.0
0.4908759124087591
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.5156,	 Acc = 0.4531
365 0.326
1671 0.486
791 0.461
82 0.268
2 0.5
0 0.0
0 0.0
0 0.0
0.4713275726630008
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5835,	 Acc1 = 0.3875,	 Acc2 = 0.4108

 ===== Epoch 161	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247  1.2989573   2.1567426
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
  3.8010299e-02  2.3172960e-01  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 6 6
train:	 Loss = 1.5151,	 Acc = 0.4289
2824 0.243
5494 0.462
2835 0.537
361 0.543
64 0.391
6 0.0
0 0.0
0 0.0
0.4886986301369863
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5286,	 Acc = 0.4236
365 0.326
1671 0.443
791 0.44
82 0.305
2 0.5
0 0.0
0 0.0
0 0.0
0.4375490966221524
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5800,	 Acc1 = 0.3736,	 Acc2 = 0.3941

 ===== Epoch 162	 =====
[ 1.7589229   2.6613789  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
  1.5843179   2.9590912 ] [ 5.3337705e-01 -2.4888235e-01  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -6.0337089e-02 -4.2690516e-01] 5 5
train:	 Loss = 1.5207,	 Acc = 0.4286
2826 0.244
5491 0.457
2836 0.546
361 0.526
64 0.422
6 0.333
0 0.0
0 0.0
0.4881251427266499
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5349,	 Acc = 0.4352
365 0.332
1671 0.474
791 0.421
82 0.244
2 0.5
0 0.0
0 0.0
0 0.0
0.45011783189316573
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5911,	 Acc1 = 0.3817,	 Acc2 = 0.4038

 ===== Epoch 163	 =====
[-0.36678174 -0.3791378   1.8522711   1.7194195  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  1.1959356e-01  2.8070408e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 6 6
train:	 Loss = 1.5176,	 Acc = 0.4302
2820 0.246
5501 0.462
2830 0.535
363 0.556
64 0.422
6 0.333
0 0.0
0 0.0
0.4892743039707896
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 5
val:	 Loss = 1.5280,	 Acc = 0.4445
365 0.326
1671 0.485
791 0.432
82 0.268
2 0.5
0 0.0
0 0.0
0 0.0
0.4615082482325216
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5764,	 Acc1 = 0.3893,	 Acc2 = 0.4130

 ===== Epoch 164	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247  1.7424685   1.4296451
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346  0.00035502  0.00176777  0.08238301 -0.00225179
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.00014811 -0.00011583] 1 2
train:	 Loss = 1.5175,	 Acc = 0.4315
2819 0.249
5500 0.461
2830 0.545
365 0.532
64 0.375
6 0.167
0 0.0
0 0.0
0.4901312036508842
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.5525,	 Acc = 0.4559
365 0.334
1671 0.496
791 0.448
82 0.268
2 0.5
0 0.0
0 0.0
0 0.0
0.47329143754909664
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5954,	 Acc1 = 0.3912,	 Acc2 = 0.4153

 ===== Epoch 165	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.00014811 -0.00011583] 3 1
train:	 Loss = 1.5161,	 Acc = 0.4283
2824 0.248
5496 0.458
2832 0.538
363 0.534
63 0.413
6 0.167
0 0.0
0 0.0
0.4865296803652968
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5122,	 Acc = 0.4521
365 0.326
1671 0.487
791 0.454
82 0.28
2 0.5
0 0.0
0 0.0
0 0.0
0.4701492537313433
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5570,	 Acc1 = 0.3976,	 Acc2 = 0.4230

 ===== Epoch 166	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247  3.2542303   2.5036147
 -0.36420733 -0.3724829   2.9939384   1.7335652  -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -7.2738630e-01  4.0519007e-02  1.3177313e-03  2.7925514e-03
  5.1528561e-01  9.3110703e-02  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 3
train:	 Loss = 1.5153,	 Acc = 0.4309
2821 0.247
5495 0.461
2834 0.541
364 0.558
64 0.406
6 0.167
0 0.0
0 0.0
0.4901289512723953
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.4943,	 Acc = 0.4555
365 0.326
1671 0.486
791 0.465
82 0.317
2 0.5
0 0.0
0 0.0
0 0.0
0.47407698350353494
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5510,	 Acc1 = 0.4083,	 Acc2 = 0.4359

 ===== Epoch 167	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913  2.2590396   1.5160387
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03 -5.7163155e-01  1.2751028e-01
 -1.4811044e-04 -1.1583284e-04] 4 3
train:	 Loss = 1.5152,	 Acc = 0.4308
2826 0.248
5485 0.465
2838 0.536
365 0.534
64 0.375
6 0.167
0 0.0
0 0.0
0.4897236812057547
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.4900,	 Acc = 0.4686
365 0.334
1671 0.485
791 0.508
82 0.341
2 0.5
0 0.0
0 0.0
0 0.0
0.4878240377062058
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5549,	 Acc1 = 0.4007,	 Acc2 = 0.4267

 ===== Epoch 168	 =====
[ 2.2446039   2.6840694  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
  1.9525656   2.9564652 ] [-0.00345012 -0.01983334  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.00014811 -0.00011583] 0 0
train:	 Loss = 1.5126,	 Acc = 0.4293
2822 0.238
5496 0.463
2831 0.543
365 0.545
64 0.344
6 0.333
0 0.0
0 0.0
0.4909837936544168
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5099,	 Acc = 0.4363
365 0.332
1671 0.468
791 0.434
82 0.28
2 0.5
0 0.0
0 0.0
0 0.0
0.4512961508248233
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5661,	 Acc1 = 0.3955,	 Acc2 = 0.4205

 ===== Epoch 169	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829   2.3820055   2.1814358  -0.42398092 -0.42041838
  3.8105388   1.4281433 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -3.5667750e-01 -5.1054835e-01  9.4095006e-04  1.8351302e-03
 -9.1933817e-01 -4.5357952e-01] 5 5
train:	 Loss = 1.5134,	 Acc = 0.4336
2817 0.251
5499 0.468
2832 0.534
366 0.544
64 0.453
6 0.333
0 0.0
0 0.0
0.4923006729782138
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5085,	 Acc = 0.4462
365 0.337
1671 0.482
791 0.441
82 0.268
2 0.0
0 0.0
0 0.0
0 0.0
0.46190102120974075
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5793,	 Acc1 = 0.3813,	 Acc2 = 0.4033

 ===== Epoch 170	 =====
[ 2.409381    2.2025282  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
  2.0600448   2.4654064 ] [-4.0206885e-01 -4.7690649e-02  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
  3.0386856e-01 -1.1748291e-01] 3 2
train:	 Loss = 1.5178,	 Acc = 0.4265
2822 0.238
5500 0.462
2828 0.531
364 0.533
64 0.453
6 0.333
0 0.0
0 0.0
0.4873316594384844
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5208,	 Acc = 0.4366
365 0.334
1671 0.472
791 0.43
82 0.244
2 0.5
0 0.0
0 0.0
0 0.0
0.4512961508248233
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5756,	 Acc1 = 0.3910,	 Acc2 = 0.4150

 ===== Epoch 171	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
  2.198798    1.8824382 ] [ 2.8070700e+00  2.5585155e+00  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
  2.7836430e-01  6.3902564e-02] 4 2
train:	 Loss = 1.5166,	 Acc = 0.4271
2816 0.24
5498 0.459
2836 0.536
364 0.547
64 0.406
6 0.167
0 0.0
0 0.0
0.4871122262773723
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.4856,	 Acc = 0.4507
365 0.184
1671 0.492
791 0.499
82 0.341
2 0.0
0 0.0
0 0.0
0 0.0
0.4890023566378633
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5555,	 Acc1 = 0.3641,	 Acc2 = 0.4165

 ===== Epoch 172	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.00014811 -0.00011583] 0 6
train:	 Loss = 1.5255,	 Acc = 0.4273
2823 0.238
5501 0.46
2827 0.539
363 0.548
64 0.359
6 0.167
0 0.0
0 0.0
0.48818628010501086
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.5085,	 Acc = 0.4449
365 0.337
1671 0.463
791 0.468
82 0.329
2 0.5
0 0.0
0 0.0
0 0.0
0.4603299293008641
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5659,	 Acc1 = 0.3953,	 Acc2 = 0.4202

 ===== Epoch 173	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  3.5344117   2.0446036  -0.4012819  -0.40874913  3.1278985   2.861962
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -1.5951499e-01  1.5974084e-01
 -2.4058432e-03  2.0827476e-03  3.3594826e-01  1.3253729e-01
 -1.4811044e-04 -1.1583284e-04] 4 2
train:	 Loss = 1.5167,	 Acc = 0.4271
2816 0.239
5493 0.458
2840 0.538
365 0.548
64 0.422
6 0.167
0 0.0
0 0.0
0.4873403284671533
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.4853,	 Acc = 0.4614
365 0.334
1671 0.49
791 0.475
82 0.305
2 0.5
0 0.0
0 0.0
0 0.0
0.4795758051846033
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5495,	 Acc1 = 0.4002,	 Acc2 = 0.4262

 ===== Epoch 174	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247  2.156157    2.8304753
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346  0.00035502  0.00176777  0.01582706 -0.00979957
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.00014811 -0.00011583] 4 0
train:	 Loss = 1.5128,	 Acc = 0.4318
2822 0.242
5494 0.47
2834 0.536
364 0.519
64 0.406
6 0.167
0 0.0
0 0.0
0.49280986076238303
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.5238,	 Acc = 0.4332
365 0.329
1671 0.463
791 0.435
82 0.268
2 0.5
0 0.0
0 0.0
0 0.0
0.4481539670070699
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5620,	 Acc1 = 0.4009,	 Acc2 = 0.4269

 ===== Epoch 175	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
  1.9146833   3.5735712 ] [ 0.00073671  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
  0.15900186 -0.06946911] 3 2
train:	 Loss = 1.5163,	 Acc = 0.4286
2825 0.245
5492 0.465
2833 0.529
364 0.527
64 0.406
6 0.0
0 0.0
0 0.0
0.4878410777486014
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.5057,	 Acc = 0.4442
365 0.334
1671 0.47
791 0.454
82 0.305
2 0.5
0 0.0
0 0.0
0 0.0
0.45993715632364496
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5582,	 Acc1 = 0.3912,	 Acc2 = 0.4153

 ===== Epoch 176	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  0.9156938   3.4792252  -0.4012819  -0.40874913  2.2806005   3.11786
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04  8.6758293e-02  5.4187584e-01
 -2.4058432e-03  2.0827476e-03  1.4106689e-01  3.4367156e-01
 -1.4811044e-04 -1.1583284e-04] 6 6
train:	 Loss = 1.5160,	 Acc = 0.4279
2819 0.25
5500 0.459
2833 0.531
363 0.551
63 0.365
6 0.167
0 0.0
0 0.0
0.4852253280091272
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5290,	 Acc = 0.4373
365 0.334
1671 0.481
791 0.415
82 0.232
2 0.5
0 0.0
0 0.0
0 0.0
0.4520816967792616
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5878,	 Acc1 = 0.3804,	 Acc2 = 0.4023

 ===== Epoch 177	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
  1.9640186   3.570945  ] [ 2.6428423e+00  4.6075759e+00  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -5.4439731e+00 -8.0290899e+00] 3 1
train:	 Loss = 1.5173,	 Acc = 0.4315
2816 0.247
5500 0.465
2832 0.541
366 0.525
64 0.375
6 0.167
0 0.0
0 0.0
0.4907618613138686
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5223,	 Acc = 0.4521
365 0.332
1671 0.491
791 0.445
82 0.268
2 0.5
0 0.0
0 0.0
0 0.0
0.46936370777690495
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5701,	 Acc1 = 0.4033,	 Acc2 = 0.4299

 ===== Epoch 178	 =====
[-0.36678174 -0.3791378   1.0874152   1.0049459  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  2.8061661e-01 -1.6559401e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 5 5
train:	 Loss = 1.5146,	 Acc = 0.4327
2822 0.25
5498 0.469
2830 0.535
365 0.523
63 0.381
6 0.167
0 0.0
0 0.0
0.49155443962565626
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5665,	 Acc = 0.4356
365 0.332
1671 0.483
791 0.406
82 0.22
2 0.5
0 0.0
0 0.0
0 0.0
0.4505106048703849
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.6173,	 Acc1 = 0.3866,	 Acc2 = 0.4098

 ===== Epoch 179	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.00014811 -0.00011583] 1 1
train:	 Loss = 1.5175,	 Acc = 0.4328
2822 0.243
5493 0.465
2834 0.545
365 0.553
64 0.391
6 0.333
0 0.0
0 0.0
0.4939511527048619
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5248,	 Acc = 0.4315
365 0.334
1671 0.468
791 0.416
82 0.268
2 0.5
0 0.0
0 0.0
0 0.0
0.44540455616653574
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5793,	 Acc1 = 0.3879,	 Acc2 = 0.4113

 ===== Epoch 180	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.00014811 -0.00011583] 2 1
train:	 Loss = 1.5123,	 Acc = 0.4317
2821 0.248
5493 0.468
2835 0.533
365 0.529
64 0.359
6 0.167
0 0.0
0 0.0
0.4909277644642246
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.4759,	 Acc = 0.4620
365 0.334
1671 0.493
791 0.468
82 0.354
2 0.5
0 0.0
0 0.0
0 0.0
0.48036135113904166
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5433,	 Acc1 = 0.4029,	 Acc2 = 0.4294

 ===== Epoch 181	 =====
[-0.36678174 -0.3791378   3.9158766   1.7625147  -0.4410947  -0.36475152
  3.293843    2.7702875  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  8.5744351e-01 -2.0870234e-01
 -3.0847333e-04  2.6414116e-04  3.7574604e-01 -6.1817676e-01
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 5 5
train:	 Loss = 1.5186,	 Acc = 0.4296
2825 0.239
5492 0.461
2833 0.541
364 0.56
64 0.453
6 0.167
0 0.0
0 0.0
0.4911519579860715
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.5248,	 Acc = 0.4620
365 0.337
1671 0.493
791 0.472
82 0.293
2 0.5
0 0.0
0 0.0
0 0.0
0.4799685781618225
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5844,	 Acc1 = 0.3963,	 Acc2 = 0.4215

 ===== Epoch 182	 =====
[ 1.7351868   2.3386707  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
  1.5279359   2.6203394 ] [ 1.1395479e-01 -1.9626299e-01  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
  2.3347409e-01 -2.9886839e-01] 5 5
train:	 Loss = 1.5143,	 Acc = 0.4303
2820 0.248
5497 0.464
2833 0.532
364 0.549
64 0.406
6 0.333
0 0.0
0 0.0
0.48893199452304886
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.5010,	 Acc = 0.4514
365 0.326
1671 0.485
791 0.458
82 0.256
2 0.5
0 0.0
0 0.0
0 0.0
0.46936370777690495
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5494,	 Acc1 = 0.4083,	 Acc2 = 0.4359

 ===== Epoch 183	 =====
[ 2.2914917   2.5554905  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
  1.8217416   2.8566778 ] [-0.11567792  0.05135757  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
  0.22633089  0.01588877] 4 4
train:	 Loss = 1.5179,	 Acc = 0.4273
2822 0.243
5501 0.46
2827 0.534
364 0.544
64 0.328
6 0.167
0 0.0
0 0.0
0.48653275507874916
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.5020,	 Acc = 0.4565
365 0.329
1671 0.487
791 0.465
82 0.329
2 0.5
0 0.0
0 0.0
0 0.0
0.4748625294579733
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5539,	 Acc1 = 0.4095,	 Acc2 = 0.4374

 ===== Epoch 184	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  6.9396420e+00  5.3354883e+00
 -1.4811044e-04 -1.1583284e-04] 2 1
train:	 Loss = 1.5146,	 Acc = 0.4294
2817 0.25
5498 0.463
2835 0.53
364 0.541
64 0.422
6 0.167
0 0.0
0 0.0
0.48716778829702295
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.5112,	 Acc = 0.4555
365 0.334
1671 0.486
791 0.464
82 0.293
2 0.5
0 0.0
0 0.0
0 0.0
0.47289866457187746
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5668,	 Acc1 = 0.3924,	 Acc2 = 0.4167

 ===== Epoch 185	 =====
[ 0.7211387   0.972203   -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829   3.2789707   3.0722005  -0.42398092 -0.42041838
  1.2477866   1.320478  ] [ 4.9810377e-01  4.0112159e-01  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
  8.3306693e-02  2.0827476e-03  9.4095006e-04  1.8351302e-03
  8.4526457e-02  3.7228238e-02] 2 2
train:	 Loss = 1.5101,	 Acc = 0.4314
2818 0.247
5492 0.465
2838 0.533
366 0.568
64 0.375
6 0.333
0 0.0
0 0.0
0.49075975359342916
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.4775,	 Acc = 0.4665
365 0.332
1671 0.499
791 0.474
82 0.329
2 0.5
0 0.0
0 0.0
0 0.0
0.48586017282010996
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5564,	 Acc1 = 0.3967,	 Acc2 = 0.4220

 ===== Epoch 186	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.00014811 -0.00011583] 6 1
train:	 Loss = 1.5148,	 Acc = 0.4286
2822 0.245
5498 0.46
2832 0.537
362 0.544
64 0.406
6 0.333
0 0.0
0 0.0
0.48778817621547593
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.4990,	 Acc = 0.4672
365 0.334
1671 0.494
791 0.483
82 0.354
2 0.5
0 0.0
0 0.0
0 0.0
0.48625294579732914
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5804,	 Acc1 = 0.3827,	 Acc2 = 0.4051

 ===== Epoch 187	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247  1.9256082   2.7993455
 -0.36420733 -0.3724829   0.6649224   1.9127135  -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.36706250e-04  1.83346274e-03  3.55022145e-04  1.76776643e-03
  2.55830646e-01  1.05933145e-01  1.31773134e-03  2.79255142e-03
  9.58728269e-02  8.35288167e-02  9.40950064e-04  1.83513016e-03
 -1.48110441e-04 -1.15832838e-04] 2 4
train:	 Loss = 1.5161,	 Acc = 0.4315
2818 0.238
5500 0.461
2831 0.554
365 0.54
64 0.391
6 0.0
0 0.0
0 0.0
0.4934976043805613
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5443,	 Acc = 0.4328
365 0.332
1671 0.471
791 0.418
82 0.244
2 0.5
0 0.0
0 0.0
0 0.0
0.4473684210526316
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.6008,	 Acc1 = 0.3936,	 Acc2 = 0.4182

 ===== Epoch 188	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247  0.40466994  0.88487786
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -2.2310426e+00 -1.4136878e+00  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 0 1
train:	 Loss = 1.5139,	 Acc = 0.4356
2820 0.247
5496 0.469
2835 0.545
363 0.556
64 0.391
6 0.167
0 0.0
0 0.0
0.49623459607485165
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5342,	 Acc = 0.4335
365 0.334
1671 0.469
791 0.42
82 0.28
2 0.5
0 0.0
0 0.0
0 0.0
0.44776119402985076
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5811,	 Acc1 = 0.3924,	 Acc2 = 0.4167

 ===== Epoch 189	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247  3.1563513   2.1278367
 -0.36420733 -0.3724829   2.4519727   1.5146062  -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.6535931e-01 -1.6075532e-01  1.3177313e-03  2.7925514e-03
  3.1879880e-02 -2.9016489e-01  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 3 5
train:	 Loss = 1.5143,	 Acc = 0.4328
2822 0.25
5497 0.464
2830 0.54
365 0.545
64 0.453
6 0.0
0 0.0
0 0.0
0.4916685688199041
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.4988,	 Acc = 0.4559
365 0.334
1671 0.48
791 0.477
82 0.305
2 0.5
0 0.0
0 0.0
0 0.0
0.47329143754909664
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5578,	 Acc1 = 0.3963,	 Acc2 = 0.4215

 ===== Epoch 190	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.00014811 -0.00011583] 1 1
train:	 Loss = 1.5170,	 Acc = 0.4257
2823 0.242
5495 0.459
2831 0.534
365 0.526
64 0.375
6 0.0
0 0.0
0 0.0
0.48499029791119735
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.5015,	 Acc = 0.4507
365 0.326
1671 0.477
791 0.468
82 0.305
2 0.5
0 0.0
0 0.0
0 0.0
0.4685781618224666
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5568,	 Acc1 = 0.3986,	 Acc2 = 0.4242

 ===== Epoch 191	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829   1.8417611   1.2060732  -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -1.1211662e-01  4.4764063e-01  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 6 6
train:	 Loss = 1.5157,	 Acc = 0.4295
2818 0.249
5497 0.459
2834 0.536
365 0.559
64 0.406
6 0.167
0 0.0
0 0.0
0.48756559434177504
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.4985,	 Acc = 0.4565
365 0.323
1671 0.493
791 0.46
82 0.28
2 0.5
0 0.0
0 0.0
0 0.0
0.4756480754124116
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5518,	 Acc1 = 0.4066,	 Acc2 = 0.4339

 ===== Epoch 192	 =====
[-0.36678174 -0.3791378   3.1868038   2.647101    2.728516    0.90266615
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -1.0155914e-01 -1.0473518e-01
 -1.7375609e-01 -1.6578716e-01  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 5 5
train:	 Loss = 1.5148,	 Acc = 0.4307
2823 0.249
5497 0.464
2831 0.535
363 0.532
64 0.406
6 0.0
0 0.0
0 0.0
0.4893277023170871
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.4889,	 Acc = 0.4569
365 0.337
1671 0.497
791 0.446
82 0.268
2 0.5
0 0.0
0 0.0
0 0.0
0.47407698350353494
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5626,	 Acc1 = 0.3949,	 Acc2 = 0.4197

 ===== Epoch 193	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829   1.1317564   1.4747956  -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255  0.09016383  0.0931107   0.00094095  0.00183513
 -0.00014811 -0.00011583] 4 4
train:	 Loss = 1.5115,	 Acc = 0.4296
2817 0.245
5501 0.46
2831 0.544
365 0.521
64 0.359
6 0.333
0 0.0
0 0.0
0.48899281396144634
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.5191,	 Acc = 0.4435
365 0.332
1671 0.479
791 0.436
82 0.293
2 0.5
0 0.0
0 0.0
0 0.0
0.4595443833464258
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5577,	 Acc1 = 0.4075,	 Acc2 = 0.4349

 ===== Epoch 194	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247  0.36528933  0.91156036
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
  1.4893897e-01  1.1851279e-01  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 6 6
train:	 Loss = 1.5152,	 Acc = 0.4321
2820 0.25
5495 0.466
2835 0.536
364 0.53
64 0.422
6 0.167
0 0.0
0 0.0
0.4905294386125057
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5324,	 Acc = 0.4232
365 0.334
1671 0.453
791 0.42
82 0.244
2 0.5
0 0.0
0 0.0
0 0.0
0.43597800471327575
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5884,	 Acc1 = 0.3819,	 Acc2 = 0.4041

 ===== Epoch 195	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829   1.1300367   2.7014632  -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
  1.5187462e-01  3.5619363e-02  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 2
train:	 Loss = 1.5119,	 Acc = 0.4316
2822 0.245
5500 0.465
2829 0.539
363 0.543
64 0.406
6 0.333
0 0.0
0 0.0
0.4918968272083999
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5489,	 Acc = 0.4157
365 0.326
1671 0.449
791 0.401
82 0.28
2 0.5
0 0.0
0 0.0
0 0.0
0.42851531814611155
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5886,	 Acc1 = 0.3763,	 Acc2 = 0.3974

 ===== Epoch 196	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829   0.7532429   1.7733759  -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
  6.4718766e+00  3.6684883e+00  1.3177313e-03  2.7925514e-03
  7.9876356e-02 -5.0096643e-01  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 5 5
train:	 Loss = 1.5103,	 Acc = 0.4340
2819 0.245
5492 0.471
2840 0.539
363 0.532
64 0.391
6 0.333
0 0.0
0 0.0
0.4949229891614375
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5481,	 Acc = 0.4469
365 0.337
1671 0.485
791 0.434
82 0.28
2 0.5
0 0.0
0 0.0
0 0.0
0.4626865671641791
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5742,	 Acc1 = 0.4056,	 Acc2 = 0.4327

 ===== Epoch 197	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.00014811 -0.00011583] 6 1
train:	 Loss = 1.5117,	 Acc = 0.4347
2821 0.25
5491 0.466
2838 0.546
365 0.534
63 0.397
6 0.167
0 0.0
0 0.0
0.49412301723154173
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.4929,	 Acc = 0.4583
365 0.334
1671 0.492
791 0.463
82 0.28
2 0.5
0 0.0
0 0.0
0 0.0
0.4760408483896308
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5429,	 Acc1 = 0.4038,	 Acc2 = 0.4304

 ===== Epoch 198	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247  2.559523    1.58974
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -1.6468142e-01 -1.4831432e-02  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 3 3
train:	 Loss = 1.5125,	 Acc = 0.4312
2821 0.241
5496 0.465
2832 0.54
366 0.566
63 0.381
6 0.167
0 0.0
0 0.0
0.49252539084788316
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.5128,	 Acc = 0.4397
365 0.329
1671 0.463
791 0.456
82 0.293
2 0.5
0 0.0
0 0.0
0 0.0
0.4556166535742341
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5630,	 Acc1 = 0.3850,	 Acc2 = 0.4078

 ===== Epoch 199	 =====
[-0.36678174 -0.3791378   1.7449222   2.2025397  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346  0.05436963  0.03219718 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.00014811 -0.00011583] 0 0
train:	 Loss = 1.5108,	 Acc = 0.4335
2820 0.248
5497 0.466
2832 0.545
365 0.515
64 0.437
6 0.0
0 0.0
0 0.0
0.4932679141944318
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.5049,	 Acc = 0.4576
365 0.334
1671 0.486
791 0.468
82 0.329
2 0.5
0 0.0
0 0.0
0 0.0
0.4752553024351925
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5591,	 Acc1 = 0.4005,	 Acc2 = 0.4264

 ===== Epoch 200	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.00014811 -0.00011583] 3 1
train:	 Loss = 1.5121,	 Acc = 0.4303
2817 0.243
5499 0.46
2832 0.547
366 0.538
64 0.391
6 0.333
0 0.0
0 0.0
0.49047564731379034
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.4845,	 Acc = 0.4675
365 0.332
1671 0.5
791 0.479
82 0.305
2 0.5
0 0.0
0 0.0
0 0.0
0.4870384917517675
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5586,	 Acc1 = 0.3984,	 Acc2 = 0.4240

 ===== Epoch 201	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913  1.1904597   2.709408
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  1.5777150e-01 -8.8650972e-02
 -1.4811044e-04 -1.1583284e-04] 4 5
train:	 Loss = 1.5208,	 Acc = 0.4245
2821 0.241
5497 0.456
2830 0.534
366 0.525
64 0.422
6 0.167
0 0.0
0 0.0
0.48362432956749973
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.4964,	 Acc = 0.4610
365 0.326
1671 0.491
791 0.47
82 0.366
2 0.5
0 0.0
0 0.0
0 0.0
0.48036135113904166
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5638,	 Acc1 = 0.3901,	 Acc2 = 0.4140

 ===== Epoch 202	 =====
[-0.36678174 -0.3791378   2.674866    2.356775   -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -3.5735744e-01  6.8393359e-03
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 2 3
train:	 Loss = 1.5130,	 Acc = 0.4283
2825 0.242
5495 0.464
2828 0.534
366 0.536
64 0.359
6 0.333
0 0.0
0 0.0
0.4884119191688549
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.5011,	 Acc = 0.4535
365 0.329
1671 0.49
791 0.45
82 0.305
2 0.5
0 0.0
0 0.0
0 0.0
0.4713275726630008
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5599,	 Acc1 = 0.4038,	 Acc2 = 0.4304

 ===== Epoch 203	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.8722847   1.067721   -0.4012819  -0.40874913  3.000048    2.1311774
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -7.2336259e+00 -3.5183079e+00
 -2.4058432e-03  2.0827476e-03 -8.3239901e-01 -3.4000129e-01
 -1.4811044e-04 -1.1583284e-04] 5 5
train:	 Loss = 1.5111,	 Acc = 0.4343
2817 0.25
5497 0.464
2836 0.542
364 0.571
64 0.406
6 0.333
0 0.0
0 0.0
0.49355537812250483
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5334,	 Acc = 0.4301
365 0.332
1671 0.467
791 0.416
82 0.244
2 0.5
0 0.0
0 0.0
0 0.0
0.44422623723487825
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5900,	 Acc1 = 0.3875,	 Acc2 = 0.4108

 ===== Epoch 204	 =====
[ 3.0052338   1.499125   -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829   2.2076585   3.3608284  -0.42398092 -0.42041838
  2.5855455   1.6723596 ] [ 3.5056254e-01 -1.9833336e-02  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.8582388e-01 -4.1035756e-02  9.4095006e-04  1.8351302e-03
 -2.3377348e-01 -1.0785567e-02] 3 3
train:	 Loss = 1.5122,	 Acc = 0.4328
2814 0.248
5497 0.462
2837 0.543
366 0.557
64 0.437
6 0.333
0 0.0
0 0.0
0.4919042189281642
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5048,	 Acc = 0.4576
365 0.332
1671 0.503
791 0.44
82 0.256
2 0.5
0 0.0
0 0.0
0 0.0
0.4756480754124116
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5475,	 Acc1 = 0.4056,	 Acc2 = 0.4327

 ===== Epoch 205	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.00014811 -0.00011583] 4 1
train:	 Loss = 1.5135,	 Acc = 0.4334
2818 0.249
5499 0.469
2835 0.539
364 0.511
63 0.413
5 0.0
0 0.0
0 0.0
0.4926990645676477
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.4798,	 Acc = 0.4641
365 0.329
1671 0.484
791 0.496
82 0.366
2 0.5
0 0.0
0 0.0
0 0.0
0.483503534956795
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5519,	 Acc1 = 0.3916,	 Acc2 = 0.4158

 ===== Epoch 206	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913  1.8414423   1.8113053
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275 -0.07144282 -0.00319188
 -0.00014811 -0.00011583] 2 0
train:	 Loss = 1.5167,	 Acc = 0.4304
2817 0.247
5504 0.463
2829 0.538
364 0.525
64 0.375
6 0.333
0 0.0
0 0.0
0.48922094216949924
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.5010,	 Acc = 0.4579
365 0.337
1671 0.491
791 0.456
82 0.329
2 0.5
0 0.0
0 0.0
0 0.0
0.4752553024351925
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5583,	 Acc1 = 0.4071,	 Acc2 = 0.4344

 ===== Epoch 207	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829   1.3927039   2.942816   -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00926299 -0.0027082   0.00094095  0.00183513
 -0.00014811 -0.00011583] 0 0
train:	 Loss = 1.5166,	 Acc = 0.4319
2822 0.248
5497 0.468
2834 0.535
364 0.505
62 0.403
5 0.4
0 0.0
0 0.0
0.4909837936544168
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.5039,	 Acc = 0.4552
365 0.323
1671 0.485
791 0.469
82 0.305
2 0.5
0 0.0
0 0.0
0 0.0
0.47407698350353494
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5758,	 Acc1 = 0.3852,	 Acc2 = 0.4081

 ===== Epoch 208	 =====
[-0.36678174 -0.3791378   1.8229944   2.461111   -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  2.1538952e-01 -2.4420334e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 5 5
train:	 Loss = 1.5183,	 Acc = 0.4273
2819 0.24
5497 0.462
2832 0.535
366 0.527
64 0.406
6 0.333
0 0.0
0 0.0
0.4875071306332002
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.5120,	 Acc = 0.4469
365 0.329
1671 0.483
791 0.445
82 0.256
2 0.5
0 0.0
0 0.0
0 0.0
0.4638648860958366
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5576,	 Acc1 = 0.4083,	 Acc2 = 0.4359

 ===== Epoch 209	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913  1.2895641   1.8605163
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03 -2.0971556e-01 -1.8919107e-01
 -1.4811044e-04 -1.1583284e-04] 2 5
train:	 Loss = 1.5078,	 Acc = 0.4303
2816 0.248
5502 0.464
2833 0.535
363 0.534
64 0.391
6 0.333
0 0.0
0 0.0
0.48905109489051096
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5248,	 Acc = 0.4349
365 0.334
1671 0.469
791 0.429
82 0.256
2 0.5
0 0.0
0 0.0
0 0.0
0.4493322859387274
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5779,	 Acc1 = 0.3941,	 Acc2 = 0.4187

 ===== Epoch 210	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913  2.716354    1.5849341
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275 -0.00648332  0.00183513
 -0.00014811 -0.00011583] 0 0
train:	 Loss = 1.5129,	 Acc = 0.4277
2822 0.242
5497 0.46
2833 0.532
363 0.581
63 0.349
6 0.333
0 0.0
0 0.0
0.4873316594384844
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.5073,	 Acc = 0.4373
365 0.326
1671 0.47
791 0.436
82 0.28
2 0.5
0 0.0
0 0.0
0 0.0
0.45326001571091906
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5616,	 Acc1 = 0.3875,	 Acc2 = 0.4108

 ===== Epoch 211	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829   1.1856667   1.2160258  -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -8.0116615e-02  4.6680441e-01  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 6 6
train:	 Loss = 1.5158,	 Acc = 0.4324
2823 0.247
5494 0.466
2833 0.542
365 0.518
63 0.397
6 0.167
0 0.0
0 0.0
0.49218125784727773
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5325,	 Acc = 0.4308
365 0.334
1671 0.464
791 0.425
82 0.244
2 0.5
0 0.0
0 0.0
0 0.0
0.44461901021209743
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5868,	 Acc1 = 0.3844,	 Acc2 = 0.4071

 ===== Epoch 212	 =====
[ 3.0128374   1.7991427  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829   1.9426974   3.3483872  -0.42398092 -0.42041838
  2.8709807   1.9953554 ] [ 0.00023043 -0.00126179  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.01268979 -0.02187198  0.00094095  0.00183513
 -0.00014811 -0.0054507 ] 0 0
train:	 Loss = 1.5109,	 Acc = 0.4283
2824 0.243
5494 0.467
2833 0.526
363 0.526
64 0.422
6 0.333
0 0.0
0 0.0
0.48812785388127855
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.5238,	 Acc = 0.4260
365 0.178
1671 0.479
791 0.444
82 0.293
2 0.0
0 0.0
0 0.0
0 0.0
0.4615082482325216
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5794,	 Acc1 = 0.3648,	 Acc2 = 0.4172

 ===== Epoch 213	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247  1.2828985   1.758729
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -2.8542539e-02 -2.6894024e-01  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 5 5
train:	 Loss = 1.5134,	 Acc = 0.4279
2822 0.245
5491 0.462
2836 0.534
365 0.529
64 0.375
6 0.0
0 0.0
0 0.0
0.4869892718557407
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.5168,	 Acc = 0.4507
365 0.332
1671 0.489
791 0.44
82 0.305
2 0.5
0 0.0
0 0.0
0 0.0
0.4677926158680283
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 1
Testing:	 Loss = 1.5589,	 Acc1 = 0.4073,	 Acc2 = 0.4346

 ===== Epoch 214	 =====
[ 0.5493517   2.2302608  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
  0.5685559   2.520552  ] [ 0.17258227  0.09778643  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
  0.13349761  0.03722824] 2 4
train:	 Loss = 1.5086,	 Acc = 0.4357
2817 0.251
5496 0.47
2839 0.542
362 0.522
64 0.406
6 0.333
0 0.0
0 0.0
0.4951522755788753
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.5007,	 Acc = 0.4497
365 0.332
1671 0.482
791 0.453
82 0.28
2 0.5
0 0.0
0 0.0
0 0.0
0.4666142969363708
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5615,	 Acc1 = 0.4035,	 Acc2 = 0.4302

 ===== Epoch 215	 =====
[-0.36678174 -0.3791378   2.0470428   1.0820637  -0.4410947  -0.36475152
  3.1842377   3.41503    -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  7.6266187e-01  1.3362856e-01
 -3.0847333e-04  2.6414116e-04  3.6695224e-01  4.9410895e-01
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 6 6
train:	 Loss = 1.5127,	 Acc = 0.4327
2815 0.25
5503 0.46
2833 0.543
364 0.563
64 0.437
5 0.4
0 0.0
0 0.0
0.49116204812407344
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.4928,	 Acc = 0.4490
365 0.332
1671 0.472
791 0.468
82 0.317
2 0.5
0 0.0
0 0.0
0 0.0
0.46582875098193244
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5610,	 Acc1 = 0.3961,	 Acc2 = 0.4212

 ===== Epoch 216	 =====
[ 0.8491963   3.057201   -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [-0.00402321  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.00014811 -0.00011583] 0 0
train:	 Loss = 1.5122,	 Acc = 0.4307
2816 0.249
5500 0.459
2834 0.542
364 0.552
64 0.375
6 0.333
0 0.0
0 0.0
0.48916514598540145
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.5126,	 Acc = 0.4541
365 0.326
1671 0.495
791 0.441
82 0.317
2 0.5
0 0.0
0 0.0
0 0.0
0.4725058915946583
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5687,	 Acc1 = 0.3951,	 Acc2 = 0.4200

 ===== Epoch 217	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829   1.4724203   2.6019363  -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.36706250e-04  1.83346274e-03  3.55022145e-04  1.76776643e-03
 -3.08473333e-04  2.64141156e-04  1.31773134e-03  2.79255142e-03
 -1.20114855e-01  1.84138656e-01  9.40950064e-04  1.83513016e-03
 -1.48110441e-04 -1.15832838e-04] 4 6
train:	 Loss = 1.5124,	 Acc = 0.4278
2821 0.244
5494 0.462
2834 0.529
365 0.564
64 0.406
6 0.167
0 0.0
0 0.0
0.4870478146753395
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.4872,	 Acc = 0.4648
365 0.332
1671 0.484
791 0.496
82 0.378
2 0.5
0 0.0
0 0.0
0 0.0
0.4838963079340141
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5445,	 Acc1 = 0.4102,	 Acc2 = 0.4381

 ===== Epoch 218	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
  1.9587327   3.563067  ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -5.4317303e+00 -8.0130854e+00] 2 1
train:	 Loss = 1.5126,	 Acc = 0.4291
2816 0.24
5498 0.464
2836 0.535
365 0.54
63 0.46
6 0.333
0 0.0
0 0.0
0.48996350364963503
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.5426,	 Acc = 0.4383
365 0.34
1671 0.471
791 0.434
82 0.256
2 0.5
0 0.0
0 0.0
0 0.0
0.45247446975648076
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5906,	 Acc1 = 0.3990,	 Acc2 = 0.4247

 ===== Epoch 219	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.00014811 -0.00011583] 6 1
train:	 Loss = 1.5169,	 Acc = 0.4325
2821 0.249
5491 0.465
2838 0.542
364 0.533
64 0.344
6 0.333
0 0.0
0 0.0
0.4914983453155312
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.5298,	 Acc = 0.4421
365 0.337
1671 0.477
791 0.435
82 0.268
2 0.5
0 0.0
0 0.0
0 0.0
0.45718774548311075
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 1
Testing:	 Loss = 1.5629,	 Acc1 = 0.4027,	 Acc2 = 0.4292

 ===== Epoch 220	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.00014811 -0.00011583] 5 1
train:	 Loss = 1.5146,	 Acc = 0.4284
2817 0.241
5501 0.46
2833 0.54
364 0.541
64 0.375
5 0.4
0 0.0
0 0.0
0.48865062164936696
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.5143,	 Acc = 0.4397
365 0.337
1671 0.458
791 0.459
82 0.329
2 0.5
0 0.0
0 0.0
0 0.0
0.4544383346425766
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5827,	 Acc1 = 0.3854,	 Acc2 = 0.4083

 ===== Epoch 221	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829   1.5091264   2.186412   -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
  1.1304913e-02  1.8413866e-01  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 3 6
train:	 Loss = 1.5136,	 Acc = 0.4305
2815 0.247
5500 0.463
2837 0.535
363 0.54
63 0.429
6 0.333
0 0.0
0 0.0
0.48933743870452734
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.5131,	 Acc = 0.4589
365 0.334
1671 0.496
791 0.455
82 0.293
2 0.5
0 0.0
0 0.0
0 0.0
0.47682639434406915
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5533,	 Acc1 = 0.4176,	 Acc2 = 0.4471

 ===== Epoch 222	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.00014811 -0.00011583] 6 1
train:	 Loss = 1.5102,	 Acc = 0.4333
2821 0.25
5500 0.469
2832 0.532
362 0.544
63 0.444
6 0.333
0 0.0
0 0.0
0.4924112746776218
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.4968,	 Acc = 0.4583
365 0.34
1671 0.487
791 0.468
82 0.305
2 0.5
0 0.0
0 0.0
0 0.0
0.4752553024351925
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5490,	 Acc1 = 0.4027,	 Acc2 = 0.4292

 ===== Epoch 223	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.00014811 -0.00011583] 0 1
train:	 Loss = 1.5079,	 Acc = 0.4325
2816 0.245
5500 0.466
2833 0.543
365 0.526
64 0.406
6 0.167
0 0.0
0 0.0
0.49258667883211676
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.5004,	 Acc = 0.4586
365 0.329
1671 0.487
791 0.472
82 0.329
2 0.5
0 0.0
0 0.0
0 0.0
0.4772191673212883
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5541,	 Acc1 = 0.4005,	 Acc2 = 0.4264

 ===== Epoch 224	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247  2.7954252   1.8120939
 -0.36420733 -0.3724829   2.8511353   1.0692239  -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
  7.7920669e-01  2.6414116e-04  1.3177313e-03  2.7925514e-03
  2.7758184e-01 -7.4991430e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 3 0
train:	 Loss = 1.5091,	 Acc = 0.4318
2822 0.248
5497 0.463
2832 0.54
363 0.537
64 0.437
6 0.167
0 0.0
0 0.0
0.4908696644601689
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.5195,	 Acc = 0.4497
365 0.329
1671 0.485
791 0.449
82 0.28
2 0.5
0 0.0
0 0.0
0 0.0
0.4670070699135899
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5603,	 Acc1 = 0.4050,	 Acc2 = 0.4319

 ===== Epoch 225	 =====
[-0.36678174 -0.3791378   2.9908113   1.8918004  -0.4410947  -0.36475152
  2.2809787   3.7890365  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -9.3402609e-02 -5.1483709e-02
 -3.0847333e-04  2.6414116e-04 -3.5929060e-01 -1.3368422e-01
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 3 3
train:	 Loss = 1.5126,	 Acc = 0.4277
2826 0.237
5487 0.462
2837 0.536
364 0.549
64 0.422
6 0.333
0 0.0
0 0.0
0.4892669559260105
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.4987,	 Acc = 0.4665
365 0.334
1671 0.501
791 0.466
82 0.341
2 0.5
0 0.0
0 0.0
0 0.0
0.48546739984289083
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5635,	 Acc1 = 0.4002,	 Acc2 = 0.4262

 ===== Epoch 226	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829   2.446812    2.4526463  -0.42398092 -0.42041838
  3.3678505   1.5909542 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -5.9548710e-02  3.2307607e-01  9.4095006e-04  1.8351302e-03
 -3.5109547e-01  3.9466432e-01] 6 6
train:	 Loss = 1.5135,	 Acc = 0.4270
2815 0.241
5500 0.458
2838 0.537
361 0.546
64 0.391
6 0.333
0 0.0
0 0.0
0.48671456266392976
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.5531,	 Acc = 0.4352
365 0.34
1671 0.464
791 0.436
82 0.256
2 0.5
0 0.0
0 0.0
0 0.0
0.44893951296150825
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.6003,	 Acc1 = 0.3912,	 Acc2 = 0.4153

 ===== Epoch 227	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.00014811 -0.00011583] 1 1
train:	 Loss = 1.5117,	 Acc = 0.4322
2814 0.247
5502 0.463
2833 0.544
365 0.553
64 0.359
6 0.333
0 0.0
0 0.0
0.4917901938426454
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.4840,	 Acc = 0.4562
365 0.332
1671 0.479
791 0.477
82 0.341
2 0.5
0 0.0
0 0.0
0 0.0
0.47407698350353494
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 1
Testing:	 Loss = 1.5477,	 Acc1 = 0.3998,	 Acc2 = 0.4257

 ===== Epoch 228	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  1.4091977   1.2603065  -0.4012819  -0.40874913  1.6837084   3.2753353
 -0.38645998 -0.3811587 ] [ 7.36706250e-04  1.83346274e-03  3.55022145e-04  1.76776643e-03
 -3.08473333e-04  2.64141156e-04  1.10630535e-01 -1.47331908e-01
 -2.40584323e-03  2.08274764e-03 -2.28273377e-01  2.69701593e-02
 -1.48110441e-04 -1.15832838e-04] 1 3
train:	 Loss = 1.5109,	 Acc = 0.4328
2818 0.251
5498 0.464
2835 0.542
364 0.541
63 0.349
6 0.333
0 0.0
0 0.0
0.4914442162902122
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5289,	 Acc = 0.4445
365 0.334
1671 0.48
791 0.439
82 0.268
2 0.5
0 0.0
0 0.0
0 0.0
0.4603299293008641
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 1
Testing:	 Loss = 1.5568,	 Acc1 = 0.4091,	 Acc2 = 0.4369

 ===== Epoch 229	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247  3.5490124   2.4057791
 -0.36420733 -0.3724829   4.282609    1.7559587  -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346  0.00035502  0.00176777 -0.00030847 -0.00225179
  0.00131773  0.00279255 -0.00240584 -0.0027082   0.00094095  0.00183513
 -0.00014811 -0.00011583] 0 0
train:	 Loss = 1.5108,	 Acc = 0.4325
2821 0.238
5493 0.466
2837 0.544
364 0.574
63 0.397
6 0.333
0 0.0
0 0.0
0.495264178934155
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5771,	 Acc = 0.4019
365 0.326
1671 0.424
791 0.402
82 0.293
2 0.5
0 0.0
0 0.0
0 0.0
0.41280439905734484
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 4
Testing:	 Loss = 1.5967,	 Acc1 = 0.3850,	 Acc2 = 0.4078

 ===== Epoch 230	 =====
[ 2.7966683   3.268978   -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
  2.5965574   3.5315552 ] [ 5.7460535e-03  1.1119233e-02  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -4.3168914e-01  1.5888767e-02] 0 0
train:	 Loss = 1.5168,	 Acc = 0.4273
2819 0.24
5500 0.459
2831 0.537
364 0.558
64 0.437
6 0.167
0 0.0
0 0.0
0.4876212207644039
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.5034,	 Acc = 0.4665
365 0.329
1671 0.493
791 0.487
82 0.354
2 0.5
0 0.0
0 0.0
0 0.0
0.48625294579732914
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 1
Testing:	 Loss = 1.5464,	 Acc1 = 0.4081,	 Acc2 = 0.4356

 ===== Epoch 231	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913  0.780428    3.1572292
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275 -0.08257923  0.10740225
 -0.00014811 -0.00011583] 1 1
train:	 Loss = 1.5119,	 Acc = 0.4316
2826 0.247
5490 0.461
2833 0.545
365 0.537
64 0.469
6 0.333
0 0.0
0 0.0
0.4912080383649235
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 5
val:	 Loss = 1.5143,	 Acc = 0.4438
365 0.337
1671 0.481
791 0.429
82 0.317
2 0.0
0 0.0
0 0.0
0 0.0
0.4591516103692066
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5695,	 Acc1 = 0.4029,	 Acc2 = 0.4294

 ===== Epoch 232	 =====
[ 2.0495367   2.1949646  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [-2.5483951e-01 -5.3881161e-02  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 3 3
train:	 Loss = 1.5129,	 Acc = 0.4316
2822 0.241
5495 0.467
2836 0.537
362 0.544
63 0.46
6 0.167
0 0.0
0 0.0
0.4929239899566309
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.5157,	 Acc = 0.4565
365 0.34
1671 0.487
791 0.464
82 0.293
2 0.5
0 0.0
0 0.0
0 0.0
0.47329143754909664
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5552,	 Acc1 = 0.4081,	 Acc2 = 0.4356

 ===== Epoch 233	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  1.1112964   2.2483532  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773 -0.01085513 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.00014811 -0.00011583] 4 0
train:	 Loss = 1.5090,	 Acc = 0.4325
2821 0.247
5491 0.461
2836 0.546
366 0.557
64 0.406
6 0.333
0 0.0
0 0.0
0.49218304233709914
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.5078,	 Acc = 0.4497
365 0.337
1671 0.478
791 0.455
82 0.329
2 0.5
0 0.0
0 0.0
0 0.0
0.46582875098193244
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 4
Testing:	 Loss = 1.5533,	 Acc1 = 0.4089,	 Acc2 = 0.4366

 ===== Epoch 234	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247  0.912797    2.9105225
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -1.5762369e-01 -6.7665935e-02  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 1 5
train:	 Loss = 1.5103,	 Acc = 0.4291
2818 0.244
5497 0.458
2835 0.542
364 0.549
64 0.422
6 0.333
0 0.0
0 0.0
0.48859228838694957
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.5161,	 Acc = 0.4346
365 0.334
1671 0.454
791 0.454
82 0.305
2 0.5
0 0.0
0 0.0
0 0.0
0.44893951296150825
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5643,	 Acc1 = 0.3974,	 Acc2 = 0.4227

 ===== Epoch 235	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247  2.8692162  -4.7162185
 -0.36420733 -0.3724829   1.9679329   1.6464794  -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -1.6444005e-02  8.0763960e+00  1.3177313e-03  2.7925514e-03
  2.0101218e-01  3.7098551e-01  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 6 6
train:	 Loss = 1.5048,	 Acc = 0.4379
2821 0.252
5497 0.47
2834 0.546
362 0.558
64 0.469
6 0.333
0 0.0
0 0.0
0.49777473467990413
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5273,	 Acc = 0.4280
365 0.334
1671 0.458
791 0.422
82 0.28
2 0.5
0 0.0
0 0.0
0 0.0
0.44147682639434405
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5925,	 Acc1 = 0.3773,	 Acc2 = 0.3986

 ===== Epoch 236	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913  1.21656     2.0499792
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03 -7.4127885e-03 -1.9924510e-01
 -1.4811044e-04 -1.1583284e-04] 3 5
train:	 Loss = 1.5064,	 Acc = 0.4345
2816 0.247
5498 0.464
2836 0.549
365 0.545
63 0.444
6 0.333
0 0.0
0 0.0
0.4947536496350365
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5005,	 Acc = 0.4473
365 0.332
1671 0.482
791 0.444
82 0.293
2 0.5
0 0.0
0 0.0
0 0.0
0.4638648860958366
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5521,	 Acc1 = 0.4040,	 Acc2 = 0.4307

 ===== Epoch 237	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  0.8819695   1.274262   -0.4012819  -0.40874913  2.1724186   1.6858169
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -8.5381307e-02  2.4162690e-01
 -2.4058432e-03  2.0827476e-03  1.3003956e-02  3.1997167e-02
 -1.4811044e-04 -1.1583284e-04] 2 4
train:	 Loss = 1.5057,	 Acc = 0.4343
2817 0.246
5500 0.46
2837 0.554
362 0.566
62 0.468
6 0.333
0 0.0
0 0.0
0.49481008326679593
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.5096,	 Acc = 0.4473
365 0.332
1671 0.476
791 0.455
82 0.305
2 0.5
0 0.0
0 0.0
0 0.0
0.4638648860958366
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5539,	 Acc1 = 0.4027,	 Acc2 = 0.4292

 ===== Epoch 238	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.00014811 -0.00011583] 6 1
train:	 Loss = 1.5109,	 Acc = 0.4322
2816 0.244
5500 0.461
2833 0.547
365 0.553
64 0.484
6 0.333
0 0.0
0 0.0
0.49258667883211676
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.5028,	 Acc = 0.4535
365 0.321
1671 0.481
791 0.469
82 0.329
2 0.5
0 0.0
0 0.0
0 0.0
0.4725058915946583
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5473,	 Acc1 = 0.4054,	 Acc2 = 0.4324

 ===== Epoch 239	 =====
[ 3.587919    3.029468   -0.42037553 -0.33554247 -0.4410947  -0.36475152
  1.1517658   2.7256303  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [-5.3142846e-01  6.6833854e-02  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04  2.9156783e-01  1.3244548e-01
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 4
train:	 Loss = 1.5038,	 Acc = 0.4311
2821 0.238
5494 0.461
2833 0.547
366 0.563
64 0.5
6 0.333
0 0.0
0 0.0
0.49332420403971244
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.5120,	 Acc = 0.4466
365 0.326
1671 0.479
791 0.451
82 0.28
2 0.5
0 0.0
0 0.0
0 0.0
0.4638648860958366
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 1
Testing:	 Loss = 1.5398,	 Acc1 = 0.4176,	 Acc2 = 0.4471

 ===== Epoch 240	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247  3.229761    2.3524141
 -0.36420733 -0.3724829   3.303059    1.539488   -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -9.3108803e-01  6.8194225e-02  1.3177313e-03  2.7925514e-03
 -2.5262991e-02  1.5060204e-01  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 2
train:	 Loss = 1.5031,	 Acc = 0.4367
2826 0.248
5488 0.466
2834 0.55
366 0.568
64 0.469
6 0.333
0 0.0
0 0.0
0.4974880109614067
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 5
val:	 Loss = 1.5481,	 Acc = 0.4466
365 0.334
1671 0.488
791 0.425
82 0.305
2 0.5
0 0.0
0 0.0
0 0.0
0.4626865671641791
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5760,	 Acc1 = 0.3978,	 Acc2 = 0.4232

 ===== Epoch 241	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.00014811 -0.00011583] 0 1
train:	 Loss = 1.5070,	 Acc = 0.4309
2821 0.247
5500 0.457
2829 0.545
364 0.58
64 0.422
6 0.333
0 0.0
0 0.0
0.4901289512723953
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.5011,	 Acc = 0.4531
365 0.334
1671 0.476
791 0.468
82 0.366
2 0.5
0 0.0
0 0.0
0 0.0
0.4701492537313433
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 1
Testing:	 Loss = 1.5622,	 Acc1 = 0.3903,	 Acc2 = 0.4143

 ===== Epoch 242	 =====
[ 0.20288777  1.2394457  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 9.1473594e-02  2.2778720e-01  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 6 6
train:	 Loss = 1.5070,	 Acc = 0.4336
2819 0.242
5491 0.46
2839 0.555
366 0.587
63 0.397
6 0.333
0 0.0
0 0.0
0.49537934968625214
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.5035,	 Acc = 0.4414
365 0.342
1671 0.467
791 0.442
82 0.341
2 0.5
0 0.0
0 0.0
0 0.0
0.4556166535742341
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5509,	 Acc1 = 0.4019,	 Acc2 = 0.4282

 ===== Epoch 243	 =====
[-0.36678174 -0.3791378   4.104548   -4.3955665   3.9909942   1.3229152
  2.7171516   3.705304    4.071558    0.81791854 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -2.0776494e+00  8.0376692e+00
 -1.1075625e+00  7.8257933e-02  9.0525955e-02  1.8021236e-01
 -5.9095091e-01  4.5201249e-02  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 5 2
train:	 Loss = 1.5033,	 Acc = 0.4360
2824 0.248
5497 0.466
2831 0.552
362 0.55
64 0.437
6 0.167
0 0.0
0 0.0
0.49668949771689497
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.4916,	 Acc = 0.4603
365 0.334
1671 0.482
791 0.482
82 0.366
2 0.5
0 0.0
0 0.0
0 0.0
0.4783974862529458
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 4
Testing:	 Loss = 1.5343,	 Acc1 = 0.4122,	 Acc2 = 0.4406

 ===== Epoch 244	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  1.2675534   2.3516238  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -1.3312578e-01  4.2587054e-01
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 6 6
train:	 Loss = 1.5019,	 Acc = 0.4358
2825 0.247
5489 0.466
2834 0.549
366 0.563
64 0.422
6 0.333
0 0.0
0 0.0
0.4967462039045553
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5166,	 Acc = 0.4462
365 0.334
1671 0.481
791 0.432
82 0.366
2 0.5
0 0.0
0 0.0
0 0.0
0.4622937941869599
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5597,	 Acc1 = 0.4060,	 Acc2 = 0.4332

 ===== Epoch 245	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.00014811 -0.00011583] 0 1
train:	 Loss = 1.5000,	 Acc = 0.4355
2822 0.248
5495 0.466
2833 0.546
364 0.571
64 0.406
6 0.333
0 0.0
0 0.0
0.49577721981282813
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 5
val:	 Loss = 1.5147,	 Acc = 0.4404
365 0.329
1671 0.469
791 0.444
82 0.329
2 0.5
0 0.0
0 0.0
0 0.0
0.45640219952867245
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5526,	 Acc1 = 0.4013,	 Acc2 = 0.4274

 ===== Epoch 246	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913  2.6845798   1.818687
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  7.3869771e-01 -1.0875900e-01
 -1.4811044e-04 -1.1583284e-04] 2 3
train:	 Loss = 1.5053,	 Acc = 0.4352
2823 0.253
5490 0.463
2835 0.543
366 0.582
64 0.484
6 0.5
0 0.0
0 0.0
0.4940075333865997
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5147,	 Acc = 0.4383
365 0.334
1671 0.463
791 0.444
82 0.354
2 0.5
0 0.0
0 0.0
0 0.0
0.45326001571091906
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 1
Testing:	 Loss = 1.5572,	 Acc1 = 0.3930,	 Acc2 = 0.4175

 ===== Epoch 247	 =====
[-0.36678174 -0.3791378   1.1455616   1.5493068  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -1.6677992e-01  1.7677664e-03
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 0 0
train:	 Loss = 1.5036,	 Acc = 0.4377
2823 0.247
5494 0.469
2832 0.55
365 0.575
64 0.437
6 0.333
0 0.0
0 0.0
0.4991439333409428
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 5
val:	 Loss = 1.5238,	 Acc = 0.4462
365 0.345
1671 0.479
791 0.436
82 0.317
2 0.5
0 0.0
0 0.0
0 0.0
0.46072270227808326
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5625,	 Acc1 = 0.4060,	 Acc2 = 0.4332

 ===== Epoch 248	 =====
[-0.36678174 -0.3791378   3.3112302   2.8671134   2.962889    1.0783257
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -3.1149939e-01 -2.4420334e-01
 -6.1847454e-01 -1.9849426e-01  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 5 5
train:	 Loss = 1.4997,	 Acc = 0.4308
2820 0.238
5497 0.458
2834 0.554
364 0.56
64 0.469
5 0.4
0 0.0
0 0.0
0.492925604746691
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5384,	 Acc = 0.4335
365 0.332
1671 0.469
791 0.42
82 0.305
2 0.5
0 0.0
0 0.0
0 0.0
0.4481539670070699
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5837,	 Acc1 = 0.3908,	 Acc2 = 0.4148

 ===== Epoch 249	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829   1.0772735   1.9201779  -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.36706250e-04  1.83346274e-03  3.55022145e-04  1.76776643e-03
 -3.08473333e-04  2.64141156e-04  1.31773134e-03  2.79255142e-03
  1.17588885e-01  4.52012494e-02  9.40950064e-04  1.83513016e-03
 -1.48110441e-04 -1.15832838e-04] 4 2
train:	 Loss = 1.4963,	 Acc = 0.4393
2817 0.252
5500 0.47
2832 0.548
365 0.573
64 0.437
6 0.5
0 0.0
0 0.0
0.4993726474278545
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5083,	 Acc = 0.4514
365 0.345
1671 0.475
791 0.461
82 0.341
2 0.5
0 0.0
0 0.0
0 0.0
0.4666142969363708
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5525,	 Acc1 = 0.4062,	 Acc2 = 0.4334

 ===== Epoch 250	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.00014811 -0.00011583] 1 1
train:	 Loss = 1.4995,	 Acc = 0.4406
2819 0.252
5498 0.469
2831 0.556
366 0.557
64 0.516
6 0.333
0 0.0
0 0.0
0.5011979463776384
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.4720,	 Acc = 0.4686
365 0.337
1671 0.477
791 0.525
82 0.341
2 0.5
0 0.0
0 0.0
0 0.0
0.4874312647289866
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 1
Testing:	 Loss = 1.5298,	 Acc1 = 0.3951,	 Acc2 = 0.4200

 ===== Epoch 251	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829   1.7001028   2.8109426  -0.42398092 -0.42041838
  2.5604374   0.9160767 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -1.5668274e-01  1.0269259e-01  9.4095006e-04  1.8351302e-03
 -6.8253078e+00 -2.6355400e+00] 1 1
train:	 Loss = 1.5016,	 Acc = 0.4343
2819 0.25
5494 0.463
2837 0.546
364 0.558
64 0.484
6 0.167
0 0.0
0 0.0
0.4934398174557901
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5027,	 Acc = 0.4456
365 0.332
1671 0.476
791 0.449
82 0.305
2 0.5
0 0.0
0 0.0
0 0.0
0.46190102120974075
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 1
Testing:	 Loss = 1.5469,	 Acc1 = 0.4009,	 Acc2 = 0.4269

 ===== Epoch 252	 =====
[-0.36678174 -0.3791378   1.1972028   2.4860609  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  9.7171754e-02 -2.4420334e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 5 5
train:	 Loss = 1.4998,	 Acc = 0.4362
2817 0.25
5497 0.464
2834 0.55
366 0.568
64 0.469
6 0.333
0 0.0
0 0.0
0.49595072430706055
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.5053,	 Acc = 0.4524
365 0.34
1671 0.471
791 0.474
82 0.366
2 0.5
0 0.0
0 0.0
0 0.0
0.4685781618224666
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 1
Testing:	 Loss = 1.5383,	 Acc1 = 0.4015,	 Acc2 = 0.4277

 ===== Epoch 253	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.00014811 -0.00011583] 0 1
train:	 Loss = 1.4951,	 Acc = 0.4363
2817 0.252
5497 0.463
2835 0.549
365 0.586
64 0.437
6 0.333
0 0.0
0 0.0
0.49560853199498117
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5251,	 Acc = 0.4205
365 0.34
1671 0.443
791 0.416
82 0.354
2 0.5
0 0.0
0 0.0
0 0.0
0.43205027494108406
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5663,	 Acc1 = 0.3943,	 Acc2 = 0.4190

 ===== Epoch 254	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.00014811 -0.00011583] 3 1
train:	 Loss = 1.5014,	 Acc = 0.4387
2819 0.25
5493 0.472
2836 0.548
366 0.555
64 0.437
6 0.333
0 0.0
0 0.0
0.49925841414717625
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5110,	 Acc = 0.4456
365 0.334
1671 0.482
791 0.435
82 0.305
2 0.5
0 0.0
0 0.0
0 0.0
0.4615082482325216
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5652,	 Acc1 = 0.3967,	 Acc2 = 0.4220

 ===== Epoch 255	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829   1.934668    1.8306037  -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.01954694  0.10748354  0.00094095  0.00183513
 -0.00014811 -0.00011583] 2 2
train:	 Loss = 1.4986,	 Acc = 0.4351
2818 0.241
5497 0.466
2838 0.551
361 0.571
64 0.422
6 0.333
0 0.0
0 0.0
0.4976043805612594
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.4999,	 Acc = 0.4565
365 0.334
1671 0.487
791 0.461
82 0.329
2 0.5
0 0.0
0 0.0
0 0.0
0.47407698350353494
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 1
Testing:	 Loss = 1.5434,	 Acc1 = 0.4071,	 Acc2 = 0.4344

 ===== Epoch 256	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913  0.6851064   3.1006362
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03 -2.7683165e-03  3.0848250e-01
 -1.4811044e-04 -1.1583284e-04] 1 6
train:	 Loss = 1.4956,	 Acc = 0.4412
2818 0.245
5496 0.473
2834 0.557
366 0.585
64 0.437
6 0.333
0 0.0
0 0.0
0.5042208532968286
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.4882,	 Acc = 0.4531
365 0.332
1671 0.474
791 0.477
82 0.341
2 0.5
0 0.0
0 0.0
0 0.0
0.47054202670856243
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5552,	 Acc1 = 0.3992,	 Acc2 = 0.4250

 ===== Epoch 257	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  1.3861524   3.3145509  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04  1.5137685e-02  1.6656467e-01
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 4
train:	 Loss = 1.4960,	 Acc = 0.4345
2822 0.248
5495 0.463
2832 0.547
366 0.568
63 0.476
6 0.333
0 0.0
0 0.0
0.4946359278703492
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.5040,	 Acc = 0.4473
365 0.337
1671 0.476
791 0.448
82 0.341
2 0.5
0 0.0
0 0.0
0 0.0
0.4630793401413983
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 4
Testing:	 Loss = 1.5341,	 Acc1 = 0.4122,	 Acc2 = 0.4406

 ===== Epoch 258	 =====
[ 2.6523244   2.1747954  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  1.7762331   2.1004252  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 2.0861173e+00 -2.3031081e-01  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -1.0799508e-01 -4.5440462e-01
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 5 5
train:	 Loss = 1.4951,	 Acc = 0.4386
2815 0.247
5501 0.468
2833 0.554
365 0.567
64 0.5
6 0.333
0 0.0
0 0.0
0.5000570190443608
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.4860,	 Acc = 0.4679
365 0.334
1671 0.506
791 0.464
82 0.329
2 0.5
0 0.0
0 0.0
0 0.0
0.4870384917517675
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5391,	 Acc1 = 0.4077,	 Acc2 = 0.4351

 ===== Epoch 259	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.00014811 -0.00011583] 4 1
train:	 Loss = 1.4986,	 Acc = 0.4354
2820 0.247
5498 0.464
2832 0.548
365 0.589
63 0.444
6 0.333
0 0.0
0 0.0
0.4960063897763578
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.4929,	 Acc = 0.4593
365 0.334
1671 0.488
791 0.465
82 0.366
2 0.5
0 0.0
0 0.0
0 0.0
0.4772191673212883
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5414,	 Acc1 = 0.4095,	 Acc2 = 0.4374

 ===== Epoch 260	 =====
[-0.36678174 -0.3791378   1.6387937   1.0888683  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  4.2737126e-01  1.3109277e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 2 2
train:	 Loss = 1.4978,	 Acc = 0.4389
2816 0.25
5501 0.47
2833 0.548
364 0.574
64 0.453
6 0.333
0 0.0
0 0.0
0.49942974452554745
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.4827,	 Acc = 0.4668
365 0.337
1671 0.501
791 0.472
82 0.305
2 0.5
0 0.0
0 0.0
0 0.0
0.48546739984289083
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5436,	 Acc1 = 0.4042,	 Acc2 = 0.4309

 ===== Epoch 261	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247  1.3582191   1.8365529
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346  0.00035502  0.00176777 -0.03560339 -0.06263408
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.00014811 -0.00011583] 2 5
train:	 Loss = 1.4971,	 Acc = 0.4378
2821 0.249
5493 0.464
2835 0.556
366 0.579
63 0.492
6 0.333
0 0.0
0 0.0
0.4985735478717334
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5107,	 Acc = 0.4397
365 0.34
1671 0.459
791 0.454
82 0.354
2 0.5
0 0.0
0 0.0
0 0.0
0.4540455616653574
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5703,	 Acc1 = 0.3887,	 Acc2 = 0.4123

 ===== Epoch 262	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913  1.148473    1.4643669
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03 -3.5262418e-01 -4.3407921e-02
 -1.4811044e-04 -1.1583284e-04] 1 1
train:	 Loss = 1.4972,	 Acc = 0.4365
2824 0.244
5500 0.466
2827 0.553
363 0.57
64 0.469
6 0.5
0 0.0
0 0.0
0.4986301369863014
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.4852,	 Acc = 0.4583
365 0.329
1671 0.481
791 0.482
82 0.341
2 0.5
0 0.0
0 0.0
0 0.0
0.47682639434406915
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5531,	 Acc1 = 0.3899,	 Acc2 = 0.4138

 ===== Epoch 263	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
  2.0829496   2.680737  ] [ 0.00073671  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.03687648  0.05323283] 3 3
train:	 Loss = 1.4959,	 Acc = 0.4359
2824 0.243
5493 0.464
2834 0.555
366 0.577
62 0.419
5 0.4
0 0.0
0 0.0
0.49817351598173515
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5151,	 Acc = 0.4435
365 0.334
1671 0.479
791 0.43
82 0.341
2 0.5
0 0.0
0 0.0
0 0.0
0.4591516103692066
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5797,	 Acc1 = 0.3850,	 Acc2 = 0.4078

 ===== Epoch 264	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.00014811 -0.00011583] 6 1
train:	 Loss = 1.4952,	 Acc = 0.4380
2818 0.246
5501 0.467
2830 0.554
366 0.59
63 0.444
6 0.333
0 0.0
0 0.0
0.4998859228838695
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.4773,	 Acc = 0.4648
365 0.334
1671 0.488
791 0.487
82 0.354
2 0.5
0 0.0
0 0.0
0 0.0
0.483503534956795
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5319,	 Acc1 = 0.4110,	 Acc2 = 0.4391

 ===== Epoch 265	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247  1.7080578   1.9744123
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 8.1621323e+00 -6.3589201e+00  3.5502214e-04  1.7677664e-03
 -3.4317297e-01  6.5678291e-02  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 2 4
train:	 Loss = 1.4982,	 Acc = 0.4384
2820 0.248
5497 0.469
2833 0.552
364 0.577
64 0.422
6 0.167
0 0.0
0 0.0
0.4998858968507531
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.4815,	 Acc = 0.4658
365 0.332
1671 0.488
791 0.491
82 0.366
2 0.5
0 0.0
0 0.0
0 0.0
0.48507462686567165
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5213,	 Acc1 = 0.4161,	 Acc2 = 0.4453

 ===== Epoch 266	 =====
[-0.36678174 -0.3791378   2.2588923   1.6627152  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.1628609e-01  1.6912955e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 2 2
train:	 Loss = 1.4926,	 Acc = 0.4396
2826 0.245
5493 0.47
2833 0.553
363 0.606
63 0.413
6 0.5
0 0.0
0 0.0
0.5022836263987212
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5225,	 Acc = 0.4510
365 0.337
1671 0.487
791 0.445
82 0.28
2 0.5
0 0.0
0 0.0
0 0.0
0.4673998428908091
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5627,	 Acc1 = 0.4011,	 Acc2 = 0.4272

 ===== Epoch 267	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913  2.0721803   1.314273
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03 -7.2660607e-01 -1.2886703e-01
 -1.4811044e-04 -1.1583284e-04] 5 1
train:	 Loss = 1.4979,	 Acc = 0.4408
2820 0.245
5499 0.474
2830 0.549
366 0.598
63 0.476
6 0.333
0 0.0
0 0.0
0.5037654039251483
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.4960,	 Acc = 0.4596
365 0.329
1671 0.469
791 0.508
82 0.378
2 0.5
0 0.0
0 0.0
0 0.0
0.4783974862529458
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 1
Testing:	 Loss = 1.5364,	 Acc1 = 0.4099,	 Acc2 = 0.4379

 ===== Epoch 268	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.00014811 -0.00011583] 4 1
train:	 Loss = 1.4900,	 Acc = 0.4375
2822 0.248
5494 0.464
2833 0.556
365 0.589
64 0.469
6 0.333
0 0.0
0 0.0
0.4986304496690253
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.4931,	 Acc = 0.4510
365 0.334
1671 0.479
791 0.455
82 0.354
2 0.5
0 0.0
0 0.0
0 0.0
0.4677926158680283
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5303,	 Acc1 = 0.4064,	 Acc2 = 0.4336

 ===== Epoch 269	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247  1.6579713   1.2428677
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.00014811 -0.00011583] 0 0
train:	 Loss = 1.4879,	 Acc = 0.4405
2826 0.249
5486 0.47
2839 0.553
364 0.599
63 0.46
6 0.333
0 0.0
0 0.0
0.5022836263987212
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.4987,	 Acc = 0.4617
365 0.337
1671 0.501
791 0.45
82 0.329
2 0.5
0 0.0
0 0.0
0 0.0
0.4795758051846033
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5594,	 Acc1 = 0.4019,	 Acc2 = 0.4282

 ===== Epoch 270	 =====
[ 0.5460727   1.594929   -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [-1.0127667e+00 -2.4217527e+00  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 5 1
train:	 Loss = 1.4958,	 Acc = 0.4381
2818 0.24
5495 0.472
2837 0.55
365 0.6
63 0.413
6 0.333
0 0.0
0 0.0
0.5017111567419575
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5390,	 Acc = 0.4466
365 0.332
1671 0.491
791 0.421
82 0.293
2 0.5
0 0.0
0 0.0
0 0.0
0.4630793401413983
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5695,	 Acc1 = 0.4017,	 Acc2 = 0.4279

 ===== Epoch 271	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913  1.8985599   0.8861364
  2.9189937   3.3739965 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  5.7722557e-01  3.9394161e-01
 -5.9389818e-01  4.3734327e-01] 6 6
train:	 Loss = 1.4978,	 Acc = 0.4361
2821 0.247
5497 0.466
2834 0.548
364 0.56
63 0.492
5 0.4
0 0.0
0 0.0
0.49697592148807485
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5009,	 Acc = 0.4545
365 0.337
1671 0.485
791 0.455
82 0.341
2 0.5
0 0.0
0 0.0
0 0.0
0.4713275726630008
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5579,	 Acc1 = 0.4054,	 Acc2 = 0.4324

 ===== Epoch 272	 =====
[ 0.25726163  2.2605147  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [-6.9211209e-01 -3.2389007e+00  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 6 1
train:	 Loss = 1.4895,	 Acc = 0.4376
2820 0.248
5496 0.461
2833 0.559
365 0.595
64 0.453
6 0.333
0 0.0
0 0.0
0.49851665905979003
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5257,	 Acc = 0.4425
365 0.332
1671 0.482
791 0.422
82 0.317
2 0.5
0 0.0
0 0.0
0 0.0
0.45836606441476824
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5731,	 Acc1 = 0.3996,	 Acc2 = 0.4254

 ===== Epoch 273	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
  0.9865772   0.96334434] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
  6.0854416e+00  7.2507825e+00  9.4095006e-04  1.8351302e-03
 -3.1801701e+00 -2.7315676e+00] 5 2
train:	 Loss = 1.4980,	 Acc = 0.4347
2819 0.239
5497 0.464
2836 0.552
362 0.594
64 0.484
6 0.333
0 0.0
0 0.0
0.4977752424415288
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5072,	 Acc = 0.4541
365 0.334
1671 0.486
791 0.454
82 0.341
2 0.5
0 0.0
0 0.0
0 0.0
0.4713275726630008
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5566,	 Acc1 = 0.4031,	 Acc2 = 0.4297

 ===== Epoch 274	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829   1.2785753   3.1816802  -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.2639531e-01 -2.0392789e-01  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 2 5
train:	 Loss = 1.4974,	 Acc = 0.4367
2826 0.247
5486 0.469
2837 0.548
365 0.564
64 0.453
6 0.167
0 0.0
0 0.0
0.49794473624115093
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5066,	 Acc = 0.4541
365 0.342
1671 0.489
791 0.448
82 0.305
2 0.5
0 0.0
0 0.0
0 0.0
0.4701492537313433
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5438,	 Acc1 = 0.4145,	 Acc2 = 0.4433

 ===== Epoch 275	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829   3.2445602   2.1938765  -0.42398092 -0.42041838
  3.7233226   1.1734229 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -7.2673464e+00 -5.0092454e+00  9.4095006e-04  1.8351302e-03
 -2.9600292e-01 -1.8150131e-01] 2 5
train:	 Loss = 1.4917,	 Acc = 0.4397
2818 0.249
5502 0.469
2830 0.55
364 0.618
64 0.469
6 0.333
0 0.0
0 0.0
0.5011407711613051
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5084,	 Acc = 0.4452
365 0.337
1671 0.485
791 0.427
82 0.293
2 0.5
0 0.0
0 0.0
0 0.0
0.46072270227808326
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5574,	 Acc1 = 0.4023,	 Acc2 = 0.4287

 ===== Epoch 276	 =====
[-0.36678174 -0.3791378   2.4272335   1.6037428  -0.4410947  -0.36475152
  1.4580978   3.5517936  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  1.4710967e-01 -1.4277196e-01
 -3.0847333e-04  2.6414116e-04  6.7912199e-02 -3.5887089e-01
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 5 5
train:	 Loss = 1.4947,	 Acc = 0.4397
2821 0.247
5492 0.473
2838 0.549
363 0.579
64 0.453
6 0.167
0 0.0
0 0.0
0.5015405682985279
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5447,	 Acc = 0.4335
365 0.332
1671 0.468
791 0.421
82 0.305
2 0.5
0 0.0
0 0.0
0 0.0
0.4481539670070699
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5799,	 Acc1 = 0.3963,	 Acc2 = 0.4215

 ===== Epoch 277	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.5108685   2.39349    -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04  4.5490974e-01  2.2115539e-01
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 6 2
train:	 Loss = 1.4926,	 Acc = 0.4405
2822 0.249
5498 0.468
2830 0.559
364 0.588
64 0.469
6 0.333
0 0.0
0 0.0
0.502054325496462
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5312,	 Acc = 0.4541
365 0.342
1671 0.498
791 0.429
82 0.305
2 0.5
0 0.0
0 0.0
0 0.0
0.4701492537313433
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5888,	 Acc1 = 0.3936,	 Acc2 = 0.4182

 ===== Epoch 278	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.00014811 -0.00011583] 3 1
train:	 Loss = 1.4935,	 Acc = 0.4360
2817 0.245
5494 0.463
2839 0.552
365 0.595
63 0.46
6 0.5
0 0.0
0 0.0
0.49754762176343104
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.4954,	 Acc = 0.4658
365 0.337
1671 0.501
791 0.464
82 0.329
2 0.5
0 0.0
0 0.0
0 0.0
0.4842890809112333
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5556,	 Acc1 = 0.4015,	 Acc2 = 0.4277

 ===== Epoch 279	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913  1.5585049   1.575092
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03 -1.9472927e-02  1.2751028e-01
 -1.4811044e-04 -1.1583284e-04] 3 4
train:	 Loss = 1.4942,	 Acc = 0.4384
2819 0.248
5503 0.467
2828 0.553
364 0.591
64 0.453
6 0.167
0 0.0
0 0.0
0.4994865944095836
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5629,	 Acc = 0.4328
365 0.334
1671 0.469
791 0.416
82 0.293
2 0.5
0 0.0
0 0.0
0 0.0
0.4469756480754124
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5989,	 Acc1 = 0.3951,	 Acc2 = 0.4200

 ===== Epoch 280	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829   6.3942666  -4.5440874   1.0614735   2.5027213
  3.848421    2.4155126 ] [ 4.3951769e+00  3.3880444e+00  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
  1.2089735e+00  1.5155842e+01  4.9198713e-02  2.8334749e-01
  4.5485663e-01  2.0260911e-01] 4 2
train:	 Loss = 1.4944,	 Acc = 0.4379
2820 0.245
5497 0.469
2831 0.551
366 0.568
64 0.531
6 0.333
0 0.0
0 0.0
0.4998858968507531
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5099,	 Acc = 0.4452
365 0.334
1671 0.474
791 0.445
82 0.354
2 0.5
0 0.0
0 0.0
0 0.0
0.46111547525530244
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5659,	 Acc1 = 0.3980,	 Acc2 = 0.4235

 ===== Epoch 281	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.00014811 -0.00011583] 1 1
train:	 Loss = 1.4918,	 Acc = 0.4374
2818 0.249
5489 0.464
2843 0.555
364 0.571
64 0.453
6 0.333
0 0.0
0 0.0
0.49794661190965095
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5610,	 Acc = 0.4456
365 0.337
1671 0.485
791 0.421
82 0.354
2 0.5
0 0.0
0 0.0
0 0.0
0.46111547525530244
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.6005,	 Acc1 = 0.3957,	 Acc2 = 0.4207

 ===== Epoch 282	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.00014811 -0.00011583] 3 1
train:	 Loss = 1.4933,	 Acc = 0.4391
2822 0.247
5495 0.468
2834 0.556
365 0.592
62 0.419
6 0.333
0 0.0
0 0.0
0.501027162748231
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5086,	 Acc = 0.4404
365 0.337
1671 0.475
791 0.426
82 0.341
2 0.5
0 0.0
0 0.0
0 0.0
0.4552238805970149
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5462,	 Acc1 = 0.4031,	 Acc2 = 0.4297

 ===== Epoch 283	 =====
[-0.36678174 -0.3791378   2.2080643   2.7106097   2.911274    0.7581361
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -1.8410744e-01 -2.5688225e-01
 -8.8422995e+00 -1.2702800e+00  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 5 5
train:	 Loss = 1.4896,	 Acc = 0.4407
2819 0.248
5495 0.466
2836 0.561
366 0.607
62 0.5
6 0.167
0 0.0
0 0.0
0.5026811180832857
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.4961,	 Acc = 0.4631
365 0.334
1671 0.499
791 0.468
82 0.268
2 0.5
0 0.0
0 0.0
0 0.0
0.48153967007069914
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5518,	 Acc1 = 0.4091,	 Acc2 = 0.4369

 ===== Epoch 284	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913  2.779145    1.424998
  3.6524048   3.4422717 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  3.0160958e-01 -5.9135157e-01
 -4.6127430e-01 -6.5096956e-01] 5 5
train:	 Loss = 1.4955,	 Acc = 0.4412
2819 0.249
5500 0.471
2832 0.553
364 0.604
63 0.492
6 0.167
0 0.0
0 0.0
0.5030233884768968
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.4871,	 Acc = 0.4411
365 0.184
1671 0.491
791 0.468
82 0.317
2 0.0
0 0.0
0 0.0
0 0.0
0.47800471327572663
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5587,	 Acc1 = 0.3691,	 Acc2 = 0.4225

 ===== Epoch 285	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.00014811 -0.00011583] 3 6
train:	 Loss = 1.4888,	 Acc = 0.4383
2821 0.235
5499 0.47
2835 0.559
361 0.596
62 0.468
6 0.333
0 0.0
0 0.0
0.5037087755334931
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5206,	 Acc = 0.4538
365 0.332
1671 0.493
791 0.444
82 0.293
2 0.5
0 0.0
0 0.0
0 0.0
0.4713275726630008
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5562,	 Acc1 = 0.4077,	 Acc2 = 0.4351

 ===== Epoch 286	 =====
[ 1.5817763   2.5252364  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 2.6632121e-01 -1.8078668e-01  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 5 5
train:	 Loss = 1.4936,	 Acc = 0.4410
2827 0.245
5496 0.471
2827 0.557
365 0.592
64 0.516
5 0.2
0 0.0
0 0.0
0.5040538997373529
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5070,	 Acc = 0.4531
365 0.337
1671 0.488
791 0.446
82 0.329
2 0.5
0 0.0
0 0.0
0 0.0
0.46975648075412413
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5646,	 Acc1 = 0.3974,	 Acc2 = 0.4227

 ===== Epoch 287	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913  1.1061087   1.890043
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03 -1.8744275e-01  4.4923866e-01
 -1.4811044e-04 -1.1583284e-04] 2 6
train:	 Loss = 1.4892,	 Acc = 0.4401
2823 0.25
5501 0.468
2827 0.556
364 0.585
63 0.476
6 0.333
0 0.0
0 0.0
0.5011984933226801
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5165,	 Acc = 0.4297
365 0.181
1671 0.48
791 0.449
82 0.341
2 0.0
0 0.0
0 0.0
0 0.0
0.46543597800471326
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5685,	 Acc1 = 0.3697,	 Acc2 = 0.4232

 ===== Epoch 288	 =====
[-0.36678174 -0.3791378   1.7660674   0.9369008  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346 -0.01187506  0.08544866 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.00014811 -0.00011583] 4 4
train:	 Loss = 1.4929,	 Acc = 0.4355
2818 0.233
5493 0.466
2839 0.558
365 0.586
63 0.508
6 0.333
0 0.0
0 0.0
0.5005703855806525
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.4858,	 Acc = 0.4617
365 0.342
1671 0.48
791 0.488
82 0.366
2 0.5
0 0.0
0 0.0
0 0.0
0.47879025923016494
0.4913589945011783
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5479,	 Acc1 = 0.3953,	 Acc2 = 0.4202

 ===== Epoch 289	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.00014811 -0.00011583] 5 1
train:	 Loss = 1.4926,	 Acc = 0.4353
2824 0.243
5493 0.461
2833 0.554
364 0.61
64 0.5
6 0.333
0 0.0
0 0.0
0.4973744292237443
0.4913589945011783
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.4677,	 Acc = 0.4861
365 0.323
1671 0.517
791 0.511
82 0.341
2 0.5
0 0.0
0 0.0
0 0.0
0.50942655145326
0.50942655145326
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5327,	 Acc1 = 0.4064,	 Acc2 = 0.4336

 ===== Epoch 290	 =====
[ 2.7507687   3.268978   -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
  2.6053674   3.5394332 ] [ 0.00125041  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.00014811 -0.00011583] 0 0
train:	 Loss = 1.4931,	 Acc = 0.4382
2821 0.246
5492 0.467
2838 0.554
363 0.59
64 0.437
6 0.333
0 0.0
0 0.0
0.4999429419148693
0.50942655145326
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.4933,	 Acc = 0.4507
365 0.332
1671 0.482
791 0.449
82 0.354
2 0.5
0 0.0
0 0.0
0 0.0
0.4677926158680283
0.50942655145326
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5377,	 Acc1 = 0.4075,	 Acc2 = 0.4349

 ===== Epoch 291	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829   1.0686715   2.9925792  -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.36706250e-04  1.83346274e-03  3.55022145e-04  1.76776643e-03
 -3.08473333e-04  2.64141156e-04  1.31773134e-03  2.79255142e-03
 -1.50970235e-01  1.07483536e-01  9.40950064e-04  1.83513016e-03
 -1.48110441e-04 -1.15832838e-04] 1 4
train:	 Loss = 1.4892,	 Acc = 0.4428
2815 0.25
5504 0.471
2831 0.56
364 0.596
64 0.469
6 0.333
0 0.0
0 0.0
0.5047325806819478
0.50942655145326
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5108,	 Acc = 0.4572
365 0.326
1671 0.507
791 0.432
82 0.268
2 0.5
0 0.0
0 0.0
0 0.0
0.4760408483896308
0.50942655145326
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5542,	 Acc1 = 0.4087,	 Acc2 = 0.4364

 ===== Epoch 292	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.00014811 -0.00011583] 4 1
train:	 Loss = 1.4901,	 Acc = 0.4379
2819 0.245
5496 0.469
2836 0.545
364 0.613
63 0.492
6 0.333
0 0.0
0 0.0
0.49982886480319455
0.50942655145326
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5347,	 Acc = 0.4418
365 0.332
1671 0.481
791 0.425
82 0.305
2 0.5
0 0.0
0 0.0
0 0.0
0.45758051846032993
0.50942655145326
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5680,	 Acc1 = 0.4031,	 Acc2 = 0.4297

 ===== Epoch 293	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.3799052   1.9217954  -0.4012819  -0.40874913  2.81054     2.6971052
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04  1.2193742e-01 -5.4311448e-01
 -2.4058432e-03  2.0827476e-03 -2.2084911e-01 -4.0032533e-01
 -1.4811044e-04 -1.1583284e-04] 5 5
train:	 Loss = 1.4887,	 Acc = 0.4413
2820 0.248
5494 0.471
2835 0.558
365 0.589
64 0.469
6 0.167
0 0.0
0 0.0
0.5036513007759014
0.50942655145326
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.4792,	 Acc = 0.4596
365 0.323
1671 0.494
791 0.46
82 0.354
2 0.5
0 0.0
0 0.0
0 0.0
0.4791830322073841
0.50942655145326
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5406,	 Acc1 = 0.4011,	 Acc2 = 0.4272

 ===== Epoch 294	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.00014811 -0.00011583] 0 1
train:	 Loss = 1.4897,	 Acc = 0.4418
2821 0.248
5499 0.469
2830 0.561
364 0.607
64 0.437
6 0.333
0 0.0
0 0.0
0.5042793563847997
0.50942655145326
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.5073,	 Acc = 0.4528
365 0.329
1671 0.481
791 0.466
82 0.293
2 0.5
0 0.0
0 0.0
0 0.0
0.47054202670856243
0.50942655145326
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5451,	 Acc1 = 0.4112,	 Acc2 = 0.4394

 ===== Epoch 295	 =====
[-0.36678174 -0.3791378   2.6370504   3.0508351   2.3469439   1.0849965
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -1.3620788e-01  1.1910905e-02
  6.8441361e-01  7.3226079e-02  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 2 2
train:	 Loss = 1.4937,	 Acc = 0.4386
2823 0.247
5489 0.468
2840 0.555
362 0.583
64 0.437
6 0.0
0 0.0
0 0.0
0.500285355553019
0.50942655145326
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.5056,	 Acc = 0.4476
365 0.329
1671 0.468
791 0.468
82 0.366
2 0.5
0 0.0
0 0.0
0 0.0
0.46465043205027495
0.50942655145326
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5731,	 Acc1 = 0.3889,	 Acc2 = 0.4125

 ===== Epoch 296	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913  1.5448881   1.0682175
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  3.6842516e-01  5.7132192e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
train:	 Loss = 1.4908,	 Acc = 0.4378
2822 0.249
5500 0.462
2828 0.56
365 0.592
63 0.413
6 0.167
0 0.0
0 0.0
0.49851632047477745
0.50942655145326
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.5003,	 Acc = 0.4610
365 0.34
1671 0.495
791 0.46
82 0.317
2 0.5
0 0.0
0 0.0
0 0.0
0.4783974862529458
0.50942655145326
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5628,	 Acc1 = 0.4005,	 Acc2 = 0.4264

 ===== Epoch 297	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  1.3068988   2.5665379  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -4.2659082e-02  3.4398448e-01
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
train:	 Loss = 1.4924,	 Acc = 0.4413
2818 0.248
5497 0.469
2838 0.56
362 0.591
63 0.429
6 0.333
0 0.0
0 0.0
0.5033082363677847
0.50942655145326
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.4808,	 Acc = 0.4723
365 0.326
1671 0.509
791 0.479
82 0.317
2 0.5
0 0.0
0 0.0
0 0.0
0.49332285938727416
0.50942655145326
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5382,	 Acc1 = 0.4079,	 Acc2 = 0.4354

 ===== Epoch 298	 =====
[-0.36678174 -0.3791378   0.8105061   1.1954722  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346 -0.05875702 -0.01851851 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.00014811 -0.00011583] 5 5
train:	 Loss = 1.4922,	 Acc = 0.4410
2821 0.25
5500 0.473
2832 0.553
362 0.564
63 0.508
6 0.333
0 0.0
0 0.0
0.5025676138308799
0.50942655145326
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 2
val:	 Loss = 1.5211,	 Acc = 0.4466
365 0.334
1671 0.477
791 0.442
82 0.366
2 0.5
0 0.0
0 0.0
0 0.0
0.4626865671641791
0.50942655145326
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 1
Testing:	 Loss = 1.5575,	 Acc1 = 0.4019,	 Acc2 = 0.4282

 ===== Epoch 299	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 0.00073671  0.00183346  0.00035502  0.00176777 -0.00030847  0.00026414
  0.00131773  0.00279255 -0.00240584  0.00208275  0.00094095  0.00183513
 -0.00014811 -0.00011583] 6 1
train:	 Loss = 1.4971,	 Acc = 0.4392
2814 0.249
5500 0.468
2835 0.552
366 0.607
64 0.391
5 0.2
0 0.0
0 0.0
0.5003420752565565
0.50942655145326
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 3
val:	 Loss = 1.5052,	 Acc = 0.4593
365 0.34
1671 0.491
791 0.463
82 0.305
2 0.5
0 0.0
0 0.0
0 0.0
0.47643362136684997
0.50942655145326
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5530,	 Acc1 = 0.4060,	 Acc2 = 0.4332

 ===== Epoch 300	 =====
[-0.36678174 -0.3791378  -0.42037553 -0.33554247  3.1693513  -4.8651958
 -0.36420733 -0.3724829   2.963542    1.3055999  -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -2.1712191e-01  7.9002805e+00  1.3177313e-03  2.7925514e-03
 -1.2583090e-01  2.1246528e-02  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 3 2
train:	 Loss = 1.4958,	 Acc = 0.4390
2819 0.248
5499 0.466
2833 0.558
363 0.595
64 0.453
6 0.333
0 0.0
0 0.0
0.5005134055904165
0.50942655145326
[-0.36678174 -0.3791378  -0.42037553 -0.33554247 -0.4410947  -0.36475152
  2.778418    1.8268982  -0.4012819  -0.40874913  2.3974822   3.0415828
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03  3.5502214e-04  1.7677664e-03
 -3.0847333e-04  2.6414116e-04 -9.4175115e-02 -1.4733191e-01
 -2.4058432e-03  2.0827476e-03  6.3012207e-01 -9.3677975e-02
 -1.4811044e-04 -1.1583284e-04] 3 5
val:	 Loss = 1.5248,	 Acc = 0.4552
365 0.337
1671 0.494
791 0.445
82 0.293
2 0.5
0 0.0
0 0.0
0 0.0
0.4721131186174391
0.50942655145326
[-0.36678174 -0.3791378   1.0764366   2.3386297  -0.4410947  -0.36475152
 -0.36420733 -0.3724829  -0.4012819  -0.40874913 -0.42398092 -0.42041838
 -0.38645998 -0.3811587 ] [ 7.3670625e-04  1.8334627e-03 -4.9582887e-02  2.3252416e-01
 -3.0847333e-04  2.6414116e-04  1.3177313e-03  2.7925514e-03
 -2.4058432e-03  2.0827476e-03  9.4095006e-04  1.8351302e-03
 -1.4811044e-04 -1.1583284e-04] 4 6
Testing:	 Loss = 1.5685,	 Acc1 = 0.4019,	 Acc2 = 0.4282
