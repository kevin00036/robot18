(0.24300000000005184, array([-1.000000e+00, -1.000000e+00,  1.189783e+03,  1.790000e-01,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 4, array([-1.000000e+00, -1.000000e+00,  1.173932e+03,  2.700000e-01,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 1)
(0.2, array([-1.00000000e+00, -1.00000000e+00,  1.79280339e+03,  5.91715458e-01,
        1.79280339e+03, -5.91715458e-01, -1.00000000e+00, -1.00000000e+00,
       -1.00000000e+00, -1.00000000e+00, -1.00000000e+00, -1.00000000e+00,
       -1.00000000e+00, -1.00000000e+00]), 1, array([-1.00000000e+00, -1.00000000e+00,  1.77293429e+03,  5.99266206e-01,
        1.77293429e+03, -5.99266206e-01, -1.00000000e+00, -1.00000000e+00,
       -1.00000000e+00, -1.00000000e+00, -1.00000000e+00, -1.00000000e+00,
       -1.00000000e+00, -1.00000000e+00]))
(0.2, array([-1.00000000e+00, -1.00000000e+00,  1.77293429e+03,  5.99266206e-01,
        1.77293429e+03, -5.99266206e-01, -1.00000000e+00, -1.00000000e+00,
       -1.00000000e+00, -1.00000000e+00, -1.00000000e+00, -1.00000000e+00,
       -1.00000000e+00, -1.00000000e+00]), 1, array([-1.00000000e+00, -1.00000000e+00,  1.74725952e+03,  6.09338117e-01,
        1.74725952e+03, -6.09338117e-01, -1.00000000e+00, -1.00000000e+00,
       -1.00000000e+00, -1.00000000e+00, -1.00000000e+00, -1.00000000e+00,
       -1.00000000e+00, -1.00000000e+00]))
(0.2, array([-1.00000000e+00, -1.00000000e+00,  1.74725952e+03,  6.09338117e-01,
        1.74725952e+03, -6.09338117e-01, -1.00000000e+00, -1.00000000e+00,
       -1.00000000e+00, -1.00000000e+00, -1.00000000e+00, -1.00000000e+00,
       -1.00000000e+00, -1.00000000e+00]), 1, array([-1.00000000e+00, -1.00000000e+00,  1.72293971e+03,  6.19223933e-01,
        1.72293971e+03, -6.19223933e-01, -1.00000000e+00, -1.00000000e+00,
       -1.00000000e+00, -1.00000000e+00, -1.00000000e+00, -1.00000000e+00,
       -1.00000000e+00, -1.00000000e+00]))
(0.2, array([-1.00000000e+00, -1.00000000e+00,  1.72293971e+03,  6.19223933e-01,
        1.72293971e+03, -6.19223933e-01, -1.00000000e+00, -1.00000000e+00,
       -1.00000000e+00, -1.00000000e+00, -1.00000000e+00, -1.00000000e+00,
       -1.00000000e+00, -1.00000000e+00]), 1, array([-1.00000000e+00, -1.00000000e+00,  1.69856028e+03,  6.29491609e-01,
        1.69856028e+03, -6.29491609e-01, -1.00000000e+00, -1.00000000e+00,
       -1.00000000e+00, -1.00000000e+00, -1.00000000e+00, -1.00000000e+00,
       -1.00000000e+00, -1.00000000e+00]))
(0.2, array([-1.00000000e+00, -1.00000000e+00,  1.69856028e+03,  6.29491609e-01,
        1.69856028e+03, -6.29491609e-01, -1.00000000e+00, -1.00000000e+00,
       -1.00000000e+00, -1.00000000e+00, -1.00000000e+00, -1.00000000e+00,
       -1.00000000e+00, -1.00000000e+00]), 2, array([-1.00000000e+00, -1.00000000e+00,  1.68400512e+03,  6.35801293e-01,
        1.68400512e+03, -6.35801293e-01, -1.00000000e+00, -1.00000000e+00,
       -1.00000000e+00, -1.00000000e+00, -1.00000000e+00, -1.00000000e+00,
       -1.00000000e+00, -1.00000000e+00]))
(0.2, array([-1.00000000e+00, -1.00000000e+00,  1.68400512e+03,  6.35801293e-01,
        1.68400512e+03, -6.35801293e-01, -1.00000000e+00, -1.00000000e+00,
       -1.00000000e+00, -1.00000000e+00, -1.00000000e+00, -1.00000000e+00,
       -1.00000000e+00, -1.00000000e+00]), 2, array([-1.00000000e+00, -1.00000000e+00,  1.67914276e+03,  6.37940147e-01,
        1.67914276e+03, -6.37940147e-01, -1.00000000e+00, -1.00000000e+00,
       -1.00000000e+00, -1.00000000e+00, -1.00000000e+00, -1.00000000e+00,
       -1.00000000e+00, -1.00000000e+00]))
(0.2, array([-1.00000000e+00, -1.00000000e+00,  1.67914276e+03,  6.37940147e-01,
        1.67914276e+03, -6.37940147e-01, -1.00000000e+00, -1.00000000e+00,
       -1.00000000e+00, -1.00000000e+00, -1.00000000e+00, -1.00000000e+00,
       -1.00000000e+00, -1.00000000e+00]), 2, array([-1.00000000e+00, -1.00000000e+00,  1.68392788e+03,  6.35835147e-01,
        1.68392788e+03, -6.35835147e-01, -1.00000000e+00, -1.00000000e+00,
       -1.00000000e+00, -1.00000000e+00, -1.00000000e+00, -1.00000000e+00,
       -1.00000000e+00, -1.00000000e+00]))
(0.2, array([-1.00000000e+00, -1.00000000e+00,  1.68392788e+03,  6.35835147e-01,
        1.68392788e+03, -6.35835147e-01, -1.00000000e+00, -1.00000000e+00,
       -1.00000000e+00, -1.00000000e+00, -1.00000000e+00, -1.00000000e+00,
       -1.00000000e+00, -1.00000000e+00]), 2, array([-1.00000000e+00, -1.00000000e+00,  1.69840508e+03,  6.29558166e-01,
        1.69840508e+03, -6.29558166e-01, -1.00000000e+00, -1.00000000e+00,
       -1.00000000e+00, -1.00000000e+00, -1.00000000e+00, -1.00000000e+00,
       -1.00000000e+00, -1.00000000e+00]))
(0.2, array([-1.00000000e+00, -1.00000000e+00,  1.69840508e+03,  6.29558166e-01,
        1.69840508e+03, -6.29558166e-01, -1.00000000e+00, -1.00000000e+00,
       -1.00000000e+00, -1.00000000e+00, -1.00000000e+00, -1.00000000e+00,
       -1.00000000e+00, -1.00000000e+00]), 2, array([-1.00000000e+00, -1.00000000e+00,  1.72270518e+03,  6.19320968e-01,
        1.72270518e+03, -6.19320968e-01, -1.00000000e+00, -1.00000000e+00,
       -1.00000000e+00, -1.00000000e+00, -1.00000000e+00, -1.00000000e+00,
       -1.00000000e+00, -1.00000000e+00]))
(0.2, array([-1.00000000e+00, -1.00000000e+00,  1.72270518e+03,  6.19320968e-01,
        1.72270518e+03, -6.19320968e-01, -1.00000000e+00, -1.00000000e+00,
       -1.00000000e+00, -1.00000000e+00, -1.00000000e+00, -1.00000000e+00,
       -1.00000000e+00, -1.00000000e+00]), 2, array([-1.00000000e+00, -1.00000000e+00,  1.74722803e+03,  6.09350699e-01,
        1.74722803e+03, -6.09350699e-01, -1.00000000e+00, -1.00000000e+00,
       -1.00000000e+00, -1.00000000e+00, -1.00000000e+00, -1.00000000e+00,
       -1.00000000e+00, -1.00000000e+00]))
14 1 14

 ===== Epoch 1	 =====
[ 0.6303424   1.4833022  -0.5993768  -0.57551634  1.4813584   2.0372813
 -0.43886673 -0.47319013  1.7054945   0.97982633 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [ 0.6303424   1.4833022  -0.59937984 -0.57549053  1.4815028   2.0373058
 -0.4389609  -0.47327355  1.7054945   0.97982633 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 2
train:	 Loss = 1.8552,	 Acc = 0.2831,	 Loss Con1 = 1.2787,	 Loss Con2 = 1.2723
3165 0.172
3143 0.283
2594 0.331
1323 0.347
892 0.363
760 0.353
71 0.437
20 0.55
0.32295808247188457
0.0
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 5
val:	 Loss = 1.6784,	 Acc = 0.3837,	 Loss Con1 = 1.3409,	 Loss Con2 = 1.3953
745 0.121
668 0.537
714 0.549
562 0.375
190 0.242
103 0.359
11 1.0
7 0.714
0.470509977827051
0.470509977827051

 ===== Epoch 2	 =====
[-0.4557189  -0.49292567  1.4808669   0.5521458  -0.6348712  -0.6508424
  2.840615    2.1383524  -0.46656546 -0.4823711   3.2709932   3.1808357
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.5072143   0.5600472  -0.6347739  -0.6507814
  2.8770142   2.131845   -0.46656546 -0.4823711   3.2957497   3.1699567
 -0.45947683 -0.42555502] 2 1
train:	 Loss = 1.5643,	 Acc = 0.4443,	 Loss Con1 = 1.2783,	 Loss Con2 = 1.4129
3161 0.173
3145 0.513
2594 0.59
1326 0.539
893 0.545
758 0.483
71 0.62
20 0.75
0.5417281707732485
0.470509977827051
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.4905,	 Acc = 0.5540,	 Loss Con1 = 1.3409,	 Loss Con2 = 1.5782
745 0.219
668 0.612
714 0.742
562 0.626
190 0.668
103 0.621
11 1.0
7 0.857
0.6647450110864745
0.6647450110864745

 ===== Epoch 3	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013  1.7858902   0.27099875 -0.4178274  -0.43225467
  2.953005    0.47958085] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355  1.8090248   0.24251324 -0.4178274  -0.43225467
  2.9707384   0.46257764] 3 3
train:	 Loss = 1.4255,	 Acc = 0.5343,	 Loss Con1 = 1.2803,	 Loss Con2 = 1.5749
3159 0.17
3144 0.597
2595 0.703
1325 0.703
892 0.698
762 0.701
71 0.718
20 0.85
0.6648881825405835
0.6647450110864745
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 1
val:	 Loss = 1.3579,	 Acc = 0.6067,	 Loss Con1 = 1.3409,	 Loss Con2 = 1.7711
745 0.219
668 0.578
714 0.821
562 0.785
190 0.747
103 0.845
11 0.909
7 0.714
0.7348115299334812
0.7348115299334812

 ===== Epoch 4	 =====
[ 1.7262443   1.1077654  -0.5993768  -0.57551634 -0.6348712  -0.6508424
  0.15301587  2.2608852   2.6526988   2.7029727   1.2769942   0.09372705
  2.594471    1.7282416 ] [ 1.7227373   1.1231284  -0.59937984 -0.57549053 -0.6347739  -0.6507814
  0.15804836  2.3101518   2.659921    2.7121294   1.2685422   0.10536259
  2.593931    1.7374815 ] 4 4
train:	 Loss = 1.2906,	 Acc = 0.5987,	 Loss Con1 = 1.2795,	 Loss Con2 = 1.6763
3162 0.161
3144 0.682
2594 0.796
1325 0.807
893 0.779
762 0.799
68 0.809
20 0.9
0.7559618441971383
0.7348115299334812
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.2255,	 Acc = 0.6460,	 Loss Con1 = 1.3409,	 Loss Con2 = 1.8046
745 0.217
668 0.657
714 0.863
562 0.827
190 0.8
103 0.845
11 0.909
7 1.0
0.7875831485587583
0.7875831485587583

 ===== Epoch 5	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.3927526   2.6059694
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.38836375  2.5778291
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 1 3
train:	 Loss = 1.2009,	 Acc = 0.6294,	 Loss Con1 = 1.2788,	 Loss Con2 = 1.7376
3169 0.155
3142 0.733
2592 0.841
1324 0.845
889 0.816
761 0.837
71 0.817
20 0.9
0.8003182179793158
0.7875831485587583
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1784,	 Acc = 0.6537,	 Loss Con1 = 1.3409,	 Loss Con2 = 1.8231
745 0.219
668 0.702
714 0.859
562 0.827
190 0.789
103 0.825
11 0.909
7 0.857
0.797339246119734
0.797339246119734

 ===== Epoch 6	 =====
[-0.00549916  0.7608245  -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711   2.146602    1.4341794
  1.3000416   3.5586474 ] [ 0.02582838  0.8142847  -0.59937984 -0.57549053 -0.6347739  -0.6507814
  1.9476854  -0.02961254 -0.46656546 -0.4823711   2.1708784   1.4373902
  1.3181621   3.5366085 ] 2 6
train:	 Loss = 1.1566,	 Acc = 0.6349,	 Loss Con1 = 1.2784,	 Loss Con2 = 1.7424
3163 0.153
3144 0.742
2597 0.848
1324 0.847
889 0.831
760 0.843
71 0.873
20 0.85
0.8080636002271436
0.797339246119734
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 5
val:	 Loss = 1.1605,	 Acc = 0.6493,	 Loss Con1 = 1.3409,	 Loss Con2 = 1.8691
745 0.219
668 0.668
714 0.867
562 0.835
190 0.774
103 0.854
11 0.818
7 1.0
0.7915742793791575
0.797339246119734

 ===== Epoch 7	 =====
[-0.4557189  -0.49292567  1.8810511   1.1121689   0.29514897  2.5987005
  3.9193103   2.41468    -0.46656546 -0.4823711   4.292931    3.2032402
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.8807815   0.94095516  0.29526702  2.4075794
  3.918996    2.2095892  -0.46656546 -0.4823711   4.292931    2.9826393
 -0.45947683 -0.42555502] 5 5
train:	 Loss = 1.1457,	 Acc = 0.6346,	 Loss Con1 = 1.2794,	 Loss Con2 = 1.7400
3166 0.154
3140 0.741
2597 0.848
1324 0.856
889 0.822
762 0.84
70 0.829
20 0.85
0.8076573506021358
0.797339246119734
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1557,	 Acc = 0.6467,	 Loss Con1 = 1.3409,	 Loss Con2 = 1.8425
745 0.219
668 0.696
714 0.87
562 0.795
190 0.747
103 0.835
11 0.909
7 0.857
0.7880266075388027
0.797339246119734

 ===== Epoch 8	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 4 0
train:	 Loss = 1.1249,	 Acc = 0.6446,	 Loss Con1 = 1.2797,	 Loss Con2 = 1.7162
3160 0.156
3146 0.761
2596 0.855
1323 0.865
890 0.833
762 0.845
71 0.859
20 0.9
0.8198228882833788
0.797339246119734
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1193,	 Acc = 0.6527,	 Loss Con1 = 1.3409,	 Loss Con2 = 1.7910
745 0.219
668 0.701
714 0.874
562 0.82
190 0.742
103 0.806
11 1.0
7 1.0
0.7960088691796009
0.797339246119734

 ===== Epoch 9	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 3 0
train:	 Loss = 1.1184,	 Acc = 0.6435,	 Loss Con1 = 1.2795,	 Loss Con2 = 1.7048
3161 0.155
3146 0.753
2595 0.859
1325 0.866
887 0.834
763 0.851
71 0.845
20 0.85
0.8187805154990349
0.797339246119734
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 4
val:	 Loss = 1.1358,	 Acc = 0.6467,	 Loss Con1 = 1.3409,	 Loss Con2 = 1.8141
745 0.215
668 0.683
714 0.868
562 0.831
190 0.732
103 0.796
11 0.818
7 1.0
0.7893569844789357
0.797339246119734

 ===== Epoch 10	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 5 0
train:	 Loss = 1.1108,	 Acc = 0.6448,	 Loss Con1 = 1.2799,	 Loss Con2 = 1.6925
3165 0.154
3141 0.76
2595 0.857
1321 0.865
892 0.841
763 0.849
71 0.859
20 0.9
0.8214245143701011
0.797339246119734
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 3
val:	 Loss = 1.1521,	 Acc = 0.6403,	 Loss Con1 = 1.3409,	 Loss Con2 = 1.8250
745 0.212
668 0.651
714 0.857
562 0.838
190 0.742
103 0.854
11 0.818
7 1.0
0.7818181818181819
0.797339246119734

 ===== Epoch 11	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634  1.0838137   2.133753
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053  1.0718102   2.1056786
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 3 3
train:	 Loss = 1.1032,	 Acc = 0.6446,	 Loss Con1 = 1.2794,	 Loss Con2 = 1.6823
3166 0.156
3143 0.758
2592 0.856
1322 0.865
892 0.84
762 0.853
71 0.859
20 0.9
0.8203817314246762
0.797339246119734
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 3
val:	 Loss = 1.1452,	 Acc = 0.6423,	 Loss Con1 = 1.3409,	 Loss Con2 = 1.7873
745 0.217
668 0.665
714 0.85
562 0.826
190 0.779
103 0.825
11 0.909
7 1.0
0.7827050997782705
0.797339246119734

 ===== Epoch 12	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 1 0
train:	 Loss = 1.1034,	 Acc = 0.6462,	 Loss Con1 = 1.2792,	 Loss Con2 = 1.6722
3166 0.154
3144 0.76
2594 0.861
1322 0.865
890 0.84
761 0.859
71 0.831
20 0.9
0.8232219950011361
0.797339246119734
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1438,	 Acc = 0.6537,	 Loss Con1 = 1.3409,	 Loss Con2 = 1.7748
745 0.219
668 0.702
714 0.861
562 0.827
190 0.758
103 0.845
11 1.0
7 1.0
0.797339246119734
0.797339246119734

 ===== Epoch 13	 =====
[ 1.9759319   0.52093637  1.9550589   2.2462683   3.0427456   1.5645767
  1.4062595   2.0655496   3.6041536   1.2042707  -0.4178274  -0.43225467
  2.7582555   0.27496815] [ 1.9759319   0.31729496  1.9547817   2.0752206   3.042925    1.3734695
  1.4060723   1.8604664   3.6041536   0.9908363  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 5 5
train:	 Loss = 1.0965,	 Acc = 0.6457,	 Loss Con1 = 1.2789,	 Loss Con2 = 1.6699
3163 0.154
3143 0.76
2597 0.86
1325 0.866
891 0.838
758 0.85
71 0.873
20 0.9
0.8224872231686542
0.797339246119734
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1149,	 Acc = 0.6473,	 Loss Con1 = 1.3409,	 Loss Con2 = 1.7736
745 0.216
668 0.707
714 0.868
562 0.799
190 0.732
103 0.816
11 0.909
7 1.0
0.78980044345898
0.797339246119734

 ===== Epoch 14	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 4 0
train:	 Loss = 1.0909,	 Acc = 0.6493,	 Loss Con1 = 1.2784,	 Loss Con2 = 1.6675
3164 0.154
3144 0.77
2597 0.864
1324 0.866
888 0.836
761 0.858
70 0.871
20 0.9
0.8274647887323944
0.797339246119734
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 5
val:	 Loss = 1.1312,	 Acc = 0.6473,	 Loss Con1 = 1.3409,	 Loss Con2 = 1.7529
745 0.219
668 0.671
714 0.871
562 0.829
190 0.732
103 0.854
11 0.818
7 1.0
0.7889135254988914
0.797339246119734

 ===== Epoch 15	 =====
[ 0.9466294   0.84796435 -0.5993768  -0.57551634 -0.6348712  -0.6508424
  2.8207726   0.7109717   0.2852849   1.2376881   2.718286    1.968486
  1.3828932   3.5333538 ] [ 0.9466294   0.84796435 -0.59937984 -0.57549053 -0.6347739  -0.6507814
  2.820514    0.71086323  0.2852849   1.2376881   2.718286    1.968486
  1.3828932   3.5333538 ] 0 0
train:	 Loss = 1.0919,	 Acc = 0.6481,	 Loss Con1 = 1.2790,	 Loss Con2 = 1.6864
3162 0.153
3144 0.768
2596 0.862
1323 0.866
891 0.828
761 0.865
71 0.887
20 0.9
0.8260277083806495
0.797339246119734
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 5
val:	 Loss = 1.1017,	 Acc = 0.6507,	 Loss Con1 = 1.3409,	 Loss Con2 = 1.8456
745 0.217
668 0.69
714 0.882
562 0.808
190 0.732
103 0.864
11 0.909
7 1.0
0.7937915742793792
0.797339246119734

 ===== Epoch 16	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 5 0
train:	 Loss = 1.0915,	 Acc = 0.6456,	 Loss Con1 = 1.2790,	 Loss Con2 = 1.6722
3162 0.156
3146 0.764
2596 0.857
1323 0.861
890 0.834
760 0.85
71 0.859
20 0.9
0.8212582330229389
0.797339246119734
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1054,	 Acc = 0.6527,	 Loss Con1 = 1.3409,	 Loss Con2 = 1.8000
745 0.219
668 0.69
714 0.874
562 0.822
190 0.747
103 0.864
11 0.909
7 1.0
0.7960088691796009
0.797339246119734

 ===== Epoch 17	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013  1.0390023   1.8873543  -0.4178274  -0.43225467
  1.8202584   0.47708726] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355  1.0401601   1.7272705  -0.4178274  -0.43225467
  1.8169414   0.3018825 ] 5 5
train:	 Loss = 1.0839,	 Acc = 0.6490,	 Loss Con1 = 1.2793,	 Loss Con2 = 1.6696
3164 0.155
3143 0.77
2596 0.861
1323 0.87
892 0.835
759 0.852
71 0.887
20 0.9
0.8266696955929124
0.797339246119734
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 5
val:	 Loss = 1.1200,	 Acc = 0.6530,	 Loss Con1 = 1.3409,	 Loss Con2 = 1.7690
745 0.217
668 0.693
714 0.874
562 0.827
190 0.742
103 0.854
11 0.818
7 1.0
0.7968957871396896
0.797339246119734

 ===== Epoch 18	 =====
[ 1.2205361   2.122996    1.4325364   0.16984896 -0.6348712  -0.6508424
  2.377052    1.8473557  -0.46656546 -0.4823711   2.7689023   3.0870712
 -0.45947683 -0.42555502] [ 1.2609041   2.1089907   1.4561365   0.18256703 -0.6347739  -0.6507814
  2.41449     1.8436226  -0.46656546 -0.4823711   2.794371    3.075242
 -0.45947683 -0.42555502] 2 2
train:	 Loss = 1.0872,	 Acc = 0.6464,	 Loss Con1 = 1.2789,	 Loss Con2 = 1.6777
3163 0.153
3144 0.767
2596 0.861
1324 0.86
890 0.836
761 0.85
70 0.857
20 0.9
0.8236229415105054
0.797339246119734
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1173,	 Acc = 0.6493,	 Loss Con1 = 1.3409,	 Loss Con2 = 1.7708
745 0.215
668 0.686
714 0.868
562 0.827
190 0.747
103 0.845
11 0.818
7 1.0
0.7929046563192904
0.797339246119734

 ===== Epoch 19	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 2 0
train:	 Loss = 1.0788,	 Acc = 0.6495,	 Loss Con1 = 1.2785,	 Loss Con2 = 1.6710
3165 0.152
3141 0.769
2599 0.867
1323 0.867
890 0.838
759 0.854
71 0.887
20 0.9
0.8283539702374191
0.797339246119734
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1131,	 Acc = 0.6457,	 Loss Con1 = 1.3409,	 Loss Con2 = 1.8003
745 0.219
668 0.684
714 0.863
562 0.815
190 0.732
103 0.845
11 0.909
7 1.0
0.7866962305986697
0.797339246119734

 ===== Epoch 20	 =====
[-0.4557189  -0.49292567  1.4225554   0.31694648 -0.6348712  -0.6508424
  2.0101178   2.1189077  -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.4223351   0.14561653 -0.6347739  -0.6507814
  2.0099      1.9138232  -0.46656546 -0.4823711   2.3537278   3.3930767
 -0.45947683 -0.42555502] 5 5
train:	 Loss = 1.0806,	 Acc = 0.6504,	 Loss Con1 = 1.2792,	 Loss Con2 = 1.6744
3165 0.153
3142 0.771
2597 0.868
1320 0.866
892 0.837
761 0.859
71 0.873
20 0.9
0.8293763489719413
0.797339246119734
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1135,	 Acc = 0.6463,	 Loss Con1 = 1.3409,	 Loss Con2 = 1.8878
745 0.217
668 0.672
714 0.864
562 0.833
190 0.726
103 0.845
11 1.0
7 1.0
0.7880266075388027
0.797339246119734

 ===== Epoch 21	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 3 0
train:	 Loss = 1.0837,	 Acc = 0.6468,	 Loss Con1 = 1.2799,	 Loss Con2 = 1.6814
3168 0.155
3142 0.769
2592 0.856
1323 0.859
891 0.841
762 0.848
70 0.9
20 0.9
0.82375
0.797339246119734
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1042,	 Acc = 0.6483,	 Loss Con1 = 1.3409,	 Loss Con2 = 1.8241
745 0.216
668 0.687
714 0.866
562 0.829
190 0.711
103 0.854
11 1.0
7 1.0
0.7911308203991131
0.797339246119734

 ===== Epoch 22	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 4 0
train:	 Loss = 1.0785,	 Acc = 0.6495,	 Loss Con1 = 1.2785,	 Loss Con2 = 1.6482
3164 0.153
3146 0.769
2598 0.868
1321 0.866
889 0.838
759 0.85
71 0.887
20 0.9
0.8279191276692413
0.797339246119734
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1456,	 Acc = 0.6427,	 Loss Con1 = 1.3409,	 Loss Con2 = 1.7670
745 0.219
668 0.663
714 0.849
562 0.827
190 0.784
103 0.835
11 0.818
7 1.0
0.7827050997782705
0.797339246119734

 ===== Epoch 23	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 6 0
train:	 Loss = 1.0811,	 Acc = 0.6496,	 Loss Con1 = 1.2785,	 Loss Con2 = 1.6500
3163 0.154
3145 0.769
2596 0.865
1323 0.868
891 0.84
759 0.847
71 0.901
20 0.9
0.8275979557069847
0.797339246119734
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1010,	 Acc = 0.6460,	 Loss Con1 = 1.3409,	 Loss Con2 = 1.8235
745 0.213
668 0.683
714 0.871
562 0.822
190 0.716
103 0.825
11 1.0
7 1.0
0.7889135254988914
0.797339246119734

 ===== Epoch 24	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 4 0
train:	 Loss = 1.0770,	 Acc = 0.6490,	 Loss Con1 = 1.2798,	 Loss Con2 = 1.6578
3160 0.155
3142 0.768
2595 0.862
1326 0.867
893 0.841
761 0.849
71 0.887
20 0.9
0.8262942779291553
0.797339246119734
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.0978,	 Acc = 0.6397,	 Loss Con1 = 1.3409,	 Loss Con2 = 1.8264
745 0.217
668 0.666
714 0.87
562 0.811
190 0.711
103 0.806
11 0.909
7 1.0
0.7791574279379158
0.797339246119734

 ===== Epoch 25	 =====
[-0.4557189  -0.49292567  1.7706927   0.6780671  -0.6348712  -0.6508424
  2.4316154   2.312331   -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.7631142   0.6979798  -0.6347739  -0.6507814
  2.4449515   2.3374836  -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 4 4
train:	 Loss = 1.0782,	 Acc = 0.6491,	 Loss Con1 = 1.2793,	 Loss Con2 = 1.6711
3164 0.153
3142 0.772
2595 0.863
1325 0.863
892 0.842
760 0.849
70 0.9
20 0.9
0.8274647887323944
0.797339246119734
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1580,	 Acc = 0.6457,	 Loss Con1 = 1.3409,	 Loss Con2 = 1.8542
745 0.216
668 0.734
714 0.859
562 0.797
190 0.653
103 0.806
11 1.0
7 1.0
0.7875831485587583
0.797339246119734

 ===== Epoch 26	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 1 0
train:	 Loss = 1.0798,	 Acc = 0.6469,	 Loss Con1 = 1.2775,	 Loss Con2 = 1.6622
3164 0.156
3146 0.768
2599 0.862
1324 0.861
885 0.827
761 0.844
69 0.899
20 0.9
0.8233757383007724
0.797339246119734
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.0910,	 Acc = 0.6383,	 Loss Con1 = 1.3409,	 Loss Con2 = 1.8518
745 0.216
668 0.66
714 0.864
562 0.808
190 0.721
103 0.845
11 1.0
7 1.0
0.7778270509977827
0.797339246119734

 ===== Epoch 27	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 6 0
train:	 Loss = 1.0739,	 Acc = 0.6495,	 Loss Con1 = 1.2790,	 Loss Con2 = 1.6734
3167 0.154
3140 0.774
2595 0.864
1324 0.863
889 0.835
762 0.853
71 0.887
20 0.9
0.8278604704010908
0.797339246119734
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1012,	 Acc = 0.6477,	 Loss Con1 = 1.3409,	 Loss Con2 = 1.8620
745 0.216
668 0.678
714 0.871
562 0.835
190 0.711
103 0.835
11 1.0
7 0.857
0.7902439024390244
0.797339246119734

 ===== Epoch 28	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013  0.3122153   0.66332453 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355  0.31913212  0.62628514 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 3 3
train:	 Loss = 1.0737,	 Acc = 0.6496,	 Loss Con1 = 1.2793,	 Loss Con2 = 1.6844
3166 0.153
3141 0.777
2596 0.863
1321 0.864
892 0.835
761 0.844
71 0.859
20 0.9
0.8282208588957055
0.797339246119734
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1213,	 Acc = 0.6337,	 Loss Con1 = 1.3409,	 Loss Con2 = 1.9857
745 0.216
668 0.663
714 0.85
562 0.806
190 0.716
103 0.825
11 0.909
7 0.857
0.7716186252771619
0.797339246119734

 ===== Epoch 29	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
  1.1440271   2.6149433 ] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
  1.1146488   2.6302388 ] 1 1
train:	 Loss = 1.0798,	 Acc = 0.6507,	 Loss Con1 = 1.2777,	 Loss Con2 = 1.6975
3167 0.152
3144 0.778
2596 0.866
1323 0.865
889 0.837
758 0.85
71 0.859
20 0.9
0.8301329394387001
0.797339246119734
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.0955,	 Acc = 0.6437,	 Loss Con1 = 1.3409,	 Loss Con2 = 1.8551
745 0.215
668 0.665
714 0.868
562 0.829
190 0.726
103 0.835
11 0.909
7 1.0
0.7853658536585366
0.797339246119734

 ===== Epoch 30	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013  0.65023166  0.4897559  -0.4178274  -0.43225467
  1.9713173   1.2095509 ] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355  0.6388655   0.54890114 -0.4178274  -0.43225467
  1.9688148   1.2344934 ] 4 4
train:	 Loss = 1.0710,	 Acc = 0.6528,	 Loss Con1 = 1.2793,	 Loss Con2 = 1.6826
3165 0.153
3137 0.779
2600 0.87
1324 0.868
891 0.838
760 0.85
71 0.901
20 0.9
0.8326706804498466
0.797339246119734
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.0933,	 Acc = 0.6400,	 Loss Con1 = 1.3409,	 Loss Con2 = 1.8521
745 0.219
668 0.657
714 0.86
562 0.824
190 0.732
103 0.816
11 1.0
7 1.0
0.7791574279379158
0.797339246119734

 ===== Epoch 31	 =====
[ 0.2526381   1.2819188  -0.5993768  -0.57551634  1.2524389   1.9837356
 -0.43886673 -0.47319013  1.3838277   0.6844747  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [ 0.26930997  1.4101622  -0.59937984 -0.57549053  1.2629676   2.0942843
 -0.4389609  -0.47327355  1.3975186   0.82017004 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 2 4
train:	 Loss = 1.0725,	 Acc = 0.6525,	 Loss Con1 = 1.2790,	 Loss Con2 = 1.6749
3164 0.153
3145 0.781
2593 0.862
1324 0.867
891 0.84
760 0.864
71 0.873
20 0.9
0.8320081781008632
0.797339246119734
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1273,	 Acc = 0.6473,	 Loss Con1 = 1.3409,	 Loss Con2 = 1.8086
745 0.22
668 0.681
714 0.864
562 0.826
190 0.721
103 0.854
11 0.909
7 1.0
0.788470066518847
0.797339246119734

 ===== Epoch 32	 =====
[ 2.3234308   1.7530274  -0.5993768  -0.57551634 -0.6348712  -0.6508424
  1.2937279   3.0663075   3.7154536   2.6767807   1.1905372   1.0306519
  3.073787    1.8939043 ] [ 2.3280604   1.7836188  -0.59937984 -0.57549053 -0.6347739  -0.6507814
  1.3190331   3.099276    3.7331603   2.693932    1.1806513   1.0706738
  3.0748088   1.9138728 ] 4 4
train:	 Loss = 1.0747,	 Acc = 0.6474,	 Loss Con1 = 1.2794,	 Loss Con2 = 1.6647
3161 0.153
3148 0.771
2594 0.861
1323 0.859
889 0.835
762 0.844
71 0.887
20 0.9
0.8249120018167367
0.797339246119734
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1027,	 Acc = 0.6467,	 Loss Con1 = 1.3409,	 Loss Con2 = 1.8663
745 0.216
668 0.683
714 0.875
562 0.826
190 0.689
103 0.825
11 1.0
7 1.0
0.7889135254988914
0.797339246119734

 ===== Epoch 33	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013  0.39072683  0.59054875 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355  0.39072683  0.80398315 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 6 4
train:	 Loss = 1.0728,	 Acc = 0.6497,	 Loss Con1 = 1.2790,	 Loss Con2 = 1.6668
3160 0.152
3147 0.775
2595 0.866
1324 0.86
889 0.834
762 0.852
71 0.859
20 0.9
0.8282243415077203
0.797339246119734
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1129,	 Acc = 0.6460,	 Loss Con1 = 1.3409,	 Loss Con2 = 1.8416
745 0.219
668 0.668
714 0.867
562 0.835
190 0.721
103 0.845
11 0.909
7 1.0
0.7871396895787139
0.797339246119734

 ===== Epoch 34	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634  0.8395455   2.2786355
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053  0.8314573   2.2492137
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 3 3
train:	 Loss = 1.0649,	 Acc = 0.6498,	 Loss Con1 = 1.2800,	 Loss Con2 = 1.6830
3161 0.153
3142 0.776
2596 0.863
1325 0.864
891 0.834
762 0.848
71 0.873
20 0.85
0.8279777449755876
0.797339246119734
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1098,	 Acc = 0.6477,	 Loss Con1 = 1.3409,	 Loss Con2 = 1.8772
745 0.219
668 0.687
714 0.87
562 0.819
190 0.711
103 0.845
11 1.0
7 1.0
0.7893569844789357
0.797339246119734

 ===== Epoch 35	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634  0.551504    2.589035
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053  0.55159914  2.588997
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 0
train:	 Loss = 1.0676,	 Acc = 0.6502,	 Loss Con1 = 1.2801,	 Loss Con2 = 1.6756
3159 0.151
3142 0.776
2600 0.864
1320 0.863
893 0.84
763 0.849
71 0.887
20 0.9
0.8290384833692814
0.797339246119734
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.0825,	 Acc = 0.6437,	 Loss Con1 = 1.3409,	 Loss Con2 = 1.8448
745 0.219
668 0.692
714 0.864
562 0.813
190 0.684
103 0.816
11 1.0
7 1.0
0.7840354767184036
0.797339246119734

 ===== Epoch 36	 =====
[-0.4557189  -0.49292567  1.8425046   0.5209067  -0.6348712  -0.6508424
  2.3371534   2.0955553  -0.19880703  3.0822582  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.8422393   0.52109283 -0.6347739  -0.6507814
  2.3369193   2.0954175  -0.19880703  3.0822582  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 0
train:	 Loss = 1.0676,	 Acc = 0.6526,	 Loss Con1 = 1.2801,	 Loss Con2 = 1.6754
3165 0.154
3140 0.781
2598 0.864
1319 0.867
893 0.84
762 0.853
71 0.887
20 0.9
0.8318754969896626
0.797339246119734
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.0975,	 Acc = 0.6463,	 Loss Con1 = 1.3409,	 Loss Con2 = 1.8794
745 0.219
668 0.678
714 0.867
562 0.826
190 0.726
103 0.816
11 1.0
7 1.0
0.7875831485587583
0.797339246119734

 ===== Epoch 37	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013  0.74407655  2.5262697  -0.4178274  -0.43225467
  1.4310352   0.5567852 ] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355  0.74407655  2.7397041  -0.4178274  -0.43225467
  1.4310352   0.7823619 ] 6 6
train:	 Loss = 1.0741,	 Acc = 0.6517,	 Loss Con1 = 1.2798,	 Loss Con2 = 1.6921
3168 0.154
3141 0.777
2593 0.867
1323 0.87
891 0.838
761 0.846
71 0.901
20 0.8
0.8309090909090909
0.797339246119734
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1023,	 Acc = 0.6437,	 Loss Con1 = 1.3409,	 Loss Con2 = 1.8416
745 0.219
668 0.683
714 0.87
562 0.813
190 0.684
103 0.835
11 1.0
7 1.0
0.7840354767184036
0.797339246119734

 ===== Epoch 38	 =====
[-0.4557189  -0.49292567  1.0007601   2.2634459   2.0287008   1.0657693
  0.02152531  1.1151361   2.4816992   0.25947416 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0067184   2.2861645   2.0190272   1.0795884
  0.00550879  1.2268325   2.456627    0.27053708 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 1 4
train:	 Loss = 1.0672,	 Acc = 0.6503,	 Loss Con1 = 1.2788,	 Loss Con2 = 1.6979
3164 0.152
3140 0.776
2599 0.862
1323 0.865
890 0.84
762 0.848
70 0.929
20 0.9
0.8292821444797819
0.797339246119734
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 5
val:	 Loss = 1.1124,	 Acc = 0.6430,	 Loss Con1 = 1.3409,	 Loss Con2 = 1.7921
745 0.219
668 0.65
714 0.87
562 0.833
190 0.721
103 0.864
11 0.909
7 1.0
0.7831485587583149
0.797339246119734

 ===== Epoch 39	 =====
[ 0.08051484  2.64656    -0.5993768  -0.57551634  1.2145989   2.3114939
 -0.43886673 -0.47319013  1.0617037   1.0025575  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [ 0.11692874  2.5693147  -0.59937984 -0.57549053  1.2386142   2.2966132
 -0.4389609  -0.47327355  1.0980372   1.0181167  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 2 2
train:	 Loss = 1.0691,	 Acc = 0.6493,	 Loss Con1 = 1.2795,	 Loss Con2 = 1.6805
3162 0.153
3144 0.775
2597 0.862
1323 0.864
889 0.838
762 0.845
71 0.873
20 0.9
0.8275039745627981
0.797339246119734
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 5
val:	 Loss = 1.1015,	 Acc = 0.6473,	 Loss Con1 = 1.3409,	 Loss Con2 = 1.8008
745 0.213
668 0.69
714 0.874
562 0.826
190 0.695
103 0.835
11 0.909
7 0.857
0.7906873614190687
0.797339246119734

 ===== Epoch 40	 =====
[ 3.6619086   2.3149574   2.1547556   1.0227894   0.2568212   1.4702144
  3.877702    2.368227   -0.46656546 -0.4823711   4.056023    3.285861
 -0.45947683 -0.42555502] [ 3.6465485   2.2955775   2.1565003   1.0055736   0.25430825  1.4104905
  3.8632078   2.3513155  -0.46656546 -0.4823711   4.0366483   3.2740915
 -0.45947683 -0.42555502] 3 3
train:	 Loss = 1.0663,	 Acc = 0.6525,	 Loss Con1 = 1.2798,	 Loss Con2 = 1.6884
3160 0.155
3145 0.782
2597 0.863
1323 0.863
889 0.844
763 0.851
71 0.859
20 0.85
0.8309491371480472
0.797339246119734
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 5
val:	 Loss = 1.1145,	 Acc = 0.6507,	 Loss Con1 = 1.3409,	 Loss Con2 = 1.8118
745 0.22
668 0.699
714 0.871
562 0.831
190 0.689
103 0.816
11 0.909
7 1.0
0.7929046563192904
0.797339246119734

 ===== Epoch 41	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 5 0
train:	 Loss = 1.0680,	 Acc = 0.6517,	 Loss Con1 = 1.2797,	 Loss Con2 = 1.7006
3160 0.151
3146 0.775
2596 0.866
1323 0.872
891 0.845
761 0.85
71 0.901
20 0.9
0.8314032697547684
0.797339246119734
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 5
val:	 Loss = 1.1163,	 Acc = 0.6470,	 Loss Con1 = 1.3409,	 Loss Con2 = 1.7923
745 0.221
668 0.681
714 0.878
562 0.826
190 0.684
103 0.806
11 0.909
7 1.0
0.7875831485587583
0.797339246119734

 ===== Epoch 42	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 1 0
train:	 Loss = 1.0640,	 Acc = 0.6507,	 Loss Con1 = 1.2798,	 Loss Con2 = 1.6990
3164 0.154
3142 0.773
2596 0.867
1323 0.864
889 0.839
763 0.853
71 0.887
20 0.9
0.8292821444797819
0.797339246119734
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1198,	 Acc = 0.6393,	 Loss Con1 = 1.3409,	 Loss Con2 = 1.8737
745 0.217
668 0.687
714 0.866
562 0.801
190 0.674
103 0.816
11 0.909
7 1.0
0.7787139689578714
0.797339246119734

 ===== Epoch 43	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 5 0
train:	 Loss = 1.0702,	 Acc = 0.6499,	 Loss Con1 = 1.2791,	 Loss Con2 = 1.7181
3165 0.153
3142 0.778
2596 0.861
1324 0.865
889 0.843
761 0.845
71 0.887
20 0.85
0.8286947631489265
0.797339246119734
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1104,	 Acc = 0.6457,	 Loss Con1 = 1.3409,	 Loss Con2 = 1.8689
745 0.213
668 0.689
714 0.877
562 0.824
190 0.674
103 0.806
11 1.0
7 1.0
0.788470066518847
0.797339246119734

 ===== Epoch 44	 =====
[-0.4557189  -0.49292567  1.6271957   2.1738763  -0.28400442  2.16825
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.6113386   2.1563547  -0.2955369   2.0281837
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 3 5
train:	 Loss = 1.0645,	 Acc = 0.6527,	 Loss Con1 = 1.2782,	 Loss Con2 = 1.6996
3166 0.155
3145 0.777
2595 0.866
1322 0.87
890 0.843
759 0.852
71 0.901
20 0.9
0.8316291751874574
0.797339246119734
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1298,	 Acc = 0.6473,	 Loss Con1 = 1.3409,	 Loss Con2 = 1.9199
745 0.219
668 0.669
714 0.875
562 0.827
190 0.732
103 0.825
11 1.0
7 1.0
0.7889135254988914
0.797339246119734

 ===== Epoch 45	 =====
[-0.4557189  -0.49292567  1.8116294   2.250881    0.57796216  0.82976764
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.7944753   2.235244    0.58463424  0.7870891
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 3 3
train:	 Loss = 1.0674,	 Acc = 0.6498,	 Loss Con1 = 1.2788,	 Loss Con2 = 1.7177
3164 0.152
3143 0.777
2598 0.858
1321 0.871
889 0.843
762 0.845
71 0.901
20 0.85
0.8286006360745116
0.797339246119734
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 5
val:	 Loss = 1.0954,	 Acc = 0.6513,	 Loss Con1 = 1.3409,	 Loss Con2 = 1.8644
745 0.219
668 0.696
714 0.864
562 0.835
190 0.7
103 0.864
11 1.0
7 1.0
0.7942350332594235
0.797339246119734

 ===== Epoch 46	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013  0.6480501   0.19816063 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355  0.6724124   0.14332561 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 3 3
train:	 Loss = 1.0665,	 Acc = 0.6502,	 Loss Con1 = 1.2798,	 Loss Con2 = 1.6978
3159 0.152
3144 0.775
2597 0.864
1325 0.865
890 0.839
762 0.85
71 0.887
20 0.9
0.8290384833692814
0.797339246119734
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 5
val:	 Loss = 1.1047,	 Acc = 0.6477,	 Loss Con1 = 1.3409,	 Loss Con2 = 1.8743
745 0.215
668 0.674
714 0.873
562 0.835
190 0.721
103 0.845
11 0.909
7 1.0
0.7906873614190687
0.797339246119734

 ===== Epoch 47	 =====
[ 0.48820654  2.8254924   1.9525173   1.3333004   0.8090391  -0.07635386
  2.4100595   3.0060875  -0.14895152  2.9150558  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [ 0.4640942   2.7501633   1.9492124   1.3147806   0.827312   -0.10426831
  2.3855577   2.984787   -0.16848895  2.687589   -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 3 3
train:	 Loss = 1.0635,	 Acc = 0.6528,	 Loss Con1 = 1.2800,	 Loss Con2 = 1.7089
3164 0.155
3139 0.785
2600 0.863
1321 0.864
892 0.84
761 0.852
71 0.873
20 0.9
0.8318945933666515
0.797339246119734
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1128,	 Acc = 0.6473,	 Loss Con1 = 1.3409,	 Loss Con2 = 1.8313
745 0.212
668 0.686
714 0.868
562 0.829
190 0.721
103 0.835
11 0.909
7 1.0
0.7911308203991131
0.797339246119734

 ===== Epoch 48	 =====
[ 0.5272944   2.551355    1.7726638   1.9890324   2.136458    0.81888956
  0.9167561   3.0325003   1.9429398   0.2124088  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [ 0.5644654   2.5114615   1.7972002   1.9798506   2.1638188   0.823302
  0.945622    2.9956198   1.9727652   0.23320454 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 2 2
train:	 Loss = 1.0655,	 Acc = 0.6506,	 Loss Con1 = 1.2796,	 Loss Con2 = 1.7105
3164 0.154
3143 0.777
2596 0.864
1323 0.862
891 0.84
760 0.847
71 0.915
20 0.85
0.8291685597455702
0.797339246119734
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 5
val:	 Loss = 1.1430,	 Acc = 0.6417,	 Loss Con1 = 1.3409,	 Loss Con2 = 1.9134
745 0.216
668 0.648
714 0.868
562 0.831
190 0.742
103 0.835
11 0.909
7 1.0
0.7822616407982261
0.797339246119734

 ===== Epoch 49	 =====
[ 2.780202    2.1339772   1.9328624   0.77443576  0.19745034 -0.22070454
  3.0552685   2.2813993  -0.46656546 -0.4823711   3.1911747   3.451087
 -0.45947683 -0.42555502] [ 2.7804954   1.9302324   1.9328126   0.6031716  -0.6347739  -0.6507814
  3.055255    2.0762162  -0.46656546 -0.4823711   3.1913114   3.2303615
 -0.45947683 -0.42555502] 5 5
train:	 Loss = 1.0611,	 Acc = 0.6514,	 Loss Con1 = 1.2794,	 Loss Con2 = 1.7272
3166 0.152
3139 0.775
2598 0.871
1323 0.864
890 0.836
761 0.855
71 0.873
20 0.85
0.8310611224721655
0.797339246119734
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 5
val:	 Loss = 1.1274,	 Acc = 0.6477,	 Loss Con1 = 1.3409,	 Loss Con2 = 1.9004
745 0.216
668 0.681
714 0.867
562 0.826
190 0.732
103 0.845
11 1.0
7 1.0
0.7902439024390244
0.797339246119734

 ===== Epoch 50	 =====
[-0.4557189  -0.49292567  0.86626905  2.3279378   1.1075468   0.2288394
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  0.8484239   2.3029373   1.1220872   0.20300932
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 3 3
train:	 Loss = 1.0664,	 Acc = 0.6513,	 Loss Con1 = 1.2797,	 Loss Con2 = 1.7098
3159 0.154
3146 0.771
2596 0.866
1325 0.869
890 0.843
761 0.857
71 0.901
20 0.85
0.8296060846861165
0.797339246119734
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1204,	 Acc = 0.6407,	 Loss Con1 = 1.3409,	 Loss Con2 = 1.9458
745 0.219
668 0.669
714 0.854
562 0.824
190 0.726
103 0.806
11 1.0
7 1.0
0.7800443458980044
0.797339246119734

 ===== Epoch 51	 =====
[ 1.2171078   1.4320642  -0.5993768  -0.57551634  2.586041    2.7343588
 -0.43886673 -0.47319013  2.9004557   2.3986182  -0.4178274  -0.43225467
  2.2282302   1.3087692 ] [ 1.217163    1.2288268  -0.59937984 -0.57549053  2.5863972   2.5433083
 -0.4389609  -0.47327355  2.9006205   2.1853461  -0.4178274  -0.43225467
  2.2282276   1.0834028 ] 5 5
train:	 Loss = 1.0595,	 Acc = 0.6521,	 Loss Con1 = 1.2795,	 Loss Con2 = 1.7041
3162 0.154
3143 0.779
2594 0.867
1326 0.863
891 0.836
762 0.852
71 0.887
19 0.895
0.8309107426754485
0.797339246119734
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1117,	 Acc = 0.6500,	 Loss Con1 = 1.3409,	 Loss Con2 = 1.8972
745 0.213
668 0.69
714 0.874
562 0.835
190 0.705
103 0.835
11 0.909
7 1.0
0.7942350332594235
0.797339246119734

 ===== Epoch 52	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634  1.3067956   1.5381621
 -0.43886673 -0.47319013  0.2995255   2.4172218  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053  1.3045753   1.5217949
 -0.4389609  -0.47327355  0.29179853  2.3568022  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 4 3
train:	 Loss = 1.0595,	 Acc = 0.6526,	 Loss Con1 = 1.2787,	 Loss Con2 = 1.7461
3166 0.154
3144 0.781
2595 0.869
1323 0.863
890 0.835
760 0.857
70 0.886
20 0.85
0.8320836173596909
0.797339246119734
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1069,	 Acc = 0.6437,	 Loss Con1 = 1.3409,	 Loss Con2 = 1.9431
745 0.219
668 0.68
714 0.867
562 0.82
190 0.695
103 0.816
11 1.0
7 1.0
0.7840354767184036
0.797339246119734

 ===== Epoch 53	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 5 0
train:	 Loss = 1.0614,	 Acc = 0.6532,	 Loss Con1 = 1.2791,	 Loss Con2 = 1.7464
3167 0.154
3138 0.781
2599 0.868
1321 0.869
891 0.837
761 0.853
71 0.873
20 0.9
0.8328599022838313
0.797339246119734
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 5
val:	 Loss = 1.0990,	 Acc = 0.6460,	 Loss Con1 = 1.3409,	 Loss Con2 = 1.8874
745 0.213
668 0.689
714 0.873
562 0.827
190 0.679
103 0.816
11 1.0
7 1.0
0.7889135254988914
0.797339246119734

 ===== Epoch 54	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 0
train:	 Loss = 1.0643,	 Acc = 0.6518,	 Loss Con1 = 1.2793,	 Loss Con2 = 1.7312
3161 0.154
3146 0.777
2594 0.865
1324 0.873
890 0.838
762 0.846
71 0.845
20 0.9
0.8303622118769161
0.797339246119734
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.0937,	 Acc = 0.6463,	 Loss Con1 = 1.3409,	 Loss Con2 = 1.9850
745 0.215
668 0.689
714 0.867
562 0.822
190 0.7
103 0.845
11 1.0
7 1.0
0.7889135254988914
0.797339246119734

 ===== Epoch 55	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.36579958  1.8150846
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.36570904  1.8148068
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 0
train:	 Loss = 1.0611,	 Acc = 0.6517,	 Loss Con1 = 1.2791,	 Loss Con2 = 1.7767
3164 0.153
3142 0.778
2594 0.867
1324 0.869
891 0.834
763 0.849
70 0.9
20 0.9
0.8309859154929577
0.797339246119734
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1019,	 Acc = 0.6487,	 Loss Con1 = 1.3409,	 Loss Con2 = 1.9156
745 0.213
668 0.698
714 0.87
562 0.82
190 0.716
103 0.825
11 1.0
7 1.0
0.7924611973392461
0.797339246119734

 ===== Epoch 56	 =====
[-0.4557189  -0.49292567  2.3382058   1.8640469   1.0082521   0.86050975
 -0.43886673 -0.47319013  0.40221724  3.3140976  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  2.3378954   1.8644274   1.0083948   0.86055243
 -0.4389609  -0.47327355  0.4022259   3.3140764  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 0
train:	 Loss = 1.0552,	 Acc = 0.6555,	 Loss Con1 = 1.2797,	 Loss Con2 = 1.7940
3160 0.155
3148 0.78
2594 0.872
1321 0.873
892 0.843
762 0.858
71 0.887
20 0.8
0.8350363306085377
0.797339246119734
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1023,	 Acc = 0.6497,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.0293
745 0.216
668 0.678
714 0.873
562 0.835
190 0.716
103 0.864
11 1.0
7 1.0
0.7929046563192904
0.797339246119734

 ===== Epoch 57	 =====
[-0.4557189  -0.49292567  0.42708352  2.4582875  -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 4 6
train:	 Loss = 1.0615,	 Acc = 0.6518,	 Loss Con1 = 1.2797,	 Loss Con2 = 1.8039
3166 0.151
3141 0.781
2597 0.868
1320 0.867
891 0.838
762 0.844
71 0.901
20 0.9
0.8318563962735742
0.797339246119734
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.0893,	 Acc = 0.6490,	 Loss Con1 = 1.3409,	 Loss Con2 = 1.9155
745 0.219
668 0.699
714 0.867
562 0.822
190 0.689
103 0.845
11 1.0
7 1.0
0.7911308203991131
0.797339246119734

 ===== Epoch 58	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 2 0
train:	 Loss = 1.0610,	 Acc = 0.6532,	 Loss Con1 = 1.2793,	 Loss Con2 = 1.7476
3161 0.153
3146 0.783
2595 0.866
1323 0.869
890 0.835
763 0.856
70 0.857
20 0.85
0.8325195866924038
0.797339246119734
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1094,	 Acc = 0.6407,	 Loss Con1 = 1.3409,	 Loss Con2 = 1.9552
745 0.217
668 0.663
714 0.859
562 0.82
190 0.732
103 0.854
11 0.909
7 0.857
0.7804878048780488
0.797339246119734

 ===== Epoch 59	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013  0.7937099   2.4206245  -0.4178274  -0.43225467
  1.9245312   1.4884084 ] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355  0.7808134   2.359642   -0.4178274  -0.43225467
  1.9294546   1.4591886 ] 3 3
train:	 Loss = 1.0583,	 Acc = 0.6512,	 Loss Con1 = 1.2790,	 Loss Con2 = 1.8046
3166 0.155
3144 0.776
2594 0.864
1324 0.869
889 0.838
761 0.85
70 0.871
20 0.9
0.829811406498523
0.797339246119734
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1336,	 Acc = 0.6500,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.0194
745 0.216
668 0.684
714 0.873
562 0.835
190 0.721
103 0.825
11 1.0
7 1.0
0.7933481152993348
0.797339246119734

 ===== Epoch 60	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 4 0
train:	 Loss = 1.0616,	 Acc = 0.6515,	 Loss Con1 = 1.2796,	 Loss Con2 = 1.7990
3160 0.151
3142 0.78
2599 0.864
1324 0.867
891 0.842
761 0.849
71 0.887
20 0.85
0.8310626702997275
0.797339246119734
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1202,	 Acc = 0.6527,	 Loss Con1 = 1.3409,	 Loss Con2 = 1.9275
745 0.216
668 0.693
714 0.866
562 0.836
190 0.737
103 0.854
11 1.0
7 1.0
0.7968957871396896
0.797339246119734

 ===== Epoch 61	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 2 0
train:	 Loss = 1.0552,	 Acc = 0.6540,	 Loss Con1 = 1.2800,	 Loss Con2 = 1.8087
3163 0.156
3140 0.78
2598 0.867
1321 0.87
893 0.84
762 0.86
71 0.873
20 0.9
0.8330494037478705
0.797339246119734
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1029,	 Acc = 0.6380,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.0914
745 0.213
668 0.645
714 0.874
562 0.822
190 0.705
103 0.835
11 1.0
7 1.0
0.7782705099778271
0.797339246119734

 ===== Epoch 62	 =====
[-0.4557189  -0.49292567  1.0807341   0.7639437  -0.6348712  -0.6508424
  1.2223074   3.1060593  -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0805506   0.592679   -0.6347739  -0.6507814
  1.2221295   2.9009538  -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 5 5
train:	 Loss = 1.0614,	 Acc = 0.6522,	 Loss Con1 = 1.2800,	 Loss Con2 = 1.8444
3160 0.153
3143 0.777
2599 0.867
1325 0.869
889 0.839
761 0.853
71 0.859
20 0.9
0.8310626702997275
0.797339246119734
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1303,	 Acc = 0.6473,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.2499
745 0.215
668 0.672
714 0.873
562 0.838
190 0.705
103 0.845
11 1.0
7 1.0
0.7902439024390244
0.797339246119734

 ===== Epoch 63	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 2 0
train:	 Loss = 1.0647,	 Acc = 0.6515,	 Loss Con1 = 1.2790,	 Loss Con2 = 1.9181
3167 0.154
3142 0.775
2593 0.864
1323 0.869
889 0.841
763 0.856
71 0.901
20 0.85
0.8304738097943416
0.797339246119734
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.0859,	 Acc = 0.6487,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.0331
745 0.211
668 0.677
714 0.875
562 0.833
190 0.721
103 0.864
11 1.0
7 1.0
0.7933481152993348
0.797339246119734

 ===== Epoch 64	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013  0.20739907  0.14405623 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 5 3
train:	 Loss = 1.0616,	 Acc = 0.6505,	 Loss Con1 = 1.2784,	 Loss Con2 = 1.8009
3162 0.15
3144 0.774
2599 0.868
1323 0.865
891 0.844
758 0.85
71 0.873
20 0.9
0.8301158301158301
0.797339246119734
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1122,	 Acc = 0.6577,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.1527
745 0.216
668 0.72
714 0.868
562 0.838
190 0.7
103 0.864
11 1.0
7 1.0
0.8035476718403548
0.8035476718403548

 ===== Epoch 65	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634  0.8640548   1.8991923
 -0.43886673 -0.47319013  1.0855231   0.0470636  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053  0.880031    1.8922786
 -0.4389609  -0.47327355  1.10227     0.06829061 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 1 2
train:	 Loss = 1.0613,	 Acc = 0.6512,	 Loss Con1 = 1.2780,	 Loss Con2 = 1.8483
3164 0.153
3145 0.778
2597 0.864
1322 0.864
892 0.841
757 0.855
71 0.887
20 0.9
0.8303044070876874
0.8035476718403548
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1115,	 Acc = 0.6470,	 Loss Con1 = 1.3409,	 Loss Con2 = 1.9735
745 0.213
668 0.696
714 0.86
562 0.824
190 0.716
103 0.835
11 1.0
7 1.0
0.7902439024390244
0.8035476718403548

 ===== Epoch 66	 =====
[-0.4557189  -0.49292567  1.3569533   2.4334867  -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.35674     2.4339526  -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 0
train:	 Loss = 1.0594,	 Acc = 0.6513,	 Loss Con1 = 1.2800,	 Loss Con2 = 1.8770
3160 0.155
3142 0.778
2597 0.866
1324 0.863
892 0.835
762 0.841
71 0.887
20 0.85
0.8292461398728429
0.8035476718403548
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1089,	 Acc = 0.6617,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.1288
745 0.215
668 0.743
714 0.878
562 0.831
190 0.684
103 0.845
11 1.0
7 1.0
0.8093126385809313
0.8093126385809313

 ===== Epoch 67	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013  2.135391    0.60955554 -0.4178274  -0.43225467
  3.2130857   0.98244977] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355  2.1353471   0.82302445  3.992038    0.11385859
  3.2130537   1.20805   ] 6 6
train:	 Loss = 1.0568,	 Acc = 0.6542,	 Loss Con1 = 1.2799,	 Loss Con2 = 1.8759
3161 0.154
3143 0.783
2595 0.867
1325 0.869
891 0.841
762 0.856
71 0.873
20 0.9
0.8338821392074486
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 5
val:	 Loss = 1.1145,	 Acc = 0.6520,	 Loss Con1 = 1.3409,	 Loss Con2 = 1.9886
745 0.211
668 0.705
714 0.877
562 0.822
190 0.705
103 0.864
11 0.909
7 1.0
0.7977827050997782
0.8093126385809313

 ===== Epoch 68	 =====
[-0.4557189  -0.49292567  1.1767527   2.4197297   0.82015866  0.35194713
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.1559622   2.4388816   0.7954915   0.33483532
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 1 1
train:	 Loss = 1.0619,	 Acc = 0.6524,	 Loss Con1 = 1.2796,	 Loss Con2 = 1.9171
3164 0.152
3143 0.78
2598 0.865
1320 0.87
892 0.841
760 0.855
71 0.873
20 0.85
0.832121762835075
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1041,	 Acc = 0.6500,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.3067
745 0.215
668 0.686
714 0.867
562 0.836
190 0.721
103 0.854
11 1.0
7 1.0
0.7937915742793792
0.8093126385809313

 ===== Epoch 69	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.19232443  1.2402213
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.19103782  1.1195904
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 3 3
train:	 Loss = 1.0537,	 Acc = 0.6559,	 Loss Con1 = 1.2797,	 Loss Con2 = 1.8992
3167 0.153
3138 0.786
2597 0.868
1323 0.872
890 0.846
762 0.86
71 0.901
20 0.9
0.8368367230996477
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1073,	 Acc = 0.6517,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.0262
745 0.217
668 0.695
714 0.871
562 0.833
190 0.705
103 0.854
11 0.909
7 1.0
0.7951219512195122
0.8093126385809313

 ===== Epoch 70	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 1 0
train:	 Loss = 1.0578,	 Acc = 0.6525,	 Loss Con1 = 1.2804,	 Loss Con2 = 1.8330
3160 0.154
3142 0.782
2598 0.861
1322 0.865
892 0.842
763 0.856
71 0.887
20 0.9
0.8312897366030881
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1021,	 Acc = 0.6527,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.0245
745 0.216
668 0.692
714 0.878
562 0.829
190 0.721
103 0.845
11 1.0
7 1.0
0.7968957871396896
0.8093126385809313

 ===== Epoch 71	 =====
[-0.4557189  -0.49292567  1.5487118   1.6280969   0.91469145 -0.13833137
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.5567696   1.6508276   0.89545757 -0.11136883
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 4 4
train:	 Loss = 1.0559,	 Acc = 0.6542,	 Loss Con1 = 1.2794,	 Loss Con2 = 1.8582
3164 0.154
3143 0.781
2594 0.869
1325 0.866
890 0.851
761 0.854
71 0.901
20 0.85
0.834166288050886
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.0993,	 Acc = 0.6443,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.1560
745 0.212
668 0.662
714 0.868
562 0.838
190 0.726
103 0.835
11 1.0
7 1.0
0.7871396895787139
0.8093126385809313

 ===== Epoch 72	 =====
[-0.4557189  -0.49292567  2.2060087   1.3206546   0.5738694   0.41464895
  3.1793852   2.8532572  -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  2.2057042   1.3209578   0.5739937   0.4146955
  3.1791084   2.8531034  -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 0
train:	 Loss = 1.0568,	 Acc = 0.6520,	 Loss Con1 = 1.2792,	 Loss Con2 = 2.0364
3163 0.153
3144 0.781
2595 0.862
1322 0.868
893 0.842
760 0.854
71 0.873
20 0.85
0.8313458262350937
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.0979,	 Acc = 0.6497,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.5317
745 0.216
668 0.699
714 0.871
562 0.82
190 0.705
103 0.845
11 1.0
7 0.857
0.7929046563192904
0.8093126385809313

 ===== Epoch 73	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
  1.1440271   2.6149433 ] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
  1.1146488   2.6302388 ] 1 1
train:	 Loss = 1.0571,	 Acc = 0.6532,	 Loss Con1 = 1.2792,	 Loss Con2 = 1.9748
3165 0.153
3141 0.783
2599 0.866
1320 0.866
892 0.837
760 0.858
71 0.873
20 0.85
0.8330114733613541
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.0809,	 Acc = 0.6563,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.2641
745 0.217
668 0.713
714 0.868
562 0.833
190 0.716
103 0.864
11 1.0
7 1.0
0.801330376940133
0.8093126385809313

 ===== Epoch 74	 =====
[-0.4557189  -0.49292567  1.6349422   2.0569856   0.6840864   0.38209102
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.6346915   2.0574002   0.6842051   0.38213226
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 0
train:	 Loss = 1.0579,	 Acc = 0.6545,	 Loss Con1 = 1.2792,	 Loss Con2 = 1.9460
3166 0.152
3143 0.787
2595 0.866
1322 0.871
889 0.845
762 0.849
71 0.901
20 0.9
0.8352647125653261
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1094,	 Acc = 0.6490,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.1234
745 0.212
668 0.693
714 0.875
562 0.822
190 0.711
103 0.845
11 0.909
7 1.0
0.7933481152993348
0.8093126385809313

 ===== Epoch 75	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 6 0
train:	 Loss = 1.0559,	 Acc = 0.6555,	 Loss Con1 = 1.2796,	 Loss Con2 = 1.9165
3162 0.153
3145 0.788
2596 0.867
1321 0.872
890 0.842
763 0.857
71 0.873
20 0.85
0.8360208948444242
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1031,	 Acc = 0.6523,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.1239
745 0.216
668 0.689
714 0.866
562 0.838
190 0.758
103 0.825
11 1.0
7 1.0
0.7964523281596453
0.8093126385809313

 ===== Epoch 76	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
  1.1234494   1.425772  ] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
  1.1542476   1.4335792 ] 2 2
train:	 Loss = 1.0604,	 Acc = 0.6536,	 Loss Con1 = 1.2781,	 Loss Con2 = 1.9911
3163 0.152
3145 0.78
2597 0.869
1322 0.871
891 0.844
760 0.855
70 0.871
20 0.85
0.8338444065871664
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1135,	 Acc = 0.6460,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.4140
745 0.217
668 0.671
714 0.871
562 0.827
190 0.711
103 0.864
11 0.909
7 1.0
0.7875831485587583
0.8093126385809313

 ===== Epoch 77	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634  1.4853992   1.6227072
 -0.43886673 -0.47319013  0.5641083   2.570416   -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053  1.458113    1.6275448
 -0.4389609  -0.47327355  0.5296635   2.6044579  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 1 1
train:	 Loss = 1.0554,	 Acc = 0.6542,	 Loss Con1 = 1.2792,	 Loss Con2 = 2.0126
3165 0.153
3143 0.788
2595 0.865
1320 0.866
891 0.842
763 0.852
71 0.901
20 0.9
0.8346018402817221
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1112,	 Acc = 0.6470,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.6081
745 0.217
668 0.68
714 0.871
562 0.824
190 0.721
103 0.825
11 1.0
7 1.0
0.7889135254988914
0.8093126385809313

 ===== Epoch 78	 =====
[-0.4557189  -0.49292567  1.8656963   1.4346967   2.9567719   0.6326848
  1.2868227   1.0492148   3.5088644   0.13861974 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.8376603   1.438417    2.930476    0.6278846
  1.2498598   1.0373325   3.4800296   0.1251962  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 1 1
train:	 Loss = 1.0582,	 Acc = 0.6535,	 Loss Con1 = 1.2802,	 Loss Con2 = 2.1258
3164 0.154
3141 0.781
2594 0.866
1324 0.872
892 0.841
762 0.853
71 0.859
20 0.9
0.8330304407087687
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1188,	 Acc = 0.6480,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.3219
745 0.215
668 0.674
714 0.875
562 0.831
190 0.721
103 0.845
11 1.0
7 1.0
0.7911308203991131
0.8093126385809313

 ===== Epoch 79	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 5 0
train:	 Loss = 1.0551,	 Acc = 0.6556,	 Loss Con1 = 1.2779,	 Loss Con2 = 1.9480
3162 0.155
3145 0.783
2599 0.87
1321 0.872
889 0.84
761 0.857
71 0.873
20 0.9
0.8352259822848058
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.0965,	 Acc = 0.6497,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.1487
745 0.211
668 0.69
714 0.873
562 0.836
190 0.705
103 0.835
11 1.0
7 1.0
0.7946784922394678
0.8093126385809313

 ===== Epoch 80	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 1 0
train:	 Loss = 1.0590,	 Acc = 0.6537,	 Loss Con1 = 1.2791,	 Loss Con2 = 1.9577
3163 0.154
3144 0.785
2595 0.867
1322 0.865
890 0.837
763 0.853
71 0.873
20 0.9
0.8332765474162408
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1014,	 Acc = 0.6480,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.2695
745 0.216
668 0.69
714 0.874
562 0.82
190 0.689
103 0.864
11 0.909
7 1.0
0.7906873614190687
0.8093126385809313

 ===== Epoch 81	 =====
[ 3.4365616   0.97633433  1.8313941   1.7354763   2.4356894   0.6794349
  0.94340056  2.2458522   2.4668686   0.12150692 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [ 3.4365578   0.97634107  1.8311328   1.735846    2.4358525   0.67948323
  0.9432407   2.2457283   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 0
train:	 Loss = 1.0570,	 Acc = 0.6540,	 Loss Con1 = 1.2791,	 Loss Con2 = 1.9406
3161 0.154
3143 0.782
2599 0.868
1322 0.868
893 0.843
760 0.853
70 0.871
20 0.9
0.833427955035767
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1097,	 Acc = 0.6460,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.1208
745 0.215
668 0.674
714 0.875
562 0.826
190 0.7
103 0.854
11 1.0
7 1.0
0.788470066518847
0.8093126385809313

 ===== Epoch 82	 =====
[-0.05491513  1.4908607  -0.5993768  -0.57551634 -0.6348712  -0.6508424
  1.8158147   0.03327088 -0.46656546 -0.4823711   2.1192486   1.5317962
 -0.45947683 -0.42555502] [-0.05167621  1.2797422  -0.59937984 -0.57549053 -0.6347739  -0.6507814
  1.8415487   0.00844059 -0.46656546 -0.4823711   2.1227634   1.5054955
 -0.45947683 -0.42555502] 3 3
train:	 Loss = 1.0536,	 Acc = 0.6532,	 Loss Con1 = 1.2795,	 Loss Con2 = 1.9273
3163 0.152
3140 0.788
2598 0.862
1325 0.868
890 0.839
761 0.85
71 0.859
20 0.9
0.8330494037478705
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.0907,	 Acc = 0.6477,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.1961
745 0.217
668 0.678
714 0.867
562 0.824
190 0.742
103 0.845
11 1.0
7 1.0
0.78980044345898
0.8093126385809313

 ===== Epoch 83	 =====
[-0.4557189  -0.49292567  1.1770707   2.0818853  -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.1768768   2.0823     -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 0
train:	 Loss = 1.0542,	 Acc = 0.6537,	 Loss Con1 = 1.2793,	 Loss Con2 = 1.9734
3163 0.155
3145 0.781
2594 0.868
1323 0.866
891 0.843
761 0.853
71 0.901
20 0.9
0.8329358319136854
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.0973,	 Acc = 0.6473,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.3809
745 0.217
668 0.68
714 0.87
562 0.826
190 0.732
103 0.816
11 1.0
7 1.0
0.7893569844789357
0.8093126385809313

 ===== Epoch 84	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.06721104  1.0966837
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.09493054  1.0882357
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 1 1
train:	 Loss = 1.0516,	 Acc = 0.6545,	 Loss Con1 = 1.2800,	 Loss Con2 = 1.9883
3163 0.154
3141 0.785
2596 0.866
1323 0.865
892 0.842
762 0.858
71 0.901
20 0.9
0.834412265758092
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1148,	 Acc = 0.6403,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.1435
745 0.216
668 0.653
714 0.864
562 0.827
190 0.737
103 0.816
11 1.0
7 1.0
0.7804878048780488
0.8093126385809313

 ===== Epoch 85	 =====
[-0.4557189  -0.49292567  0.8187712   0.8798443  -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  0.8186158   0.7085967  -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 5 5
train:	 Loss = 1.0549,	 Acc = 0.6543,	 Loss Con1 = 1.2800,	 Loss Con2 = 1.9331
3165 0.155
3141 0.782
2595 0.866
1321 0.875
893 0.847
763 0.849
70 0.857
20 0.85
0.8339202544587072
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1104,	 Acc = 0.6380,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.2380
745 0.216
668 0.639
714 0.857
562 0.836
190 0.737
103 0.835
11 1.0
7 1.0
0.7773835920177383
0.8093126385809313

 ===== Epoch 86	 =====
[-0.4557189  -0.49292567  1.2500427   2.1751685   1.7287873   0.7209273
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.2498536   2.3471231   1.728912    0.9121378
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 6 6
train:	 Loss = 1.0556,	 Acc = 0.6521,	 Loss Con1 = 1.2794,	 Loss Con2 = 2.0134
3166 0.153
3142 0.784
2592 0.861
1323 0.869
891 0.84
763 0.847
71 0.901
20 0.85
0.8317427857305157
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.0925,	 Acc = 0.6533,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.3794
745 0.217
668 0.69
714 0.873
562 0.84
190 0.721
103 0.845
11 1.0
7 1.0
0.797339246119734
0.8093126385809313

 ===== Epoch 87	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634  0.23130243  1.9312414
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053  0.23674159  2.080424
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 6 6
train:	 Loss = 1.0506,	 Acc = 0.6556,	 Loss Con1 = 1.2796,	 Loss Con2 = 2.0799
3163 0.152
3147 0.789
2590 0.866
1323 0.872
892 0.844
762 0.854
71 0.873
20 0.9
0.8364565587734242
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1239,	 Acc = 0.6457,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.1432
745 0.215
668 0.669
714 0.87
562 0.836
190 0.721
103 0.825
11 0.909
7 1.0
0.7880266075388027
0.8093126385809313

 ===== Epoch 88	 =====
[ 0.5052629   0.26050878  1.1642796   1.8439885   1.4955243   0.16586843
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [ 0.48112395  0.334435    1.1755221   1.8693092   1.4806595   0.18714277
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 4 4
train:	 Loss = 1.0509,	 Acc = 0.6573,	 Loss Con1 = 1.2787,	 Loss Con2 = 1.9465
3165 0.155
3143 0.79
2595 0.871
1322 0.87
892 0.842
760 0.857
71 0.873
20 0.9
0.8380097693967965
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1042,	 Acc = 0.6430,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.1847
745 0.212
668 0.651
714 0.87
562 0.835
190 0.742
103 0.845
11 1.0
7 1.0
0.7853658536585366
0.8093126385809313

 ===== Epoch 89	 =====
[ 2.9820724   2.1345448   1.5818079   0.32817444 -0.19449486  1.76738
  3.3110213   1.6812508   1.933648    3.321559    3.7549765   2.5250032
 -0.45947683 -0.42555502] [ 2.9817789   1.9310006   1.5813524   0.15680179 -0.19458544  1.5767235
  3.310445    1.4762201   1.9334841   3.1083527   3.7547684   2.3044715
  3.0685008   3.52508   ] 5 5
train:	 Loss = 1.0537,	 Acc = 0.6542,	 Loss Con1 = 1.2786,	 Loss Con2 = 1.9430
3161 0.153
3147 0.781
2599 0.869
1323 0.868
888 0.846
759 0.858
71 0.873
20 0.85
0.8342227773362099
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1048,	 Acc = 0.6433,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.1265
745 0.215
668 0.668
714 0.868
562 0.82
190 0.742
103 0.825
11 0.909
7 1.0
0.7849223946784922
0.8093126385809313

 ===== Epoch 90	 =====
[-0.4557189  -0.49292567  0.9299266   2.2148244   2.2542827   1.2907958
  0.50384957  0.47447488  3.011442    0.6085058  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  0.9297592   2.2152584   2.2544444   1.2908305
  0.5037078   0.47437137  3.011442    0.6085058  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 0
train:	 Loss = 1.0536,	 Acc = 0.6551,	 Loss Con1 = 1.2803,	 Loss Con2 = 2.0093
3161 0.154
3144 0.787
2594 0.866
1323 0.868
892 0.837
763 0.858
71 0.901
20 0.85
0.8350175996366527
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1111,	 Acc = 0.6483,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.5101
745 0.212
668 0.678
714 0.867
562 0.838
190 0.721
103 0.864
11 1.0
7 1.0
0.7924611973392461
0.8093126385809313

 ===== Epoch 91	 =====
[ 0.4199459   2.5876184   1.9443579   1.2767138   0.8643696  -0.15819372
  2.338193    2.9415622  -0.19544561  2.14964    -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [ 0.4001185   2.5003262   1.9419798   1.2580727   0.8835243  -0.18407461
  2.3146374   2.919158   -0.2016248   1.8490654  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 3 3
train:	 Loss = 1.0566,	 Acc = 0.6538,	 Loss Con1 = 1.2792,	 Loss Con2 = 2.0596
3163 0.15
3146 0.788
2594 0.863
1324 0.871
888 0.841
762 0.854
71 0.887
20 0.9
0.8346394094264622
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1087,	 Acc = 0.6447,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.2678
745 0.216
668 0.678
714 0.868
562 0.827
190 0.705
103 0.816
11 0.909
7 1.0
0.7862527716186253
0.8093126385809313

 ===== Epoch 92	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634  1.0838294   2.2995584
 -0.43886673 -0.47319013  0.400814    0.9489426  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053  1.0986258   2.3259723
 -0.4389609  -0.47327355  0.38929912  1.0390106  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 4 4
train:	 Loss = 1.0537,	 Acc = 0.6550,	 Loss Con1 = 1.2798,	 Loss Con2 = 2.0143
3162 0.154
3142 0.787
2597 0.866
1324 0.871
891 0.835
761 0.859
71 0.887
20 0.9
0.8349988644106291
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1144,	 Acc = 0.6523,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.2056
745 0.219
668 0.699
714 0.87
562 0.829
190 0.711
103 0.845
11 1.0
7 1.0
0.7955654101995565
0.8093126385809313

 ===== Epoch 93	 =====
[-0.01400131  2.4447503  -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013  1.1659131   3.2819393  -0.4178274  -0.43225467
  1.9059588   2.0244062 ] [-0.0301378   2.2636118  -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355  1.1400757   3.2456698  -0.4178274  -0.43225467
  1.9035225   1.9948711 ] 3 3
train:	 Loss = 1.0520,	 Acc = 0.6562,	 Loss Con1 = 1.2791,	 Loss Con2 = 1.9453
3163 0.153
3141 0.792
2599 0.868
1324 0.863
890 0.843
761 0.855
70 0.9
20 0.9
0.8371379897785349
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1076,	 Acc = 0.6497,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.1247
745 0.213
668 0.705
714 0.868
562 0.819
190 0.716
103 0.825
11 1.0
7 1.0
0.7937915742793792
0.8093126385809313

 ===== Epoch 94	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 6 0
train:	 Loss = 1.0584,	 Acc = 0.6537,	 Loss Con1 = 1.2802,	 Loss Con2 = 2.1017
3161 0.152
3145 0.786
2596 0.866
1321 0.864
891 0.836
763 0.855
71 0.901
20 0.9
0.8335415010786874
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1125,	 Acc = 0.6500,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.3586
745 0.216
668 0.687
714 0.875
562 0.822
190 0.732
103 0.845
11 0.909
7 1.0
0.7933481152993348
0.8093126385809313

 ===== Epoch 95	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711   0.3270031   3.0279255
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711   0.34383556  3.1023831
 -0.45947683 -0.42555502] 4 4
train:	 Loss = 1.0544,	 Acc = 0.6562,	 Loss Con1 = 1.2790,	 Loss Con2 = 2.0840
3162 0.151
3144 0.789
2599 0.87
1322 0.871
892 0.845
759 0.855
70 0.871
20 0.9
0.8376107199636611
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1038,	 Acc = 0.6507,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.3148
745 0.217
668 0.705
714 0.877
562 0.815
190 0.684
103 0.845
11 1.0
7 1.0
0.7937915742793792
0.8093126385809313

 ===== Epoch 96	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 6 0
train:	 Loss = 1.0561,	 Acc = 0.6549,	 Loss Con1 = 1.2791,	 Loss Con2 = 2.0451
3168 0.153
3144 0.787
2593 0.867
1319 0.869
890 0.843
763 0.856
71 0.901
20 0.9
0.8355681818181818
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.0964,	 Acc = 0.6533,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.2399
745 0.216
668 0.696
714 0.87
562 0.843
190 0.716
103 0.835
11 0.909
7 1.0
0.7977827050997782
0.8093126385809313

 ===== Epoch 97	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.07425489  2.2042692
  1.9400731   3.235362  ] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.07430308  2.2043307
  1.9400324   3.2353895 ] 0 0
train:	 Loss = 1.0500,	 Acc = 0.6556,	 Loss Con1 = 1.2784,	 Loss Con2 = 2.3362
3163 0.153
3146 0.788
2596 0.869
1322 0.872
892 0.841
760 0.853
69 0.87
20 0.8
0.8361158432708689
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1061,	 Acc = 0.6487,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.1877
745 0.215
668 0.689
714 0.87
562 0.829
190 0.721
103 0.816
11 1.0
7 1.0
0.7920177383592018
0.8093126385809313

 ===== Epoch 98	 =====
[ 1.7421672   0.3530386  -0.5993768  -0.57551634  2.5610144   1.9181015
  0.7464819   2.123902    3.1399384   1.4352436  -0.4178274  -0.43225467
  2.6526027   0.43125978] [ 1.7561798   0.3336698  -0.59937984 -0.57549053  2.5555685   1.9086708
  0.7405134   2.0855067   3.1423233   1.4218103  -0.4178274  -0.43225467
  2.6635506   0.4202211 ] 4 3
train:	 Loss = 1.0529,	 Acc = 0.6567,	 Loss Con1 = 1.2791,	 Loss Con2 = 2.0093
3162 0.152
3144 0.79
2598 0.87
1322 0.874
891 0.838
761 0.857
70 0.9
20 0.9
0.8379513967749261
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1161,	 Acc = 0.6540,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.2440
745 0.217
668 0.693
714 0.87
562 0.84
190 0.726
103 0.854
11 1.0
7 1.0
0.7982261640798226
0.8093126385809313

 ===== Epoch 99	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 4 0
train:	 Loss = 1.0516,	 Acc = 0.6551,	 Loss Con1 = 1.2800,	 Loss Con2 = 1.9318
3165 0.153
3141 0.786
2593 0.866
1324 0.872
891 0.843
763 0.861
71 0.901
20 0.85
0.8356242190162445
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1016,	 Acc = 0.6393,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.2188
745 0.213
668 0.654
714 0.867
562 0.822
190 0.726
103 0.825
11 1.0
7 1.0
0.7800443458980044
0.8093126385809313

 ===== Epoch 100	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 4 0
train:	 Loss = 1.0514,	 Acc = 0.6538,	 Loss Con1 = 1.2791,	 Loss Con2 = 2.1452
3165 0.156
3144 0.787
2594 0.859
1322 0.872
891 0.841
761 0.849
71 0.901
20 0.9
0.8327842780870158
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.0992,	 Acc = 0.6473,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.6321
745 0.212
668 0.675
714 0.871
562 0.835
190 0.721
103 0.845
11 1.0
7 1.0
0.7911308203991131
0.8093126385809313

 ===== Epoch 101	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
  0.46369314  1.7882166  -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
  0.42576325  1.7973946  -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 1 1
train:	 Loss = 1.0530,	 Acc = 0.6539,	 Loss Con1 = 1.2795,	 Loss Con2 = 2.1628
3165 0.151
3141 0.783
2593 0.867
1325 0.873
892 0.844
761 0.855
71 0.887
20 0.9
0.8348290355560605
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1204,	 Acc = 0.6440,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.4529
745 0.213
668 0.635
714 0.877
562 0.845
190 0.753
103 0.845
11 1.0
7 1.0
0.7862527716186253
0.8093126385809313

 ===== Epoch 102	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 0
train:	 Loss = 1.0510,	 Acc = 0.6560,	 Loss Con1 = 1.2798,	 Loss Con2 = 2.1040
3161 0.152
3147 0.788
2592 0.867
1323 0.869
891 0.845
763 0.86
71 0.915
20 0.9
0.8367207902804588
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.0995,	 Acc = 0.6527,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.1445
745 0.216
668 0.678
714 0.878
562 0.843
190 0.742
103 0.816
11 1.0
7 1.0
0.7968957871396896
0.8093126385809313

 ===== Epoch 103	 =====
[ 3.5420632   1.3816749   1.7641524   2.0617337   2.509837    1.084887
  0.8777504   2.3674052   2.675235    0.56311244 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [ 3.5420587   1.178027    1.7638881   1.8906554   2.5100014   0.89378107
  0.877581    2.1623023   2.6752362   0.34966984 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 5 5
train:	 Loss = 1.0556,	 Acc = 0.6540,	 Loss Con1 = 1.2791,	 Loss Con2 = 1.9867
3163 0.153
3143 0.785
2599 0.866
1321 0.861
890 0.845
761 0.859
71 0.887
20 0.9
0.8338444065871664
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1168,	 Acc = 0.6497,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.1797
745 0.217
668 0.678
714 0.874
562 0.831
190 0.737
103 0.835
11 1.0
7 0.857
0.7924611973392461
0.8093126385809313

 ===== Epoch 104	 =====
[ 1.0833253   1.5153742  -0.5993768  -0.57551634 -0.6348712  -0.6508424
  0.1902481   0.67844987 -0.46656546 -0.4823711   1.6245111   0.6741297
  2.516626    2.45806   ] [ 1.0833253   1.3117329  -0.59937984 -0.57549053 -0.6347739  -0.6507814
  0.19012217  0.47339594  2.1958468   3.1775365   1.6245111   0.45352882
  2.516626    2.2324834 ] 5 5
train:	 Loss = 1.0535,	 Acc = 0.6552,	 Loss Con1 = 1.2795,	 Loss Con2 = 2.0232
3164 0.152
3144 0.788
2597 0.868
1319 0.87
891 0.841
763 0.857
70 0.857
20 0.9
0.8359836437982735
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1098,	 Acc = 0.6480,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.2062
745 0.215
668 0.686
714 0.871
562 0.827
190 0.716
103 0.825
11 1.0
7 1.0
0.7911308203991131
0.8093126385809313

 ===== Epoch 105	 =====
[-0.4557189  -0.49292567  1.1024078   2.1409965   1.0541576   0.14367156
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.1210468   2.1290321   1.0729789   0.15752874
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 2 2
train:	 Loss = 1.0530,	 Acc = 0.6540,	 Loss Con1 = 1.2793,	 Loss Con2 = 2.2818
3166 0.151
3138 0.79
2600 0.865
1322 0.868
889 0.832
762 0.856
71 0.901
20 0.9
0.8348102703930925
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.0959,	 Acc = 0.6547,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.5149
745 0.217
668 0.705
714 0.87
562 0.84
190 0.716
103 0.816
11 1.0
7 1.0
0.7991130820399113
0.8093126385809313

 ===== Epoch 106	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013  0.62471277  0.25920573 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355  0.6480501   0.19816063 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 3 3
train:	 Loss = 1.0492,	 Acc = 0.6550,	 Loss Con1 = 1.2792,	 Loss Con2 = 2.2510
3164 0.151
3144 0.787
2595 0.866
1322 0.874
891 0.846
761 0.857
71 0.873
20 0.9
0.8360972285324852
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.0929,	 Acc = 0.6477,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.5789
745 0.213
668 0.675
714 0.87
562 0.836
190 0.732
103 0.825
11 1.0
7 1.0
0.7911308203991131
0.8093126385809313

 ===== Epoch 107	 =====
[-0.4557189  -0.49292567  1.2682564   2.3680434   0.9047651   0.4333312
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.2866613   2.3878033   0.89339036  0.46511975
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 4 4
train:	 Loss = 1.0505,	 Acc = 0.6546,	 Loss Con1 = 1.2784,	 Loss Con2 = 2.1638
3165 0.154
3141 0.784
2598 0.867
1323 0.876
891 0.838
761 0.85
70 0.886
19 0.895
0.8346018402817221
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1322,	 Acc = 0.6460,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.6260
745 0.213
668 0.668
714 0.861
562 0.845
190 0.726
103 0.845
11 1.0
7 1.0
0.7889135254988914
0.8093126385809313

 ===== Epoch 108	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 1 0
train:	 Loss = 1.0551,	 Acc = 0.6543,	 Loss Con1 = 1.2796,	 Loss Con2 = 2.2715
3163 0.152
3145 0.788
2593 0.864
1324 0.867
891 0.841
761 0.855
71 0.915
20 0.9
0.8347529812606473
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1049,	 Acc = 0.6467,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.4167
745 0.215
668 0.654
714 0.87
562 0.843
190 0.758
103 0.835
11 1.0
7 1.0
0.7893569844789357
0.8093126385809313

 ===== Epoch 109	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 4 0
train:	 Loss = 1.0512,	 Acc = 0.6558,	 Loss Con1 = 1.2801,	 Loss Con2 = 2.0627
3161 0.152
3143 0.789
2597 0.868
1324 0.867
889 0.839
763 0.858
71 0.901
20 0.9
0.8364936981946179
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.0996,	 Acc = 0.6457,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.3872
745 0.217
668 0.654
714 0.878
562 0.833
190 0.737
103 0.825
11 1.0
7 1.0
0.7871396895787139
0.8093126385809313

 ===== Epoch 110	 =====
[-0.4557189  -0.49292567  1.3949564   0.6642557  -0.6348712  -0.6508424
  2.7231965   2.3251019  -0.46656546 -0.4823711   3.1927826   3.3941922
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.4055704   0.56420493 -0.6347739  -0.6507814
  2.737145    2.1984859  -0.46656546 -0.4823711   3.2019067   3.2568133
 -0.45947683 -0.42555502] 2 5
train:	 Loss = 1.0486,	 Acc = 0.6573,	 Loss Con1 = 1.2799,	 Loss Con2 = 2.0200
3162 0.153
3142 0.793
2598 0.868
1323 0.87
889 0.838
763 0.858
71 0.915
20 0.85
0.8381785146491029
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1072,	 Acc = 0.6493,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.2927
745 0.213
668 0.68
714 0.88
562 0.835
190 0.711
103 0.825
11 1.0
7 1.0
0.7933481152993348
0.8093126385809313

 ===== Epoch 111	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013  1.6175287   0.7065618  -0.4178274  -0.43225467
  2.8026688   1.0477823 ] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355  1.6519595   0.7229974  -0.4178274  -0.43225467
  2.8321116   1.0550739 ] 2 2
train:	 Loss = 1.0489,	 Acc = 0.6575,	 Loss Con1 = 1.2786,	 Loss Con2 = 1.9738
3162 0.153
3145 0.791
2596 0.869
1324 0.873
891 0.845
760 0.857
70 0.9
20 0.9
0.8387463093345446
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1589,	 Acc = 0.6393,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.1926
745 0.212
668 0.65
714 0.863
562 0.84
190 0.721
103 0.816
11 0.909
7 1.0
0.7804878048780488
0.8093126385809313

 ===== Epoch 112	 =====
[-0.4557189  -0.49292567  1.06432     0.5484866  -0.6348712  -0.6508424
  2.7367945   2.164165   -0.46656546 -0.4823711   3.401588    3.0333602
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0641382   0.5486768  -0.6347739  -0.6507814
  2.73654     2.1640255  -0.46656546 -0.4823711   3.401588    3.0333602
 -0.45947683 -0.42555502] 0 0
train:	 Loss = 1.0532,	 Acc = 0.6563,	 Loss Con1 = 1.2786,	 Loss Con2 = 2.1334
3165 0.153
3143 0.795
2596 0.865
1321 0.872
893 0.845
761 0.842
70 0.9
19 0.789
0.8373281835737817
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1334,	 Acc = 0.6437,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.3984
745 0.212
668 0.648
714 0.871
562 0.842
190 0.742
103 0.845
11 0.909
7 1.0
0.7862527716186253
0.8093126385809313

 ===== Epoch 113	 =====
[-0.4557189  -0.49292567  1.1242661   1.9494749  -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.1318227   1.9647683  -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 3 4
train:	 Loss = 1.0527,	 Acc = 0.6557,	 Loss Con1 = 1.2778,	 Loss Con2 = 2.2317
3166 0.153
3142 0.786
2598 0.87
1323 0.872
892 0.843
758 0.854
69 0.899
20 0.9
0.8365144285389684
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.0958,	 Acc = 0.6530,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.3061
745 0.216
668 0.702
714 0.873
562 0.833
190 0.716
103 0.816
11 1.0
7 1.0
0.797339246119734
0.8093126385809313

 ===== Epoch 114	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 0
train:	 Loss = 1.0481,	 Acc = 0.6570,	 Loss Con1 = 1.2792,	 Loss Con2 = 2.3003
3159 0.154
3144 0.789
2601 0.867
1322 0.873
892 0.846
759 0.855
71 0.887
20 0.85
0.8373254625950732
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.0999,	 Acc = 0.6447,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.4350
745 0.215
668 0.651
714 0.873
562 0.835
190 0.758
103 0.825
11 1.0
7 1.0
0.7866962305986697
0.8093126385809313

 ===== Epoch 115	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 1 0
train:	 Loss = 1.0532,	 Acc = 0.6537,	 Loss Con1 = 1.2795,	 Loss Con2 = 2.0226
3163 0.153
3143 0.787
2598 0.868
1319 0.864
892 0.836
762 0.848
71 0.873
20 0.9
0.8333901192504259
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1463,	 Acc = 0.6463,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.1179
745 0.216
668 0.671
714 0.863
562 0.836
190 0.753
103 0.816
11 0.909
7 1.0
0.788470066518847
0.8093126385809313

 ===== Epoch 116	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013  0.3678229   3.108417   -0.4178274  -0.43225467
  1.3290422   0.99905884] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355  0.3678229   3.3218513  -0.4178274  -0.43225467
  1.3290422   1.2246355 ] 6 6
train:	 Loss = 1.0489,	 Acc = 0.6559,	 Loss Con1 = 1.2794,	 Loss Con2 = 1.9760
3161 0.155
3141 0.79
2597 0.861
1325 0.874
892 0.843
761 0.859
71 0.887
20 0.9
0.8358124219370955
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1126,	 Acc = 0.6467,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.1599
745 0.216
668 0.66
714 0.87
562 0.836
190 0.747
103 0.845
11 1.0
7 1.0
0.7889135254988914
0.8093126385809313

 ===== Epoch 117	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
  0.7916358   0.8024661 ] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
  0.7636141   0.77705115] 1 1
train:	 Loss = 1.0514,	 Acc = 0.6568,	 Loss Con1 = 1.2797,	 Loss Con2 = 2.0023
3165 0.153
3142 0.79
2595 0.869
1321 0.871
892 0.844
762 0.86
71 0.887
20 0.85
0.8377825741224583
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1137,	 Acc = 0.6463,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.9255
745 0.216
668 0.669
714 0.873
562 0.829
190 0.721
103 0.845
11 1.0
7 1.0
0.788470066518847
0.8093126385809313

 ===== Epoch 118	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634  0.5151514   1.3657886
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053  0.4876065   1.3684667
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 1 1
train:	 Loss = 1.0533,	 Acc = 0.6558,	 Loss Con1 = 1.2792,	 Loss Con2 = 2.4518
3165 0.152
3143 0.786
2595 0.869
1321 0.871
892 0.851
762 0.856
70 0.9
20 0.9
0.8369873906622742
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.0987,	 Acc = 0.6483,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.5181
745 0.216
668 0.681
714 0.87
562 0.831
190 0.716
103 0.845
11 1.0
7 1.0
0.7911308203991131
0.8093126385809313

 ===== Epoch 119	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634  1.0434741   2.0970502
 -0.43886673 -0.47319013  0.20050144  0.8049729  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053  1.0699207   2.0831203
 -0.4389609  -0.47327355  0.23723045  0.850931   -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 2 2
train:	 Loss = 1.0521,	 Acc = 0.6536,	 Loss Con1 = 1.2797,	 Loss Con2 = 2.1506
3160 0.154
3145 0.781
2595 0.871
1322 0.866
892 0.836
763 0.849
71 0.901
20 0.9
0.8327656675749319
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1023,	 Acc = 0.6480,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.3482
745 0.212
668 0.705
714 0.867
562 0.813
190 0.716
103 0.825
11 1.0
7 1.0
0.7920177383592018
0.8093126385809313

 ===== Epoch 120	 =====
[ 3.5844965   0.90198934  2.1592014   1.5918709   2.517721    0.56397635
  1.4050864   2.3307605   2.3505907   0.13353552 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [ 3.597712    0.88191134  2.1516902   1.5752721   2.5279536   0.5482411
  1.3915676   2.290891    2.3755734   0.11188869 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 3 3
train:	 Loss = 1.0538,	 Acc = 0.6544,	 Loss Con1 = 1.2786,	 Loss Con2 = 2.0890
3163 0.15
3145 0.792
2597 0.865
1321 0.869
890 0.836
761 0.849
71 0.901
20 0.9
0.8354344122657581
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1287,	 Acc = 0.6477,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.0990
745 0.213
668 0.684
714 0.871
562 0.833
190 0.711
103 0.816
11 1.0
7 1.0
0.7911308203991131
0.8093126385809313

 ===== Epoch 121	 =====
[-0.4557189  -0.49292567  2.0267591   1.9032346   0.55605406  0.7711928
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  2.026474    1.7321367   0.5561779   0.58009624
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 5 5
train:	 Loss = 1.0494,	 Acc = 0.6552,	 Loss Con1 = 1.2794,	 Loss Con2 = 1.9441
3164 0.154
3142 0.79
2594 0.864
1324 0.867
892 0.843
761 0.855
71 0.887
20 0.9
0.8353021353930031
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1288,	 Acc = 0.6370,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.1869
745 0.211
668 0.639
714 0.86
562 0.826
190 0.774
103 0.825
11 1.0
7 0.857
0.7778270509977827
0.8093126385809313

 ===== Epoch 122	 =====
[-0.4557189  -0.49292567  0.8314971   1.5152179  -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  0.8588234   1.5083424  -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 2 2
train:	 Loss = 1.0490,	 Acc = 0.6546,	 Loss Con1 = 1.2793,	 Loss Con2 = 2.0768
3165 0.152
3145 0.787
2594 0.864
1321 0.869
891 0.847
761 0.857
71 0.915
20 0.9
0.8351698284675678
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1196,	 Acc = 0.6470,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.2246
745 0.22
668 0.678
714 0.873
562 0.819
190 0.726
103 0.825
11 1.0
7 1.0
0.7880266075388027
0.8093126385809313

 ===== Epoch 123	 =====
[-0.4557189  -0.49292567  0.99510175  2.2185438   0.9476559   0.05722338
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0177248   2.2014232   0.970418    0.07684191
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 2 2
train:	 Loss = 1.0485,	 Acc = 0.6573,	 Loss Con1 = 1.2800,	 Loss Con2 = 2.0175
3160 0.153
3144 0.795
2596 0.869
1324 0.868
892 0.841
761 0.852
71 0.887
20 0.9
0.8384423251589465
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1098,	 Acc = 0.6533,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.2312
745 0.22
668 0.677
714 0.874
562 0.84
190 0.763
103 0.825
11 1.0
7 1.0
0.7964523281596453
0.8093126385809313

 ===== Epoch 124	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
  1.5395155   0.76263034] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
  1.5395155   0.76263034] 0 0
train:	 Loss = 1.0495,	 Acc = 0.6559,	 Loss Con1 = 1.2791,	 Loss Con2 = 2.0138
3163 0.156
3141 0.79
2597 0.866
1324 0.869
893 0.843
760 0.847
70 0.9
20 0.9
0.8356615559341284
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1199,	 Acc = 0.6450,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.1330
745 0.216
668 0.69
714 0.864
562 0.811
190 0.726
103 0.825
11 1.0
7 0.857
0.7866962305986697
0.8093126385809313

 ===== Epoch 125	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 2 0
train:	 Loss = 1.0496,	 Acc = 0.6553,	 Loss Con1 = 1.2781,	 Loss Con2 = 2.0812
3165 0.154
3144 0.788
2598 0.866
1321 0.872
890 0.836
759 0.856
71 0.873
20 0.9
0.8355106213790753
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1025,	 Acc = 0.6527,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.3196
745 0.219
668 0.683
714 0.88
562 0.831
190 0.726
103 0.864
11 0.909
7 1.0
0.7960088691796009
0.8093126385809313

 ===== Epoch 126	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.14455949  0.23356289
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.12025917  0.2868173
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 2 4
train:	 Loss = 1.0494,	 Acc = 0.6537,	 Loss Con1 = 1.2792,	 Loss Con2 = 1.9667
3160 0.152
3146 0.778
2597 0.87
1323 0.873
891 0.843
761 0.855
70 0.871
20 0.9
0.8337874659400545
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1055,	 Acc = 0.6443,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.2216
745 0.212
668 0.672
714 0.866
562 0.829
190 0.732
103 0.825
11 1.0
7 1.0
0.7871396895787139
0.8093126385809313

 ===== Epoch 127	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634  0.00557405  0.48197132
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053  0.00568563  0.5202446
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 5 4
train:	 Loss = 1.0500,	 Acc = 0.6531,	 Loss Con1 = 1.2794,	 Loss Con2 = 2.0014
3165 0.153
3142 0.784
2595 0.863
1323 0.866
891 0.841
761 0.857
71 0.901
20 0.9
0.8328978757241849
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1217,	 Acc = 0.6530,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.2734
745 0.215
668 0.693
714 0.871
562 0.835
190 0.732
103 0.864
11 1.0
7 0.857
0.7977827050997782
0.8093126385809313

 ===== Epoch 128	 =====
[-0.05584877  1.6497489  -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013  1.0648216   3.1287348  -0.4178274  -0.43225467
  1.8985302   1.90521   ] [-0.05582492  1.4349222  -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355  1.0408376   3.0872238  -0.4178274  -0.43225467
  1.8976978   1.8752699 ] 3 3
train:	 Loss = 1.0469,	 Acc = 0.6560,	 Loss Con1 = 1.2792,	 Loss Con2 = 2.0588
3165 0.154
3145 0.786
2594 0.87
1321 0.873
892 0.84
760 0.855
71 0.873
20 0.9
0.8364194024764285
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1335,	 Acc = 0.6500,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.2437
745 0.216
668 0.671
714 0.874
562 0.842
190 0.732
103 0.845
11 1.0
7 1.0
0.7933481152993348
0.8093126385809313

 ===== Epoch 129	 =====
[ 1.4940134   0.927759   -0.5993768  -0.57551634  2.77811     2.0838575
  1.0841115   2.8794575   3.1769102   1.7275645  -0.4178274  -0.43225467
  2.4124374   0.696016  ] [ 1.5069355   0.8861071  -0.59937984 -0.57549053  2.7667134   2.0696125
  1.0616847   2.8375072   3.1757617   1.70528    -0.4178274  -0.43225467
  2.4276452   0.67458683] 3 3
train:	 Loss = 1.0459,	 Acc = 0.6562,	 Loss Con1 = 1.2802,	 Loss Con2 = 2.0301
3161 0.154
3143 0.787
2597 0.868
1323 0.868
891 0.843
762 0.86
71 0.915
20 0.9
0.8364936981946179
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1127,	 Acc = 0.6477,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.2107
745 0.215
668 0.66
714 0.868
562 0.835
190 0.768
103 0.864
11 1.0
7 1.0
0.7906873614190687
0.8093126385809313

 ===== Epoch 130	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 6 0
train:	 Loss = 1.0528,	 Acc = 0.6552,	 Loss Con1 = 1.2796,	 Loss Con2 = 2.0011
3161 0.152
3147 0.788
2591 0.865
1324 0.873
891 0.841
763 0.852
71 0.901
20 0.9
0.8356988758941751
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1299,	 Acc = 0.6480,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.2665
745 0.215
668 0.663
714 0.87
562 0.835
190 0.774
103 0.845
11 0.909
7 1.0
0.7911308203991131
0.8093126385809313

 ===== Epoch 131	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.12653886  1.4901681  -0.4178274  -0.43225467
  1.3844441   1.5776364 ] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.16415289  1.4701716  -0.4178274  -0.43225467
  1.3536013   1.5733564 ] 1 1
train:	 Loss = 1.0516,	 Acc = 0.6551,	 Loss Con1 = 1.2795,	 Loss Con2 = 2.0825
3157 0.152
3146 0.785
2599 0.865
1322 0.873
892 0.846
761 0.857
71 0.859
20 0.9
0.8352059925093633
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1237,	 Acc = 0.6477,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.1022
745 0.213
668 0.669
714 0.873
562 0.833
190 0.753
103 0.835
11 0.909
7 1.0
0.7911308203991131
0.8093126385809313

 ===== Epoch 132	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 5 0
train:	 Loss = 1.0473,	 Acc = 0.6558,	 Loss Con1 = 1.2788,	 Loss Con2 = 2.1552
3164 0.153
3145 0.791
2595 0.867
1321 0.864
893 0.84
759 0.859
71 0.901
20 0.9
0.8365515674693321
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1115,	 Acc = 0.6503,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.1704
745 0.216
668 0.696
714 0.87
562 0.829
190 0.705
103 0.835
11 1.0
7 1.0
0.7937915742793792
0.8093126385809313

 ===== Epoch 133	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
  1.2013507   2.128504  ] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
  1.2057652   2.1705472 ] 4 4
train:	 Loss = 1.0488,	 Acc = 0.6554,	 Loss Con1 = 1.2799,	 Loss Con2 = 2.0654
3161 0.153
3148 0.787
2593 0.867
1323 0.867
889 0.845
763 0.86
71 0.887
20 0.9
0.8358124219370955
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1185,	 Acc = 0.6487,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.1288
745 0.217
668 0.68
714 0.87
562 0.831
190 0.726
103 0.835
11 1.0
7 1.0
0.7911308203991131
0.8093126385809313

 ===== Epoch 134	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.25924692  0.31631815
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.27597627  0.6979795
 -0.45947683 -0.42555502] 4 6
train:	 Loss = 1.0550,	 Acc = 0.6557,	 Loss Con1 = 1.2794,	 Loss Con2 = 2.0570
3163 0.156
3141 0.787
2597 0.866
1321 0.861
893 0.847
762 0.862
71 0.901
20 0.9
0.8350936967632028
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1002,	 Acc = 0.6503,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.1569
745 0.212
668 0.686
714 0.873
562 0.838
190 0.726
103 0.825
11 1.0
7 1.0
0.7951219512195122
0.8093126385809313

 ===== Epoch 135	 =====
[ 0.23859313 -0.01752723 -0.5993768  -0.57551634  1.9680121   1.3441204
 -0.43886673 -0.47319013  2.1375685   0.56830466 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053  1.9402571   1.3450608
 -0.4389609  -0.47327355  2.1043942   0.55294335 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 1 1
train:	 Loss = 1.0490,	 Acc = 0.6547,	 Loss Con1 = 1.2794,	 Loss Con2 = 2.0265
3167 0.153
3144 0.785
2592 0.868
1322 0.869
890 0.844
762 0.857
71 0.901
20 0.85
0.8353596182252017
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1413,	 Acc = 0.6460,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.1314
745 0.216
668 0.672
714 0.867
562 0.827
190 0.726
103 0.854
11 1.0
7 1.0
0.7880266075388027
0.8093126385809313

 ===== Epoch 136	 =====
[ 3.055492    1.6640065  -0.5993768  -0.57551634 -0.6348712  -0.6508424
  2.6603248   0.9689096   0.6219779   2.86878     2.9198167   2.1124516
 -0.45947683 -0.42555502] [ 3.0532677   1.6396596  -0.59937984 -0.57549053 -0.6347739  -0.6507814
  2.6713912   0.9448444   0.60194045  2.8047335   2.9152675   2.092366
 -0.45947683 -0.42555502] 3 3
train:	 Loss = 1.0491,	 Acc = 0.6551,	 Loss Con1 = 1.2797,	 Loss Con2 = 2.0349
3160 0.152
3143 0.788
2598 0.867
1324 0.866
890 0.839
763 0.858
70 0.9
20 0.9
0.8356039963669392
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1177,	 Acc = 0.6460,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.0796
745 0.217
668 0.654
714 0.875
562 0.829
190 0.768
103 0.825
11 0.909
7 1.0
0.7875831485587583
0.8093126385809313

 ===== Epoch 137	 =====
[ 1.5849342   2.7783637   2.0267591  -0.15429993 -0.6348712  -0.6508424
  2.8209403   1.1781354   0.35742015  2.1498125   2.8043532   2.4302766
 -0.45947683 -0.42555502] [ 1.5849342   2.574722   -0.59937984 -0.57549053 -0.6347739  -0.6507814
  2.8206816   0.9730708   0.35742015  1.936378    2.8043532   2.2096756
 -0.45947683 -0.42555502] 5 5
train:	 Loss = 1.0487,	 Acc = 0.6578,	 Loss Con1 = 1.2796,	 Loss Con2 = 1.9809
3165 0.152
3138 0.788
2597 0.869
1325 0.877
891 0.852
761 0.866
71 0.915
20 0.9
0.8397137339543338
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1429,	 Acc = 0.6423,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.1547
745 0.215
668 0.63
714 0.875
562 0.843
190 0.758
103 0.825
11 1.0
7 1.0
0.7835920177383592
0.8093126385809313

 ===== Epoch 138	 =====
[-0.4557189  -0.49292567  1.0491259   1.071961   -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0489601   0.94089824 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 5 5
train:	 Loss = 1.0534,	 Acc = 0.6547,	 Loss Con1 = 1.2803,	 Loss Con2 = 2.4215
3160 0.154
3144 0.783
2598 0.867
1320 0.864
892 0.85
763 0.858
71 0.887
20 0.85
0.8342415985467757
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1261,	 Acc = 0.6403,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.5840
745 0.216
668 0.632
714 0.874
562 0.833
190 0.753
103 0.825
11 1.0
7 1.0
0.7804878048780488
0.8093126385809313

 ===== Epoch 139	 =====
[ 1.2847598   3.1370246   1.8206869   1.2628225   1.3894843  -0.23206039
  1.5921926   2.8883271  -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [ 1.2847596   3.1370246   1.820424    1.2631172   1.3896266  -0.23200507
  1.5919958   2.8881721  -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 0
train:	 Loss = 1.0480,	 Acc = 0.6543,	 Loss Con1 = 1.2797,	 Loss Con2 = 2.1469
3161 0.153
3143 0.781
2598 0.869
1322 0.868
892 0.842
761 0.855
71 0.915
20 0.9
0.8341092312932894
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1520,	 Acc = 0.6470,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.3662
745 0.212
668 0.671
714 0.868
562 0.827
190 0.758
103 0.854
11 1.0
7 1.0
0.7906873614190687
0.8093126385809313

 ===== Epoch 140	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 0
train:	 Loss = 1.0486,	 Acc = 0.6562,	 Loss Con1 = 1.2797,	 Loss Con2 = 2.1768
3164 0.153
3144 0.785
2595 0.871
1321 0.873
891 0.843
762 0.861
71 0.901
20 0.9
0.8371194911403907
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1401,	 Acc = 0.6473,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.2086
745 0.216
668 0.672
714 0.853
562 0.835
190 0.789
103 0.845
11 0.909
7 1.0
0.78980044345898
0.8093126385809313

 ===== Epoch 141	 =====
[ 1.800185    1.1240267   2.2001066  -0.1686602  -0.6348712  -0.6508424
  2.0816467   0.9670276  -0.46656546 -0.4823711   1.5015981   2.872794
 -0.45947683 -0.42555502] [ 1.800185    1.164755    2.1998029  -0.13427766 -0.6347739  -0.6507814
  2.0814252   1.0079029  -0.46656546 -0.4823711   1.5015981   2.916914
 -0.45947683 -0.42555502] 6 4
train:	 Loss = 1.0454,	 Acc = 0.6553,	 Loss Con1 = 1.2794,	 Loss Con2 = 2.0216
3158 0.154
3145 0.785
2600 0.867
1322 0.87
892 0.845
761 0.85
70 0.886
20 0.9
0.8349602724177072
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1175,	 Acc = 0.6497,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.1753
745 0.216
668 0.675
714 0.874
562 0.838
190 0.732
103 0.825
11 1.0
7 1.0
0.7929046563192904
0.8093126385809313

 ===== Epoch 142	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 1 0
train:	 Loss = 1.0447,	 Acc = 0.6557,	 Loss Con1 = 1.2784,	 Loss Con2 = 2.0712
3165 0.152
3147 0.79
2596 0.866
1324 0.87
885 0.837
760 0.864
71 0.901
20 0.85
0.8368737930251051
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1123,	 Acc = 0.6487,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.3336
745 0.216
668 0.671
714 0.867
562 0.829
190 0.768
103 0.854
11 1.0
7 1.0
0.7915742793791575
0.8093126385809313

 ===== Epoch 143	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 6 0
train:	 Loss = 1.0469,	 Acc = 0.6573,	 Loss Con1 = 1.2782,	 Loss Con2 = 2.3017
3164 0.151
3142 0.793
2600 0.869
1322 0.869
890 0.846
760 0.857
71 0.915
19 0.895
0.83905043162199
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1079,	 Acc = 0.6510,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.5857
745 0.213
668 0.693
714 0.87
562 0.831
190 0.721
103 0.854
11 1.0
7 1.0
0.7955654101995565
0.8093126385809313

 ===== Epoch 144	 =====
[ 0.3079707   2.6350486  -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013  1.4804382   2.9722002  -0.4178274  -0.43225467
  2.0610387   1.7786341 ] [ 0.3082617   2.6345885  -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355  1.480688    2.9720058  -0.4178274  -0.43225467
  2.0612907   1.7786396 ] 0 0
train:	 Loss = 1.0437,	 Acc = 0.6562,	 Loss Con1 = 1.2803,	 Loss Con2 = 2.2676
3160 0.153
3145 0.788
2596 0.869
1322 0.868
892 0.846
762 0.857
71 0.901
20 0.9
0.8368528610354223
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1168,	 Acc = 0.6547,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.3538
745 0.217
668 0.711
714 0.875
562 0.827
190 0.689
103 0.854
11 1.0
7 1.0
0.7991130820399113
0.8093126385809313

 ===== Epoch 145	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013  0.02057096  0.7652269  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355  0.0485603   0.81624824 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 2 2
train:	 Loss = 1.0467,	 Acc = 0.6554,	 Loss Con1 = 1.2798,	 Loss Con2 = 2.1975
3164 0.152
3142 0.787
2595 0.872
1323 0.866
891 0.843
762 0.853
71 0.901
20 0.9
0.8364379827351204
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1277,	 Acc = 0.6460,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.2970
745 0.215
668 0.65
714 0.873
562 0.835
190 0.779
103 0.835
11 1.0
7 1.0
0.788470066518847
0.8093126385809313

 ===== Epoch 146	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 3 0
train:	 Loss = 1.0467,	 Acc = 0.6554,	 Loss Con1 = 1.2775,	 Loss Con2 = 2.4487
3164 0.152
3143 0.786
2599 0.868
1323 0.871
891 0.847
759 0.854
69 0.913
20 0.85
0.836210813266697
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1029,	 Acc = 0.6443,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.7897
745 0.213
668 0.66
714 0.863
562 0.836
190 0.742
103 0.864
11 1.0
7 0.857
0.7866962305986697
0.8093126385809313

 ===== Epoch 147	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013  0.62471277  0.25920573 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355  0.6480501   0.19816063 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 3 3
train:	 Loss = 1.0426,	 Acc = 0.6579,	 Loss Con1 = 1.2791,	 Loss Con2 = 2.3646
3159 0.152
3140 0.791
2603 0.872
1323 0.87
892 0.845
760 0.859
71 0.901
20 0.9
0.8393688273356794
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1077,	 Acc = 0.6490,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.5590
745 0.215
668 0.696
714 0.868
562 0.827
190 0.7
103 0.835
11 1.0
7 1.0
0.7924611973392461
0.8093126385809313

 ===== Epoch 148	 =====
[-0.4557189  -0.49292567  1.5603349   1.837851    2.5609274   0.9000731
  0.7614993   1.5085932   2.9775968   0.32021803 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.5600998   1.6667436   2.5610957   0.70897484
  0.7613445   1.3035216   2.9775968   0.10678374 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 5 5
train:	 Loss = 1.0533,	 Acc = 0.6537,	 Loss Con1 = 1.2787,	 Loss Con2 = 2.6069
3164 0.152
3142 0.785
2598 0.869
1320 0.87
893 0.84
761 0.845
70 0.871
20 0.9
0.834166288050886
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1060,	 Acc = 0.6583,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.7264
745 0.215
668 0.717
714 0.873
562 0.833
190 0.737
103 0.845
11 1.0
7 1.0
0.8048780487804879
0.8093126385809313

 ===== Epoch 149	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 6 0
train:	 Loss = 1.0470,	 Acc = 0.6562,	 Loss Con1 = 1.2789,	 Loss Con2 = 2.2610
3161 0.152
3144 0.785
2599 0.873
1323 0.868
891 0.844
760 0.861
70 0.886
20 0.85
0.8369478823662996
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1335,	 Acc = 0.6440,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.6031
745 0.212
668 0.644
714 0.874
562 0.843
190 0.732
103 0.864
11 1.0
7 1.0
0.7866962305986697
0.8093126385809313

 ===== Epoch 150	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013  0.4324032   3.184229   -0.4178274  -0.43225467
  1.4082421   1.2852896 ] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355  0.4041128   3.245666   -0.4178274  -0.43225467
  1.377877    1.2760947 ] 1 1
train:	 Loss = 1.0422,	 Acc = 0.6554,	 Loss Con1 = 1.2788,	 Loss Con2 = 2.2583
3164 0.156
3141 0.787
2600 0.868
1321 0.863
892 0.842
759 0.858
71 0.887
20 0.9
0.834961381190368
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1238,	 Acc = 0.6420,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.3869
745 0.212
668 0.648
714 0.88
562 0.824
190 0.737
103 0.854
11 0.909
7 0.857
0.7840354767184036
0.8093126385809313

 ===== Epoch 151	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
  1.2644713   2.0319343 ] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
  1.2644713   2.0319347 ] 0 0
train:	 Loss = 1.0502,	 Acc = 0.6570,	 Loss Con1 = 1.2801,	 Loss Con2 = 2.3949
3161 0.153
3143 0.789
2596 0.87
1324 0.872
893 0.845
760 0.854
71 0.887
20 0.85
0.8377427046667424
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1635,	 Acc = 0.6407,	 Loss Con1 = 1.3409,	 Loss Con2 = 3.3989
745 0.216
668 0.674
714 0.87
562 0.806
190 0.695
103 0.845
11 1.0
7 1.0
0.7809312638580931
0.8093126385809313

 ===== Epoch 152	 =====
[ 2.082093    1.490366    1.5696709   0.07269578 -0.6348712  -0.6508424
  2.4192257   1.6728376  -0.46656546 -0.4823711   2.7098165   2.9549701
 -0.45947683 -0.42555502] [ 2.0835369   1.4566602   1.5859523   0.05485835 -0.6347739  -0.6507814
  2.4174507   1.6454599  -0.46656546 -0.4823711   2.6942616   2.9362445
 -0.45947683 -0.42555502] 3 3
train:	 Loss = 1.0486,	 Acc = 0.6557,	 Loss Con1 = 1.2785,	 Loss Con2 = 2.5843
3159 0.152
3149 0.785
2597 0.869
1323 0.867
891 0.851
758 0.858
71 0.901
20 0.85
0.8363037802247701
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1193,	 Acc = 0.6540,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.5588
745 0.213
668 0.717
714 0.877
562 0.82
190 0.684
103 0.864
11 1.0
7 1.0
0.7995565410199557
0.8093126385809313

 ===== Epoch 153	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 2 0
train:	 Loss = 1.0473,	 Acc = 0.6552,	 Loss Con1 = 1.2787,	 Loss Con2 = 2.2914
3163 0.154
3144 0.79
2597 0.867
1322 0.86
890 0.844
761 0.852
71 0.901
20 0.85
0.8352072685973878
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1115,	 Acc = 0.6450,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.4123
745 0.217
668 0.65
714 0.868
562 0.833
190 0.758
103 0.864
11 1.0
7 1.0
0.7862527716186253
0.8093126385809313

 ===== Epoch 154	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 1 0
train:	 Loss = 1.0461,	 Acc = 0.6558,	 Loss Con1 = 1.2779,	 Loss Con2 = 2.2388
3167 0.152
3141 0.789
2598 0.869
1323 0.868
889 0.847
759 0.856
71 0.887
20 0.9
0.8371775934552892
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1275,	 Acc = 0.6473,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.6997
745 0.216
668 0.675
714 0.878
562 0.824
190 0.726
103 0.816
11 1.0
7 1.0
0.78980044345898
0.8093126385809313

 ===== Epoch 155	 =====
[-0.4557189  -0.49292567  2.0267591   2.4176185   0.55605406  1.3446153
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  2.026474    2.2465954   0.5561779   1.153511
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 5 5
train:	 Loss = 1.0466,	 Acc = 0.6542,	 Loss Con1 = 1.2795,	 Loss Con2 = 2.2870
3163 0.153
3139 0.782
2599 0.864
1324 0.872
891 0.845
761 0.862
71 0.901
20 0.85
0.8342986939239069
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1585,	 Acc = 0.6417,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.5489
745 0.219
668 0.623
714 0.873
562 0.84
190 0.763
103 0.864
11 1.0
7 0.857
0.7813747228381375
0.8093126385809313

 ===== Epoch 156	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 5 0
train:	 Loss = 1.0429,	 Acc = 0.6557,	 Loss Con1 = 1.2782,	 Loss Con2 = 2.1875
3165 0.153
3140 0.787
2598 0.868
1324 0.867
891 0.853
759 0.855
71 0.901
20 0.9
0.8366465977507668
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.0957,	 Acc = 0.6507,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.3911
745 0.217
668 0.68
714 0.878
562 0.82
190 0.758
103 0.835
11 1.0
7 1.0
0.7937915742793792
0.8093126385809313

 ===== Epoch 157	 =====
[ 3.5420594   0.974387    1.7641464   1.7188084   2.5098345   0.7026012
  0.8777434   1.9574937   2.6752355   0.1362371  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [ 3.5420594   0.8522019   1.7638893   1.6162778   2.510002    0.58796066
  0.8775824   1.8343906   2.6752355   0.00817622 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 5
train:	 Loss = 1.0454,	 Acc = 0.6573,	 Loss Con1 = 1.2792,	 Loss Con2 = 2.2000
3165 0.152
3145 0.789
2594 0.869
1322 0.874
890 0.846
761 0.865
71 0.901
20 0.9
0.8388049528569805
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1197,	 Acc = 0.6523,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.3583
745 0.217
668 0.702
714 0.88
562 0.811
190 0.726
103 0.845
11 1.0
7 0.857
0.7960088691796009
0.8093126385809313

 ===== Epoch 158	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 4 0
train:	 Loss = 1.0542,	 Acc = 0.6547,	 Loss Con1 = 1.2791,	 Loss Con2 = 2.2845
3161 0.153
3145 0.783
2599 0.868
1319 0.867
890 0.855
763 0.849
71 0.901
20 0.9
0.8346769615078915
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1125,	 Acc = 0.6483,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.6081
745 0.213
668 0.672
714 0.861
562 0.819
190 0.821
103 0.854
11 1.0
7 1.0
0.7920177383592018
0.8093126385809313

 ===== Epoch 159	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
  0.9182326   0.8957333 ] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
  0.9182326   1.1213099 ] 6 6
train:	 Loss = 1.0536,	 Acc = 0.6538,	 Loss Con1 = 1.2793,	 Loss Con2 = 2.4204
3162 0.152
3142 0.786
2598 0.865
1322 0.866
892 0.848
762 0.852
70 0.886
20 0.9
0.8340903929139223
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.0991,	 Acc = 0.6487,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.4882
745 0.216
668 0.686
714 0.868
562 0.819
190 0.732
103 0.874
11 1.0
7 1.0
0.7915742793791575
0.8093126385809313

 ===== Epoch 160	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 4 0
train:	 Loss = 1.0434,	 Acc = 0.6557,	 Loss Con1 = 1.2799,	 Loss Con2 = 2.3571
3161 0.152
3141 0.785
2596 0.869
1324 0.872
892 0.849
763 0.853
71 0.901
20 0.9
0.8366072442375383
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1197,	 Acc = 0.6543,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.6196
745 0.216
668 0.681
714 0.877
562 0.827
190 0.795
103 0.845
11 1.0
7 1.0
0.7991130820399113
0.8093126385809313

 ===== Epoch 161	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634  2.3128698   1.5637723
  0.63295615  3.0872703   2.4433064   1.014372   -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053  2.3406224   1.5609676
  0.661239    3.039464    2.4796822   1.0224856  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 2 2
train:	 Loss = 1.0485,	 Acc = 0.6550,	 Loss Con1 = 1.2787,	 Loss Con2 = 2.2636
3164 0.151
3145 0.791
2597 0.866
1324 0.869
887 0.843
760 0.85
71 0.887
20 0.9
0.8360972285324852
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1018,	 Acc = 0.6503,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.5672
745 0.216
668 0.671
714 0.868
562 0.829
190 0.795
103 0.845
11 1.0
7 1.0
0.7937915742793792
0.8093126385809313

 ===== Epoch 162	 =====
[-0.4557189  -0.49292567  0.8485825   2.3024905   1.1219507   0.2029599
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  0.8308993   2.2766066   1.1369522   0.17731166
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 3 3
train:	 Loss = 1.0421,	 Acc = 0.6564,	 Loss Con1 = 1.2789,	 Loss Con2 = 2.2577
3163 0.152
3148 0.787
2593 0.866
1322 0.874
890 0.852
761 0.866
71 0.887
20 0.9
0.8375922771152754
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1279,	 Acc = 0.6450,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.4985
745 0.216
668 0.638
714 0.874
562 0.827
190 0.816
103 0.835
11 1.0
7 1.0
0.7866962305986697
0.8093126385809313

 ===== Epoch 163	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 0
train:	 Loss = 1.0456,	 Acc = 0.6553,	 Loss Con1 = 1.2780,	 Loss Con2 = 2.2220
3166 0.153
3144 0.791
2596 0.863
1322 0.865
889 0.853
761 0.858
70 0.857
20 0.9
0.8360599863667348
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1271,	 Acc = 0.6507,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.4065
745 0.217
668 0.671
714 0.874
562 0.829
190 0.758
103 0.874
11 1.0
7 1.0
0.7937915742793792
0.8093126385809313

 ===== Epoch 164	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 6 0
train:	 Loss = 1.0454,	 Acc = 0.6545,	 Loss Con1 = 1.2781,	 Loss Con2 = 2.1857
3162 0.154
3145 0.781
2598 0.865
1323 0.868
891 0.856
760 0.858
70 0.886
19 0.842
0.8342039518510107
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1164,	 Acc = 0.6537,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.3607
745 0.216
668 0.69
714 0.875
562 0.835
190 0.726
103 0.864
11 1.0
7 1.0
0.7982261640798226
0.8093126385809313

 ===== Epoch 165	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634  1.4305522   1.6324497
 -0.43886673 -0.47319013  0.49548995  2.6409307  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053  1.4032925   1.6375476
 -0.4389609  -0.47327355  0.46161762  2.6800816  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 1 1
train:	 Loss = 1.0465,	 Acc = 0.6562,	 Loss Con1 = 1.2790,	 Loss Con2 = 2.0976
3163 0.151
3143 0.785
2597 0.87
1323 0.874
891 0.854
760 0.851
71 0.915
20 0.9
0.8374787052810903
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1249,	 Acc = 0.6453,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.3107
745 0.217
668 0.653
714 0.864
562 0.831
190 0.774
103 0.864
11 1.0
7 1.0
0.7866962305986697
0.8093126385809313

 ===== Epoch 166	 =====
[-0.4557189  -0.49292567  2.293421    1.2950372   0.99323046  0.18559976
  2.875338    2.8028634   0.32939214  2.5713303  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  2.2942328   1.4668422   0.99423134  0.3776151
  2.876426    3.0072238   0.33084098  2.7835174  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 6 6
train:	 Loss = 1.0450,	 Acc = 0.6554,	 Loss Con1 = 1.2804,	 Loss Con2 = 2.2212
3163 0.152
3144 0.79
2592 0.863
1323 0.868
892 0.855
763 0.855
71 0.873
20 0.9
0.836342986939239
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1140,	 Acc = 0.6500,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.4148
745 0.215
668 0.672
714 0.881
562 0.822
190 0.763
103 0.845
11 1.0
7 1.0
0.7937915742793792
0.8093126385809313

 ===== Epoch 167	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 4 0
train:	 Loss = 1.0416,	 Acc = 0.6562,	 Loss Con1 = 1.2776,	 Loss Con2 = 2.0598
3166 0.152
3142 0.785
2599 0.871
1321 0.87
891 0.848
760 0.864
69 0.928
20 0.85
0.8377641445126107
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1270,	 Acc = 0.6417,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.3297
745 0.215
668 0.636
714 0.868
562 0.831
190 0.774
103 0.854
11 1.0
7 1.0
0.7827050997782705
0.8093126385809313

 ===== Epoch 168	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
  1.316863    2.4915137 ] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
  1.2928722   2.5456245 ] 1 4
train:	 Loss = 1.0502,	 Acc = 0.6540,	 Loss Con1 = 1.2796,	 Loss Con2 = 2.2774
3162 0.153
3140 0.782
2600 0.869
1322 0.862
892 0.85
761 0.854
71 0.887
20 0.85
0.8338632750397457
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1026,	 Acc = 0.6497,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.4416
745 0.219
668 0.68
714 0.866
562 0.833
190 0.732
103 0.864
11 1.0
7 1.0
0.7920177383592018
0.8093126385809313

 ===== Epoch 169	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.16213782  2.0363472  -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.16224596  2.2411566  -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 6 4
train:	 Loss = 1.0396,	 Acc = 0.6573,	 Loss Con1 = 1.2799,	 Loss Con2 = 2.1366
3167 0.153
3141 0.79
2593 0.87
1321 0.868
893 0.849
763 0.861
70 0.9
20 0.9
0.8387683217816158
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1122,	 Acc = 0.6517,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.2560
745 0.213
668 0.708
714 0.866
562 0.82
190 0.726
103 0.864
11 0.909
7 1.0
0.7964523281596453
0.8093126385809313

 ===== Epoch 170	 =====
[-0.4557189  -0.49292567  0.8168746   2.3691149   0.7810151  -0.1264266
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  0.8056755   2.3540304   0.7923229  -0.1431788
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 3
train:	 Loss = 1.0456,	 Acc = 0.6568,	 Loss Con1 = 1.2787,	 Loss Con2 = 2.1078
3164 0.152
3146 0.79
2593 0.869
1323 0.866
890 0.849
761 0.862
71 0.901
20 0.9
0.838255338482508
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1595,	 Acc = 0.6480,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.2710
745 0.216
668 0.656
714 0.87
562 0.835
190 0.779
103 0.864
11 1.0
7 1.0
0.7906873614190687
0.8093126385809313

 ===== Epoch 171	 =====
[-0.4557189  -0.49292567  2.183638    1.8383155   1.2789621   0.62928474
 -0.43886673 -0.47319013  0.2543169   1.3423536  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  2.1841173   1.8520592   1.2652004   0.64582485
 -0.4389609  -0.47327355  0.2401857   1.419329   -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 1 4
train:	 Loss = 1.0387,	 Acc = 0.6588,	 Loss Con1 = 1.2793,	 Loss Con2 = 2.1267
3162 0.15
3148 0.794
2591 0.871
1322 0.874
892 0.851
762 0.862
71 0.901
20 0.85
0.8413581648875766
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.0952,	 Acc = 0.6483,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.9071
745 0.212
668 0.69
714 0.868
562 0.817
190 0.742
103 0.854
11 1.0
7 1.0
0.7924611973392461
0.8093126385809313

 ===== Epoch 172	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.12455241  0.85885113
  1.7766157   3.1226077 ] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.09631664  0.94233394
  1.8027085   3.1056294 ] 2 2
train:	 Loss = 1.0455,	 Acc = 0.6559,	 Loss Con1 = 1.2783,	 Loss Con2 = 2.3460
3165 0.151
3145 0.787
2599 0.871
1319 0.87
889 0.841
761 0.863
71 0.915
19 0.895
0.8375553788481199
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1374,	 Acc = 0.6497,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.5181
745 0.215
668 0.65
714 0.874
562 0.831
190 0.832
103 0.854
11 1.0
7 1.0
0.7933481152993348
0.8093126385809313

 ===== Epoch 173	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634  0.6256576   0.80935264
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053  0.652957    0.8190871
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 2 2
train:	 Loss = 1.0440,	 Acc = 0.6555,	 Loss Con1 = 1.2790,	 Loss Con2 = 2.1322
3164 0.152
3143 0.787
2594 0.868
1323 0.866
891 0.851
763 0.856
71 0.901
19 0.947
0.8364379827351204
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1118,	 Acc = 0.6473,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.1879
745 0.217
668 0.657
714 0.864
562 0.835
190 0.779
103 0.864
11 1.0
7 1.0
0.7893569844789357
0.8093126385809313

 ===== Epoch 174	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 0
train:	 Loss = 1.0481,	 Acc = 0.6544,	 Loss Con1 = 1.2783,	 Loss Con2 = 2.4267
3162 0.152
3146 0.784
2596 0.866
1325 0.866
889 0.852
759 0.855
71 0.887
20 0.9
0.8347717465364524
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1280,	 Acc = 0.6467,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.4309
745 0.212
668 0.653
714 0.87
562 0.829
190 0.805
103 0.864
11 0.909
7 1.0
0.7902439024390244
0.8093126385809313

 ===== Epoch 175	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
  1.0078441   1.042637   -0.46656546 -0.4823711   1.5387622   3.1897993
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
  1.0444301   1.0560507  -0.46656546 -0.4823711   1.5634413   3.1693192
 -0.45947683 -0.42555502] 2 2
train:	 Loss = 1.0459,	 Acc = 0.6553,	 Loss Con1 = 1.2786,	 Loss Con2 = 2.2991
3160 0.152
3146 0.787
2597 0.866
1326 0.871
888 0.851
760 0.854
71 0.873
20 0.9
0.83594459582198
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1295,	 Acc = 0.6480,	 Loss Con1 = 1.3409,	 Loss Con2 = 3.1478
745 0.213
668 0.668
714 0.878
562 0.826
190 0.753
103 0.845
11 1.0
7 1.0
0.7915742793791575
0.8093126385809313

 ===== Epoch 176	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634  0.04835807  0.7547789
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053  0.07547319  0.77451044
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 2 2
train:	 Loss = 1.0426,	 Acc = 0.6570,	 Loss Con1 = 1.2789,	 Loss Con2 = 2.4080
3166 0.151
3144 0.788
2592 0.872
1323 0.873
891 0.85
761 0.859
71 0.873
20 0.9
0.8389002499431947
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1121,	 Acc = 0.6470,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.6411
745 0.217
668 0.662
714 0.868
562 0.836
190 0.747
103 0.845
11 1.0
7 1.0
0.7889135254988914
0.8093126385809313

 ===== Epoch 177	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 0
train:	 Loss = 1.0444,	 Acc = 0.6543,	 Loss Con1 = 1.2777,	 Loss Con2 = 2.2526
3167 0.152
3141 0.782
2598 0.867
1322 0.87
892 0.846
757 0.861
71 0.915
20 0.9
0.8351323713214407
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1505,	 Acc = 0.6417,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.5645
745 0.211
668 0.665
714 0.868
562 0.815
190 0.742
103 0.845
11 1.0
7 1.0
0.7840354767184036
0.8093126385809313

 ===== Epoch 178	 =====
[ 0.9420332   0.36324203  2.043179    0.31797925 -0.6348712  -0.6508424
  1.2481818   0.82113415 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [ 0.9420332   0.1596006   2.0428922   0.14664951 -0.6347739  -0.6507814
  1.2480025   0.6160772  -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 5 5
train:	 Loss = 1.0419,	 Acc = 0.6554,	 Loss Con1 = 1.2788,	 Loss Con2 = 2.2451
3164 0.154
3143 0.785
2598 0.866
1322 0.873
888 0.841
762 0.86
71 0.901
20 0.9
0.8356428895956384
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1143,	 Acc = 0.6450,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.5715
745 0.216
668 0.659
714 0.877
562 0.822
190 0.737
103 0.854
11 1.0
7 1.0
0.7866962305986697
0.8093126385809313

 ===== Epoch 179	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.19114503  1.1195534
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.18810703  1.0000387
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 3 3
train:	 Loss = 1.0426,	 Acc = 0.6566,	 Loss Con1 = 1.2799,	 Loss Con2 = 2.1285
3159 0.152
3146 0.787
2596 0.871
1321 0.869
893 0.852
762 0.853
71 0.887
20 0.9
0.8374389828584402
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1070,	 Acc = 0.6493,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.4328
745 0.217
668 0.677
714 0.873
562 0.827
190 0.737
103 0.854
11 1.0
7 1.0
0.7920177383592018
0.8093126385809313

 ===== Epoch 180	 =====
[-0.4557189  -0.49292567  1.5480413   1.8372234   2.2960365   0.7263096
  0.5748226   2.1299894   2.4550788   0.06288275 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.5478076   1.6661158   2.296199    0.53521365
  0.5746773   1.9249047  -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 5 5
train:	 Loss = 1.0444,	 Acc = 0.6559,	 Loss Con1 = 1.2776,	 Loss Con2 = 2.2045
3162 0.152
3144 0.787
2598 0.866
1325 0.872
888 0.843
761 0.863
71 0.901
19 0.842
0.8367022484669544
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1191,	 Acc = 0.6437,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.4226
745 0.215
668 0.671
714 0.87
562 0.815
190 0.726
103 0.864
11 0.909
7 1.0
0.7853658536585366
0.8093126385809313

 ===== Epoch 181	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634  0.43166813  1.374082
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053  0.40390062  1.3771694
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 1 1
train:	 Loss = 1.0480,	 Acc = 0.6559,	 Loss Con1 = 1.2800,	 Loss Con2 = 2.3147
3162 0.151
3146 0.788
2596 0.869
1320 0.87
891 0.847
762 0.86
71 0.887
20 0.85
0.8373836020894845
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1201,	 Acc = 0.6383,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.4507
745 0.216
668 0.621
714 0.874
562 0.819
190 0.789
103 0.854
11 0.909
7 1.0
0.7778270509977827
0.8093126385809313

 ===== Epoch 182	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
  0.21125066  3.1139758  -0.46656546 -0.4823711   1.642247    3.3488
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
  0.19463356  3.0822363  -0.46656546 -0.4823711   1.6283575   3.2735739
 -0.45947683 -0.42555502] 5 3
train:	 Loss = 1.0446,	 Acc = 0.6541,	 Loss Con1 = 1.2797,	 Loss Con2 = 2.1258
3166 0.153
3137 0.786
2599 0.866
1324 0.866
889 0.84
762 0.858
71 0.873
20 0.85
0.8342422176778005
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1319,	 Acc = 0.6443,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.3461
745 0.212
668 0.672
714 0.866
562 0.824
190 0.737
103 0.854
11 0.909
7 1.0
0.7871396895787139
0.8093126385809313

 ===== Epoch 183	 =====
[ 1.68245     1.3917351  -0.5993768  -0.57551634 -0.6348712  -0.6508424
  0.29099402  3.0078442   2.7918167   2.8620005   1.1320944   0.32308418
  2.5926323   1.8955535 ] [ 1.6967427   1.3641236  -0.59937984 -0.57549053 -0.6347739  -0.6507814
  0.2821056   2.928561    2.786369    2.8431027   1.1524744   0.306046
  2.6009126   1.878826  ] 2 3
train:	 Loss = 1.0485,	 Acc = 0.6537,	 Loss Con1 = 1.2795,	 Loss Con2 = 2.2379
3165 0.152
3141 0.785
2597 0.868
1322 0.868
890 0.838
762 0.856
71 0.887
20 0.85
0.8341474497330456
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1214,	 Acc = 0.6477,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.5685
745 0.216
668 0.657
714 0.878
562 0.822
190 0.774
103 0.864
11 1.0
7 1.0
0.7902439024390244
0.8093126385809313

 ===== Epoch 184	 =====
[ 0.95485175  1.2988673  -0.5993768  -0.57551634 -0.6348712  -0.6508424
  2.7080991   0.6093589   0.44205248  2.175819    2.8469446   1.7804904
  1.7574844   3.331877  ] [ 0.95485175  1.095226   -0.59937984 -0.57549053 -0.6347739  -0.6507814
  2.7078462   0.4043064   0.44205248  1.9623845   2.8469446   1.5598894
  1.7574844   3.1062999 ] 5 5
train:	 Loss = 1.0461,	 Acc = 0.6544,	 Loss Con1 = 1.2791,	 Loss Con2 = 2.2955
3165 0.152
3141 0.787
2597 0.869
1322 0.866
891 0.841
761 0.853
71 0.859
20 0.9
0.8350562308303987
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1030,	 Acc = 0.6487,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.4932
745 0.216
668 0.678
714 0.867
562 0.822
190 0.763
103 0.864
11 0.909
7 1.0
0.7915742793791575
0.8093126385809313

 ===== Epoch 185	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 6 0
train:	 Loss = 1.0414,	 Acc = 0.6587,	 Loss Con1 = 1.2798,	 Loss Con2 = 2.1800
3161 0.154
3139 0.791
2599 0.871
1325 0.87
892 0.85
761 0.863
71 0.887
20 0.9
0.8397865334393096
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1118,	 Acc = 0.6440,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.3517
745 0.217
668 0.671
714 0.866
562 0.811
190 0.758
103 0.835
11 1.0
7 1.0
0.7849223946784922
0.8093126385809313

 ===== Epoch 186	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
  0.9304849   1.6914939 ] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
  0.8992564   1.688299  ] 1 1
train:	 Loss = 1.0393,	 Acc = 0.6562,	 Loss Con1 = 1.2784,	 Loss Con2 = 2.0954
3163 0.151
3145 0.79
2596 0.87
1325 0.861
890 0.849
758 0.86
71 0.887
20 0.9
0.8375922771152754
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1266,	 Acc = 0.6507,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.4229
745 0.215
668 0.68
714 0.875
562 0.829
190 0.753
103 0.835
11 1.0
7 1.0
0.7946784922394678
0.8093126385809313

 ===== Epoch 187	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634  1.028228    1.7299783
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053  1.0364113   1.7617238
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 4 4
train:	 Loss = 1.0382,	 Acc = 0.6568,	 Loss Con1 = 1.2790,	 Loss Con2 = 2.0961
3166 0.154
3142 0.788
2594 0.87
1324 0.872
889 0.843
762 0.86
71 0.887
20 0.9
0.837536923426494
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.0950,	 Acc = 0.6537,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.3739
745 0.212
668 0.678
714 0.868
562 0.829
190 0.826
103 0.864
11 1.0
7 1.0
0.7995565410199557
0.8093126385809313

 ===== Epoch 188	 =====
[ 0.5014934   1.3346326   1.7365515   0.6073801  -0.6348712  -0.6508424
  0.9348996   1.5985678  -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [ 0.4597068   1.3250475   1.7095118   0.6011971  -0.6347739  -0.6507814
  0.8967368   1.5990735  -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 1 1
train:	 Loss = 1.0401,	 Acc = 0.6564,	 Loss Con1 = 1.2797,	 Loss Con2 = 2.2128
3165 0.152
3144 0.791
2593 0.872
1323 0.866
891 0.846
761 0.85
71 0.859
20 0.9
0.8376689764852892
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1032,	 Acc = 0.6517,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.5387
745 0.216
668 0.665
714 0.875
562 0.833
190 0.795
103 0.864
11 0.909
7 1.0
0.7955654101995565
0.8093126385809313

 ===== Epoch 189	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 4 0
train:	 Loss = 1.0395,	 Acc = 0.6563,	 Loss Con1 = 1.2792,	 Loss Con2 = 2.2396
3164 0.152
3144 0.786
2594 0.87
1324 0.867
891 0.848
760 0.867
71 0.915
20 0.85
0.8375738300772376
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1312,	 Acc = 0.6470,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.4847
745 0.215
668 0.672
714 0.873
562 0.817
190 0.753
103 0.864
11 1.0
7 1.0
0.78980044345898
0.8093126385809313

 ===== Epoch 190	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
  1.5395155   0.76263034] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
  1.5395155   0.76263034] 0 0
train:	 Loss = 1.0433,	 Acc = 0.6556,	 Loss Con1 = 1.2797,	 Loss Con2 = 2.0999
3165 0.153
3141 0.785
2597 0.871
1320 0.868
891 0.844
763 0.858
71 0.901
20 0.8
0.8361922072020902
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1359,	 Acc = 0.6470,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.4452
745 0.213
668 0.677
714 0.868
562 0.824
190 0.732
103 0.874
11 1.0
7 1.0
0.7902439024390244
0.8093126385809313

 ===== Epoch 191	 =====
[ 0.7221299   0.69766206 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711   1.5861026   1.2818546
 -0.45947683 -0.42555502] [ 0.760759    0.72611165 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711   1.6161088   1.2892286
 -0.45947683 -0.42555502] 2 2
train:	 Loss = 1.0455,	 Acc = 0.6570,	 Loss Con1 = 1.2790,	 Loss Con2 = 2.1568
3161 0.154
3144 0.784
2599 0.873
1322 0.869
890 0.847
761 0.862
71 0.901
20 0.9
0.8374020665379811
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1155,	 Acc = 0.6407,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.4621
745 0.215
668 0.653
714 0.867
562 0.811
190 0.758
103 0.864
11 1.0
7 1.0
0.7813747228381375
0.8093126385809313

 ===== Epoch 192	 =====
[-0.4557189  -0.49292567  2.2582438   2.1373336   0.3584737   2.0660293
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  2.2579336   2.3092427   0.35859314  2.2571917
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 6 6
train:	 Loss = 1.0386,	 Acc = 0.6563,	 Loss Con1 = 1.2792,	 Loss Con2 = 2.1491
3167 0.153
3142 0.79
2594 0.869
1322 0.869
892 0.841
760 0.862
71 0.887
20 0.85
0.8374048403590502
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1084,	 Acc = 0.6477,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.2849
745 0.215
668 0.654
714 0.87
562 0.829
190 0.816
103 0.835
11 1.0
7 1.0
0.7906873614190687
0.8093126385809313

 ===== Epoch 193	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 3 0
train:	 Loss = 1.0414,	 Acc = 0.6569,	 Loss Con1 = 1.2793,	 Loss Con2 = 2.1709
3162 0.153
3145 0.787
2594 0.871
1323 0.871
892 0.842
761 0.866
71 0.901
20 0.9
0.8378378378378378
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1225,	 Acc = 0.6440,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.2135
745 0.216
668 0.663
714 0.874
562 0.819
190 0.742
103 0.835
11 1.0
7 0.857
0.7853658536585366
0.8093126385809313

 ===== Epoch 194	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 2 0
train:	 Loss = 1.0466,	 Acc = 0.6550,	 Loss Con1 = 1.2792,	 Loss Con2 = 2.1870
3164 0.153
3141 0.787
2596 0.866
1323 0.871
892 0.842
761 0.857
71 0.859
20 0.85
0.8353021353930031
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1261,	 Acc = 0.6400,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.5317
745 0.212
668 0.666
714 0.875
562 0.81
190 0.7
103 0.845
11 1.0
7 0.857
0.7813747228381375
0.8093126385809313

 ===== Epoch 195	 =====
[-0.4557189  -0.49292567  1.9731493   1.4309539   0.7193958   0.08166248
  2.5415378   3.1111035  -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.9730917   1.4312477   0.71970564  0.08189618
  2.5415158   3.1108017  -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 0
train:	 Loss = 1.0470,	 Acc = 0.6559,	 Loss Con1 = 1.2792,	 Loss Con2 = 2.2640
3165 0.152
3143 0.787
2595 0.871
1320 0.868
891 0.845
763 0.858
71 0.873
20 0.9
0.8369873906622742
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1295,	 Acc = 0.6467,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.7317
745 0.216
668 0.659
714 0.868
562 0.824
190 0.789
103 0.864
11 0.909
7 1.0
0.7889135254988914
0.8093126385809313

 ===== Epoch 196	 =====
[-0.4557189  -0.49292567  1.0808647   2.1354957   0.70329934 -0.15563738
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0805244   1.964597   -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 5 5
train:	 Loss = 1.0414,	 Acc = 0.6552,	 Loss Con1 = 1.2786,	 Loss Con2 = 2.4258
3166 0.154
3137 0.782
2599 0.869
1326 0.87
889 0.852
761 0.857
70 0.886
20 0.85
0.8353783231083844
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1023,	 Acc = 0.6457,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.5502
745 0.211
668 0.653
714 0.881
562 0.82
190 0.779
103 0.854
11 1.0
7 1.0
0.7893569844789357
0.8093126385809313

 ===== Epoch 197	 =====
[-0.4557189  -0.49292567  1.2733877   1.6950158   1.6858512   0.16178612
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.2644083   1.6708139   1.7013078   0.14264381
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 3 3
train:	 Loss = 1.0402,	 Acc = 0.6571,	 Loss Con1 = 1.2793,	 Loss Con2 = 2.3080
3163 0.152
3144 0.793
2596 0.867
1321 0.87
891 0.844
763 0.864
70 0.9
20 0.9
0.8386144236229415
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1048,	 Acc = 0.6470,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.5390
745 0.213
668 0.665
714 0.874
562 0.826
190 0.753
103 0.864
11 1.0
7 1.0
0.7902439024390244
0.8093126385809313

 ===== Epoch 198	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.16213782  1.0115948  -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.16224596  1.216426   -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 6 4
train:	 Loss = 1.0385,	 Acc = 0.6588,	 Loss Con1 = 1.2787,	 Loss Con2 = 2.2441
3165 0.155
3145 0.791
2595 0.874
1324 0.872
890 0.843
758 0.86
71 0.887
20 0.9
0.8398273315915029
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1045,	 Acc = 0.6393,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.5874
745 0.215
668 0.659
714 0.86
562 0.815
190 0.737
103 0.854
11 1.0
7 1.0
0.77960088691796
0.8093126385809313

 ===== Epoch 199	 =====
[-0.4557189  -0.49292567  1.6693826   1.9264224   0.8444222   0.29149297
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.6568025   1.9076296   0.8583175   0.26025516
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 3 3
train:	 Loss = 1.0446,	 Acc = 0.6546,	 Loss Con1 = 1.2786,	 Loss Con2 = 2.2504
3164 0.154
3143 0.783
2596 0.868
1324 0.87
890 0.844
761 0.85
70 0.9
20 0.85
0.8343934575193094
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1236,	 Acc = 0.6447,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.9217
745 0.212
668 0.654
714 0.875
562 0.819
190 0.789
103 0.845
11 1.0
7 0.857
0.7875831485587583
0.8093126385809313

 ===== Epoch 200	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 4 0
train:	 Loss = 1.0447,	 Acc = 0.6562,	 Loss Con1 = 1.2796,	 Loss Con2 = 2.4881
3166 0.152
3138 0.789
2596 0.871
1324 0.867
890 0.849
763 0.853
71 0.901
20 0.9
0.837536923426494
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1074,	 Acc = 0.6560,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.6952
745 0.216
668 0.71
714 0.875
562 0.831
190 0.705
103 0.864
11 1.0
7 1.0
0.801330376940133
0.8093126385809313

 ===== Epoch 201	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634  0.34276122  2.4829185
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053  0.3598061   2.5260615
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 4 4
train:	 Loss = 1.0397,	 Acc = 0.6578,	 Loss Con1 = 1.2797,	 Loss Con2 = 2.3072
3165 0.153
3143 0.795
2594 0.867
1322 0.872
891 0.845
762 0.854
71 0.901
20 0.85
0.839145745768488
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1204,	 Acc = 0.6473,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.7113
745 0.217
668 0.66
714 0.867
562 0.833
190 0.763
103 0.864
11 1.0
7 1.0
0.7893569844789357
0.8093126385809313

 ===== Epoch 202	 =====
[ 0.9711833   0.6350094  -0.5993768  -0.57551634 -0.6348712  -0.6508424
  2.1430953   0.27126187  0.54178107  3.1253176   2.6117268   1.4428778
  1.9281377   3.1632595 ] [ 0.9702826   0.84067833 -0.59937984 -0.57549053 -0.6347739  -0.6507814
  2.1418273   0.47698689 -0.46656546 -0.4823711   2.6113994   1.664341
  1.9287057   3.3898902 ] 6 6
train:	 Loss = 1.0456,	 Acc = 0.6555,	 Loss Con1 = 1.2794,	 Loss Con2 = 2.2065
3163 0.152
3144 0.784
2598 0.872
1321 0.868
890 0.84
761 0.863
71 0.901
20 0.9
0.8364565587734242
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.0971,	 Acc = 0.6493,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.4143
745 0.212
668 0.693
714 0.875
562 0.815
190 0.732
103 0.845
11 1.0
7 1.0
0.7937915742793792
0.8093126385809313

 ===== Epoch 203	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634  0.311943    1.4784782
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053  0.31564534  1.5343922
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 4 4
train:	 Loss = 1.0419,	 Acc = 0.6557,	 Loss Con1 = 1.2783,	 Loss Con2 = 2.1581
3165 0.153
3144 0.786
2597 0.868
1322 0.868
887 0.849
762 0.857
71 0.901
20 0.9
0.8363058048392593
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1656,	 Acc = 0.6363,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.5126
745 0.211
668 0.627
714 0.861
562 0.822
190 0.8
103 0.845
11 1.0
7 0.857
0.776940133037694
0.8093126385809313

 ===== Epoch 204	 =====
[-0.4557189  -0.49292567  2.1441777   2.4391308   0.6926253   1.4462829
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  2.1655877   2.4276109   0.7214837   1.4435655
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 2 2
train:	 Loss = 1.0401,	 Acc = 0.6579,	 Loss Con1 = 1.2794,	 Loss Con2 = 2.0960
3162 0.153
3143 0.79
2598 0.871
1323 0.872
892 0.848
759 0.86
71 0.901
20 0.9
0.839200545082898
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1120,	 Acc = 0.6460,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.3888
745 0.215
668 0.665
714 0.87
562 0.826
190 0.753
103 0.854
11 1.0
7 1.0
0.788470066518847
0.8093126385809313

 ===== Epoch 205	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013  0.40016854  0.02620259 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355  0.42808154  0.0889328  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 2 2
train:	 Loss = 1.0386,	 Acc = 0.6588,	 Loss Con1 = 1.2788,	 Loss Con2 = 2.1659
3165 0.152
3139 0.797
2598 0.869
1324 0.872
890 0.846
762 0.864
70 0.9
20 0.85
0.8409633079631944
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1099,	 Acc = 0.6433,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.2363
745 0.217
668 0.675
714 0.866
562 0.811
190 0.737
103 0.835
11 0.909
7 1.0
0.7840354767184036
0.8093126385809313

 ===== Epoch 206	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 1 0
train:	 Loss = 1.0402,	 Acc = 0.6558,	 Loss Con1 = 1.2798,	 Loss Con2 = 2.1904
3167 0.153
3137 0.789
2595 0.869
1324 0.868
892 0.843
762 0.858
71 0.887
20 0.9
0.8368367230996477
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1338,	 Acc = 0.6380,	 Loss Con1 = 1.3409,	 Loss Con2 = 3.0746
745 0.215
668 0.675
714 0.861
562 0.804
190 0.705
103 0.825
11 1.0
7 0.857
0.7778270509977827
0.8093126385809313

 ===== Epoch 207	 =====
[-0.4557189  -0.49292567  2.1167305   1.7331727   1.344551    0.46459302
 -0.43886673 -0.47319013  0.3275685   0.6790085  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  2.106903    1.7168941   1.3559327   0.44017798
 -0.4389609  -0.47327355  0.3447222   0.5892647  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 3 3
train:	 Loss = 1.0451,	 Acc = 0.6557,	 Loss Con1 = 1.2795,	 Loss Con2 = 2.5801
3161 0.153
3143 0.785
2598 0.87
1324 0.872
891 0.842
760 0.861
71 0.873
20 0.85
0.8361530600658567
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1239,	 Acc = 0.6433,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.4191
745 0.215
668 0.666
714 0.873
562 0.822
190 0.721
103 0.825
11 1.0
7 1.0
0.7849223946784922
0.8093126385809313

 ===== Epoch 208	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.3464858   0.29105893 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.36429077  0.9417854  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 4 6
train:	 Loss = 1.0404,	 Acc = 0.6562,	 Loss Con1 = 1.2801,	 Loss Con2 = 2.1888
3161 0.153
3146 0.787
2596 0.869
1321 0.874
892 0.841
761 0.857
71 0.887
20 0.9
0.8368343363233791
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1165,	 Acc = 0.6497,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.3649
745 0.217
668 0.68
714 0.873
562 0.831
190 0.716
103 0.864
11 1.0
7 1.0
0.7924611973392461
0.8093126385809313

 ===== Epoch 209	 =====
[-0.4557189  -0.49292567  1.2741174   2.1757677   1.4148599   0.5252595
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.250828    2.1910312   1.3891358   0.51539916
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 1 1
train:	 Loss = 1.0435,	 Acc = 0.6540,	 Loss Con1 = 1.2799,	 Loss Con2 = 2.2923
3162 0.156
3142 0.782
2597 0.866
1323 0.865
891 0.844
763 0.852
70 0.9
20 0.85
0.8327276856688621
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1261,	 Acc = 0.6477,	 Loss Con1 = 1.3409,	 Loss Con2 = 3.0263
745 0.212
668 0.665
714 0.873
562 0.831
190 0.763
103 0.864
11 1.0
7 0.857
0.7915742793791575
0.8093126385809313

 ===== Epoch 210	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634  1.1156318   2.2501717
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053  1.1157681   2.4413316
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 6 6
train:	 Loss = 1.0399,	 Acc = 0.6572,	 Loss Con1 = 1.2775,	 Loss Con2 = 2.3478
3167 0.155
3141 0.787
2599 0.87
1323 0.87
890 0.846
757 0.865
71 0.887
20 0.85
0.837859334166572
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1126,	 Acc = 0.6390,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.3808
745 0.216
668 0.633
714 0.871
562 0.829
190 0.742
103 0.835
11 1.0
7 1.0
0.7787139689578714
0.8093126385809313

 ===== Epoch 211	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 4 0
train:	 Loss = 1.0396,	 Acc = 0.6583,	 Loss Con1 = 1.2794,	 Loss Con2 = 2.4844
3163 0.153
3142 0.791
2596 0.873
1323 0.866
892 0.848
761 0.865
71 0.915
20 0.9
0.8396365701306077
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1056,	 Acc = 0.6450,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.9199
745 0.217
668 0.641
714 0.875
562 0.827
190 0.779
103 0.864
11 1.0
7 1.0
0.7862527716186253
0.8093126385809313

 ===== Epoch 212	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 4 0
train:	 Loss = 1.0411,	 Acc = 0.6568,	 Loss Con1 = 1.2787,	 Loss Con2 = 2.3656
3166 0.153
3144 0.789
2594 0.868
1323 0.871
891 0.85
759 0.859
71 0.887
20 0.9
0.8379913655987276
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1047,	 Acc = 0.6467,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.5362
745 0.211
668 0.659
714 0.873
562 0.833
190 0.768
103 0.864
11 0.909
7 1.0
0.7906873614190687
0.8093126385809313

 ===== Epoch 213	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013  2.1362715   0.1819971  -0.4178274  -0.43225467
  3.2137277   0.5308244 ] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355  2.135171    0.3962938  -0.4178274  -0.43225467
  3.2129254   0.7569912 ] 6 6
train:	 Loss = 1.0385,	 Acc = 0.6557,	 Loss Con1 = 1.2797,	 Loss Con2 = 2.1974
3163 0.153
3138 0.788
2599 0.868
1324 0.868
890 0.845
763 0.857
71 0.901
20 0.85
0.836229415105054
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1196,	 Acc = 0.6517,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.3628
745 0.217
668 0.666
714 0.875
562 0.833
190 0.784
103 0.854
11 1.0
7 1.0
0.7951219512195122
0.8093126385809313

 ===== Epoch 214	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.20034741  2.461844
  1.8337584   3.3114522 ] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.22932507  2.5727324
  1.8090644   3.3304484 ] 1 1
train:	 Loss = 1.0427,	 Acc = 0.6556,	 Loss Con1 = 1.2795,	 Loss Con2 = 2.1651
3164 0.155
3143 0.788
2596 0.865
1321 0.872
892 0.841
761 0.857
71 0.901
20 0.85
0.8356428895956384
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.0949,	 Acc = 0.6440,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.3001
745 0.217
668 0.65
714 0.87
562 0.82
190 0.789
103 0.835
11 1.0
7 1.0
0.7849223946784922
0.8093126385809313

 ===== Epoch 215	 =====
[ 1.3232068   0.5732473  -0.5993768  -0.57551634  2.507014    2.1039925
  0.69660485  2.8751175   2.893071    1.6480782  -0.4178274  -0.43225467
  2.331105    0.5375474 ] [ 1.3043329   0.61648625 -0.59937984 -0.57549053  2.519214    2.1193297
  0.71936136  2.9292119   2.8932164   1.6722527  -0.4178274  -0.43225467
  2.314265    0.5589368 ] 4 4
train:	 Loss = 1.0404,	 Acc = 0.6571,	 Loss Con1 = 1.2791,	 Loss Con2 = 2.2191
3157 0.151
3145 0.791
2601 0.867
1323 0.871
891 0.85
761 0.859
70 0.914
20 0.9
0.8382703438883214
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.0978,	 Acc = 0.6487,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.5272
745 0.215
668 0.656
714 0.874
562 0.831
190 0.789
103 0.864
11 1.0
7 1.0
0.7920177383592018
0.8093126385809313

 ===== Epoch 216	 =====
[-0.4557189  -0.49292567  2.6069849   2.279246    0.9547139   1.7043896
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  2.606495    2.1082966   0.95464545  1.5133977
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 5 5
train:	 Loss = 1.0399,	 Acc = 0.6570,	 Loss Con1 = 1.2788,	 Loss Con2 = 2.2037
3162 0.152
3145 0.789
2597 0.873
1323 0.867
890 0.847
760 0.855
71 0.915
20 0.85
0.8384056325232796
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.0933,	 Acc = 0.6507,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.4060
745 0.213
668 0.668
714 0.874
562 0.831
190 0.784
103 0.864
11 1.0
7 1.0
0.7951219512195122
0.8093126385809313

 ===== Epoch 217	 =====
[-0.4557189  -0.49292567  2.0857491   1.6240021   1.4408078   0.3285866
 -0.43886673 -0.47319013  0.47981146  0.233597   -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  2.107222    1.6175311   1.4638901   0.33534727
  2.0425255   3.1761065   0.50889456  0.26727206 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 2 5
train:	 Loss = 1.0378,	 Acc = 0.6566,	 Loss Con1 = 1.2798,	 Loss Con2 = 2.1922
3162 0.154
3146 0.79
2593 0.867
1323 0.87
890 0.843
763 0.862
71 0.901
20 0.85
0.8371564842153078
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1208,	 Acc = 0.6447,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.4923
745 0.215
668 0.653
714 0.875
562 0.827
190 0.747
103 0.854
11 1.0
7 1.0
0.7866962305986697
0.8093126385809313

 ===== Epoch 218	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 0
train:	 Loss = 1.0398,	 Acc = 0.6560,	 Loss Con1 = 1.2789,	 Loss Con2 = 2.2706
3165 0.151
3145 0.789
2594 0.87
1322 0.866
892 0.844
759 0.864
71 0.887
20 0.85
0.8374417812109508
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1070,	 Acc = 0.6467,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.6288
745 0.217
668 0.66
714 0.874
562 0.829
190 0.737
103 0.864
11 1.0
7 1.0
0.788470066518847
0.8093126385809313

 ===== Epoch 219	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 6 0
train:	 Loss = 1.0363,	 Acc = 0.6566,	 Loss Con1 = 1.2799,	 Loss Con2 = 2.2243
3163 0.154
3141 0.789
2596 0.867
1323 0.871
891 0.847
763 0.856
71 0.901
20 0.85
0.8370244179443498
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1147,	 Acc = 0.6467,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.3536
745 0.215
668 0.669
714 0.868
562 0.833
190 0.742
103 0.835
11 1.0
7 1.0
0.7893569844789357
0.8093126385809313

 ===== Epoch 220	 =====
[-0.4557189  -0.49292567  1.8836575   2.0899127   0.7975822   0.69380283
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.907358    2.0801864   0.8244522   0.7045444
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 2 2
train:	 Loss = 1.0376,	 Acc = 0.6556,	 Loss Con1 = 1.2796,	 Loss Con2 = 2.1786
3164 0.154
3138 0.787
2599 0.866
1324 0.869
889 0.844
763 0.862
71 0.887
20 0.9
0.8359836437982735
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1006,	 Acc = 0.6500,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.3373
745 0.215
668 0.659
714 0.874
562 0.836
190 0.789
103 0.854
11 1.0
7 1.0
0.7937915742793792
0.8093126385809313

 ===== Epoch 221	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.31006584  0.09005687
 -0.43886673 -0.47319013  1.8768005   0.2250128  -0.4178274  -0.43225467
  3.0324218   0.30890787] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.30525044 -0.0451986
 -0.4389609  -0.47327355  1.8829585   0.05833119 -0.4178274  -0.43225467
  3.037464    0.13083988] 5 5
train:	 Loss = 1.0390,	 Acc = 0.6564,	 Loss Con1 = 1.2789,	 Loss Con2 = 2.2147
3164 0.153
3144 0.791
2594 0.869
1324 0.863
891 0.843
760 0.864
71 0.873
20 0.85
0.8372330758746025
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1316,	 Acc = 0.6393,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.8973
745 0.213
668 0.626
714 0.868
562 0.82
190 0.816
103 0.845
11 1.0
7 1.0
0.7800443458980044
0.8093126385809313

 ===== Epoch 222	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
  1.8973916   2.0232577 ] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
  1.8949237   1.9933755 ] 3 3
train:	 Loss = 1.0391,	 Acc = 0.6568,	 Loss Con1 = 1.2776,	 Loss Con2 = 2.3230
3163 0.153
3146 0.791
2594 0.868
1326 0.867
891 0.847
759 0.859
71 0.901
18 0.889
0.8378194207836457
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1126,	 Acc = 0.6480,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.4979
745 0.215
668 0.656
714 0.874
562 0.826
190 0.795
103 0.864
11 1.0
7 1.0
0.7911308203991131
0.8093126385809313

 ===== Epoch 223	 =====
[-0.4557189  -0.49292567  1.5268657   0.83996195  2.5358706  -0.22188745
  0.72331464  0.27672756 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.4978306   0.83592856  2.5154479  -0.23423998
  0.6916683   0.23385617 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 1 1
train:	 Loss = 1.0385,	 Acc = 0.6569,	 Loss Con1 = 1.2794,	 Loss Con2 = 2.3106
3162 0.152
3144 0.792
2596 0.869
1322 0.873
891 0.84
762 0.858
71 0.873
20 0.85
0.8382920735861913
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1144,	 Acc = 0.6440,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.3593
745 0.215
668 0.66
714 0.87
562 0.82
190 0.747
103 0.864
11 1.0
7 1.0
0.7858093126385809
0.8093126385809313

 ===== Epoch 224	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 4 0
train:	 Loss = 1.0379,	 Acc = 0.6580,	 Loss Con1 = 1.2801,	 Loss Con2 = 2.1759
3162 0.155
3143 0.793
2595 0.867
1322 0.871
892 0.844
763 0.86
71 0.887
20 0.85
0.8386327503974563
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1402,	 Acc = 0.6543,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.7707
745 0.216
668 0.671
714 0.881
562 0.826
190 0.811
103 0.864
11 1.0
7 1.0
0.7991130820399113
0.8093126385809313

 ===== Epoch 225	 =====
[ 1.6540809   0.48851535 -0.5993768  -0.57551634  2.6004136   1.9802825
  0.7953653   2.3682036   3.1277044   1.5251901  -0.4178274  -0.43225467
  2.5815723   0.5068491 ] [ 1.6752007   0.4536051  -0.59937984 -0.57549053  2.590415    1.9649063
  0.7813964   2.308875    3.130163    1.5026486  -0.4178274  -0.43225467
  2.5990043   0.48762375] 3 3
train:	 Loss = 1.0397,	 Acc = 0.6572,	 Loss Con1 = 1.2794,	 Loss Con2 = 2.2941
3160 0.152
3143 0.791
2598 0.868
1324 0.872
890 0.842
762 0.865
71 0.887
20 0.9
0.8384423251589465
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1298,	 Acc = 0.6447,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.3800
745 0.213
668 0.66
714 0.87
562 0.826
190 0.753
103 0.854
11 1.0
7 1.0
0.7871396895787139
0.8093126385809313

 ===== Epoch 226	 =====
[-0.4557189  -0.49292567  1.7546089   1.7054296   0.5389657   0.15197594
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.7810363   1.6991999   0.56259364  0.17652464
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 2 2
train:	 Loss = 1.0406,	 Acc = 0.6579,	 Loss Con1 = 1.2790,	 Loss Con2 = 2.1866
3165 0.154
3144 0.793
2595 0.869
1322 0.871
891 0.845
760 0.862
71 0.901
20 0.8
0.8390321481313189
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1000,	 Acc = 0.6450,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.6016
745 0.212
668 0.668
714 0.88
562 0.817
190 0.737
103 0.845
11 0.909
7 1.0
0.7880266075388027
0.8093126385809313

 ===== Epoch 227	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 3 0
train:	 Loss = 1.0409,	 Acc = 0.6568,	 Loss Con1 = 1.2791,	 Loss Con2 = 2.2607
3162 0.154
3145 0.791
2595 0.864
1325 0.873
888 0.846
762 0.86
71 0.901
20 0.9
0.8374971610265728
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1162,	 Acc = 0.6423,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.3920
745 0.211
668 0.642
714 0.87
562 0.838
190 0.768
103 0.835
11 0.909
7 1.0
0.7849223946784922
0.8093126385809313

 ===== Epoch 228	 =====
[ 0.5659154   0.5882411  -0.5993768  -0.57551634  1.801656    2.3906002
 -0.43886673 -0.47319013  1.278263    1.9565475  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [ 0.5659154   0.5882411  -0.59937984 -0.57549053  1.8018076   2.3906202
 -0.4389609  -0.47327355  1.278263    1.9565475  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 0
train:	 Loss = 1.0324,	 Acc = 0.6588,	 Loss Con1 = 1.2804,	 Loss Con2 = 2.2530
3160 0.153
3143 0.796
2597 0.87
1322 0.869
892 0.843
763 0.862
71 0.887
20 0.9
0.840258855585831
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1173,	 Acc = 0.6477,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.5029
745 0.212
668 0.683
714 0.881
562 0.808
190 0.747
103 0.835
11 1.0
7 1.0
0.7915742793791575
0.8093126385809313

 ===== Epoch 229	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634  0.59755164  0.4774897
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053  0.5976764   0.6686737
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 6 6
train:	 Loss = 1.0374,	 Acc = 0.6558,	 Loss Con1 = 1.2794,	 Loss Con2 = 2.2606
3166 0.155
3143 0.787
2597 0.87
1319 0.868
891 0.843
761 0.853
71 0.887
20 0.85
0.8360599863667348
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1076,	 Acc = 0.6507,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.5412
745 0.215
668 0.68
714 0.874
562 0.829
190 0.747
103 0.864
11 1.0
7 0.857
0.7946784922394678
0.8093126385809313

 ===== Epoch 230	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 5 0
train:	 Loss = 1.0399,	 Acc = 0.6555,	 Loss Con1 = 1.2798,	 Loss Con2 = 2.2871
3163 0.153
3140 0.789
2599 0.865
1322 0.868
892 0.848
761 0.857
71 0.887
20 0.9
0.8361158432708689
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1126,	 Acc = 0.6537,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.4081
745 0.215
668 0.675
714 0.868
562 0.833
190 0.821
103 0.854
11 1.0
7 1.0
0.798669623059867
0.8093126385809313

 ===== Epoch 231	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 5 0
train:	 Loss = 1.0407,	 Acc = 0.6559,	 Loss Con1 = 1.2785,	 Loss Con2 = 2.6431
3161 0.151
3144 0.794
2599 0.867
1322 0.868
892 0.835
759 0.858
71 0.873
20 0.9
0.8370614284092199
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1114,	 Acc = 0.6473,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.9581
745 0.213
668 0.666
714 0.871
562 0.822
190 0.774
103 0.864
11 1.0
7 1.0
0.7906873614190687
0.8093126385809313

 ===== Epoch 232	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 6 0
train:	 Loss = 1.0380,	 Acc = 0.6564,	 Loss Con1 = 1.2796,	 Loss Con2 = 2.5187
3162 0.155
3141 0.79
2598 0.865
1323 0.868
892 0.846
761 0.858
71 0.887
20 0.9
0.8363615716556893
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1100,	 Acc = 0.6420,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.6503
745 0.215
668 0.654
714 0.871
562 0.822
190 0.732
103 0.864
11 1.0
7 0.857
0.7831485587583149
0.8093126385809313

 ===== Epoch 233	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 5 0
train:	 Loss = 1.0379,	 Acc = 0.6562,	 Loss Con1 = 1.2786,	 Loss Con2 = 2.2220
3163 0.153
3142 0.789
2597 0.866
1323 0.872
890 0.838
762 0.866
71 0.887
20 0.9
0.8367972742759796
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.0971,	 Acc = 0.6473,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.5247
745 0.212
668 0.671
714 0.874
562 0.819
190 0.779
103 0.845
11 0.909
7 1.0
0.7911308203991131
0.8093126385809313

 ===== Epoch 234	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013  0.13213459  0.01066751 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355  0.10633544  0.11276814 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 4 4
train:	 Loss = 1.0365,	 Acc = 0.6559,	 Loss Con1 = 1.2794,	 Loss Con2 = 2.3008
3162 0.154
3145 0.79
2597 0.867
1322 0.862
890 0.848
761 0.854
71 0.887
20 0.9
0.8360208948444242
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.0856,	 Acc = 0.6450,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.4589
745 0.216
668 0.648
714 0.866
562 0.831
190 0.789
103 0.864
11 1.0
7 0.857
0.7866962305986697
0.8093126385809313

 ===== Epoch 235	 =====
[-0.4557189  -0.49292567  1.0390961   2.3637388   2.2622752   1.4057771
  0.40740424  0.9728867   2.9065864   0.7410962  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0574893   2.3862176   2.264691    1.4241799
  0.3970323   1.0620891   2.8910196   0.7631165  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 4 4
train:	 Loss = 1.0414,	 Acc = 0.6544,	 Loss Con1 = 1.2788,	 Loss Con2 = 2.1341
3165 0.153
3143 0.789
2595 0.864
1320 0.867
893 0.838
761 0.858
71 0.901
20 0.85
0.8348290355560605
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.0919,	 Acc = 0.6447,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.2850
745 0.213
668 0.665
714 0.874
562 0.817
190 0.742
103 0.864
11 1.0
7 1.0
0.7871396895787139
0.8093126385809313

 ===== Epoch 236	 =====
[-0.4557189  -0.49292567  0.25892726 -0.19376156 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 3 5
train:	 Loss = 1.0421,	 Acc = 0.6550,	 Loss Con1 = 1.2796,	 Loss Con2 = 2.2875
3158 0.152
3145 0.786
2598 0.864
1325 0.874
891 0.842
760 0.863
71 0.859
20 0.85
0.8354143019296254
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.0947,	 Acc = 0.6463,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.2385
745 0.215
668 0.68
714 0.874
562 0.813
190 0.732
103 0.845
11 1.0
7 1.0
0.7889135254988914
0.8093126385809313

 ===== Epoch 237	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711   0.41486636  0.22330706
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 5 5
train:	 Loss = 1.0370,	 Acc = 0.6578,	 Loss Con1 = 1.2790,	 Loss Con2 = 2.1058
3165 0.151
3141 0.793
2596 0.869
1324 0.867
891 0.852
761 0.867
70 0.886
20 0.9
0.839940929228672
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1253,	 Acc = 0.6400,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.4761
745 0.213
668 0.665
714 0.873
562 0.802
190 0.732
103 0.835
11 1.0
7 1.0
0.7809312638580931
0.8093126385809313

 ===== Epoch 238	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 4 0
train:	 Loss = 1.0367,	 Acc = 0.6576,	 Loss Con1 = 1.2786,	 Loss Con2 = 2.1766
3164 0.152
3146 0.792
2599 0.87
1316 0.872
892 0.841
760 0.866
71 0.901
20 0.85
0.8392776010904135
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1164,	 Acc = 0.6490,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.3211
745 0.217
668 0.656
714 0.87
562 0.833
190 0.795
103 0.864
11 1.0
7 1.0
0.7915742793791575
0.8093126385809313

 ===== Epoch 239	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 4 0
train:	 Loss = 1.0352,	 Acc = 0.6562,	 Loss Con1 = 1.2792,	 Loss Con2 = 2.1930
3160 0.153
3144 0.783
2599 0.871
1322 0.872
891 0.845
761 0.865
71 0.887
20 0.9
0.8368528610354223
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1180,	 Acc = 0.6443,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.4226
745 0.217
668 0.644
714 0.873
562 0.831
190 0.774
103 0.835
11 1.0
7 1.0
0.7853658536585366
0.8093126385809313

 ===== Epoch 240	 =====
[ 0.6303424   1.4833022  -0.5993768  -0.57551634  1.4813584   2.0372813
 -0.43886673 -0.47319013  1.7054945   0.97982633 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [ 0.6303424   1.4833022  -0.59937984 -0.57549053  1.4815028   2.0373058
 -0.4389609  -0.47327355  1.7054945   0.97982633 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 0
train:	 Loss = 1.0405,	 Acc = 0.6569,	 Loss Con1 = 1.2795,	 Loss Con2 = 2.2311
3166 0.153
3139 0.791
2598 0.865
1323 0.871
890 0.849
761 0.867
71 0.887
20 0.9
0.8383321972279028
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1352,	 Acc = 0.6410,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.2661
745 0.209
668 0.666
714 0.868
562 0.817
190 0.737
103 0.825
11 1.0
7 1.0
0.7835920177383592
0.8093126385809313

 ===== Epoch 241	 =====
[ 2.0716534  -0.04708537 -0.5993768  -0.57551634 -0.6348712  -0.6508424
  2.4292872   0.06596041  0.6963182   2.3803887   2.8378763   1.1324972
  2.0597315   2.7080848 ] [ 2.0894675   0.02003663 -0.59937984 -0.57549053 -0.6347739  -0.6507814
  2.4444222   0.13583572  0.6888788   2.4259844   2.843205    1.2087926
  2.0526679   2.7827103 ] 6 6
train:	 Loss = 1.0378,	 Acc = 0.6562,	 Loss Con1 = 1.2786,	 Loss Con2 = 2.1233
3162 0.151
3144 0.79
2598 0.869
1324 0.866
891 0.847
758 0.86
71 0.887
20 0.85
0.8374971610265728
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1200,	 Acc = 0.6400,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.3761
745 0.216
668 0.632
714 0.866
562 0.835
190 0.768
103 0.845
11 0.909
7 1.0
0.7800443458980044
0.8093126385809313

 ===== Epoch 242	 =====
[-0.4557189  -0.49292567  0.7552021   2.1468024   1.208999    0.0647592
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  0.76448905  2.1649086   1.1993719   0.07897759
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 4 4
train:	 Loss = 1.0333,	 Acc = 0.6585,	 Loss Con1 = 1.2786,	 Loss Con2 = 2.7327
3163 0.153
3144 0.795
2597 0.866
1322 0.872
891 0.848
761 0.866
70 0.886
20 0.9
0.8400908574673481
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.0913,	 Acc = 0.6467,	 Loss Con1 = 1.3409,	 Loss Con2 = 3.0154
745 0.219
668 0.659
714 0.875
562 0.829
190 0.742
103 0.845
11 1.0
7 1.0
0.7880266075388027
0.8093126385809313

 ===== Epoch 243	 =====
[ 0.5604225   2.7613108  -0.5993768  -0.57551634 -0.6348712  -0.6508424
  1.9655486   0.8891734  -0.46656546 -0.4823711   2.4841106   2.1605325
 -0.45947683 -0.42555502] [ 0.5356184   2.6891708  -0.59937984 -0.57549053 -0.6347739  -0.6507814
  1.9774172   0.8573629  -0.46656546 -0.4823711   2.4778423   2.1369157
 -0.45947683 -0.42555502] 3 3
train:	 Loss = 1.0359,	 Acc = 0.6572,	 Loss Con1 = 1.2795,	 Loss Con2 = 2.5813
3164 0.155
3142 0.788
2593 0.869
1325 0.875
891 0.842
762 0.864
71 0.873
20 0.9
0.8378009995456611
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1163,	 Acc = 0.6430,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.6148
745 0.217
668 0.668
714 0.874
562 0.815
190 0.711
103 0.835
11 1.0
7 1.0
0.7835920177383592
0.8093126385809313

 ===== Epoch 244	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 2 0
train:	 Loss = 1.0334,	 Acc = 0.6591,	 Loss Con1 = 1.2798,	 Loss Con2 = 2.3780
3160 0.151
3143 0.795
2597 0.87
1324 0.871
892 0.846
761 0.87
71 0.901
20 0.9
0.8412806539509536
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1262,	 Acc = 0.6427,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.5064
745 0.217
668 0.654
714 0.87
562 0.815
190 0.758
103 0.864
11 1.0
7 0.857
0.7831485587583149
0.8093126385809313

 ===== Epoch 245	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
  1.246746    1.4553007 ] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
  1.2776117   1.4620266 ] 2 2
train:	 Loss = 1.0362,	 Acc = 0.6575,	 Loss Con1 = 1.2800,	 Loss Con2 = 2.2993
3158 0.153
3146 0.792
2598 0.868
1323 0.869
890 0.84
762 0.867
71 0.873
20 0.9
0.8384790011350738
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1355,	 Acc = 0.6450,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.3633
745 0.217
668 0.645
714 0.87
562 0.835
190 0.763
103 0.864
11 1.0
7 1.0
0.7862527716186253
0.8093126385809313

 ===== Epoch 246	 =====
[-0.4557189  -0.49292567  2.125916    1.7505224   1.3323059   0.48985
 -0.43886673 -0.47319013  0.3100563   0.7745111  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  2.1164358   1.7335362   1.3446922   0.46463892
 -0.4389609  -0.47327355  0.3275685   0.6790085  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 3 3
train:	 Loss = 1.0374,	 Acc = 0.6568,	 Loss Con1 = 1.2795,	 Loss Con2 = 2.2827
3161 0.152
3147 0.792
2595 0.866
1322 0.868
889 0.846
763 0.864
71 0.887
20 0.9
0.8378562507096627
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.0968,	 Acc = 0.6503,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.7279
745 0.212
668 0.683
714 0.875
562 0.827
190 0.742
103 0.864
11 1.0
7 0.857
0.7951219512195122
0.8093126385809313

 ===== Epoch 247	 =====
[ 0.630342    1.4833022  -0.5993768  -0.57551634  1.4813582   2.0372813
 -0.43886673 -0.47319013  1.7054943   0.9798262  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [ 0.6303425   1.4833024  -0.59937984 -0.57549053  1.4815028   2.0373058
 -0.4389609  -0.47327355  1.7054948   0.97982633 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 0
train:	 Loss = 1.0348,	 Acc = 0.6579,	 Loss Con1 = 1.2786,	 Loss Con2 = 2.4608
3165 0.153
3148 0.793
2594 0.867
1322 0.874
888 0.848
760 0.862
71 0.887
20 0.9
0.8393729410428263
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1210,	 Acc = 0.6393,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.9122
745 0.209
668 0.654
714 0.88
562 0.804
190 0.737
103 0.845
11 1.0
7 1.0
0.7813747228381375
0.8093126385809313

 ===== Epoch 248	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 4 0
train:	 Loss = 1.0414,	 Acc = 0.6575,	 Loss Con1 = 1.2782,	 Loss Con2 = 2.3217
3165 0.153
3140 0.79
2599 0.869
1323 0.873
892 0.848
761 0.862
69 0.913
19 0.842
0.8389185504941498
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1018,	 Acc = 0.6460,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.6283
745 0.213
668 0.665
714 0.875
562 0.826
190 0.742
103 0.845
11 1.0
7 1.0
0.7889135254988914
0.8093126385809313

 ===== Epoch 249	 =====
[ 1.427979    2.6441948  -0.5993768  -0.57551634 -0.6348712  -0.6508424
  1.9894869   0.26298845 -0.46656546 -0.4823711   2.3759134   1.5925703
  1.6419749   3.493786  ] [ 1.427979    2.6849232  -0.59937984 -0.57549053 -0.6347739  -0.6507814
  1.9892702   0.30387875 -0.46656546 -0.4823711   2.3759134   1.6366905
  1.6419749   3.5389013 ] 5 4
train:	 Loss = 1.0357,	 Acc = 0.6560,	 Loss Con1 = 1.2790,	 Loss Con2 = 2.3455
3166 0.154
3140 0.784
2599 0.866
1321 0.874
891 0.846
760 0.866
71 0.915
20 0.9
0.8366280390820268
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1104,	 Acc = 0.6500,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.4557
745 0.211
668 0.65
714 0.871
562 0.835
190 0.858
103 0.845
11 1.0
7 1.0
0.7951219512195122
0.8093126385809313

 ===== Epoch 250	 =====
[ 1.7421672   0.3530386  -0.5993768  -0.57551634  2.5610144   1.9181015
  0.7464819   2.123902    3.1399384   1.4352436  -0.4178274  -0.43225467
  2.6526027   0.43125978] [ 1.7561798   0.3336698  -0.59937984 -0.57549053  2.5555685   1.9086708
  0.7405134   2.0855067   3.1423233   1.4218103  -0.4178274  -0.43225467
  2.6635506   0.4202211 ] 4 3
train:	 Loss = 1.0366,	 Acc = 0.6558,	 Loss Con1 = 1.2790,	 Loss Con2 = 2.2882
3164 0.153
3143 0.79
2596 0.864
1323 0.872
892 0.846
759 0.856
71 0.873
20 0.9
0.8365515674693321
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1026,	 Acc = 0.6490,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.4387
745 0.213
668 0.681
714 0.873
562 0.822
190 0.742
103 0.864
11 1.0
7 1.0
0.7929046563192904
0.8093126385809313

 ===== Epoch 251	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 5 0
train:	 Loss = 1.0348,	 Acc = 0.6586,	 Loss Con1 = 1.2795,	 Loss Con2 = 2.1833
3162 0.155
3142 0.796
2596 0.866
1323 0.862
892 0.846
762 0.873
71 0.887
20 0.9
0.8394276629570747
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1166,	 Acc = 0.6450,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.5758
745 0.216
668 0.642
714 0.87
562 0.833
190 0.789
103 0.854
11 1.0
7 1.0
0.7866962305986697
0.8093126385809313

 ===== Epoch 252	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
  0.4736643   0.25892434 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
  0.49818194  0.25715953 -0.46656546 -0.4823711   1.040291    3.4795866
 -0.45947683 -0.42555502] 2 3
train:	 Loss = 1.0378,	 Acc = 0.6578,	 Loss Con1 = 1.2792,	 Loss Con2 = 2.3682
3164 0.152
3143 0.792
2594 0.869
1325 0.872
891 0.842
760 0.868
71 0.901
20 0.85
0.8395047705588369
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1034,	 Acc = 0.6477,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.3633
745 0.215
668 0.671
714 0.871
562 0.82
190 0.779
103 0.835
11 1.0
7 1.0
0.7906873614190687
0.8093126385809313

 ===== Epoch 253	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 4 0
train:	 Loss = 1.0333,	 Acc = 0.6575,	 Loss Con1 = 1.2775,	 Loss Con2 = 2.2094
3165 0.155
3144 0.793
2600 0.867
1322 0.868
887 0.843
759 0.867
71 0.873
20 0.9
0.838350562308304
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1026,	 Acc = 0.6483,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.3254
745 0.215
668 0.666
714 0.874
562 0.82
190 0.789
103 0.845
11 1.0
7 1.0
0.7915742793791575
0.8093126385809313

 ===== Epoch 254	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013  1.6210828   0.1133377  -0.4178274  -0.43225467
  2.819583    0.28934786] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355  1.6497983   0.13866669 -0.4178274  -0.43225467
  2.8439028   0.30283388] 2 2
train:	 Loss = 1.0335,	 Acc = 0.6579,	 Loss Con1 = 1.2776,	 Loss Con2 = 2.2385
3164 0.153
3145 0.79
2598 0.868
1322 0.869
890 0.849
759 0.868
70 0.929
20 0.9
0.8392776010904135
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1077,	 Acc = 0.6490,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.3596
745 0.213
668 0.654
714 0.875
562 0.836
190 0.789
103 0.854
11 1.0
7 1.0
0.7929046563192904
0.8093126385809313

 ===== Epoch 255	 =====
[ 0.8332992   3.0383945  -0.5993768  -0.57551634  0.70414466  2.5271308
 -0.43886673 -0.47319013  0.7742572   0.30650762 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [ 0.86181515  3.0867796  -0.59937984 -0.57549053  0.7216325   2.5582757
 -0.4389609  -0.47327355  0.75215787  0.3604011  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 4 4
train:	 Loss = 1.0354,	 Acc = 0.6574,	 Loss Con1 = 1.2794,	 Loss Con2 = 2.4303
3161 0.153
3140 0.798
2599 0.861
1325 0.87
891 0.842
761 0.863
71 0.887
20 0.85
0.8385375269671852
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1234,	 Acc = 0.6477,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.6872
745 0.217
668 0.665
714 0.871
562 0.824
190 0.763
103 0.864
11 1.0
7 1.0
0.78980044345898
0.8093126385809313

 ===== Epoch 256	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 5 0
train:	 Loss = 1.0335,	 Acc = 0.6570,	 Loss Con1 = 1.2792,	 Loss Con2 = 2.2669
3159 0.152
3144 0.791
2601 0.867
1323 0.868
890 0.847
760 0.862
71 0.915
20 0.9
0.8382336247020094
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1060,	 Acc = 0.6420,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.5112
745 0.217
668 0.654
714 0.868
562 0.817
190 0.753
103 0.854
11 1.0
7 0.857
0.7822616407982261
0.8093126385809313

 ===== Epoch 257	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634  0.99908185  2.0997522
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053  0.9739717   2.1138937
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 1 1
train:	 Loss = 1.0351,	 Acc = 0.6557,	 Loss Con1 = 1.2791,	 Loss Con2 = 2.3444
3160 0.153
3146 0.789
2598 0.864
1320 0.873
892 0.839
762 0.858
70 0.914
20 0.9
0.8360581289736603
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1000,	 Acc = 0.6453,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.3264
745 0.217
668 0.657
714 0.873
562 0.82
190 0.763
103 0.864
11 1.0
7 0.857
0.7866962305986697
0.8093126385809313

 ===== Epoch 258	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634  0.6256176   0.2652499
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053  0.6016467   0.2441528
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 1 1
train:	 Loss = 1.0387,	 Acc = 0.6568,	 Loss Con1 = 1.2790,	 Loss Con2 = 2.3245
3164 0.154
3143 0.793
2597 0.864
1323 0.868
890 0.838
761 0.866
70 0.929
20 0.9
0.8374602453430259
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1382,	 Acc = 0.6393,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.5755
745 0.211
668 0.641
714 0.864
562 0.822
190 0.784
103 0.854
11 1.0
7 0.857
0.7809312638580931
0.8093126385809313

 ===== Epoch 259	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634  1.1717829   2.0338497
 -0.43886673 -0.47319013  0.380385    0.99163365 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053  1.1976352   2.022398
 -0.4389609  -0.47327355  0.41669378  1.0196617  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 2 2
train:	 Loss = 1.0382,	 Acc = 0.6571,	 Loss Con1 = 1.2800,	 Loss Con2 = 2.2842
3163 0.153
3144 0.786
2594 0.869
1323 0.875
892 0.844
761 0.871
71 0.887
20 0.9
0.8380465644520159
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1283,	 Acc = 0.6397,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.4331
745 0.216
668 0.656
714 0.87
562 0.822
190 0.711
103 0.816
11 1.0
7 1.0
0.77960088691796
0.8093126385809313

 ===== Epoch 260	 =====
[-0.4557189  -0.49292567  1.06432     0.5827789  -0.6348712  -0.6508424
  2.7367945   2.2051551  -0.46656546 -0.4823711   3.401588    3.0774803
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0641382   0.5486768  -0.6347739  -0.6507814
  2.73654     2.1640255  -0.46656546 -0.4823711   3.401588    3.0333602
 -0.45947683 -0.42555502] 0 3
train:	 Loss = 1.0379,	 Acc = 0.6548,	 Loss Con1 = 1.2797,	 Loss Con2 = 2.3437
3165 0.151
3142 0.79
2593 0.864
1323 0.866
891 0.846
763 0.861
71 0.873
20 0.85
0.8358514142905827
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1026,	 Acc = 0.6480,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.2470
745 0.209
668 0.672
714 0.861
562 0.826
190 0.826
103 0.845
11 1.0
7 0.714
0.7929046563192904
0.8093126385809313

 ===== Epoch 261	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.13225368  0.7080461
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.13938323  0.8116536
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 4 4
train:	 Loss = 1.0336,	 Acc = 0.6570,	 Loss Con1 = 1.2789,	 Loss Con2 = 2.2921
3164 0.152
3145 0.788
2596 0.869
1322 0.878
891 0.845
759 0.859
71 0.887
20 0.9
0.8384825079509314
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.0996,	 Acc = 0.6470,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.4807
745 0.213
668 0.681
714 0.873
562 0.819
190 0.732
103 0.845
11 1.0
7 1.0
0.7902439024390244
0.8093126385809313

 ===== Epoch 262	 =====
[-0.4557189  -0.49292567  1.0992994   2.1567533   1.0386755   0.14727843
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.1118084   2.1758945   1.0266212   0.16881439
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 4 4
train:	 Loss = 1.0370,	 Acc = 0.6578,	 Loss Con1 = 1.2788,	 Loss Con2 = 2.2742
3163 0.15
3145 0.796
2597 0.87
1321 0.866
893 0.844
758 0.864
71 0.915
20 0.9
0.8402044293015333
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1920,	 Acc = 0.6290,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.3553
745 0.213
668 0.644
714 0.854
562 0.811
190 0.668
103 0.845
11 1.0
7 1.0
0.7662971175166298
0.8093126385809313

 ===== Epoch 263	 =====
[ 0.2526381   0.67099464 -0.5993768  -0.57551634  1.2524389   1.410313
 -0.43886673 -0.47319013  1.3838277   0.04417158 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [ 0.2526381   0.874636   -0.59937984 -0.57549053  1.2525784   1.6014843
 -0.4389609  -0.47327355  1.3838277   0.257606   -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 6 6
train:	 Loss = 1.0383,	 Acc = 0.6573,	 Loss Con1 = 1.2793,	 Loss Con2 = 2.1435
3162 0.154
3146 0.793
2597 0.869
1319 0.869
892 0.843
762 0.858
70 0.871
20 0.9
0.8381785146491029
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.0979,	 Acc = 0.6453,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.3504
745 0.217
668 0.668
714 0.871
562 0.815
190 0.763
103 0.835
11 1.0
7 0.857
0.7866962305986697
0.8093126385809313

 ===== Epoch 264	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.20274188  2.005744
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.22833693  2.0558224
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 1 1
train:	 Loss = 1.0339,	 Acc = 0.6559,	 Loss Con1 = 1.2786,	 Loss Con2 = 2.3143
3167 0.152
3145 0.796
2598 0.866
1317 0.866
890 0.834
760 0.863
71 0.859
20 0.8
0.8371775934552892
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.0974,	 Acc = 0.6453,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.6624
745 0.209
668 0.668
714 0.867
562 0.822
190 0.774
103 0.864
11 1.0
7 0.857
0.7893569844789357
0.8093126385809313

 ===== Epoch 265	 =====
[-0.4557189  -0.49292567  2.4247692   0.278634   -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  2.4380803   0.26484984 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 3 3
train:	 Loss = 1.0396,	 Acc = 0.6552,	 Loss Con1 = 1.2797,	 Loss Con2 = 2.6820
3157 0.151
3147 0.792
2598 0.866
1321 0.863
892 0.841
762 0.856
71 0.887
20 0.9
0.8357734649869482
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1030,	 Acc = 0.6433,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.7877
745 0.211
668 0.672
714 0.87
562 0.819
190 0.726
103 0.845
11 1.0
7 1.0
0.7862527716186253
0.8093126385809313

 ===== Epoch 266	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013  0.6764103   0.24477999 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355  0.70694697  0.29015326 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 2 2
train:	 Loss = 1.0299,	 Acc = 0.6568,	 Loss Con1 = 1.2793,	 Loss Con2 = 2.4761
3160 0.153
3144 0.79
2598 0.87
1323 0.867
891 0.844
762 0.857
70 0.9
20 0.85
0.8373069936421436
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1006,	 Acc = 0.6450,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.3520
745 0.217
668 0.66
714 0.871
562 0.815
190 0.768
103 0.854
11 1.0
7 1.0
0.7862527716186253
0.8093126385809313

 ===== Epoch 267	 =====
[ 0.02654133  1.9001952  -0.5993768  -0.57551634 -0.6348712  -0.6508424
  1.8092      0.13184996 -0.46656546 -0.4823711   2.1667087   1.5952523
 -0.45947683 -0.42555502] [-0.01477261  1.9341217  -0.59937984 -0.57549053 -0.6347739  -0.6507814
  1.780181    0.10900239 -0.46656546 -0.4823711   2.136105    1.5931077
 -0.45947683 -0.42555502] 1 1
train:	 Loss = 1.0328,	 Acc = 0.6583,	 Loss Con1 = 1.2792,	 Loss Con2 = 2.3633
3164 0.153
3144 0.798
2595 0.871
1322 0.866
891 0.84
762 0.858
70 0.9
20 0.9
0.8399591094956838
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1093,	 Acc = 0.6387,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.5009
745 0.213
668 0.617
714 0.874
562 0.835
190 0.779
103 0.845
11 1.0
7 0.857
0.7791574279379158
0.8093126385809313

 ===== Epoch 268	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 0
train:	 Loss = 1.0346,	 Acc = 0.6562,	 Loss Con1 = 1.2786,	 Loss Con2 = 2.3330
3167 0.152
3140 0.79
2598 0.868
1322 0.867
891 0.848
759 0.86
71 0.887
20 0.9
0.8377457107146915
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.0936,	 Acc = 0.6460,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.5212
745 0.217
668 0.665
714 0.874
562 0.82
190 0.753
103 0.835
11 1.0
7 1.0
0.7875831485587583
0.8093126385809313

 ===== Epoch 269	 =====
[ 0.9605463   0.9763703  -0.5993768  -0.57551634  0.7509166   0.38921088
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [ 0.9605333   0.9763649  -0.59937984 -0.57549053  0.75103676  0.3892523
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 0
train:	 Loss = 1.0333,	 Acc = 0.6563,	 Loss Con1 = 1.2790,	 Loss Con2 = 2.4712
3161 0.154
3144 0.792
2596 0.866
1326 0.863
890 0.843
760 0.859
71 0.901
20 0.9
0.8367207902804588
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.0920,	 Acc = 0.6493,	 Loss Con1 = 1.3409,	 Loss Con2 = 3.2028
745 0.216
668 0.662
714 0.867
562 0.84
190 0.774
103 0.864
11 1.0
7 1.0
0.7924611973392461
0.8093126385809313

 ===== Epoch 270	 =====
[-0.4557189  -0.49292567  0.87973565  2.430605    1.8382989   1.0784515
 -0.23810811  1.0157548   2.2515042   0.16782351 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  0.87042385  2.453969    1.8177611   1.0831345
 -0.26733124  1.0515547   2.2236805   0.160506   -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 4 1
train:	 Loss = 1.0406,	 Acc = 0.6548,	 Loss Con1 = 1.2784,	 Loss Con2 = 2.8914
3166 0.152
3143 0.788
2596 0.866
1322 0.865
891 0.844
760 0.863
70 0.871
20 0.85
0.8356055441945013
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1235,	 Acc = 0.6423,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.6230
745 0.215
668 0.647
714 0.868
562 0.819
190 0.789
103 0.845
11 1.0
7 1.0
0.7835920177383592
0.8093126385809313

 ===== Epoch 271	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 4 0
train:	 Loss = 1.0355,	 Acc = 0.6577,	 Loss Con1 = 1.2795,	 Loss Con2 = 2.6258
3161 0.154
3143 0.79
2599 0.871
1322 0.868
890 0.842
762 0.862
71 0.915
20 0.85
0.8383104348813444
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.0905,	 Acc = 0.6433,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.7900
745 0.216
668 0.663
714 0.866
562 0.808
190 0.784
103 0.854
11 1.0
7 0.857
0.7844789356984478
0.8093126385809313

 ===== Epoch 272	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634  1.1137822   2.1502864
 -0.43886673 -0.47319013  0.5688615   0.75549656 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053  1.0889992   2.1642215
 -0.4389609  -0.47327355  0.53424907  0.7223729  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 1 1
train:	 Loss = 1.0402,	 Acc = 0.6567,	 Loss Con1 = 1.2798,	 Loss Con2 = 2.6024
3162 0.151
3141 0.791
2600 0.868
1319 0.87
892 0.851
763 0.856
71 0.915
20 0.9
0.8381785146491029
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1063,	 Acc = 0.6447,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.6687
745 0.215
668 0.65
714 0.859
562 0.827
190 0.816
103 0.874
11 1.0
7 0.857
0.7866962305986697
0.8093126385809313

 ===== Epoch 273	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 1 0
train:	 Loss = 1.0341,	 Acc = 0.6566,	 Loss Con1 = 1.2783,	 Loss Con2 = 2.5969
3164 0.154
3146 0.79
2597 0.868
1319 0.867
891 0.843
760 0.867
71 0.887
20 0.9
0.8373466606088141
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1119,	 Acc = 0.6430,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.6163
745 0.212
668 0.674
714 0.864
562 0.813
190 0.747
103 0.845
11 1.0
7 1.0
0.7853658536585366
0.8093126385809313

 ===== Epoch 274	 =====
[-0.4557189  -0.49292567  1.7075157   2.2395048   1.3715724   0.7897233
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.7072918   2.2399695   1.3717034   0.7898064
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 0
train:	 Loss = 1.0336,	 Acc = 0.6578,	 Loss Con1 = 1.2797,	 Loss Con2 = 2.4542
3161 0.152
3144 0.793
2596 0.869
1322 0.871
892 0.844
762 0.861
71 0.901
20 0.9
0.839332349267628
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1125,	 Acc = 0.6437,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.9996
745 0.217
668 0.672
714 0.866
562 0.804
190 0.758
103 0.864
11 1.0
7 0.857
0.7844789356984478
0.8093126385809313

 ===== Epoch 275	 =====
[4.033204   2.4626887  1.8810511  0.32344726 0.29514897 1.7194526
 3.9193103  1.4719079  2.5990717  2.8353467  4.292931   2.1884756
 3.605906   3.2596762 ] [4.033204   2.5848737  1.8807815  0.42649633 0.29526702 1.8341645
 3.918996   1.5947509  2.5990717  2.9634075  4.292931   2.3208363
 3.605906   3.3950222 ] 6 6
train:	 Loss = 1.0356,	 Acc = 0.6560,	 Loss Con1 = 1.2792,	 Loss Con2 = 2.3370
3163 0.151
3144 0.791
2597 0.867
1322 0.872
889 0.841
762 0.858
71 0.873
20 0.9
0.8374787052810903
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1238,	 Acc = 0.6413,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.3143
745 0.216
668 0.663
714 0.861
562 0.804
190 0.779
103 0.845
11 1.0
7 1.0
0.7818181818181819
0.8093126385809313

 ===== Epoch 276	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.373042    2.2648065
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.37293795  2.264839
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 0
train:	 Loss = 1.0351,	 Acc = 0.6581,	 Loss Con1 = 1.2797,	 Loss Con2 = 2.1721
3158 0.152
3143 0.795
2598 0.868
1325 0.869
892 0.846
762 0.86
70 0.9
20 0.9
0.8396140749148695
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 2
val:	 Loss = 1.0967,	 Acc = 0.6453,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.2853
745 0.217
668 0.657
714 0.87
562 0.824
190 0.763
103 0.864
11 1.0
7 0.857
0.7866962305986697
0.8093126385809313

 ===== Epoch 277	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.2241779   1.4598262  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.2245976   1.7971659  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 4 4
train:	 Loss = 1.0306,	 Acc = 0.6583,	 Loss Con1 = 1.2796,	 Loss Con2 = 2.2587
3164 0.154
3147 0.795
2590 0.866
1323 0.873
891 0.841
762 0.865
71 0.901
20 0.9
0.8396183552930486
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 2
val:	 Loss = 1.1326,	 Acc = 0.6317,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.2556
745 0.212
668 0.617
714 0.846
562 0.822
190 0.8
103 0.864
11 1.0
7 1.0
0.7702882483370288
0.8093126385809313

 ===== Epoch 278	 =====
[ 1.5253706   0.22589044 -0.5993768  -0.57551634  2.4081407   1.9610115
  0.5339858   2.2856104   2.9110968   1.4312917  -0.4178274  -0.43225467
  2.4932873   0.3565264 ] [ 1.5003289   0.26082048 -0.59937984 -0.57549053  2.4184275   1.9773709
  0.54738235  2.3598616   2.9073827   1.4552364  -0.4178274  -0.43225467
  2.4743745   0.37563732] 4 4
train:	 Loss = 1.0310,	 Acc = 0.6580,	 Loss Con1 = 1.2795,	 Loss Con2 = 2.2148
3162 0.152
3143 0.794
2598 0.869
1321 0.871
891 0.846
763 0.862
71 0.887
19 0.895
0.8396547808312514
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1092,	 Acc = 0.6467,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.6753
745 0.216
668 0.677
714 0.874
562 0.815
190 0.747
103 0.835
11 1.0
7 0.857
0.7889135254988914
0.8093126385809313

 ===== Epoch 279	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 4 0
train:	 Loss = 1.0312,	 Acc = 0.6585,	 Loss Con1 = 1.2789,	 Loss Con2 = 2.2419
3160 0.153
3145 0.796
2597 0.868
1324 0.873
891 0.84
761 0.866
71 0.901
19 0.895
0.8400317892824705
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1010,	 Acc = 0.6517,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.5233
745 0.213
668 0.665
714 0.873
562 0.827
190 0.832
103 0.864
11 1.0
7 0.857
0.7964523281596453
0.8093126385809313

 ===== Epoch 280	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634  1.4853992   1.6227072
 -0.43886673 -0.47319013  0.5641083   2.570416   -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053  1.458113    1.6275448
 -0.4389609  -0.47327355  0.5296635   2.6044579  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 1 1
train:	 Loss = 1.0343,	 Acc = 0.6564,	 Loss Con1 = 1.2780,	 Loss Con2 = 2.3932
3165 0.153
3146 0.788
2595 0.865
1323 0.872
888 0.848
761 0.867
70 0.9
20 0.9
0.8374417812109508
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1068,	 Acc = 0.6430,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.4209
745 0.216
668 0.668
714 0.867
562 0.811
190 0.758
103 0.835
11 1.0
7 0.857
0.7840354767184036
0.8093126385809313

 ===== Epoch 281	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634  1.0362767   1.7616955
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053  1.04366     1.7923334
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 4 4
train:	 Loss = 1.0323,	 Acc = 0.6577,	 Loss Con1 = 1.2792,	 Loss Con2 = 2.3017
3162 0.152
3143 0.793
2597 0.869
1324 0.875
891 0.842
761 0.859
71 0.887
19 0.895
0.839200545082898
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1103,	 Acc = 0.6443,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.5307
745 0.215
668 0.647
714 0.867
562 0.824
190 0.805
103 0.864
11 1.0
7 0.857
0.7862527716186253
0.8093126385809313

 ===== Epoch 282	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
  0.8197975   0.8267567 ] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
  0.7916358   0.8024661 ] 1 1
train:	 Loss = 1.0348,	 Acc = 0.6568,	 Loss Con1 = 1.2788,	 Loss Con2 = 2.3381
3164 0.151
3143 0.788
2597 0.87
1323 0.872
890 0.847
760 0.864
71 0.901
20 0.9
0.8384825079509314
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.0912,	 Acc = 0.6443,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.4931
745 0.216
668 0.666
714 0.873
562 0.819
190 0.737
103 0.845
11 1.0
7 0.857
0.7858093126385809
0.8093126385809313

 ===== Epoch 283	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 5 0
train:	 Loss = 1.0319,	 Acc = 0.6578,	 Loss Con1 = 1.2788,	 Loss Con2 = 2.3511
3163 0.152
3143 0.794
2597 0.869
1322 0.869
891 0.843
761 0.863
71 0.887
20 0.9
0.8395229982964225
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.0968,	 Acc = 0.6440,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.3730
745 0.215
668 0.675
714 0.873
562 0.802
190 0.753
103 0.835
11 1.0
7 1.0
0.7858093126385809
0.8093126385809313

 ===== Epoch 284	 =====
[ 0.33369222  1.0172042   1.9455591   0.9917536  -0.6348712  -0.6508424
  2.0270178   2.5667756  -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [ 0.32401454  1.1235363   1.9430219   1.0109296  -0.6347739  -0.6507814
  2.0446858   2.5944102  -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 4 4
train:	 Loss = 1.0313,	 Acc = 0.6566,	 Loss Con1 = 1.2788,	 Loss Con2 = 2.2431
3164 0.152
3142 0.785
2601 0.875
1320 0.874
889 0.84
761 0.858
71 0.887
20 0.9
0.8380281690140845
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1031,	 Acc = 0.6420,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.7517
745 0.213
668 0.653
714 0.868
562 0.817
190 0.779
103 0.845
11 1.0
7 0.857
0.7835920177383592
0.8093126385809313

 ===== Epoch 285	 =====
[-0.04607511  1.964492   -0.5993768  -0.57551634 -0.6348712  -0.6508424
  1.7586766   0.09140914 -0.46656546 -0.4823711   2.1128478   1.5914431
 -0.45947683 -0.42555502] [-0.0464041   1.9648364  -0.59937984 -0.57549053 -0.6347739  -0.6507814
  1.7582439   0.09112563 -0.46656546 -0.4823711   2.1126032   1.5914255
 -0.45947683 -0.42555502] 0 0
train:	 Loss = 1.0336,	 Acc = 0.6578,	 Loss Con1 = 1.2796,	 Loss Con2 = 2.4114
3160 0.152
3148 0.791
2593 0.872
1322 0.871
892 0.844
762 0.861
71 0.887
20 0.9
0.8393505903723887
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.0995,	 Acc = 0.6517,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.5128
745 0.212
668 0.695
714 0.87
562 0.819
190 0.784
103 0.825
11 1.0
7 1.0
0.7968957871396896
0.8093126385809313

 ===== Epoch 286	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.20412502  3.4634578
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.20412502  3.2428567
 -0.45947683 -0.42555502] 5 5
train:	 Loss = 1.0356,	 Acc = 0.6573,	 Loss Con1 = 1.2796,	 Loss Con2 = 2.2425
3161 0.152
3142 0.791
2600 0.869
1320 0.873
893 0.839
761 0.866
71 0.859
20 0.9
0.8384239809242648
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 2
val:	 Loss = 1.1075,	 Acc = 0.6420,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.2775
745 0.215
668 0.647
714 0.873
562 0.826
190 0.763
103 0.816
11 1.0
7 1.0
0.7831485587583149
0.8093126385809313

 ===== Epoch 287	 =====
[ 4.625789    0.6029842  -0.5993768  -0.57551634 -0.6348712  -0.6508424
  4.5914516   0.19257875  2.0425212   0.07548586  3.8357747   1.0974908
  1.9684973   1.7291685 ] [ 4.644651    0.58796114 -0.59937984 -0.57549053 -0.6347739  -0.6507814
  4.614943    0.18044859  2.068288    0.05179161  3.8451416   1.0823383
  1.9700997   1.7001338 ] 3 3
train:	 Loss = 1.0347,	 Acc = 0.6568,	 Loss Con1 = 1.2792,	 Loss Con2 = 2.2966
3161 0.152
3146 0.79
2595 0.87
1325 0.871
888 0.845
763 0.857
70 0.871
20 0.9
0.8380833427955036
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1031,	 Acc = 0.6430,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.7729
745 0.215
668 0.668
714 0.863
562 0.827
190 0.726
103 0.845
11 1.0
7 0.857
0.7844789356984478
0.8093126385809313

 ===== Epoch 288	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 2 0
train:	 Loss = 1.0364,	 Acc = 0.6573,	 Loss Con1 = 1.2783,	 Loss Con2 = 2.4683
3165 0.151
3143 0.79
2594 0.871
1324 0.869
893 0.845
758 0.871
71 0.873
20 0.9
0.8392593434056571
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.0814,	 Acc = 0.6397,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.4851
745 0.216
668 0.639
714 0.871
562 0.826
190 0.753
103 0.816
11 1.0
7 1.0
0.77960088691796
0.8093126385809313

 ===== Epoch 289	 =====
[-0.4557189  -0.49292567  1.414878    2.2601218   1.4203051   0.6854703
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.3923916   2.2753077   1.393783    0.6777084
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 1 1
train:	 Loss = 1.0335,	 Acc = 0.6583,	 Loss Con1 = 1.2794,	 Loss Con2 = 2.6371
3166 0.154
3145 0.79
2592 0.871
1323 0.873
889 0.846
763 0.872
70 0.886
20 0.9
0.8399227448307203
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.0966,	 Acc = 0.6447,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.6295
745 0.216
668 0.678
714 0.863
562 0.822
190 0.726
103 0.835
11 1.0
7 1.0
0.7862527716186253
0.8093126385809313

 ===== Epoch 290	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.19779068  2.08716
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.6958057   2.478942   -0.18240972  2.0574384
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 5
train:	 Loss = 1.0293,	 Acc = 0.6573,	 Loss Con1 = 1.2797,	 Loss Con2 = 2.5644
3161 0.153
3143 0.79
2598 0.873
1323 0.871
889 0.843
763 0.855
71 0.887
20 0.9
0.8383104348813444
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1202,	 Acc = 0.6440,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.5277
745 0.219
668 0.648
714 0.868
562 0.829
190 0.774
103 0.825
11 1.0
7 1.0
0.7844789356984478
0.8093126385809313

 ===== Epoch 291	 =====
[ 1.0287329   1.7367147  -0.5993768  -0.57551634 -0.6348712  -0.6508424
  2.773167    1.0709279   0.5025774   2.578851    2.8998694   2.257831
 -0.45947683 -0.42555502] [ 0.9869092   1.7423397  -0.59937984 -0.57549053 -0.6347739  -0.6507814
  2.7360713   1.0648216   0.468206    2.6154466   2.869907    2.2623174
 -0.45947683 -0.42555502] 1 1
train:	 Loss = 1.0294,	 Acc = 0.6588,	 Loss Con1 = 1.2787,	 Loss Con2 = 2.3399
3164 0.153
3144 0.793
2597 0.872
1323 0.869
890 0.845
759 0.866
71 0.901
20 0.9
0.8405270331667424
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1043,	 Acc = 0.6457,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.4877
745 0.216
668 0.675
714 0.868
562 0.826
190 0.737
103 0.816
11 1.0
7 0.857
0.7875831485587583
0.8093126385809313

 ===== Epoch 292	 =====
[ 2.9246628   2.8791802   1.7204756   1.0881327   0.12926243  2.7605705
  3.680642    2.4501448  -0.46656546 -0.4823711   4.1062074   3.2633648
 -0.45947683 -0.42555502] [ 2.8914638   2.8947687   1.6920984   1.0877707  -0.6347739  -0.6507814
  3.6457744   2.4578812  -0.46656546 -0.4823711   4.082282    3.2727706
 -0.45947683 -0.42555502] 1 6
train:	 Loss = 1.0338,	 Acc = 0.6552,	 Loss Con1 = 1.2788,	 Loss Con2 = 2.4975
3166 0.152
3140 0.788
2598 0.869
1322 0.868
889 0.843
762 0.857
71 0.887
20 0.85
0.8361735969097932
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.0977,	 Acc = 0.6420,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.4064
745 0.216
668 0.656
714 0.871
562 0.815
190 0.747
103 0.854
11 1.0
7 0.857
0.7827050997782705
0.8093126385809313

 ===== Epoch 293	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.00454938  0.9162081
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.00443796  0.91624784
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 0
train:	 Loss = 1.0276,	 Acc = 0.6602,	 Loss Con1 = 1.2803,	 Loss Con2 = 2.3373
3155 0.152
3144 0.799
2599 0.87
1325 0.866
892 0.852
762 0.869
71 0.873
20 0.9
0.8420515148076705
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1261,	 Acc = 0.6450,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.5454
745 0.213
668 0.656
714 0.856
562 0.824
190 0.842
103 0.845
11 0.909
7 1.0
0.7875831485587583
0.8093126385809313

 ===== Epoch 294	 =====
[ 4.033204    2.7885149   1.8810511   0.5977852   0.29514897  2.025278
  3.9193103   1.7998286   2.5990717   3.176842    4.292931    2.5414371
 -0.45947683 -0.42555502] [ 4.033204    2.9921565   1.8807815   0.76946896  0.29526702  2.216441
  3.918996    2.0046432  -0.46656546 -0.4823711   4.292931    2.762038
 -0.45947683 -0.42555502] 6 6
train:	 Loss = 1.0406,	 Acc = 0.6583,	 Loss Con1 = 1.2791,	 Loss Con2 = 2.3466
3163 0.152
3148 0.792
2592 0.87
1323 0.868
891 0.847
760 0.874
71 0.901
20 0.9
0.8402044293015333
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1016,	 Acc = 0.6483,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.5242
745 0.217
668 0.675
714 0.863
562 0.822
190 0.774
103 0.864
11 1.0
7 1.0
0.7906873614190687
0.8093126385809313

 ===== Epoch 295	 =====
[-0.4557189  -0.49292567  1.4343532   0.49053735 -0.33134407  2.3856645
  3.1033688   1.9503168  -0.46656546 -0.4823711   3.603986    2.8329942
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.4446931   0.46876597 -0.3458121   2.2313244
  3.0963974   1.9285098  -0.46656546 -0.4823711   3.5898955   2.818
 -0.45947683 -0.42555502] 3 5
train:	 Loss = 1.0280,	 Acc = 0.6599,	 Loss Con1 = 1.2788,	 Loss Con2 = 2.3314
3161 0.151
3143 0.797
2598 0.869
1322 0.877
890 0.846
763 0.868
71 0.901
20 0.9
0.8425116384693994
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1049,	 Acc = 0.6420,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.7060
745 0.215
668 0.641
714 0.864
562 0.836
190 0.774
103 0.835
11 1.0
7 1.0
0.7831485587583149
0.8093126385809313

 ===== Epoch 296	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 3 0
train:	 Loss = 1.0294,	 Acc = 0.6598,	 Loss Con1 = 1.2798,	 Loss Con2 = 2.3708
3159 0.154
3143 0.793
2600 0.873
1323 0.874
891 0.848
761 0.863
71 0.873
20 0.9
0.8412986718129186
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1129,	 Acc = 0.6433,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.4678
745 0.219
668 0.662
714 0.861
562 0.822
190 0.742
103 0.864
11 1.0
7 1.0
0.7835920177383592
0.8093126385809313

 ===== Epoch 297	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
  1.6991969   3.422021  ] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.32257774  3.4996655
  1.7231429   3.401     ] 2 3
train:	 Loss = 1.0322,	 Acc = 0.6598,	 Loss Con1 = 1.2791,	 Loss Con2 = 2.4322
3159 0.153
3149 0.792
2595 0.872
1325 0.872
889 0.848
760 0.871
71 0.901
20 0.9
0.8414121920762856
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.0972,	 Acc = 0.6470,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.3498
745 0.213
668 0.663
714 0.861
562 0.836
190 0.774
103 0.864
11 1.0
7 1.0
0.7902439024390244
0.8093126385809313

 ===== Epoch 298	 =====
[-0.4557189  -0.49292567 -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711  -0.4178274  -0.43225467
  1.428222    0.11938963] [-0.4557189  -0.49292567 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
  1.4067869   0.14686117] 4 4
train:	 Loss = 1.0334,	 Acc = 0.6576,	 Loss Con1 = 1.2788,	 Loss Con2 = 2.2498
3164 0.151
3142 0.795
2598 0.864
1323 0.87
890 0.852
760 0.864
71 0.901
20 0.9
0.8396183552930486
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1118,	 Acc = 0.6357,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.2981
745 0.215
668 0.659
714 0.854
562 0.806
190 0.747
103 0.825
11 1.0
7 0.857
0.7747228381374723
0.8093126385809313

 ===== Epoch 299	 =====
[ 0.9830448   0.544989   -0.5993768  -0.57551634 -0.6348712  -0.6508424
 -0.43886673 -0.47319013 -0.46656546 -0.4823711   1.7536634   1.1903892
  1.4619597   3.6017044 ] [ 0.9830448   0.74863034 -0.59937984 -0.57549053 -0.6347739  -0.6507814
 -0.4389609  -0.47327355 -0.46656546 -0.4823711   1.7536634   1.4109901
 -0.45947683 -0.42555502] 6 4
train:	 Loss = 1.0341,	 Acc = 0.6588,	 Loss Con1 = 1.2803,	 Loss Con2 = 2.5158
3160 0.153
3142 0.791
2596 0.872
1325 0.869
892 0.851
762 0.866
71 0.901
20 0.9
0.8403723887375113
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 6
val:	 Loss = 1.1072,	 Acc = 0.6453,	 Loss Con1 = 1.3409,	 Loss Con2 = 3.2917
745 0.216
668 0.669
714 0.87
562 0.826
190 0.737
103 0.835
11 1.0
7 0.857
0.7871396895787139
0.8093126385809313

 ===== Epoch 300	 =====
[ 1.2540509   1.6261883  -0.5993768  -0.57551634 -0.6348712  -0.6508424
  0.2835762   1.1913444   2.4236386   3.2724378   1.6690947   0.7328365
  2.6739902   2.398245  ] [ 1.2540509   1.422547   -0.59937984 -0.57549053 -0.6347739  -0.6507814
  0.28344554  0.98627955  2.4236386   3.0590036   1.6690947   0.5122355
  2.6739902   2.1726687 ] 5 5
train:	 Loss = 1.0269,	 Acc = 0.6589,	 Loss Con1 = 1.2790,	 Loss Con2 = 2.8755
3163 0.152
3144 0.793
2597 0.869
1322 0.877
889 0.848
762 0.869
71 0.901
20 0.85
0.8411130039750142
0.8093126385809313
[ 3.4365578   0.97634107  1.8313969   1.7354822   2.4356868   0.67944026
  0.9434046   2.2458694   2.4668605   0.12151366 -0.4178274  -0.43225467
 -0.45947683 -0.42555502] [-0.4557189  -0.49292567  1.0928433   2.1477163   1.044983    0.1367033
 -0.4389609  -0.47327355 -0.46656546 -0.4823711  -0.4178274  -0.43225467
 -0.45947683 -0.42555502] 0 2
val:	 Loss = 1.0896,	 Acc = 0.6457,	 Loss Con1 = 1.3409,	 Loss Con2 = 2.9192
745 0.215
668 0.681
714 0.866
562 0.822
190 0.726
103 0.845
11 1.0
7 0.857
0.7880266075388027
0.8093126385809313
