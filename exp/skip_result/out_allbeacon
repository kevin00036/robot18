(0.24300000000005184, array([-1.000000e+00, -1.000000e+00,  1.189783e+03,  1.790000e-01,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 4, array([-1.000000e+00, -1.000000e+00,  1.173932e+03,  2.700000e-01,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 1)
(0.24, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.543573e+03, -3.290000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.370508e+03,  2.799000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]))
(0.238, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.370508e+03,  2.799000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.356274e+03, -3.410000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]))
(0.236, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.356274e+03, -3.410000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.427118e+03, -3.330000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]))
(0.2350000000000001, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.427118e+03, -3.330000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.245642e+03, -3.340000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]))
(0.248, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.245642e+03, -3.340000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.415149e+03, -3.430000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]))
(0.256, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.415149e+03, -3.430000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.211029e+03, -3.400000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]))
(0.256, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.211029e+03, -3.400000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([-1.000000e+00, -1.000000e+00,  2.398328e+03,  4.980000e-01,
        2.350128e+03, -3.360000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]))
(0.2670000000000001, array([-1.000000e+00, -1.000000e+00,  2.398328e+03,  4.980000e-01,
        2.350128e+03, -3.360000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.383771e+03, -3.390000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]))
(0.2609999999999997, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.383771e+03, -3.390000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.435205e+03, -3.500000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]))
(0.28500000000000014, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.435205e+03, -3.500000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.406091e+03, -3.470000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]))
14 1 14

 ===== Epoch 1	 =====
[-0.36602148 -0.3783333   1.5083969   0.9231153  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.710092    0.8638864  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 3 4
train:	 Loss = 1.8818,	 Acc = 0.2340
2922 0.221
5681 0.226
2915 0.245
375 0.331
69 0.42
6 0.333
0 0.0
0 0.0
0.23822684059252708
0.0
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 5
val:	 Loss = 1.9314,	 Acc = 0.1903
380 0.316
1718 0.154
815 0.215
85 0.141
2 0.0
0 0.0
0 0.0
0 0.0
0.17213740458015267
0.17213740458015267
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 0
Testing:	 Loss = 1.9237,	 Acc1 = 0.2078,	 Acc2 = 0.1943

 ===== Epoch 2	 =====
[-0.36602148 -0.3783333   2.339387    2.9483051  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   2.2859974   2.8162403  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 2
train:	 Loss = 1.8046,	 Acc = 0.2950
2922 0.239
5679 0.301
2917 0.32
375 0.427
69 0.435
6 0.167
0 0.0
0 0.0
0.31295600265310636
0.17213740458015267
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.8488,	 Acc = 0.2923
380 0.334
1718 0.317
815 0.242
85 0.094
2 0.0
0 0.0
0 0.0
0 0.0
0.2862595419847328
0.2862595419847328
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8471,	 Acc1 = 0.2765,	 Acc2 = 0.2771

 ===== Epoch 3	 =====
[-0.36602148 -0.3783333   2.438894    2.193956   -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   2.6933553   2.1933897  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 2
train:	 Loss = 1.6512,	 Acc = 0.3819
2926 0.247
5677 0.419
2915 0.431
375 0.488
69 0.42
6 0.333
0 0.0
0 0.0
0.4254589692545897
0.2862595419847328
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.8020,	 Acc = 0.3467
380 0.342
1718 0.394
815 0.27
85 0.153
2 0.0
0 0.0
0 0.0
0 0.0
0.3473282442748092
0.3473282442748092
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.7937,	 Acc1 = 0.3196,	 Acc2 = 0.3290

 ===== Epoch 4	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.6541451   1.8817724
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.6458104   1.9212065
 -0.38555372 -0.3798593 ] 1 2
train:	 Loss = 1.5880,	 Acc = 0.4031
2928 0.246
5680 0.443
2911 0.473
374 0.471
69 0.478
6 0.167
0 0.0
0 0.0
0.4539823008849557
0.3473282442748092
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.8171,	 Acc = 0.3087
380 0.347
1718 0.333
815 0.253
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.30305343511450383
0.3473282442748092
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.7671,	 Acc1 = 0.3305,	 Acc2 = 0.3422

 ===== Epoch 5	 =====
[-0.36602148 -0.3783333   0.13722287  1.5664642  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   0.11152866  1.5252405  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 0
train:	 Loss = 1.5578,	 Acc = 0.4120
2923 0.253
5683 0.447
2912 0.493
376 0.476
69 0.493
5 0.4
0 0.0
0 0.0
0.4634604754007739
0.3473282442748092
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.7438,	 Acc = 0.3757
380 0.353
1718 0.405
815 0.346
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.3790076335877863
0.3790076335877863
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.7585,	 Acc1 = 0.3336,	 Acc2 = 0.3459

 ===== Epoch 6	 =====
[-0.36602148 -0.3783333   2.2301311   1.6072398  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   2.113388    1.4776775  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 5
train:	 Loss = 1.5397,	 Acc = 0.4218
2924 0.252
5677 0.457
2915 0.515
377 0.483
69 0.435
6 0.333
0 0.0
0 0.0
0.47666961521450685
0.3790076335877863
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.7648,	 Acc = 0.3550
380 0.345
1718 0.372
815 0.341
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.3564885496183206
0.3790076335877863
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.7718,	 Acc1 = 0.3192,	 Acc2 = 0.3285

 ===== Epoch 7	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 1
train:	 Loss = 1.5317,	 Acc = 0.4241
2928 0.251
5677 0.465
2913 0.513
376 0.481
68 0.397
6 0.333
0 0.0
0 0.0
0.48030973451327436
0.3790076335877863
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 0
val:	 Loss = 1.7319,	 Acc = 0.3763
380 0.347
1718 0.413
815 0.337
85 0.153
2 0.0
0 0.0
0 0.0
0 0.0
0.38053435114503814
0.38053435114503814
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.7558,	 Acc1 = 0.3175,	 Acc2 = 0.3265

 ===== Epoch 8	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  1.7855879   2.2699487  -0.40141198 -0.40778467  3.4187596   2.2736487
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  1.9035444   2.2476256  -0.40141198 -0.40778467  2.7217026   2.2736487
 -0.38555372 -0.3798593 ] 2 3
train:	 Loss = 1.5179,	 Acc = 0.4276
2926 0.253
5676 0.465
2915 0.521
376 0.495
69 0.493
6 0.333
0 0.0
0 0.0
0.4842955098429551
0.38053435114503814
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.7529,	 Acc = 0.3677
380 0.35
1718 0.399
815 0.328
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.3702290076335878
0.38053435114503814
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.7584,	 Acc1 = 0.3388,	 Acc2 = 0.3521

 ===== Epoch 9	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.5573833   2.8683333  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 5
train:	 Loss = 1.5137,	 Acc = 0.4293
2926 0.251
5678 0.47
2913 0.523
376 0.471
69 0.493
6 0.167
0 0.0
0 0.0
0.4871709798717098
0.38053435114503814
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.7994,	 Acc = 0.3223
380 0.347
1718 0.353
815 0.261
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.3187022900763359
0.38053435114503814
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.7793,	 Acc1 = 0.3495,	 Acc2 = 0.3651

 ===== Epoch 10	 =====
[-0.36602148 -0.3783333   1.1855044   2.662876   -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.2738988   2.7822666  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.5025,	 Acc = 0.4349
2923 0.254
5679 0.472
2914 0.539
377 0.472
69 0.478
6 0.167
0 0.0
0 0.0
0.49342177998894415
0.38053435114503814
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.7258,	 Acc = 0.3800
380 0.345
1718 0.419
815 0.333
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.3851145038167939
0.3851145038167939
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.7533,	 Acc1 = 0.3297,	 Acc2 = 0.3412

 ===== Epoch 11	 =====
[-0.36602148 -0.3783333   3.043656    2.3389359  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   3.0592864   2.3768475  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 2
train:	 Loss = 1.4965,	 Acc = 0.4357
2925 0.253
5680 0.476
2912 0.529
376 0.524
69 0.435
6 0.333
0 0.0
0 0.0
0.49463673559659405
0.3851145038167939
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.7284,	 Acc = 0.3957
380 0.345
1718 0.446
815 0.333
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.4030534351145038
0.4030534351145038
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.7588,	 Acc1 = 0.3268,	 Acc2 = 0.3377

 ===== Epoch 12	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   4.294857    2.7126203
 -0.3640846  -0.3725409   4.6449847   2.2001662  -0.42342317 -0.42019257
  3.0075908   0.95330817] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  3.8562515  -4.2443185
 -0.3640846  -0.3725409   5.312682    2.2101202  -0.42342317 -0.42019257
  2.7139566   0.98217595] 2 2
train:	 Loss = 1.4933,	 Acc = 0.4352
2924 0.253
5680 0.476
2914 0.528
376 0.508
68 0.529
6 0.333
0 0.0
0 0.0
0.4941397611676249
0.4030534351145038
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.7497,	 Acc = 0.3590
380 0.342
1718 0.398
815 0.301
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.3614503816793893
0.4030534351145038
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.7847,	 Acc1 = 0.3322,	 Acc2 = 0.3442

 ===== Epoch 13	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 1
train:	 Loss = 1.4855,	 Acc = 0.4405
2924 0.252
5677 0.476
2916 0.546
377 0.536
68 0.544
6 0.167
0 0.0
0 0.0
0.5013268465280849
0.4030534351145038
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.7034,	 Acc = 0.3893
380 0.339
1718 0.424
815 0.358
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.3965648854961832
0.4030534351145038
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.7527,	 Acc1 = 0.3361,	 Acc2 = 0.3489

 ===== Epoch 14	 =====
[-0.36602148 -0.3783333   0.71477234  0.88913554 -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   0.7113957   0.90465474 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 0
train:	 Loss = 1.4836,	 Acc = 0.4398
2923 0.245
5680 0.479
2913 0.547
377 0.525
69 0.478
6 0.5
0 0.0
0 0.0
0.5025981205085682
0.4030534351145038
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.8215,	 Acc = 0.3527
380 0.342
1718 0.385
815 0.309
85 0.176
2 0.0
0 0.0
0 0.0
0 0.0
0.35419847328244275
0.4030534351145038
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8211,	 Acc1 = 0.3454,	 Acc2 = 0.3601

 ===== Epoch 15	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.7405185   1.3099781
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.8481089   1.5120777
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.4853,	 Acc = 0.4366
2924 0.247
5682 0.477
2912 0.538
375 0.512
69 0.464
6 0.333
0 0.0
0 0.0
0.49800973020787265
0.4030534351145038
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.7552,	 Acc = 0.3410
380 0.342
1718 0.376
815 0.282
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.34083969465648856
0.4030534351145038
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.7808,	 Acc1 = 0.3518,	 Acc2 = 0.3678

 ===== Epoch 16	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.2443697   2.2095683
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.410679    2.2465377
 -0.38555372 -0.3798593 ] 4 2
train:	 Loss = 1.4796,	 Acc = 0.4362
2923 0.242
5683 0.48
2912 0.538
376 0.505
68 0.441
6 0.167
0 0.0
0 0.0
0.49917081260364843
0.4030534351145038
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.7001,	 Acc = 0.4013
380 0.347
1718 0.449
815 0.347
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.40916030534351144
0.40916030534351144
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.7623,	 Acc1 = 0.3336,	 Acc2 = 0.3459

 ===== Epoch 17	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 1
train:	 Loss = 1.4699,	 Acc = 0.4449
2923 0.249
5680 0.48
2915 0.561
375 0.536
69 0.522
6 0.333
0 0.0
0 0.0
0.5083471531232725
0.40916030534351144
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.7247,	 Acc = 0.3587
380 0.339
1718 0.393
815 0.312
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.3614503816793893
0.40916030534351144
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.7666,	 Acc1 = 0.3414,	 Acc2 = 0.3554

 ===== Epoch 18	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   1.3116518   3.2851534  -0.42342317 -0.42019257
  2.8709157   2.3232164 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   1.348269    3.2378721  -0.42342317 -0.42019257
  2.916767    2.2864757 ] 2 2
train:	 Loss = 1.4716,	 Acc = 0.4437
2927 0.238
5675 0.486
2916 0.558
375 0.512
69 0.507
6 0.167
0 0.0
0 0.0
0.5102311691184603
0.40916030534351144
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.7564,	 Acc = 0.3500
380 0.197
1718 0.405
815 0.325
85 0.176
2 0.0
0 0.0
0 0.0
0 0.0
0.37213740458015265
0.40916030534351144
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.7803,	 Acc1 = 0.3188,	 Acc2 = 0.3603

 ===== Epoch 19	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  3.4375434   2.3648214  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 6
train:	 Loss = 1.4680,	 Acc = 0.4466
2924 0.251
5683 0.486
2911 0.556
375 0.525
69 0.478
6 0.333
0 0.0
0 0.0
0.5097302078726227
0.40916030534351144
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.7267,	 Acc = 0.3893
380 0.337
1718 0.431
815 0.345
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.3969465648854962
0.40916030534351144
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.7668,	 Acc1 = 0.3530,	 Acc2 = 0.3693

 ===== Epoch 20	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.1947432   1.8867016
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.393252    1.931065
 -0.38555372 -0.3798593 ] 3 2
train:	 Loss = 1.4679,	 Acc = 0.4470
2925 0.244
5683 0.489
2911 0.555
374 0.543
69 0.536
6 0.333
0 0.0
0 0.0
0.5127723100740904
0.40916030534351144
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.7666,	 Acc = 0.3810
380 0.339
1718 0.432
815 0.314
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.38702290076335877
0.40916030534351144
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8237,	 Acc1 = 0.3414,	 Acc2 = 0.3554

 ===== Epoch 21	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.8106031   1.7289653
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.8454564   1.8152274
 -0.38555372 -0.3798593 ] 2 2
train:	 Loss = 1.4586,	 Acc = 0.4520
2922 0.247
5680 0.496
2915 0.558
376 0.54
69 0.522
6 0.333
0 0.0
0 0.0
0.5182401061242539
0.40916030534351144
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.7420,	 Acc = 0.3963
380 0.345
1718 0.44
815 0.35
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.40381679389312974
0.40916030534351144
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.7870,	 Acc1 = 0.3497,	 Acc2 = 0.3653

 ===== Epoch 22	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  3.2167957   2.155543   -0.40141198 -0.40778467  3.0444696   3.1929557
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  3.202191    1.9964913  -0.40141198 -0.40778467  2.7262487   3.0623305
 -0.38555372 -0.3798593 ] 5 5
train:	 Loss = 1.4596,	 Acc = 0.4503
2925 0.246
5682 0.491
2910 0.563
376 0.543
69 0.551
6 0.167
0 0.0
0 0.0
0.5163109587526263
0.40916030534351144
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.7843,	 Acc = 0.3737
380 0.342
1718 0.419
815 0.314
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.3782442748091603
0.40916030534351144
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8093,	 Acc1 = 0.3536,	 Acc2 = 0.3700

 ===== Epoch 23	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   2.2193334   1.9413625
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  2.2708154   1.9793223
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 4
train:	 Loss = 1.4610,	 Acc = 0.4511
2923 0.25
5683 0.491
2910 0.561
377 0.541
69 0.536
6 0.333
0 0.0
0 0.0
0.515975677169707
0.40916030534351144
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.7965,	 Acc = 0.3823
380 0.339
1718 0.44
815 0.301
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.3885496183206107
0.40916030534351144
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8199,	 Acc1 = 0.3592,	 Acc2 = 0.3767

 ===== Epoch 24	 =====
[-0.36602148 -0.3783333   1.3885809   2.8327742  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.4018326   2.6056035  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 5
train:	 Loss = 1.4535,	 Acc = 0.4556
2920 0.246
5684 0.502
2915 0.564
374 0.54
69 0.493
6 0.167
0 0.0
0 0.0
0.5233200707338639
0.40916030534351144
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.7452,	 Acc = 0.3803
380 0.195
1718 0.455
815 0.33
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.4072519083969466
0.40916030534351144
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.7961,	 Acc1 = 0.3227,	 Acc2 = 0.3651

 ===== Epoch 25	 =====
[ 2.8519578   2.4948     -0.4208693  -0.3341335  -0.4409929  -0.3635196
  1.207599    2.4624846  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 2.1916094   2.5049257  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  1.2244507   2.456904   -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 4
train:	 Loss = 1.4553,	 Acc = 0.4530
2923 0.249
5684 0.496
2909 0.561
377 0.528
69 0.58
6 0.5
0 0.0
0 0.0
0.5188501934770592
0.40916030534351144
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.8144,	 Acc = 0.3700
380 0.345
1718 0.415
815 0.306
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.3736641221374046
0.40916030534351144
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8341,	 Acc1 = 0.3629,	 Acc2 = 0.3812

 ===== Epoch 26	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   3.0647125   1.466058   -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   3.3204622   1.466058   -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.4521,	 Acc = 0.4547
2924 0.252
5679 0.5
2913 0.557
377 0.546
69 0.522
6 0.333
0 0.0
0 0.0
0.5203449800973021
0.40916030534351144
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.8194,	 Acc = 0.3737
380 0.334
1718 0.421
815 0.314
85 0.165
2 0.0
0 0.0
0 0.0
0 0.0
0.3793893129770992
0.40916030534351144
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8491,	 Acc1 = 0.3547,	 Acc2 = 0.3713

 ===== Epoch 27	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   1.148589    2.9566712  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   1.0782144   3.1408203  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 6
train:	 Loss = 1.4511,	 Acc = 0.4577
2922 0.253
5680 0.499
2914 0.568
377 0.557
69 0.536
6 0.333
0 0.0
0 0.0
0.5238779571081141
0.40916030534351144
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.7382,	 Acc = 0.4043
380 0.35
1718 0.451
815 0.356
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.4122137404580153
0.4122137404580153
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8086,	 Acc1 = 0.3311,	 Acc2 = 0.3429

 ===== Epoch 28	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.7526418   3.1362693
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.6147457   3.0672596
 -0.38555372 -0.3798593 ] 2 3
train:	 Loss = 1.4509,	 Acc = 0.4546
2920 0.25
5683 0.499
2914 0.564
376 0.516
69 0.536
6 0.333
0 0.0
0 0.0
0.5206675508399646
0.4122137404580153
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.7841,	 Acc = 0.3837
380 0.334
1718 0.422
815 0.345
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.39083969465648855
0.4122137404580153
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8211,	 Acc1 = 0.3528,	 Acc2 = 0.3690

 ===== Epoch 29	 =====
[-0.36602148 -0.3783333   2.1306238   0.85742116 -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.883107    0.8321776  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 2
train:	 Loss = 1.4537,	 Acc = 0.4568
2923 0.249
5681 0.502
2912 0.561
377 0.576
69 0.507
6 0.333
0 0.0
0 0.0
0.5239358761746822
0.4122137404580153
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.7860,	 Acc = 0.3793
380 0.195
1718 0.439
815 0.362
85 0.165
2 0.0
0 0.0
0 0.0
0 0.0
0.40610687022900765
0.4122137404580153
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8295,	 Acc1 = 0.3113,	 Acc2 = 0.3514

 ===== Epoch 30	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 0
train:	 Loss = 1.4597,	 Acc = 0.4508
2926 0.239
5677 0.493
2914 0.561
377 0.565
68 0.632
6 0.333
0 0.0
0 0.0
0.5192435301924353
0.4122137404580153
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.7327,	 Acc = 0.4003
380 0.342
1718 0.458
815 0.331
85 0.176
2 0.0
0 0.0
0 0.0
0 0.0
0.40877862595419845
0.4122137404580153
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.7965,	 Acc1 = 0.3555,	 Acc2 = 0.3723

 ===== Epoch 31	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.7219384   1.9434742  -0.40141198 -0.40778467  2.5110703   3.1855621
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.9527967   2.1443815  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.4483,	 Acc = 0.4543
2927 0.239
5677 0.499
2915 0.569
374 0.551
69 0.536
6 0.333
0 0.0
0 0.0
0.5238358588651698
0.4122137404580153
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.7825,	 Acc = 0.4050
380 0.339
1718 0.448
815 0.368
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.41450381679389314
0.41450381679389314
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8173,	 Acc1 = 0.3501,	 Acc2 = 0.3658

 ===== Epoch 32	 =====
[-0.36602148 -0.3783333   3.3332438   2.909795    2.1662414   1.232339
 -0.3640846  -0.3725409   0.95749235  1.3242133  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   3.404911   -4.1393037   2.296412    1.2613808
 -0.3640846  -0.3725409   0.9855261   1.3565638  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 4
train:	 Loss = 1.4519,	 Acc = 0.4537
2923 0.249
5685 0.498
2909 0.557
376 0.559
69 0.536
6 0.333
0 0.0
0 0.0
0.5197346600331675
0.41450381679389314
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.8097,	 Acc = 0.4000
380 0.35
1718 0.428
815 0.38
85 0.259
2 0.0
0 0.0
0 0.0
0 0.0
0.4072519083969466
0.41450381679389314
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8390,	 Acc1 = 0.3431,	 Acc2 = 0.3574

 ===== Epoch 33	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.03779     2.5127115  -0.40141198 -0.40778467  2.752767    2.5028594
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.0507088   2.4010966  -0.40141198 -0.40778467  3.065306    2.379628
 -0.38555372 -0.3798593 ] 3 5
train:	 Loss = 1.4451,	 Acc = 0.4569
2924 0.242
5682 0.501
2911 0.572
376 0.556
69 0.565
6 0.333
0 0.0
0 0.0
0.5263157894736842
0.41450381679389314
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.7598,	 Acc = 0.3927
380 0.192
1718 0.448
815 0.393
85 0.176
2 0.0
0 0.0
0 0.0
0 0.0
0.4217557251908397
0.4217557251908397
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.7965,	 Acc1 = 0.3216,	 Acc2 = 0.3638

 ===== Epoch 34	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.4473,	 Acc = 0.4570
2923 0.249
5682 0.496
2912 0.571
377 0.576
68 0.559
6 0.5
0 0.0
0 0.0
0.5242675511332228
0.4217557251908397
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.7536,	 Acc = 0.3903
380 0.339
1718 0.428
815 0.356
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.3977099236641221
0.4217557251908397
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8088,	 Acc1 = 0.3390,	 Acc2 = 0.3524

 ===== Epoch 35	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   2.6203814   2.9660015
 -0.3640846  -0.3725409   1.9284266   2.3046834  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  2.8740447   2.9662137
 -0.3640846  -0.3725409   1.9284266   2.3046834  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.4428,	 Acc = 0.4566
2921 0.246
5679 0.5
2916 0.573
377 0.538
69 0.478
6 0.333
0 0.0
0 0.0
0.5245937879960207
0.4217557251908397
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.7720,	 Acc = 0.3853
380 0.347
1718 0.431
815 0.324
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.39083969465648855
0.4217557251908397
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8338,	 Acc1 = 0.3474,	 Acc2 = 0.3626

 ===== Epoch 36	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 1
train:	 Loss = 1.4407,	 Acc = 0.4519
2926 0.244
5681 0.493
2911 0.566
375 0.563
69 0.493
6 0.333
0 0.0
0 0.0
0.5192435301924353
0.4217557251908397
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.7306,	 Acc = 0.4190
380 0.347
1718 0.466
815 0.378
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.42938931297709926
0.42938931297709926
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8015,	 Acc1 = 0.3528,	 Acc2 = 0.3690

 ===== Epoch 37	 =====
[-0.36602148 -0.3783333   3.5355082   2.6017122   3.3056026   1.0367463
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   3.447961   -4.5854917  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 2
train:	 Loss = 1.4370,	 Acc = 0.4608
2925 0.245
5682 0.504
2911 0.576
376 0.582
68 0.515
6 0.333
0 0.0
0 0.0
0.5306867190091784
0.42938931297709926
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.7371,	 Acc = 0.3987
380 0.342
1718 0.442
815 0.356
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.4068702290076336
0.42938931297709926
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.7788,	 Acc1 = 0.3643,	 Acc2 = 0.3830

 ===== Epoch 38	 =====
[ 2.7446604   3.2694073  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 1.937126    3.2086535  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.8083657   3.5750291 ] 1 5
train:	 Loss = 1.4375,	 Acc = 0.4579
2921 0.245
5687 0.503
2908 0.568
377 0.562
69 0.536
6 0.333
0 0.0
0 0.0
0.526693931690063
0.42938931297709926
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.7552,	 Acc = 0.3957
380 0.334
1718 0.456
815 0.314
85 0.247
2 0.0
0 0.0
0 0.0
0 0.0
0.40458015267175573
0.42938931297709926
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.7980,	 Acc1 = 0.3691,	 Acc2 = 0.3887

 ===== Epoch 39	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   1.250432    2.2325168  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   1.2876205   2.3021948  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 4
train:	 Loss = 1.4351,	 Acc = 0.4626
2926 0.249
5682 0.509
2911 0.572
374 0.553
69 0.551
6 0.333
0 0.0
0 0.0
0.5315195753151958
0.42938931297709926
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.8052,	 Acc = 0.3973
380 0.35
1718 0.447
815 0.336
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.40419847328244274
0.42938931297709926
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8260,	 Acc1 = 0.3629,	 Acc2 = 0.3812

 ===== Epoch 40	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  1.6002265   1.0951988  -0.40141198 -0.40778467  1.8769004   3.0771184
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  1.6007891   1.0979892  -0.40141198 -0.40778467  2.036389    3.0771184
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.4384,	 Acc = 0.4585
2921 0.245
5684 0.507
2912 0.56
376 0.561
69 0.594
6 0.333
0 0.0
0 0.0
0.527246601083232
0.42938931297709926
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.7733,	 Acc = 0.4017
380 0.342
1718 0.453
815 0.342
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.41030534351145037
0.42938931297709926
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8225,	 Acc1 = 0.3551,	 Acc2 = 0.3718

 ===== Epoch 41	 =====
[-0.36602148 -0.3783333   2.4831648   2.4386096  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   2.489067    2.417616   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 2
train:	 Loss = 1.4308,	 Acc = 0.4601
2922 0.246
5684 0.503
2912 0.577
375 0.563
69 0.536
6 0.333
0 0.0
0 0.0
0.5290736237010834
0.42938931297709926
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.7676,	 Acc = 0.4220
380 0.337
1718 0.472
815 0.377
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.43435114503816796
0.43435114503816796
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.7924,	 Acc1 = 0.3732,	 Acc2 = 0.3936

 ===== Epoch 42	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 1
train:	 Loss = 1.4302,	 Acc = 0.4630
2921 0.243
5681 0.51
2914 0.579
377 0.56
69 0.536
6 0.333
0 0.0
0 0.0
0.5340997015585277
0.43435114503816796
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.7961,	 Acc = 0.3947
380 0.339
1718 0.446
815 0.331
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.4026717557251908
0.43435114503816796
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8283,	 Acc1 = 0.3635,	 Acc2 = 0.3820

 ===== Epoch 43	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  0.8097212   1.5613705
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  0.6983434   1.6550264
 -0.38555372 -0.3798593 ] 1 4
train:	 Loss = 1.4258,	 Acc = 0.4642
2927 0.249
5684 0.51
2908 0.581
374 0.545
69 0.493
6 0.5
0 0.0
0 0.0
0.5340117243667736
0.43435114503816796
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.8342,	 Acc = 0.3910
380 0.329
1718 0.437
815 0.345
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.4
0.43435114503816796
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8468,	 Acc1 = 0.3573,	 Acc2 = 0.3745

 ===== Epoch 44	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   3.1463287  -4.6198835
 -0.3640846  -0.3725409   2.7431645   1.8268908  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  3.2457635   2.4460952
 -0.3640846  -0.3725409   2.7517476   1.9264311  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 3 3
train:	 Loss = 1.4263,	 Acc = 0.4613
2925 0.245
5677 0.504
2915 0.579
377 0.578
68 0.559
6 0.167
0 0.0
0 0.0
0.5312396328651996
0.43435114503816796
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.7598,	 Acc = 0.3877
380 0.2
1718 0.455
815 0.356
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.4148854961832061
0.43435114503816796
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8022,	 Acc1 = 0.3326,	 Acc2 = 0.3770

 ===== Epoch 45	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  0.84040636  1.3641999
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  0.8642734   1.5589057
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.4292,	 Acc = 0.4628
2924 0.249
5680 0.506
2913 0.575
376 0.582
69 0.609
6 0.167
0 0.0
0 0.0
0.5318443166740381
0.43435114503816796
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.8403,	 Acc = 0.3780
380 0.182
1718 0.455
815 0.328
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.4064885496183206
0.43435114503816796
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8626,	 Acc1 = 0.3346,	 Acc2 = 0.3810

 ===== Epoch 46	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 6
train:	 Loss = 1.4252,	 Acc = 0.4654
2924 0.251
5684 0.513
2908 0.571
377 0.594
69 0.493
6 0.333
0 0.0
0 0.0
0.5347191508182221
0.43435114503816796
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.7758,	 Acc = 0.4313
380 0.345
1718 0.473
815 0.412
85 0.165
2 0.0
0 0.0
0 0.0
0 0.0
0.4438931297709924
0.4438931297709924
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.7986,	 Acc1 = 0.3588,	 Acc2 = 0.3762

 ===== Epoch 47	 =====
[-0.36602148 -0.3783333   1.6042482   2.0376494  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.7551734   1.9012159  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 5
train:	 Loss = 1.4176,	 Acc = 0.4643
2926 0.245
5678 0.51
2912 0.576
377 0.586
69 0.623
6 0.333
0 0.0
0 0.0
0.5351692103516921
0.4438931297709924
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.7784,	 Acc = 0.4313
380 0.339
1718 0.473
815 0.41
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.4446564885496183
0.4446564885496183
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8087,	 Acc1 = 0.3526,	 Acc2 = 0.3688

 ===== Epoch 48	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.2665881   0.8502969  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.4234,	 Acc = 0.4607
2925 0.238
5680 0.508
2912 0.577
377 0.562
68 0.559
6 0.333
0 0.0
0 0.0
0.532787791662059
0.4446564885496183
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.8380,	 Acc = 0.3963
380 0.345
1718 0.454
815 0.323
85 0.176
2 0.0
0 0.0
0 0.0
0 0.0
0.40381679389312974
0.4446564885496183
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8349,	 Acc1 = 0.3660,	 Acc2 = 0.3849

 ===== Epoch 49	 =====
[-0.36602148 -0.3783333   1.0766557   1.5460764  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0765158   1.5478896  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.4216,	 Acc = 0.4631
2926 0.246
5679 0.508
2913 0.577
375 0.571
69 0.565
6 0.333
0 0.0
0 0.0
0.533178500331785
0.4446564885496183
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.7966,	 Acc = 0.3943
380 0.347
1718 0.449
815 0.325
85 0.176
2 0.0
0 0.0
0 0.0
0 0.0
0.40114503816793895
0.4446564885496183
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8174,	 Acc1 = 0.3608,	 Acc2 = 0.3787

 ===== Epoch 50	 =====
[ 1.6067183   2.6314955  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 2.773662    2.7707222  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.4204,	 Acc = 0.4607
2922 0.247
5687 0.504
2908 0.575
376 0.564
69 0.507
6 0.333
0 0.0
0 0.0
0.5296263541896971
0.4446564885496183
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.7979,	 Acc = 0.3820
380 0.326
1718 0.417
815 0.355
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.3900763358778626
0.4446564885496183
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8230,	 Acc1 = 0.3536,	 Acc2 = 0.3700

 ===== Epoch 51	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   2.3712685   1.7696553  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   2.2328098   1.7024657  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 2
train:	 Loss = 1.4175,	 Acc = 0.4681
2923 0.247
5683 0.513
2912 0.585
375 0.584
69 0.58
6 0.333
0 0.0
0 0.0
0.5395245992260918
0.4446564885496183
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.7598,	 Acc = 0.4093
380 0.337
1718 0.443
815 0.39
85 0.247
2 0.0
0 0.0
0 0.0
0 0.0
0.4198473282442748
0.4446564885496183
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8119,	 Acc1 = 0.3532,	 Acc2 = 0.3695

 ===== Epoch 52	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 1
train:	 Loss = 1.4178,	 Acc = 0.4685
2925 0.248
5679 0.516
2913 0.584
377 0.562
68 0.529
6 0.5
0 0.0
0 0.0
0.5397545062479265
0.4446564885496183
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.7670,	 Acc = 0.4160
380 0.345
1718 0.47
815 0.355
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.4263358778625954
0.4446564885496183
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8006,	 Acc1 = 0.3625,	 Acc2 = 0.3807

 ===== Epoch 53	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 1
train:	 Loss = 1.4176,	 Acc = 0.4609
2922 0.236
5680 0.51
2917 0.576
374 0.564
69 0.58
6 0.167
0 0.0
0 0.0
0.5333849215122706
0.4446564885496183
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.8350,	 Acc = 0.4050
380 0.337
1718 0.436
815 0.387
85 0.271
2 0.0
0 0.0
0 0.0
0 0.0
0.4148854961832061
0.4446564885496183
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8363,	 Acc1 = 0.3441,	 Acc2 = 0.3586

 ===== Epoch 54	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 1
train:	 Loss = 1.4176,	 Acc = 0.4598
2924 0.246
5681 0.505
2912 0.567
376 0.593
69 0.522
6 0.5
0 0.0
0 0.0
0.5289694825298541
0.4446564885496183
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.7564,	 Acc = 0.4047
380 0.347
1718 0.457
815 0.34
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.4129770992366412
0.4446564885496183
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.7982,	 Acc1 = 0.3547,	 Acc2 = 0.3713

 ===== Epoch 55	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 1
train:	 Loss = 1.4198,	 Acc = 0.4624
2923 0.242
5679 0.509
2914 0.577
377 0.581
69 0.536
6 0.0
0 0.0
0 0.0
0.5337755666113875
0.4446564885496183
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.7563,	 Acc = 0.4307
380 0.342
1718 0.47
815 0.407
85 0.259
2 0.0
0 0.0
0 0.0
0 0.0
0.4435114503816794
0.4446564885496183
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.7986,	 Acc1 = 0.3534,	 Acc2 = 0.3698

 ===== Epoch 56	 =====
[ 1.4389935   2.6163073  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.1743622   2.895324  ] [ 1.6825874   2.6289642  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.2272701   2.966181  ] 3 4
train:	 Loss = 1.4133,	 Acc = 0.4629
2921 0.251
5676 0.512
2919 0.562
377 0.581
69 0.551
6 0.5
0 0.0
0 0.0
0.5313363545926827
0.4446564885496183
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.7707,	 Acc = 0.4043
380 0.332
1718 0.46
815 0.337
85 0.259
2 0.0
0 0.0
0 0.0
0 0.0
0.4148854961832061
0.4446564885496183
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8037,	 Acc1 = 0.3602,	 Acc2 = 0.3780

 ===== Epoch 57	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.5196582   1.3641999
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.7121065   1.3913109
 -0.38555372 -0.3798593 ] 6 4
train:	 Loss = 1.4126,	 Acc = 0.4647
2923 0.244
5683 0.509
2911 0.579
376 0.612
69 0.551
6 0.5
0 0.0
0 0.0
0.5360972913211719
0.4446564885496183
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.8448,	 Acc = 0.4107
380 0.35
1718 0.475
815 0.329
85 0.176
2 0.0
0 0.0
0 0.0
0 0.0
0.41946564885496185
0.4446564885496183
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8403,	 Acc1 = 0.3670,	 Acc2 = 0.3862

 ===== Epoch 58	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.8802308   2.6381378 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.0861273   2.6801276 ] 3 4
train:	 Loss = 1.4155,	 Acc = 0.4616
2927 0.24
5680 0.51
2910 0.57
376 0.62
69 0.493
6 0.333
0 0.0
0 0.0
0.533348080964495
0.4446564885496183
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.7518,	 Acc = 0.4077
380 0.337
1718 0.445
815 0.382
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.4179389312977099
0.4446564885496183
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.7923,	 Acc1 = 0.3478,	 Acc2 = 0.3631

 ===== Epoch 59	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.8265148   2.9809976
 -0.38555372 -0.3798593 ] 1 5
train:	 Loss = 1.4112,	 Acc = 0.4667
2923 0.247
5683 0.514
2914 0.575
374 0.61
69 0.551
5 0.4
0 0.0
0 0.0
0.5378662244333886
0.4446564885496183
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.7690,	 Acc = 0.4060
380 0.337
1718 0.464
815 0.344
85 0.141
2 0.0
0 0.0
0 0.0
0 0.0
0.41603053435114506
0.4446564885496183
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8071,	 Acc1 = 0.3608,	 Acc2 = 0.3787

 ===== Epoch 60	 =====
[ 2.635521    2.2087526  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   1.9656152   3.267734   -0.42342317 -0.42019257
  3.2002616   2.614519  ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   1.8826547   3.2453372  -0.42342317 -0.42019257
  3.1901207   2.622392  ] 2 0
train:	 Loss = 1.4091,	 Acc = 0.4657
2925 0.242
5679 0.513
2914 0.583
375 0.581
69 0.478
6 0.167
0 0.0
0 0.0
0.5379851819086586
0.4446564885496183
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.7896,	 Acc = 0.4087
380 0.342
1718 0.435
815 0.399
85 0.271
2 0.0
0 0.0
0 0.0
0 0.0
0.4183206106870229
0.4446564885496183
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8039,	 Acc1 = 0.3600,	 Acc2 = 0.3777

 ===== Epoch 61	 =====
[-0.36602148 -0.3783333   2.0473619   1.151912   -0.4409929  -0.3635196
  1.2553446   3.7516403  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   2.1515653   1.2534511  -0.44088638 -0.36343393
  1.3957683   3.7907054  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 4
train:	 Loss = 1.4091,	 Acc = 0.4658
2928 0.251
5678 0.512
2913 0.573
374 0.594
69 0.594
6 0.333
0 0.0
0 0.0
0.5355088495575221
0.4446564885496183
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.8012,	 Acc = 0.4263
380 0.35
1718 0.473
815 0.389
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.43740458015267175
0.4446564885496183
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8039,	 Acc1 = 0.3650,	 Acc2 = 0.3837

 ===== Epoch 62	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   2.8537548  -5.053299
 -0.3640846  -0.3725409   1.9598953   1.956293   -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  3.121603    1.8815222
 -0.3640846  -0.3725409   2.2127845   1.8667068  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 3 4
train:	 Loss = 1.4056,	 Acc = 0.4680
2922 0.244
5683 0.513
2911 0.584
377 0.607
69 0.536
6 0.5
0 0.0
0 0.0
0.5402387795710811
0.4446564885496183
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.7480,	 Acc = 0.4290
380 0.337
1718 0.46
815 0.432
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.44236641221374046
0.4446564885496183
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.7923,	 Acc1 = 0.3528,	 Acc2 = 0.3690

 ===== Epoch 63	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 1
train:	 Loss = 1.4077,	 Acc = 0.4655
2928 0.242
5681 0.513
2909 0.577
375 0.611
69 0.522
6 0.5
0 0.0
0 0.0
0.5377212389380531
0.4446564885496183
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.7582,	 Acc = 0.4090
380 0.337
1718 0.456
815 0.362
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.41946564885496185
0.4446564885496183
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.7737,	 Acc1 = 0.3596,	 Acc2 = 0.3772

 ===== Epoch 64	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   1.2853318   1.3764719  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   1.3825976   1.525782   -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 4
train:	 Loss = 1.4065,	 Acc = 0.4669
2924 0.242
5679 0.515
2914 0.58
376 0.598
69 0.565
6 0.167
0 0.0
0 0.0
0.5394736842105263
0.4446564885496183
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.8426,	 Acc = 0.3983
380 0.337
1718 0.447
815 0.345
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.4072519083969466
0.4446564885496183
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8599,	 Acc1 = 0.3491,	 Acc2 = 0.3646

 ===== Epoch 65	 =====
[-0.36602148 -0.3783333   2.206168    1.5370151  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   2.2271066   1.5954529  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 2
train:	 Loss = 1.4061,	 Acc = 0.4621
2924 0.237
5681 0.509
2912 0.575
376 0.62
69 0.536
6 0.333
0 0.0
0 0.0
0.5348297213622291
0.4446564885496183
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.8123,	 Acc = 0.4047
380 0.334
1718 0.459
815 0.346
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.4148854961832061
0.4446564885496183
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8348,	 Acc1 = 0.3596,	 Acc2 = 0.3772

 ===== Epoch 66	 =====
[ 2.3107748   2.6947806  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.9573867   2.955684  ] [ 2.3564243   2.704906   -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.9723763   2.955684  ] 0 0
train:	 Loss = 1.4007,	 Acc = 0.4687
2923 0.248
5679 0.514
2914 0.58
377 0.621
69 0.551
6 0.333
0 0.0
0 0.0
0.5399668325041459
0.4446564885496183
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.7757,	 Acc = 0.4153
380 0.345
1718 0.477
815 0.344
85 0.176
2 0.0
0 0.0
0 0.0
0 0.0
0.4255725190839695
0.4446564885496183
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8187,	 Acc1 = 0.3592,	 Acc2 = 0.3767

 ===== Epoch 67	 =====
[-0.36602148 -0.3783333   2.339387    2.9483051  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   2.2859974   2.8162403  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 5
train:	 Loss = 1.4027,	 Acc = 0.4673
2926 0.247
5686 0.515
2905 0.578
376 0.604
69 0.507
6 0.333
0 0.0
0 0.0
0.5385976553859766
0.4446564885496183
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.8010,	 Acc = 0.4140
380 0.337
1718 0.452
815 0.398
85 0.165
2 0.0
0 0.0
0 0.0
0 0.0
0.4251908396946565
0.4446564885496183
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8414,	 Acc1 = 0.3359,	 Acc2 = 0.3487

 ===== Epoch 68	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 1
train:	 Loss = 1.3987,	 Acc = 0.4708
2925 0.249
5679 0.516
2912 0.585
377 0.594
69 0.609
6 0.333
0 0.0
0 0.0
0.5424084927568285
0.4446564885496183
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.7608,	 Acc = 0.4293
380 0.347
1718 0.473
815 0.396
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.44122137404580153
0.4446564885496183
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8035,	 Acc1 = 0.3584,	 Acc2 = 0.3757

 ===== Epoch 69	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  0.96326107  1.6225806  -0.40141198 -0.40778467  2.1845136   1.8768431
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  1.1300849   1.54445    -0.40141198 -0.40778467  2.6660142   1.778258
 -0.38555372 -0.3798593 ] 2 2
train:	 Loss = 1.3983,	 Acc = 0.4680
2927 0.252
5678 0.511
2911 0.584
377 0.613
69 0.435
6 0.333
0 0.0
0 0.0
0.5379935847804447
0.4446564885496183
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.8030,	 Acc = 0.4113
380 0.332
1718 0.477
815 0.335
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.42290076335877863
0.4446564885496183
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8534,	 Acc1 = 0.3528,	 Acc2 = 0.3690

 ===== Epoch 70	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   2.1055114   2.8326428
 -0.3640846  -0.3725409   1.0290095   2.0409021  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  2.386572    2.9062002
 -0.3640846  -0.3725409   1.0690596   2.0832067  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 2
train:	 Loss = 1.4028,	 Acc = 0.4652
2922 0.238
5679 0.512
2915 0.58
377 0.607
69 0.594
6 0.5
0 0.0
0 0.0
0.5386911342029627
0.4446564885496183
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.7535,	 Acc = 0.4290
380 0.347
1718 0.48
815 0.384
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.44083969465648853
0.4446564885496183
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.7906,	 Acc1 = 0.3685,	 Acc2 = 0.3879

 ===== Epoch 71	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 1
train:	 Loss = 1.3976,	 Acc = 0.4714
2925 0.239
5678 0.522
2913 0.586
377 0.623
69 0.493
6 0.5
0 0.0
0 0.0
0.5466106380625898
0.4446564885496183
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.7949,	 Acc = 0.4157
380 0.339
1718 0.464
815 0.371
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.4267175572519084
0.4446564885496183
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8136,	 Acc1 = 0.3542,	 Acc2 = 0.3708

 ===== Epoch 72	 =====
[ 3.5794175   1.679691   -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.9436624   1.8639561 ] [ 2.4001565   1.7303188  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.6650176   1.924316  ] 3 4
train:	 Loss = 1.3965,	 Acc = 0.4723
2923 0.246
5683 0.517
2910 0.591
377 0.626
69 0.536
6 0.5
0 0.0
0 0.0
0.5454947484798232
0.4446564885496183
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.8417,	 Acc = 0.3927
380 0.339
1718 0.424
815 0.372
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.40038167938931296
0.4446564885496183
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8281,	 Acc1 = 0.3627,	 Acc2 = 0.3810

 ===== Epoch 73	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 1
train:	 Loss = 1.3988,	 Acc = 0.4714
2925 0.237
5681 0.52
2912 0.59
375 0.624
69 0.594
6 0.333
0 0.0
0 0.0
0.5472741346898153
0.4446564885496183
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.8525,	 Acc = 0.4000
380 0.332
1718 0.462
815 0.325
85 0.176
2 0.0
0 0.0
0 0.0
0 0.0
0.40992366412213743
0.4446564885496183
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8522,	 Acc1 = 0.3580,	 Acc2 = 0.3752

 ===== Epoch 74	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   2.5390265   1.2123352
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   2.6271536   3.0857646   2.6050944   1.1924762
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 5
train:	 Loss = 1.3965,	 Acc = 0.4686
2924 0.239
5677 0.515
2916 0.589
376 0.609
69 0.58
6 0.333
0 0.0
0 0.0
0.5427908005307386
0.4446564885496183
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.7899,	 Acc = 0.4197
380 0.337
1718 0.47
815 0.375
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.4316793893129771
0.4446564885496183
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8251,	 Acc1 = 0.3561,	 Acc2 = 0.3730

 ===== Epoch 75	 =====
[ 1.6128734   1.3809863  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.5447122   1.5988972 ] [ 1.690673    1.231634   -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.5910057   1.4676799 ] 2 5
train:	 Loss = 1.3929,	 Acc = 0.4705
2921 0.236
5687 0.517
2909 0.594
376 0.612
69 0.609
6 0.5
0 0.0
0 0.0
0.546147894329612
0.4446564885496183
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.8019,	 Acc = 0.4003
380 0.342
1718 0.459
815 0.325
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.40877862595419845
0.4446564885496183
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8538,	 Acc1 = 0.3472,	 Acc2 = 0.3623

 ===== Epoch 76	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 1
train:	 Loss = 1.3996,	 Acc = 0.4657
2924 0.239
5678 0.513
2915 0.578
376 0.636
69 0.522
6 0.333
0 0.0
0 0.0
0.5388102609464839
0.4446564885496183
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.8142,	 Acc = 0.3967
380 0.337
1718 0.439
815 0.36
85 0.165
2 0.0
0 0.0
0 0.0
0 0.0
0.40534351145038167
0.4446564885496183
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8277,	 Acc1 = 0.3450,	 Acc2 = 0.3596

 ===== Epoch 77	 =====
[ 2.2212496   2.3606362  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.2272131   2.6013973 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.2267716   2.5987728 ] 0 4
train:	 Loss = 1.3901,	 Acc = 0.4734
2926 0.253
5678 0.519
2913 0.59
376 0.593
69 0.565
6 0.167
0 0.0
0 0.0
0.5447909754479098
0.4446564885496183
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.8388,	 Acc = 0.4130
380 0.35
1718 0.455
815 0.379
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.4221374045801527
0.4446564885496183
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8574,	 Acc1 = 0.3538,	 Acc2 = 0.3703

 ===== Epoch 78	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 1
train:	 Loss = 1.3937,	 Acc = 0.4671
2926 0.241
5679 0.513
2912 0.584
376 0.612
69 0.493
6 0.5
0 0.0
0 0.0
0.5401459854014599
0.4446564885496183
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.7653,	 Acc = 0.4233
380 0.342
1718 0.458
815 0.412
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.4351145038167939
0.4446564885496183
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.7969,	 Acc1 = 0.3551,	 Acc2 = 0.3718

 ===== Epoch 79	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   2.344377    2.0284595  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   2.3357956   2.0185056  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 0
train:	 Loss = 1.3927,	 Acc = 0.4713
2924 0.242
5678 0.516
2915 0.592
376 0.622
69 0.609
6 0.5
0 0.0
0 0.0
0.5454444935869085
0.4446564885496183
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.7395,	 Acc = 0.4270
380 0.339
1718 0.472
815 0.396
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.4396946564885496
0.4446564885496183
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.7912,	 Acc1 = 0.3547,	 Acc2 = 0.3713

 ===== Epoch 80	 =====
[-0.36602148 -0.3783333   2.7191393   1.8337712  -0.4409929  -0.3635196
  2.1602404   3.7711728  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   2.986181    1.8898914  -0.44088638 -0.36343393
  2.2793202   3.7879152  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 3 2
train:	 Loss = 1.3916,	 Acc = 0.4707
2923 0.25
5680 0.512
2913 0.591
377 0.607
69 0.565
6 0.333
0 0.0
0 0.0
0.5419568822553897
0.4446564885496183
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.8918,	 Acc = 0.4167
380 0.35
1718 0.469
815 0.36
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.4263358778625954
0.4446564885496183
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8890,	 Acc1 = 0.3549,	 Acc2 = 0.3715

 ===== Epoch 81	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   2.8455787   2.6829357  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   2.7363002   2.7799876  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 4
train:	 Loss = 1.3915,	 Acc = 0.4708
2923 0.247
5678 0.519
2916 0.582
376 0.612
69 0.536
6 0.333
0 0.0
0 0.0
0.5430624654505252
0.4446564885496183
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.8227,	 Acc = 0.4133
380 0.342
1718 0.462
815 0.363
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.42366412213740456
0.4446564885496183
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8557,	 Acc1 = 0.3604,	 Acc2 = 0.3782

 ===== Epoch 82	 =====
[-0.36602148 -0.3783333   1.635928    1.0884831  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.8059409   1.2036232  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 6
train:	 Loss = 1.3953,	 Acc = 0.4682
2924 0.242
5678 0.514
2915 0.584
377 0.618
68 0.618
6 0.5
0 0.0
0 0.0
0.5414639540026537
0.4446564885496183
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.8398,	 Acc = 0.3943
380 0.326
1718 0.437
815 0.357
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.40419847328244274
0.4446564885496183
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8589,	 Acc1 = 0.3406,	 Acc2 = 0.3544

 ===== Epoch 83	 =====
[-0.36602148 -0.3783333   3.4290953   2.4386096   3.6463037   1.0545275
  2.8061934   3.5284097  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   3.5137558   2.537656    3.7840471   1.1102352
  2.861803    3.6288636  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 3 3
train:	 Loss = 1.3929,	 Acc = 0.4672
2925 0.242
5684 0.513
2908 0.582
376 0.62
69 0.551
6 0.333
0 0.0
0 0.0
0.5398650890191308
0.4446564885496183
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.7998,	 Acc = 0.4030
380 0.2
1718 0.472
815 0.371
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.43244274809160305
0.4446564885496183
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8331,	 Acc1 = 0.3398,	 Acc2 = 0.3857

 ===== Epoch 84	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 0
train:	 Loss = 1.3907,	 Acc = 0.4687
2925 0.244
5678 0.516
2913 0.583
377 0.607
69 0.522
6 0.5
0 0.0
0 0.0
0.5411920822735817
0.4446564885496183
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.8248,	 Acc = 0.4147
380 0.345
1718 0.476
815 0.339
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.4248091603053435
0.4446564885496183
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8351,	 Acc1 = 0.3617,	 Acc2 = 0.3797

 ===== Epoch 85	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   1.501604    2.2051432  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   1.5868552   2.1230228  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 3 5
train:	 Loss = 1.3938,	 Acc = 0.4685
2925 0.244
5680 0.517
2913 0.577
375 0.616
69 0.594
6 0.333
0 0.0
0 0.0
0.5410814995023775
0.4446564885496183
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.7714,	 Acc = 0.4147
380 0.342
1718 0.456
815 0.379
85 0.247
2 0.0
0 0.0
0 0.0
0 0.0
0.4251908396946565
0.4446564885496183
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8262,	 Acc1 = 0.3311,	 Acc2 = 0.3429

 ===== Epoch 86	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 1
train:	 Loss = 1.3877,	 Acc = 0.4705
2924 0.248
5680 0.514
2914 0.588
376 0.62
68 0.588
6 0.333
0 0.0
0 0.0
0.5424590888987174
0.4446564885496183
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.8250,	 Acc = 0.4217
380 0.329
1718 0.466
815 0.391
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.4351145038167939
0.4446564885496183
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8325,	 Acc1 = 0.3617,	 Acc2 = 0.3797

 ===== Epoch 87	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 3 1
train:	 Loss = 1.3912,	 Acc = 0.4691
2924 0.239
5683 0.514
2909 0.59
377 0.615
69 0.594
6 0.333
0 0.0
0 0.0
0.5433436532507739
0.4446564885496183
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.8329,	 Acc = 0.4133
380 0.342
1718 0.47
815 0.348
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.42366412213740456
0.4446564885496183
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8620,	 Acc1 = 0.3528,	 Acc2 = 0.3690

 ===== Epoch 88	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 1
train:	 Loss = 1.3902,	 Acc = 0.4713
2921 0.248
5683 0.513
2913 0.595
377 0.605
68 0.529
6 0.333
0 0.0
0 0.0
0.5432740134851332
0.4446564885496183
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.8001,	 Acc = 0.4080
380 0.329
1718 0.462
815 0.346
85 0.271
2 0.0
0 0.0
0 0.0
0 0.0
0.41946564885496185
0.4446564885496183
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8229,	 Acc1 = 0.3619,	 Acc2 = 0.3800

 ===== Epoch 89	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   2.4653096   1.7880001
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  2.7697506   1.777054
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 3 0
train:	 Loss = 1.3885,	 Acc = 0.4719
2922 0.247
5681 0.516
2914 0.593
376 0.606
69 0.565
6 0.167
0 0.0
0 0.0
0.5444395312845457
0.4446564885496183
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.8014,	 Acc = 0.4133
380 0.326
1718 0.477
815 0.34
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.4259541984732824
0.4446564885496183
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8282,	 Acc1 = 0.3670,	 Acc2 = 0.3862

 ===== Epoch 90	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.3287253   1.6451678
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.4056292   1.7289653
 -0.38555372 -0.3798593 ] 1 4
train:	 Loss = 1.3853,	 Acc = 0.4715
2918 0.249
5683 0.512
2916 0.596
376 0.593
69 0.638
6 0.167
0 0.0
0 0.0
0.5433149171270718
0.4446564885496183
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.7833,	 Acc = 0.4200
380 0.324
1718 0.468
815 0.377
85 0.306
2 0.0
0 0.0
0 0.0
0 0.0
0.43396946564885497
0.4446564885496183
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8215,	 Acc1 = 0.3643,	 Acc2 = 0.3830

 ===== Epoch 91	 =====
[-0.36602148 -0.3783333   1.9125184   3.01853    -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.78604     3.0178175  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.3890,	 Acc = 0.4724
2925 0.247
5688 0.52
2907 0.588
374 0.604
68 0.559
6 0.167
0 0.0
0 0.0
0.5452836448081388
0.4446564885496183
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.8016,	 Acc = 0.4047
380 0.326
1718 0.456
815 0.348
85 0.259
2 0.0
0 0.0
0 0.0
0 0.0
0.41603053435114506
0.4446564885496183
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8288,	 Acc1 = 0.3557,	 Acc2 = 0.3725

 ===== Epoch 92	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   1.5942924   2.1429307  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   1.5044658   2.1329768  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 3 0
train:	 Loss = 1.3889,	 Acc = 0.4735
2922 0.25
5677 0.518
2917 0.592
377 0.599
69 0.609
6 0.333
0 0.0
0 0.0
0.5456555383594959
0.4446564885496183
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.8774,	 Acc = 0.3933
380 0.326
1718 0.422
815 0.385
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.4030534351145038
0.4446564885496183
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8691,	 Acc1 = 0.3528,	 Acc2 = 0.3690

 ===== Epoch 93	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 1
train:	 Loss = 1.3866,	 Acc = 0.4693
2922 0.239
5684 0.516
2912 0.589
377 0.621
68 0.5
5 0.4
0 0.0
0 0.0
0.5437762546982091
0.4446564885496183
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.7892,	 Acc = 0.4047
380 0.342
1718 0.464
815 0.33
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.41374045801526715
0.4446564885496183
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8521,	 Acc1 = 0.3433,	 Acc2 = 0.3576

 ===== Epoch 94	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  1.9035444   2.2476256  -0.40141198 -0.40778467  2.7217026   2.2736487
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.0304885   2.4178388  -0.40141198 -0.40778467  2.9876444   2.4042742
 -0.38555372 -0.3798593 ] 2 6
train:	 Loss = 1.3902,	 Acc = 0.4743
2925 0.248
5680 0.524
2912 0.583
376 0.628
69 0.565
6 0.333
0 0.0
0 0.0
0.5477164657746323
0.4446564885496183
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.8546,	 Acc = 0.4027
380 0.339
1718 0.467
815 0.313
85 0.259
2 0.0
0 0.0
0 0.0
0 0.0
0.4118320610687023
0.4446564885496183
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8737,	 Acc1 = 0.3544,	 Acc2 = 0.3710

 ===== Epoch 95	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  3.0149205   1.7511469
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.4023442   1.8053688
 -0.38555372 -0.3798593 ] 2 2
train:	 Loss = 1.3843,	 Acc = 0.4753
2925 0.249
5680 0.522
2911 0.593
377 0.61
69 0.522
6 0.333
0 0.0
0 0.0
0.5484905451730621
0.4446564885496183
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.8433,	 Acc = 0.3943
380 0.332
1718 0.448
815 0.324
85 0.282
2 0.0
0 0.0
0 0.0
0 0.0
0.4034351145038168
0.4446564885496183
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8786,	 Acc1 = 0.3524,	 Acc2 = 0.3685

 ===== Epoch 96	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.6594479   2.2687194
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.5211736   2.1011245
 -0.38555372 -0.3798593 ] 5 5
train:	 Loss = 1.3885,	 Acc = 0.4738
2925 0.241
5682 0.524
2912 0.59
375 0.613
68 0.603
6 0.333
0 0.0
0 0.0
0.5491540418002875
0.4446564885496183
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.7909,	 Acc = 0.4253
380 0.339
1718 0.459
815 0.416
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.43778625954198475
0.4446564885496183
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8284,	 Acc1 = 0.3594,	 Acc2 = 0.3770

 ===== Epoch 97	 =====
[ 2.3532863   2.1252165  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.1906183   2.357333  ] [ 2.3346446   2.1252165  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.377556    2.357333  ] 0 0
train:	 Loss = 1.3861,	 Acc = 0.4693
2926 0.239
5681 0.519
2909 0.579
377 0.629
69 0.652
6 0.333
0 0.0
0 0.0
0.5436850254368503
0.4446564885496183
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.8430,	 Acc = 0.4027
380 0.326
1718 0.459
815 0.341
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.41374045801526715
0.4446564885496183
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8587,	 Acc1 = 0.3648,	 Acc2 = 0.3834

 ===== Epoch 98	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 1
train:	 Loss = 1.3856,	 Acc = 0.4718
2922 0.239
5681 0.518
2913 0.595
377 0.607
69 0.58
6 0.333
0 0.0
0 0.0
0.5468715454344462
0.4446564885496183
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.7713,	 Acc = 0.4083
380 0.342
1718 0.46
815 0.35
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.4179389312977099
0.4446564885496183
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8205,	 Acc1 = 0.3542,	 Acc2 = 0.3708

 ===== Epoch 99	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  1.8001512   0.7501532
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 3 2
train:	 Loss = 1.3823,	 Acc = 0.4751
2923 0.252
5676 0.519
2918 0.592
376 0.614
69 0.58
6 0.333
0 0.0
0 0.0
0.5470425649530127
0.4446564885496183
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.8074,	 Acc = 0.4027
380 0.205
1718 0.46
815 0.393
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.4312977099236641
0.4446564885496183
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8674,	 Acc1 = 0.3210,	 Acc2 = 0.3631

 ===== Epoch 100	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   1.8386      1.6452302  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   1.7110101   1.8244025  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.3811,	 Acc = 0.4748
2923 0.244
5682 0.521
2911 0.598
377 0.615
69 0.478
6 0.5
0 0.0
0 0.0
0.5493642896627972
0.4446564885496183
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.8928,	 Acc = 0.3940
380 0.337
1718 0.441
815 0.345
85 0.176
2 0.0
0 0.0
0 0.0
0 0.0
0.4022900763358779
0.4446564885496183
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9004,	 Acc1 = 0.3518,	 Acc2 = 0.3678

 ===== Epoch 101	 =====
[-0.36602148 -0.3783333   2.4441738   3.0728977  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  0.836283    0.79905325
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 2
train:	 Loss = 1.3862,	 Acc = 0.4723
2925 0.239
5679 0.522
2912 0.587
377 0.621
69 0.594
6 0.333
0 0.0
0 0.0
0.5478270485458365
0.4446564885496183
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.8035,	 Acc = 0.4100
380 0.339
1718 0.45
815 0.382
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.4202290076335878
0.4446564885496183
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8341,	 Acc1 = 0.3427,	 Acc2 = 0.3569

 ===== Epoch 102	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 1
train:	 Loss = 1.3854,	 Acc = 0.4718
2923 0.244
5679 0.519
2914 0.589
377 0.605
69 0.58
6 0.333
0 0.0
0 0.0
0.5454947484798232
0.4446564885496183
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.7955,	 Acc = 0.4103
380 0.332
1718 0.449
815 0.384
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.4217557251908397
0.4446564885496183
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8386,	 Acc1 = 0.3476,	 Acc2 = 0.3628

 ===== Epoch 103	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.066437    2.7052479  -0.40141198 -0.40778467  2.919833    3.0993
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.175406    2.655021   -0.40141198 -0.40778467  2.687607    3.0746536
 -0.38555372 -0.3798593 ] 3 3
train:	 Loss = 1.3781,	 Acc = 0.4788
2922 0.248
5683 0.53
2911 0.592
377 0.597
69 0.565
6 0.333
0 0.0
0 0.0
0.5531726730046429
0.4446564885496183
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.8470,	 Acc = 0.3897
380 0.195
1718 0.453
815 0.371
85 0.165
2 0.0
0 0.0
0 0.0
0 0.0
0.4179389312977099
0.4446564885496183
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8942,	 Acc1 = 0.3190,	 Acc2 = 0.3606

 ===== Epoch 104	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   3.0057707   1.4723846
 -0.3640846  -0.3725409   2.454802    0.8140702  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  3.0115764   1.5281084
 -0.3640846  -0.3725409   2.4467916   0.8265127  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 2
train:	 Loss = 1.3836,	 Acc = 0.4721
2926 0.238
5682 0.521
2912 0.59
374 0.628
68 0.559
6 0.167
0 0.0
0 0.0
0.5477770404777704
0.4446564885496183
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.8251,	 Acc = 0.4050
380 0.342
1718 0.459
815 0.34
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.41412213740458015
0.4446564885496183
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8608,	 Acc1 = 0.3505,	 Acc2 = 0.3663

 ===== Epoch 105	 =====
[-0.36602148 -0.3783333   0.6051113   2.5450795  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   0.5676227   2.490093   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 2
train:	 Loss = 1.3862,	 Acc = 0.4700
2921 0.239
5684 0.516
2912 0.593
376 0.604
69 0.522
6 0.333
0 0.0
0 0.0
0.5447109539073726
0.4446564885496183
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.8094,	 Acc = 0.3893
380 0.324
1718 0.439
815 0.333
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.3988549618320611
0.4446564885496183
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8209,	 Acc1 = 0.3631,	 Acc2 = 0.3815

 ===== Epoch 106	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   1.3419743   2.0384135  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   1.2847606   1.7796093  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 5
train:	 Loss = 1.3820,	 Acc = 0.4732
2922 0.245
5681 0.519
2914 0.595
377 0.61
68 0.5
6 0.167
0 0.0
0 0.0
0.5467609993367234
0.4446564885496183
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.8655,	 Acc = 0.3973
380 0.342
1718 0.455
815 0.323
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.40534351145038167
0.4446564885496183
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9063,	 Acc1 = 0.3516,	 Acc2 = 0.3675

 ===== Epoch 107	 =====
[-0.36602148 -0.3783333   1.1351418   1.2017487  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0822006   1.2964846  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 3 4
train:	 Loss = 1.3814,	 Acc = 0.4738
2920 0.232
5679 0.525
2917 0.596
377 0.613
69 0.58
6 0.333
0 0.0
0 0.0
0.5517241379310345
0.4446564885496183
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.8633,	 Acc = 0.4040
380 0.342
1718 0.455
815 0.348
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.4129770992366412
0.4446564885496183
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8926,	 Acc1 = 0.3441,	 Acc2 = 0.3586

 ===== Epoch 108	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.9587088   3.559283  ] [ 2.5098352   3.3833199  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 2
train:	 Loss = 1.3828,	 Acc = 0.4734
2922 0.24
5679 0.519
2916 0.598
376 0.622
69 0.551
6 0.333
0 0.0
0 0.0
0.5488613751934557
0.4446564885496183
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.8580,	 Acc = 0.4293
380 0.337
1718 0.474
815 0.401
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.44274809160305345
0.4446564885496183
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8764,	 Acc1 = 0.3573,	 Acc2 = 0.3745

 ===== Epoch 109	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.6689198   1.7486823
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.6393704   1.8571261
 -0.38555372 -0.3798593 ] 2 2
train:	 Loss = 1.3844,	 Acc = 0.4745
2924 0.247
5680 0.524
2914 0.586
375 0.611
69 0.565
6 0.333
0 0.0
0 0.0
0.5479876160990712
0.4446564885496183
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.7966,	 Acc = 0.4097
380 0.345
1718 0.455
815 0.361
85 0.259
2 0.0
0 0.0
0 0.0
0 0.0
0.41908396946564885
0.4446564885496183
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8396,	 Acc1 = 0.3439,	 Acc2 = 0.3583

 ===== Epoch 110	 =====
[ 0.5921431   1.6417199  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 0.5262838   1.6012176  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 3 5
train:	 Loss = 1.3815,	 Acc = 0.4737
2926 0.239
5677 0.52
2914 0.596
376 0.649
69 0.522
6 0.167
0 0.0
0 0.0
0.5496571554965716
0.4446564885496183
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.8739,	 Acc = 0.4133
380 0.2
1718 0.49
815 0.378
85 0.176
2 0.0
0 0.0
0 0.0
0 0.0
0.4442748091603053
0.4446564885496183
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8838,	 Acc1 = 0.3315,	 Acc2 = 0.3757

 ===== Epoch 111	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.2576908   1.202621  ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.2726818   1.2603565 ] 3 4
train:	 Loss = 1.3842,	 Acc = 0.4747
2925 0.252
5678 0.518
2913 0.594
377 0.61
69 0.594
6 0.333
0 0.0
0 0.0
0.5468318036049984
0.4446564885496183
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.8705,	 Acc = 0.3993
380 0.203
1718 0.479
815 0.344
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.42786259541984734
0.4446564885496183
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8884,	 Acc1 = 0.3311,	 Acc2 = 0.3752

 ===== Epoch 112	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   1.7163029   1.070086
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  1.7394085   1.014658
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 3 3
train:	 Loss = 1.3843,	 Acc = 0.4747
2922 0.239
5680 0.521
2915 0.599
376 0.622
69 0.58
6 0.333
0 0.0
0 0.0
0.5507406588547424
0.4446564885496183
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.7960,	 Acc = 0.3950
380 0.337
1718 0.44
815 0.347
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.4034351145038168
0.4446564885496183
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8381,	 Acc1 = 0.3466,	 Acc2 = 0.3616

 ===== Epoch 113	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.4094166   2.0148623
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.3995675   1.9433881
 -0.38555372 -0.3798593 ] 3 3
train:	 Loss = 1.3765,	 Acc = 0.4767
2920 0.242
5685 0.526
2913 0.599
375 0.595
69 0.565
6 0.333
0 0.0
0 0.0
0.5523872679045093
0.4446564885496183
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.8779,	 Acc = 0.4020
380 0.334
1718 0.448
815 0.357
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.4118320610687023
0.4446564885496183
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8926,	 Acc1 = 0.3509,	 Acc2 = 0.3668

 ===== Epoch 114	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 1
train:	 Loss = 1.3825,	 Acc = 0.4735
2923 0.235
5684 0.523
2910 0.597
376 0.614
69 0.58
6 0.333
0 0.0
0 0.0
0.5504698728579326
0.4446564885496183
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.8576,	 Acc = 0.4173
380 0.337
1718 0.453
815 0.396
85 0.271
2 0.0
0 0.0
0 0.0
0 0.0
0.42900763358778626
0.4446564885496183
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8805,	 Acc1 = 0.3571,	 Acc2 = 0.3743

 ===== Epoch 115	 =====
[ 4.150317    2.813756   -0.4208693  -0.3341335  -0.4409929  -0.3635196
  1.1800756   2.2978525  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 2.8353503   2.8745096  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  1.2064756   2.4652753  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 4
train:	 Loss = 1.3834,	 Acc = 0.4717
2926 0.237
5677 0.518
2916 0.596
375 0.624
68 0.574
6 0.333
0 0.0
0 0.0
0.5476664454766644
0.4446564885496183
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.8261,	 Acc = 0.4177
380 0.326
1718 0.45
815 0.413
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.4309160305343511
0.4446564885496183
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8585,	 Acc1 = 0.3481,	 Acc2 = 0.3633

 ===== Epoch 116	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.0563262   2.5713096  -0.40141198 -0.40778467  3.1202376   2.6162322
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.046215    2.5796807  -0.40141198 -0.40778467  3.1274352   2.658131
 -0.38555372 -0.3798593 ] 3 3
train:	 Loss = 1.3842,	 Acc = 0.4747
2920 0.242
5684 0.522
2912 0.599
377 0.594
69 0.565
6 0.333
0 0.0
0 0.0
0.5497347480106101
0.4446564885496183
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.8788,	 Acc = 0.3773
380 0.187
1718 0.452
815 0.325
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.4049618320610687
0.4446564885496183
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9012,	 Acc1 = 0.3315,	 Acc2 = 0.3757

 ===== Epoch 117	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   2.908516    1.2769318  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   2.947995    1.2047652  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 2
train:	 Loss = 1.3793,	 Acc = 0.4772
2923 0.242
5680 0.526
2913 0.599
377 0.61
69 0.594
6 0.333
0 0.0
0 0.0
0.5532338308457712
0.4446564885496183
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.8785,	 Acc = 0.3977
380 0.347
1718 0.433
815 0.367
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.4049618320610687
0.4446564885496183
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8822,	 Acc1 = 0.3567,	 Acc2 = 0.3738

 ===== Epoch 118	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  0.61575747  3.084512
  3.5154996   2.6433866 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  0.665384    3.224996
 -0.38555372 -0.3798593 ] 4 4
train:	 Loss = 1.3822,	 Acc = 0.4758
2919 0.245
5682 0.52
2915 0.6
377 0.618
69 0.536
6 0.333
0 0.0
0 0.0
0.5501160349209857
0.4446564885496183
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.8260,	 Acc = 0.4137
380 0.334
1718 0.468
815 0.358
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.4251908396946565
0.4446564885496183
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8766,	 Acc1 = 0.3483,	 Acc2 = 0.3636

 ===== Epoch 119	 =====
[-0.36602148 -0.3783333   3.4319391   3.047979    2.1356854   1.3501393
 -0.3640846  -0.3725409   0.86022663  1.5432014  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   3.4239998   3.054056    2.270434    1.3858536
 -0.3640846  -0.3725409   0.9569193   1.5929717  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 4
train:	 Loss = 1.3857,	 Acc = 0.4713
2922 0.234
5681 0.519
2914 0.595
376 0.622
69 0.594
6 0.333
0 0.0
0 0.0
0.5480875525093964
0.4446564885496183
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.8663,	 Acc = 0.3953
380 0.332
1718 0.458
815 0.312
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.40458015267175573
0.4446564885496183
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8881,	 Acc1 = 0.3637,	 Acc2 = 0.3822

 ===== Epoch 120	 =====
[-0.36602148 -0.3783333   1.2378985   1.3512594  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.183736    1.4414388  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 4
train:	 Loss = 1.3820,	 Acc = 0.4804
2926 0.25
5680 0.531
2911 0.59
376 0.649
69 0.58
6 0.333
0 0.0
0 0.0
0.5549657155496571
0.4446564885496183
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.7939,	 Acc = 0.4117
380 0.337
1718 0.463
815 0.356
85 0.247
2 0.0
0 0.0
0 0.0
0 0.0
0.42251908396946564
0.4446564885496183
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8373,	 Acc1 = 0.3631,	 Acc2 = 0.3815

 ===== Epoch 121	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   1.8744315   2.0369363
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  1.7741731   2.2327135
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.3847,	 Acc = 0.4751
2926 0.242
5677 0.525
2914 0.596
376 0.606
69 0.478
6 0.333
0 0.0
0 0.0
0.5506525105065251
0.4446564885496183
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.7998,	 Acc = 0.4043
380 0.342
1718 0.441
815 0.377
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.4133587786259542
0.4446564885496183
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8484,	 Acc1 = 0.3452,	 Acc2 = 0.3598

 ===== Epoch 122	 =====
[-0.36602148 -0.3783333   2.641158    1.7635465  -0.4409929  -0.3635196
  2.7702448   3.8241901  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   2.963032    1.8966862  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 3
train:	 Loss = 1.3823,	 Acc = 0.4738
2924 0.231
5678 0.525
2914 0.595
377 0.631
69 0.551
6 0.333
0 0.0
0 0.0
0.5524104378593543
0.4446564885496183
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.8170,	 Acc = 0.3983
380 0.195
1718 0.458
815 0.385
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.42786259541984734
0.4446564885496183
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8570,	 Acc1 = 0.3235,	 Acc2 = 0.3661

 ===== Epoch 123	 =====
[-0.36602148 -0.3783333   1.7553374   1.9017305  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.747457    1.7019037  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 5
train:	 Loss = 1.3774,	 Acc = 0.4781
2924 0.24
5680 0.53
2912 0.596
377 0.621
69 0.551
6 0.333
0 0.0
0 0.0
0.5550641309155241
0.4446564885496183
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.8746,	 Acc = 0.4053
380 0.35
1718 0.443
815 0.371
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.4133587786259542
0.4446564885496183
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8733,	 Acc1 = 0.3650,	 Acc2 = 0.3837

 ===== Epoch 124	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   0.7301053   2.8770957
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  0.7300783   2.752832
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 5
train:	 Loss = 1.3815,	 Acc = 0.4746
2927 0.243
5678 0.525
2911 0.589
377 0.615
69 0.565
6 0.167
0 0.0
0 0.0
0.5494967370866055
0.4446564885496183
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.9076,	 Acc = 0.3857
380 0.195
1718 0.456
815 0.346
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.4133587786259542
0.4446564885496183
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9023,	 Acc1 = 0.3384,	 Acc2 = 0.3839

 ===== Epoch 125	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 0
train:	 Loss = 1.3805,	 Acc = 0.4757
2926 0.244
5679 0.524
2911 0.593
377 0.637
69 0.536
6 0.167
0 0.0
0 0.0
0.5507631055076311
0.4446564885496183
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.8353,	 Acc = 0.4097
380 0.337
1718 0.45
815 0.38
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.4202290076335878
0.4446564885496183
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8677,	 Acc1 = 0.3433,	 Acc2 = 0.3576

 ===== Epoch 126	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.0765464   1.3789878
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.2822537   1.3641999
 -0.38555372 -0.3798593 ] 3 3
train:	 Loss = 1.3810,	 Acc = 0.4750
2924 0.234
5680 0.525
2912 0.599
377 0.621
69 0.565
6 0.333
0 0.0
0 0.0
0.5528527200353825
0.4446564885496183
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.9025,	 Acc = 0.3837
380 0.347
1718 0.421
815 0.34
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.3889312977099237
0.4446564885496183
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8773,	 Acc1 = 0.3654,	 Acc2 = 0.3842

 ===== Epoch 127	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.0244024   3.4333148 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.3364265   0.80226415
  2.031897    3.4569337 ] 4 4
train:	 Loss = 1.3831,	 Acc = 0.4751
2926 0.242
5677 0.527
2914 0.591
376 0.59
69 0.623
6 0.333
0 0.0
0 0.0
0.5505419155054192
0.4446564885496183
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.8209,	 Acc = 0.4337
380 0.345
1718 0.477
815 0.406
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.44656488549618323
0.44656488549618323
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8422,	 Acc1 = 0.3567,	 Acc2 = 0.3738

 ===== Epoch 128	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.2193336   1.2734783 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.2576908   1.202621  ] 3 3
train:	 Loss = 1.3740,	 Acc = 0.4759
2926 0.24
5681 0.526
2910 0.597
377 0.605
68 0.618
6 0.333
0 0.0
0 0.0
0.5523114355231143
0.44656488549618323
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.8829,	 Acc = 0.4067
380 0.337
1718 0.471
815 0.329
85 0.176
2 0.0
0 0.0
0 0.0
0 0.0
0.416793893129771
0.44656488549618323
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9017,	 Acc1 = 0.3643,	 Acc2 = 0.3830

 ===== Epoch 129	 =====
[-0.36602148 -0.3783333   3.3080628  -4.2961664   2.1983256   1.0523047
 -0.3640846  -0.3725409   0.9700784   0.9633804  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   3.6437206   2.8751278   2.1630824   1.1813626
 -0.3640846  -0.3725409   0.8688083   1.1425526  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 2
train:	 Loss = 1.3790,	 Acc = 0.4740
2925 0.242
5684 0.524
2909 0.593
376 0.593
68 0.559
6 0.167
0 0.0
0 0.0
0.548932876257879
0.44656488549618323
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.9027,	 Acc = 0.4070
380 0.339
1718 0.45
815 0.368
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.416793893129771
0.44656488549618323
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9066,	 Acc1 = 0.3454,	 Acc2 = 0.3601

 ===== Epoch 130	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   1.3625726   2.252425   -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   1.2939154   2.4888327  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.3858,	 Acc = 0.4756
2923 0.241
5683 0.525
2913 0.595
375 0.624
68 0.574
6 0.333
0 0.0
0 0.0
0.5514648977335544
0.44656488549618323
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.8711,	 Acc = 0.4003
380 0.337
1718 0.464
815 0.314
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.40954198473282444
0.44656488549618323
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8628,	 Acc1 = 0.3592,	 Acc2 = 0.3767

 ===== Epoch 131	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   1.8014785   1.3723656
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  2.0423605   1.456981
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 2
train:	 Loss = 1.3844,	 Acc = 0.4765
2920 0.244
5682 0.524
2916 0.597
375 0.613
69 0.565
6 0.333
0 0.0
0 0.0
0.5516136162687887
0.44656488549618323
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.8145,	 Acc = 0.3977
380 0.205
1718 0.451
815 0.389
85 0.271
2 0.0
0 0.0
0 0.0
0 0.0
0.4255725190839695
0.44656488549618323
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8555,	 Acc1 = 0.3122,	 Acc2 = 0.3524

 ===== Epoch 132	 =====
[-0.36602148 -0.3783333   1.8398169   1.6684033  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   2.049218    1.5818634  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 2
train:	 Loss = 1.3757,	 Acc = 0.4764
2922 0.248
5680 0.527
2914 0.588
377 0.629
69 0.493
6 0.333
0 0.0
0 0.0
0.5501879283661286
0.44656488549618323
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.8669,	 Acc = 0.3817
380 0.332
1718 0.433
815 0.315
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.3889312977099237
0.44656488549618323
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8655,	 Acc1 = 0.3551,	 Acc2 = 0.3718

 ===== Epoch 133	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 1
train:	 Loss = 1.3787,	 Acc = 0.4759
2929 0.242
5680 0.525
2909 0.596
375 0.616
69 0.536
6 0.333
0 0.0
0 0.0
0.5517203230445846
0.44656488549618323
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.8894,	 Acc = 0.3907
380 0.347
1718 0.434
815 0.339
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.3969465648854962
0.44656488549618323
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8602,	 Acc1 = 0.3664,	 Acc2 = 0.3854

 ===== Epoch 134	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  3.0486376   1.5884813
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.7205665   1.5884813
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.3766,	 Acc = 0.4751
2923 0.238
5684 0.523
2909 0.596
377 0.647
69 0.594
6 0.333
0 0.0
0 0.0
0.5516860143725816
0.44656488549618323
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.8245,	 Acc = 0.4130
380 0.342
1718 0.451
815 0.387
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.4232824427480916
0.44656488549618323
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8775,	 Acc1 = 0.3303,	 Acc2 = 0.3419

 ===== Epoch 135	 =====
[ 1.5929196   2.1429362  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.0513529   2.737863  ] [ 1.2894555   1.9176421  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.0129957   2.4728043 ] 5 5
train:	 Loss = 1.3766,	 Acc = 0.4778
2922 0.25
5681 0.522
2913 0.598
377 0.631
69 0.551
6 0.333
0 0.0
0 0.0
0.5512933893433561
0.44656488549618323
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.8514,	 Acc = 0.4073
380 0.342
1718 0.451
815 0.368
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.416793893129771
0.44656488549618323
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8949,	 Acc1 = 0.3503,	 Acc2 = 0.3661

 ===== Epoch 136	 =====
[-0.36602148 -0.3783333   1.5079901   1.6955872  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.5988102   1.720023   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 4
train:	 Loss = 1.3753,	 Acc = 0.4769
2922 0.243
5683 0.525
2912 0.599
376 0.62
69 0.536
6 0.333
0 0.0
0 0.0
0.5525093964183064
0.44656488549618323
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.8357,	 Acc = 0.3830
380 0.197
1718 0.461
815 0.325
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.40992366412213743
0.44656488549618323
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8544,	 Acc1 = 0.3326,	 Acc2 = 0.3770

 ===== Epoch 137	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   1.2658799   2.9964874  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   1.3339657   2.9964874  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.3807,	 Acc = 0.4744
2926 0.233
5684 0.527
2908 0.597
376 0.612
68 0.515
6 0.333
0 0.0
0 0.0
0.5526432205264322
0.44656488549618323
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.8806,	 Acc = 0.3767
380 0.189
1718 0.445
815 0.339
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.40381679389312974
0.44656488549618323
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9029,	 Acc1 = 0.3161,	 Acc2 = 0.3571

 ===== Epoch 138	 =====
[ 2.8048756   2.3479793  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  1.2036679   1.7118726  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 2.4457095   2.5226455  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  1.295787    1.8597628  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
train:	 Loss = 1.3803,	 Acc = 0.4718
2927 0.239
5678 0.518
2914 0.594
376 0.628
67 0.567
6 0.167
0 0.0
0 0.0
0.5470633779449176
0.44656488549618323
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.8878,	 Acc = 0.3933
380 0.324
1718 0.44
815 0.344
85 0.247
2 0.0
0 0.0
0 0.0
0 0.0
0.4034351145038168
0.44656488549618323
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8862,	 Acc1 = 0.3571,	 Acc2 = 0.3743

 ===== Epoch 139	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   2.65514     2.9593337
 -0.3640846  -0.3725409   1.9307154   2.2972178  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  2.6341288   2.9662137
 -0.3640846  -0.3725409   1.9307154   2.3046834  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.3737,	 Acc = 0.4784
2923 0.243
5680 0.529
2914 0.597
376 0.617
69 0.565
6 0.333
0 0.0
0 0.0
0.5545605306799337
0.44656488549618323
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.8407,	 Acc = 0.4160
380 0.324
1718 0.463
815 0.382
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.42938931297709926
0.44656488549618323
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8713,	 Acc1 = 0.3398,	 Acc2 = 0.3534

 ===== Epoch 140	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 1
train:	 Loss = 1.3770,	 Acc = 0.4758
2924 0.246
5677 0.523
2916 0.597
376 0.601
69 0.565
6 0.167
0 0.0
0 0.0
0.5500884564352057
0.44656488549618323
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.9484,	 Acc = 0.3993
380 0.342
1718 0.439
815 0.362
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.4076335877862595
0.44656488549618323
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9329,	 Acc1 = 0.3586,	 Acc2 = 0.3760

 ===== Epoch 141	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 2.0737388   2.16825    -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 3 6
train:	 Loss = 1.3774,	 Acc = 0.4750
2924 0.238
5682 0.526
2912 0.592
375 0.627
69 0.551
6 0.333
0 0.0
0 0.0
0.5515258735072976
0.44656488549618323
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.9059,	 Acc = 0.4297
380 0.337
1718 0.486
815 0.377
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.4431297709923664
0.44656488549618323
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9208,	 Acc1 = 0.3549,	 Acc2 = 0.3715

 ===== Epoch 142	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 1
train:	 Loss = 1.3748,	 Acc = 0.4759
2924 0.242
5679 0.524
2914 0.594
376 0.633
69 0.609
6 0.167
0 0.0
0 0.0
0.5516364440513047
0.44656488549618323
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.8810,	 Acc = 0.4140
380 0.345
1718 0.475
815 0.339
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.42404580152671756
0.44656488549618323
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8685,	 Acc1 = 0.3664,	 Acc2 = 0.3854

 ===== Epoch 143	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  1.4283465   1.1621679  -0.40141198 -0.40778467  1.8359855   3.291541
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  1.3968917   1.2709926  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 6
train:	 Loss = 1.3736,	 Acc = 0.4791
2924 0.239
5685 0.533
2907 0.595
377 0.613
69 0.594
6 0.333
0 0.0
0 0.0
0.5567226890756303
0.44656488549618323
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.8938,	 Acc = 0.3890
380 0.2
1718 0.449
815 0.371
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.416412213740458
0.44656488549618323
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9016,	 Acc1 = 0.3109,	 Acc2 = 0.3509

 ===== Epoch 144	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.7806756   2.8060088
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.762492    2.7493224
 -0.38555372 -0.3798593 ] 5 2
train:	 Loss = 1.3774,	 Acc = 0.4762
2924 0.243
5680 0.528
2913 0.591
377 0.607
68 0.529
6 0.333
0 0.0
0 0.0
0.5515258735072976
0.44656488549618323
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.9236,	 Acc = 0.4077
380 0.342
1718 0.459
815 0.352
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.417175572519084
0.44656488549618323
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8914,	 Acc1 = 0.3485,	 Acc2 = 0.3638

 ===== Epoch 145	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   2.3851      1.0967577
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  2.3716724   1.0991216
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.3718,	 Acc = 0.4780
2929 0.241
5673 0.526
2916 0.603
376 0.625
68 0.5
6 0.333
0 0.0
0 0.0
0.5548180108419073
0.44656488549618323
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.9116,	 Acc = 0.4150
380 0.339
1718 0.481
815 0.333
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.4259541984732824
0.44656488549618323
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8848,	 Acc1 = 0.3652,	 Acc2 = 0.3839

 ===== Epoch 146	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.6802841   1.696925
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.7939348   1.8226211
 -0.38555372 -0.3798593 ] 2 6
train:	 Loss = 1.3719,	 Acc = 0.4792
2921 0.228
5683 0.533
2912 0.607
377 0.607
69 0.58
6 0.167
0 0.0
0 0.0
0.5601856969161048
0.44656488549618323
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.9023,	 Acc = 0.4027
380 0.345
1718 0.46
815 0.33
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.41106870229007636
0.44656488549618323
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8736,	 Acc1 = 0.3635,	 Acc2 = 0.3820

 ===== Epoch 147	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 1
train:	 Loss = 1.3785,	 Acc = 0.4766
2922 0.232
5679 0.527
2917 0.601
375 0.643
69 0.565
6 0.333
0 0.0
0 0.0
0.5556046871545435
0.44656488549618323
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.8529,	 Acc = 0.4047
380 0.339
1718 0.436
815 0.387
85 0.247
2 0.0
0 0.0
0 0.0
0 0.0
0.41412213740458015
0.44656488549618323
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8841,	 Acc1 = 0.3493,	 Acc2 = 0.3648

 ===== Epoch 148	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  3.0482862   1.2235563  -0.40141198 -0.40778467  3.395272    1.8029042
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  3.140966    1.3268002  -0.40141198 -0.40778467  3.426337    1.8891662
 -0.38555372 -0.3798593 ] 2 2
train:	 Loss = 1.3728,	 Acc = 0.4791
2926 0.239
5677 0.529
2913 0.601
377 0.658
69 0.507
6 0.167
0 0.0
0 0.0
0.5569564255695643
0.44656488549618323
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.9995,	 Acc = 0.3970
380 0.347
1718 0.458
815 0.308
85 0.247
2 0.0
0 0.0
0 0.0
0 0.0
0.40419847328244274
0.44656488549618323
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9554,	 Acc1 = 0.3577,	 Acc2 = 0.3750

 ===== Epoch 149	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   0.4952053   0.7967008
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  0.4320932   1.0102124
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.3742,	 Acc = 0.4797
2923 0.244
5682 0.526
2912 0.604
376 0.625
69 0.58
6 0.167
0 0.0
0 0.0
0.5557766721945826
0.44656488549618323
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.8965,	 Acc = 0.4097
380 0.329
1718 0.459
815 0.366
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.4213740458015267
0.44656488549618323
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8915,	 Acc1 = 0.3608,	 Acc2 = 0.3787

 ===== Epoch 150	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 1
train:	 Loss = 1.3763,	 Acc = 0.4775
2923 0.239
5680 0.524
2914 0.605
376 0.62
69 0.623
6 0.333
0 0.0
0 0.0
0.5544499723604202
0.44656488549618323
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.8103,	 Acc = 0.4380
380 0.342
1718 0.481
815 0.413
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.45190839694656487
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8638,	 Acc1 = 0.3483,	 Acc2 = 0.3636

 ===== Epoch 151	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 1
train:	 Loss = 1.3708,	 Acc = 0.4805
2923 0.246
5682 0.529
2913 0.601
376 0.625
68 0.529
6 0.333
0 0.0
0 0.0
0.5562189054726369
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.8928,	 Acc = 0.3987
380 0.347
1718 0.435
815 0.364
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.40610687022900765
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9014,	 Acc1 = 0.3439,	 Acc2 = 0.3583

 ===== Epoch 152	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.4049957   1.8201566
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.1920905   1.8226211
 -0.38555372 -0.3798593 ] 3 0
train:	 Loss = 1.3739,	 Acc = 0.4791
2927 0.242
5677 0.527
2913 0.604
376 0.644
69 0.493
6 0.167
0 0.0
0 0.0
0.5559119566419644
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.9233,	 Acc = 0.4000
380 0.342
1718 0.465
815 0.312
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.4083969465648855
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9099,	 Acc1 = 0.3573,	 Acc2 = 0.3745

 ===== Epoch 153	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 1
train:	 Loss = 1.3709,	 Acc = 0.4831
2924 0.248
5679 0.531
2913 0.606
377 0.634
69 0.536
6 0.167
0 0.0
0 0.0
0.559265811587793
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.8396,	 Acc = 0.3897
380 0.176
1718 0.442
815 0.39
85 0.282
2 0.0
0 0.0
0 0.0
0 0.0
0.4206106870229008
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8555,	 Acc1 = 0.3227,	 Acc2 = 0.3651

 ===== Epoch 154	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   1.3797358   0.99821943 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   1.3145118   1.2470696  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.3743,	 Acc = 0.4799
2920 0.243
5686 0.53
2911 0.603
376 0.614
69 0.507
6 0.167
0 0.0
0 0.0
0.5563660477453581
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.9293,	 Acc = 0.3880
380 0.189
1718 0.465
815 0.336
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.416793893129771
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9264,	 Acc1 = 0.3303,	 Acc2 = 0.3743

 ===== Epoch 155	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   1.6315097   1.007852
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  1.632821    1.030217
 -0.3640846  -0.3725409   5.3229795   0.9459609  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 3 6
train:	 Loss = 1.3767,	 Acc = 0.4738
2926 0.24
5681 0.521
2910 0.599
376 0.604
69 0.522
6 0.167
0 0.0
0 0.0
0.5493253704932537
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.9445,	 Acc = 0.4070
380 0.353
1718 0.474
815 0.313
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.4148854961832061
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9163,	 Acc1 = 0.3652,	 Acc2 = 0.3839

 ===== Epoch 156	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 1
train:	 Loss = 1.3739,	 Acc = 0.4771
2925 0.233
5678 0.532
2914 0.593
376 0.628
69 0.594
6 0.167
0 0.0
0 0.0
0.556120756386155
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.9775,	 Acc = 0.3890
380 0.332
1718 0.445
815 0.317
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.3973282442748092
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9494,	 Acc1 = 0.3536,	 Acc2 = 0.3700

 ===== Epoch 157	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   1.4678469   2.603304   -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   1.4089158   2.697867   -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 4
train:	 Loss = 1.3722,	 Acc = 0.4806
2919 0.24
5684 0.53
2914 0.605
376 0.614
69 0.594
6 0.333
0 0.0
0 0.0
0.5581832246657089
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.9025,	 Acc = 0.3933
380 0.339
1718 0.448
815 0.324
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.40114503816793895
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8727,	 Acc1 = 0.3594,	 Acc2 = 0.3770

 ===== Epoch 158	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   2.7502458   1.7302114
 -0.3640846  -0.3725409   2.0193975   1.4137994  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  2.7204683   1.7726085
 -0.3640846  -0.3725409   2.1103704   1.4610809  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 4
train:	 Loss = 1.3712,	 Acc = 0.4820
2921 0.248
5682 0.527
2913 0.608
377 0.637
69 0.536
6 0.333
0 0.0
0 0.0
0.5575328838288935
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.8594,	 Acc = 0.4273
380 0.35
1718 0.477
815 0.384
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.4385496183206107
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8683,	 Acc1 = 0.3443,	 Acc2 = 0.3588

 ===== Epoch 159	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 3 1
train:	 Loss = 1.3728,	 Acc = 0.4789
2926 0.24
5683 0.532
2909 0.595
375 0.621
69 0.565
6 0.167
0 0.0
0 0.0
0.5561822605618226
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.8867,	 Acc = 0.4017
380 0.353
1718 0.448
815 0.346
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.40877862595419845
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8839,	 Acc1 = 0.3615,	 Acc2 = 0.3795

 ===== Epoch 160	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  1.3412838   2.5406156  -0.40141198 -0.40778467  2.6766207   2.0222564
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  1.2693869   2.5406156  -0.40141198 -0.40778467  2.6732118   2.0222564
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.3749,	 Acc = 0.4821
2914 0.24
5686 0.527
2916 0.614
377 0.647
69 0.565
6 0.167
0 0.0
0 0.0
0.559973492379059
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.8697,	 Acc = 0.3967
380 0.35
1718 0.44
815 0.346
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.4034351145038168
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8681,	 Acc1 = 0.3464,	 Acc2 = 0.3613

 ===== Epoch 161	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   1.8803661   3.2328951  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   1.8883764   3.2751997  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 4
train:	 Loss = 1.3670,	 Acc = 0.4860
2922 0.252
5682 0.532
2914 0.609
375 0.643
69 0.551
6 0.333
0 0.0
0 0.0
0.5614636303338492
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.8499,	 Acc = 0.4087
380 0.332
1718 0.467
815 0.345
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.4198473282442748
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8677,	 Acc1 = 0.3481,	 Acc2 = 0.3633

 ===== Epoch 162	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.1420846   2.1085184
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.2394452   2.1898513
 -0.38555372 -0.3798593 ] 5 4
train:	 Loss = 1.3703,	 Acc = 0.4789
2922 0.242
5681 0.531
2914 0.597
376 0.604
69 0.594
6 0.333
0 0.0
0 0.0
0.555383594959098
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.8944,	 Acc = 0.3967
380 0.342
1718 0.449
815 0.334
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.40458015267175573
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9241,	 Acc1 = 0.3464,	 Acc2 = 0.3613

 ===== Epoch 163	 =====
[-0.36602148 -0.3783333   4.5305834  -3.9994102   3.0550425   1.6368603
 -0.3640846  -0.3725409   2.0491488   1.7771208  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  2.830111    1.685922
 -0.3640846  -0.3725409   2.0869122   1.8169369  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 3 3
train:	 Loss = 1.3722,	 Acc = 0.4829
2927 0.248
5675 0.53
2916 0.607
375 0.619
69 0.609
6 0.167
0 0.0
0 0.0
0.5588983519522177
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.9330,	 Acc = 0.3923
380 0.334
1718 0.453
815 0.314
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.40076335877862596
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9079,	 Acc1 = 0.3563,	 Acc2 = 0.3733

 ===== Epoch 164	 =====
[-0.36602148 -0.3783333   1.500679    0.90952337 -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.4042693   1.1356758  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.3745,	 Acc = 0.4761
2924 0.232
5681 0.526
2913 0.604
376 0.62
68 0.544
6 0.167
0 0.0
0 0.0
0.554953560371517
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.9067,	 Acc = 0.3970
380 0.342
1718 0.446
815 0.339
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.4049618320610687
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9086,	 Acc1 = 0.3538,	 Acc2 = 0.3703

 ===== Epoch 165	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  1.6378613   2.9759147  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  1.6249423   2.8224437  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 5
train:	 Loss = 1.3710,	 Acc = 0.4820
2924 0.241
5684 0.534
2911 0.6
374 0.642
69 0.536
6 0.333
0 0.0
0 0.0
0.5599292348518354
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.8536,	 Acc = 0.3840
380 0.342
1718 0.43
815 0.325
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.3900763358778626
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8849,	 Acc1 = 0.3505,	 Acc2 = 0.3663

 ===== Epoch 166	 =====
[ 0.21529981  1.5379329  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 0.21406418  1.5354013  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.3758,	 Acc = 0.4766
2927 0.235
5675 0.524
2914 0.604
377 0.637
69 0.623
6 0.333
0 0.0
0 0.0
0.5549164915385466
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.9041,	 Acc = 0.3877
380 0.337
1718 0.438
815 0.325
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.3950381679389313
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9069,	 Acc1 = 0.3503,	 Acc2 = 0.3661

 ===== Epoch 167	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   1.8168576   2.3021948  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   1.9227049   2.2325168  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 2
train:	 Loss = 1.3727,	 Acc = 0.4779
2925 0.248
5681 0.523
2912 0.601
376 0.628
69 0.507
5 0.4
0 0.0
0 0.0
0.5523609421652107
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.9759,	 Acc = 0.3830
380 0.347
1718 0.444
815 0.293
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.3881679389312977
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9539,	 Acc1 = 0.3615,	 Acc2 = 0.3795

 ===== Epoch 168	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 1
train:	 Loss = 1.3758,	 Acc = 0.4785
2919 0.243
5679 0.527
2918 0.601
377 0.623
69 0.565
6 0.167
0 0.0
0 0.0
0.5546469223118576
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.8892,	 Acc = 0.3540
380 0.195
1718 0.424
815 0.297
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.37709923664122136
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8978,	 Acc1 = 0.3183,	 Acc2 = 0.3598

 ===== Epoch 169	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 0
train:	 Loss = 1.3722,	 Acc = 0.4769
2927 0.245
5677 0.525
2913 0.595
376 0.636
69 0.536
6 0.167
0 0.0
0 0.0
0.5520407034620064
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.9093,	 Acc = 0.4000
380 0.35
1718 0.448
815 0.344
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.4072519083969466
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8835,	 Acc1 = 0.3580,	 Acc2 = 0.3752

 ===== Epoch 170	 =====
[-0.36602148 -0.3783333   3.9607506  -4.5249624   4.0675955   1.1878861
  2.8471982   3.6288636  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   3.5564003   2.6486368   3.6277957   1.2391536
  2.8966281   3.698623   -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 3 3
train:	 Loss = 1.3694,	 Acc = 0.4809
2923 0.24
5683 0.528
2913 0.612
374 0.623
69 0.536
6 0.333
0 0.0
0 0.0
0.5587617468214483
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.8966,	 Acc = 0.4047
380 0.345
1718 0.451
815 0.353
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.4133587786259542
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9044,	 Acc1 = 0.3423,	 Acc2 = 0.3564

 ===== Epoch 171	 =====
[ 2.9149811  2.2644432 -0.4208693 -0.3341335 -0.4409929 -0.3635196
 -0.3640846 -0.3725409  6.0707774  3.2080102  0.9741356  2.384557
  3.7791517  2.3678303] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   5.353304    3.2005446   0.9836063   2.4215264
  3.917152    2.3494601 ] 4 4
train:	 Loss = 1.3703,	 Acc = 0.4814
2928 0.249
5681 0.528
2907 0.606
377 0.613
69 0.594
6 0.167
0 0.0
0 0.0
0.5567477876106195
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.9926,	 Acc = 0.4003
380 0.342
1718 0.454
815 0.336
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.40877862595419845
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9486,	 Acc1 = 0.3580,	 Acc2 = 0.3752

 ===== Epoch 172	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   1.9421588   0.81904715 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   1.8059874   0.8513978  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 2
train:	 Loss = 1.3705,	 Acc = 0.4801
2922 0.243
5677 0.527
2917 0.609
377 0.621
69 0.522
6 0.333
0 0.0
0 0.0
0.556710148131771
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.9629,	 Acc = 0.3960
380 0.347
1718 0.442
815 0.34
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.4030534351145038
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9402,	 Acc1 = 0.3571,	 Acc2 = 0.3743

 ===== Epoch 173	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 3 1
train:	 Loss = 1.3749,	 Acc = 0.4759
2927 0.234
5678 0.526
2913 0.6
375 0.624
69 0.58
6 0.167
0 0.0
0 0.0
0.5540316336688419
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.9468,	 Acc = 0.3773
380 0.189
1718 0.462
815 0.307
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.40458015267175573
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9624,	 Acc1 = 0.3266,	 Acc2 = 0.3698

 ===== Epoch 174	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.1347616   3.1362693
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.1862829   3.2471778
 -0.38555372 -0.3798593 ] 6 1
train:	 Loss = 1.3741,	 Acc = 0.4751
2926 0.245
5679 0.518
2914 0.604
374 0.623
69 0.507
6 0.333
0 0.0
0 0.0
0.5496571554965716
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.8743,	 Acc = 0.3980
380 0.347
1718 0.458
815 0.317
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.40534351145038167
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8953,	 Acc1 = 0.3520,	 Acc2 = 0.3680

 ===== Epoch 175	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 1
train:	 Loss = 1.3722,	 Acc = 0.4807
2928 0.242
5676 0.529
2913 0.606
376 0.636
69 0.507
6 0.333
0 0.0
0 0.0
0.558075221238938
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.9532,	 Acc = 0.3953
380 0.339
1718 0.452
815 0.324
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.4034351145038168
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9288,	 Acc1 = 0.3542,	 Acc2 = 0.3708

 ===== Epoch 176	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.0578576   0.7948702
 -0.38555372 -0.3798593 ] 1 6
train:	 Loss = 1.3781,	 Acc = 0.4760
2922 0.238
5681 0.525
2916 0.598
374 0.634
69 0.565
6 0.167
0 0.0
0 0.0
0.5530621269069201
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.9184,	 Acc = 0.3747
380 0.2
1718 0.435
815 0.346
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.4
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9502,	 Acc1 = 0.3282,	 Acc2 = 0.3718

 ===== Epoch 177	 =====
[ 1.6319873   1.4189571  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 5
train:	 Loss = 1.3699,	 Acc = 0.4795
2921 0.241
5680 0.526
2915 0.607
377 0.634
69 0.522
6 0.333
0 0.0
0 0.0
0.5565380789211893
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.9548,	 Acc = 0.4093
380 0.342
1718 0.473
815 0.331
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.41908396946564885
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9197,	 Acc1 = 0.3586,	 Acc2 = 0.3760

 ===== Epoch 178	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.1390613   2.9815788  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 5
train:	 Loss = 1.3730,	 Acc = 0.4809
2925 0.239
5675 0.531
2917 0.607
376 0.628
69 0.536
6 0.167
0 0.0
0 0.0
0.5593276567510782
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.9451,	 Acc = 0.3883
380 0.332
1718 0.455
815 0.294
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.3965648854961832
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9262,	 Acc1 = 0.3580,	 Acc2 = 0.3752

 ===== Epoch 179	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   1.706373    2.8059711
 -0.3640846  -0.3725409   0.8075886   0.9459609  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  1.8047359   2.752832
 -0.3640846  -0.3725409   0.7292054   0.92854136 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 2
train:	 Loss = 1.3731,	 Acc = 0.4816
2924 0.248
5678 0.528
2916 0.603
375 0.651
69 0.536
6 0.167
0 0.0
0 0.0
0.5571649712516585
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.9455,	 Acc = 0.3790
380 0.189
1718 0.466
815 0.306
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.4064885496183206
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9422,	 Acc1 = 0.3260,	 Acc2 = 0.3690

 ===== Epoch 180	 =====
[ 0.21529981  1.5379329  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 0.21495906  1.5404643  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.3788,	 Acc = 0.4754
2921 0.24
5683 0.522
2913 0.601
376 0.614
69 0.478
6 0.333
0 0.0
0 0.0
0.5512324527467669
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.9106,	 Acc = 0.3913
380 0.192
1718 0.467
815 0.345
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.4202290076335878
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9247,	 Acc1 = 0.3185,	 Acc2 = 0.3601

 ===== Epoch 181	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   2.9560034   0.8513978  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 5
train:	 Loss = 1.3684,	 Acc = 0.4844
2926 0.245
5679 0.528
2911 0.615
377 0.645
69 0.609
6 0.333
0 0.0
0 0.0
0.5617120106171201
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.8978,	 Acc = 0.4073
380 0.35
1718 0.471
815 0.323
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.41564885496183207
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9212,	 Acc1 = 0.3580,	 Acc2 = 0.3752

 ===== Epoch 182	 =====
[ 2.6610825   3.4617932  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.4167407   1.8176919
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.6773784   1.8423382
 -0.38555372 -0.3798593 ] 3 2
train:	 Loss = 1.3673,	 Acc = 0.4812
2921 0.246
5683 0.529
2912 0.602
377 0.645
69 0.522
6 0.167
0 0.0
0 0.0
0.5572012821929921
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.9201,	 Acc = 0.4133
380 0.339
1718 0.466
815 0.361
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.42404580152671756
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9210,	 Acc1 = 0.3485,	 Acc2 = 0.3638

 ===== Epoch 183	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   1.6108845   1.8791285
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  1.8750305   2.0371132
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.3714,	 Acc = 0.4780
2925 0.245
5680 0.523
2912 0.605
376 0.614
69 0.551
6 0.333
0 0.0
0 0.0
0.5533561871060488
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.8564,	 Acc = 0.4037
380 0.345
1718 0.453
815 0.345
85 0.247
2 0.0
0 0.0
0 0.0
0 0.0
0.4122137404580153
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8864,	 Acc1 = 0.3577,	 Acc2 = 0.3750

 ===== Epoch 184	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   1.7482007   2.5933497   4.118848    1.2508268
  3.3104832   2.9688056 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   1.7350397   2.4739015   3.7036443   1.1349894
  3.2853534   2.837588  ] 5 5
train:	 Loss = 1.3696,	 Acc = 0.4797
2923 0.242
5680 0.528
2914 0.605
376 0.622
69 0.536
6 0.167
0 0.0
0 0.0
0.5564400221116639
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.8462,	 Acc = 0.3997
380 0.339
1718 0.469
815 0.304
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.4083969465648855
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9029,	 Acc1 = 0.3555,	 Acc2 = 0.3723

 ===== Epoch 185	 =====
[ 5.654931    3.3959768  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  4.148092    1.4914327  -0.40141198 -0.40778467  3.4861917   2.1627402
 -0.38555372 -0.3798593 ] [ 4.3006225   3.3250978  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  4.0042977   1.4328347  -0.40141198 -0.40778467  3.8578303   2.0937304
 -0.38555372 -0.3798593 ] 5 5
train:	 Loss = 1.3725,	 Acc = 0.4774
2925 0.235
5676 0.527
2916 0.6
376 0.638
69 0.594
6 0.333
0 0.0
0 0.0
0.555678425301338
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.9447,	 Acc = 0.3927
380 0.2
1718 0.47
815 0.342
85 0.176
2 0.0
0 0.0
0 0.0
0 0.0
0.4206106870229008
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9253,	 Acc1 = 0.3258,	 Acc2 = 0.3688

 ===== Epoch 186	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  1.5434952   2.8894129  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  1.5985423   2.663392   -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 5
train:	 Loss = 1.3711,	 Acc = 0.4804
2927 0.245
5680 0.53
2909 0.599
377 0.639
69 0.565
6 0.167
0 0.0
0 0.0
0.5566862072779559
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.9584,	 Acc = 0.3757
380 0.205
1718 0.456
815 0.303
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.40038167938931296
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9199,	 Acc1 = 0.3293,	 Acc2 = 0.3730

 ===== Epoch 187	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 0
train:	 Loss = 1.3725,	 Acc = 0.4809
2922 0.237
5685 0.533
2909 0.607
377 0.621
69 0.493
6 0.167
0 0.0
0 0.0
0.5595843466725624
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.9009,	 Acc = 0.4147
380 0.332
1718 0.462
815 0.379
85 0.176
2 0.0
0 0.0
0 0.0
0 0.0
0.4267175572519084
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9255,	 Acc1 = 0.3528,	 Acc2 = 0.3690

 ===== Epoch 188	 =====
[-0.36602148 -0.3783333   2.7971213   1.4214841  -0.4409929  -0.3635196
  2.7382293   3.360987   -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   2.6624887   1.645281   -0.44088638 -0.36343393
  2.6298213   3.595379   -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.3679,	 Acc = 0.4808
2925 0.241
5681 0.53
2912 0.61
376 0.612
68 0.485
6 0.167
0 0.0
0 0.0
0.5584429945814442
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.9023,	 Acc = 0.3850
380 0.337
1718 0.441
815 0.312
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.3919847328244275
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8877,	 Acc1 = 0.3604,	 Acc2 = 0.3782

 ===== Epoch 189	 =====
[-0.36602148 -0.3783333   2.276839    2.2913644  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   2.123948    2.211509   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 2
train:	 Loss = 1.3697,	 Acc = 0.4807
2924 0.239
5679 0.529
2913 0.608
377 0.629
69 0.565
6 0.333
0 0.0
0 0.0
0.5589340999557718
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.9015,	 Acc = 0.3973
380 0.342
1718 0.451
815 0.333
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.40534351145038167
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9211,	 Acc1 = 0.3507,	 Acc2 = 0.3666

 ===== Epoch 190	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   0.8433909   3.0427315  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 2
train:	 Loss = 1.3695,	 Acc = 0.4824
2926 0.243
5678 0.533
2913 0.601
376 0.657
69 0.507
6 0.5
0 0.0
0 0.0
0.559721300597213
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.8382,	 Acc = 0.4160
380 0.339
1718 0.455
815 0.393
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.42709923664122135
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8991,	 Acc1 = 0.3379,	 Acc2 = 0.3511

 ===== Epoch 191	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   3.1985958   2.1031146  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 6
train:	 Loss = 1.3733,	 Acc = 0.4773
2923 0.235
5682 0.532
2912 0.594
376 0.622
69 0.536
6 0.167
0 0.0
0 0.0
0.555444997236042
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.9289,	 Acc = 0.3873
380 0.332
1718 0.442
815 0.319
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.39541984732824426
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9076,	 Acc1 = 0.3518,	 Acc2 = 0.3678

 ===== Epoch 192	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 1
train:	 Loss = 1.3689,	 Acc = 0.4790
2922 0.248
5679 0.525
2916 0.6
377 0.623
68 0.588
6 0.167
0 0.0
0 0.0
0.553614857395534
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.8857,	 Acc = 0.4063
380 0.339
1718 0.459
815 0.352
85 0.176
2 0.0
0 0.0
0 0.0
0 0.0
0.41603053435114506
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8970,	 Acc1 = 0.3514,	 Acc2 = 0.3673

 ===== Epoch 193	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 1
train:	 Loss = 1.3694,	 Acc = 0.4849
2922 0.247
5683 0.531
2911 0.615
377 0.605
69 0.609
6 0.167
0 0.0
0 0.0
0.5615741764315719
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.9303,	 Acc = 0.4147
380 0.342
1718 0.461
815 0.368
85 0.259
2 0.0
0 0.0
0 0.0
0 0.0
0.4251908396946565
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9053,	 Acc1 = 0.3499,	 Acc2 = 0.3656

 ===== Epoch 194	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.5211736   2.044438
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.5211736   2.0395088
 -0.38555372 -0.3798593 ] 0 3
train:	 Loss = 1.3690,	 Acc = 0.4831
2928 0.24
5680 0.532
2909 0.614
376 0.625
69 0.522
6 0.167
0 0.0
0 0.0
0.5618362831858407
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.9392,	 Acc = 0.3957
380 0.355
1718 0.447
815 0.323
85 0.247
2 0.0
0 0.0
0 0.0
0 0.0
0.4015267175572519
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9176,	 Acc1 = 0.3553,	 Acc2 = 0.3720

 ===== Epoch 195	 =====
[-0.36602148 -0.3783333   1.3134426   1.7363627  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.3770589   1.645281   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 5
train:	 Loss = 1.3649,	 Acc = 0.4856
2926 0.244
5679 0.535
2913 0.607
375 0.675
69 0.536
6 0.333
0 0.0
0 0.0
0.5638133156381332
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.9942,	 Acc = 0.3937
380 0.184
1718 0.467
815 0.36
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.42404580152671756
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9561,	 Acc1 = 0.3274,	 Acc2 = 0.3723

 ===== Epoch 196	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
train:	 Loss = 1.3757,	 Acc = 0.4745
2924 0.227
5677 0.524
2917 0.603
375 0.651
69 0.551
6 0.167
0 0.0
0 0.0
0.5544007076514816
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.9222,	 Acc = 0.4030
380 0.342
1718 0.467
815 0.323
85 0.165
2 0.0
0 0.0
0 0.0
0 0.0
0.4118320610687023
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9060,	 Acc1 = 0.3573,	 Acc2 = 0.3745

 ===== Epoch 197	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  4.217399    1.5595319 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  4.163169    1.6382623 ] 2 4
train:	 Loss = 1.3664,	 Acc = 0.4834
2926 0.239
5676 0.538
2916 0.606
375 0.605
69 0.551
6 0.167
0 0.0
0 0.0
0.5623755806237558
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.9344,	 Acc = 0.3743
380 0.195
1718 0.439
815 0.34
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.40038167938931296
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9264,	 Acc1 = 0.3167,	 Acc2 = 0.3579

 ===== Epoch 198	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 0
train:	 Loss = 1.3671,	 Acc = 0.4826
2924 0.243
5679 0.529
2913 0.615
377 0.631
69 0.464
6 0.167
0 0.0
0 0.0
0.5600398053958425
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.9482,	 Acc = 0.3800
380 0.334
1718 0.427
815 0.321
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.38664122137404583
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9472,	 Acc1 = 0.3400,	 Acc2 = 0.3536

 ===== Epoch 199	 =====
[-0.36602148 -0.3783333   1.7809255   2.9868155  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.821375    3.026877   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 4
train:	 Loss = 1.3727,	 Acc = 0.4782
2924 0.238
5683 0.526
2910 0.601
376 0.657
69 0.536
6 0.167
0 0.0
0 0.0
0.5557275541795665
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.9799,	 Acc = 0.4210
380 0.337
1718 0.475
815 0.373
85 0.176
2 0.0
0 0.0
0 0.0
0 0.0
0.43320610687022904
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9586,	 Acc1 = 0.3594,	 Acc2 = 0.3770

 ===== Epoch 200	 =====
[ 2.045796    2.1783757  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.150056    2.417693  ] [ 2.148564    2.2036896  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.0345426   2.4203174 ] 3 4
train:	 Loss = 1.3679,	 Acc = 0.4801
2924 0.249
5685 0.526
2911 0.601
374 0.639
68 0.574
6 0.333
0 0.0
0 0.0
0.554953560371517
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.9812,	 Acc = 0.3850
380 0.347
1718 0.441
815 0.304
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.39045801526717555
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9320,	 Acc1 = 0.3670,	 Acc2 = 0.3862

 ===== Epoch 201	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   3.4532006   2.0234826  -0.42342317 -0.42019257
  4.037514    0.9611812 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   3.5178533   2.0533447  -0.42342317 -0.42019257
  3.5516515   1.0294142 ] 2 2
train:	 Loss = 1.3690,	 Acc = 0.4803
2920 0.241
5681 0.528
2916 0.606
376 0.628
69 0.536
6 0.167
0 0.0
0 0.0
0.5574712643678161
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.8939,	 Acc = 0.3867
380 0.334
1718 0.451
815 0.301
85 0.165
2 0.0
0 0.0
0 0.0
0 0.0
0.39427480916030533
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9159,	 Acc1 = 0.3551,	 Acc2 = 0.3718

 ===== Epoch 202	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 1
train:	 Loss = 1.3690,	 Acc = 0.4827
2926 0.245
5678 0.531
2913 0.605
376 0.644
69 0.551
6 0.167
0 0.0
0 0.0
0.559610705596107
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.9804,	 Acc = 0.3890
380 0.35
1718 0.427
815 0.35
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.3946564885496183
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9391,	 Acc1 = 0.3600,	 Acc2 = 0.3777

 ===== Epoch 203	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   1.3333926   3.0014641  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   1.3333926   2.9989755  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.3725,	 Acc = 0.4801
2921 0.244
5682 0.526
2913 0.602
377 0.668
69 0.551
6 0.333
0 0.0
0 0.0
0.5564275450425555
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.9138,	 Acc = 0.3973
380 0.345
1718 0.455
815 0.323
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.4049618320610687
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9093,	 Acc1 = 0.3534,	 Acc2 = 0.3698

 ===== Epoch 204	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.6378552   1.1867466
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.7348365   1.2532916
 -0.38555372 -0.3798593 ] 1 2
train:	 Loss = 1.3672,	 Acc = 0.4823
2923 0.244
5676 0.531
2917 0.603
377 0.658
69 0.551
6 0.167
0 0.0
0 0.0
0.559314538419016
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.9633,	 Acc = 0.4073
380 0.347
1718 0.454
815 0.358
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.41603053435114506
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9308,	 Acc1 = 0.3641,	 Acc2 = 0.3827

 ===== Epoch 205	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   2.8946233   2.9660015
 -0.3640846  -0.3725409   1.9284266   2.3146374  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  2.8954394   2.9662137
 -0.3640846  -0.3725409   1.9284266   2.3146374  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.3685,	 Acc = 0.4830
2926 0.242
5676 0.532
2915 0.612
376 0.614
69 0.565
6 0.167
0 0.0
0 0.0
0.5608272506082725
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 2.0372,	 Acc = 0.3793
380 0.353
1718 0.421
815 0.324
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.383206106870229
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9574,	 Acc1 = 0.3619,	 Acc2 = 0.3800

 ===== Epoch 206	 =====
[-0.36602148 -0.3783333   2.438894    2.193956   -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   2.6933553   2.1933897  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.3658,	 Acc = 0.4817
2924 0.243
5679 0.528
2915 0.611
376 0.628
68 0.515
6 0.167
0 0.0
0 0.0
0.5588235294117647
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.8852,	 Acc = 0.3953
380 0.342
1718 0.438
815 0.352
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.4030534351145038
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8953,	 Acc1 = 0.3542,	 Acc2 = 0.3708

 ===== Epoch 207	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   2.6413229   1.4685465  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  1.8887831   2.93065
 -0.3640846  -0.3725409   2.896501    1.2420926  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 2
train:	 Loss = 1.3763,	 Acc = 0.4776
2926 0.242
5677 0.53
2915 0.598
376 0.601
68 0.426
6 0.333
0 0.0
0 0.0
0.5537491705374917
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.9295,	 Acc = 0.3813
380 0.195
1718 0.461
815 0.32
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.4083969465648855
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9264,	 Acc1 = 0.3359,	 Acc2 = 0.3810

 ===== Epoch 208	 =====
[-0.36602148 -0.3783333   2.4831648   2.4386096  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   2.489067    2.417616   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 2
train:	 Loss = 1.3700,	 Acc = 0.4850
2925 0.241
5685 0.533
2909 0.615
374 0.642
69 0.507
6 0.167
0 0.0
0 0.0
0.563750967599248
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 2.0069,	 Acc = 0.3920
380 0.339
1718 0.448
815 0.318
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.39961832061068703
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9707,	 Acc1 = 0.3495,	 Acc2 = 0.3651

 ===== Epoch 209	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 1
train:	 Loss = 1.3653,	 Acc = 0.4861
2924 0.244
5681 0.538
2911 0.605
377 0.658
69 0.536
6 0.333
0 0.0
0 0.0
0.5643520566121185
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.9401,	 Acc = 0.3933
380 0.337
1718 0.461
815 0.299
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.4015267175572519
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9520,	 Acc1 = 0.3563,	 Acc2 = 0.3733

 ===== Epoch 210	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 1
train:	 Loss = 1.3673,	 Acc = 0.4822
2926 0.238
5679 0.533
2911 0.608
377 0.634
69 0.522
6 0.167
0 0.0
0 0.0
0.5613802256138023
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.9447,	 Acc = 0.4173
380 0.345
1718 0.445
815 0.413
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.42786259541984734
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9306,	 Acc1 = 0.3505,	 Acc2 = 0.3663

 ===== Epoch 211	 =====
[-0.36602148 -0.3783333   0.64856964  1.3059531  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   0.6614416   1.282895   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.3741,	 Acc = 0.4817
2919 0.242
5681 0.527
2916 0.613
377 0.647
69 0.507
6 0.167
0 0.0
0 0.0
0.5590673002541717
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.9048,	 Acc = 0.3897
380 0.189
1718 0.445
815 0.387
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.41870229007633586
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8985,	 Acc1 = 0.3241,	 Acc2 = 0.3668

 ===== Epoch 212	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   2.6272569   2.3770008
 -0.3640846  -0.3725409   2.572664    1.2396042  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  2.8373704   2.366077
 -0.3640846  -0.3725409   2.53948     1.2321388  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 3
train:	 Loss = 1.3691,	 Acc = 0.4834
2920 0.246
5683 0.53
2913 0.608
377 0.645
69 0.551
6 0.333
0 0.0
0 0.0
0.5601237842617153
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.8569,	 Acc = 0.3967
380 0.334
1718 0.45
815 0.333
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.40572519083969466
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8866,	 Acc1 = 0.3571,	 Acc2 = 0.3743

 ===== Epoch 213	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   0.41384917  1.3101315
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  0.38624856  1.6170174
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.3691,	 Acc = 0.4815
2925 0.245
5680 0.529
2913 0.605
375 0.645
69 0.522
6 0.167
0 0.0
0 0.0
0.5581112462678315
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.8756,	 Acc = 0.3780
380 0.192
1718 0.451
815 0.331
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.4049618320610687
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8690,	 Acc1 = 0.3348,	 Acc2 = 0.3797

 ===== Epoch 214	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   1.5090415   2.3245914  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   1.5084702   2.237494   -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 3 5
train:	 Loss = 1.3691,	 Acc = 0.4818
2922 0.245
5680 0.53
2915 0.604
376 0.657
69 0.435
6 0.167
0 0.0
0 0.0
0.5581472474021667
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.9285,	 Acc = 0.4003
380 0.345
1718 0.463
815 0.315
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.4083969465648855
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9461,	 Acc1 = 0.3493,	 Acc2 = 0.3648

 ===== Epoch 215	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   1.1983653   3.292619   -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 1.5035932   0.83926773 -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.075105    1.0241655 ] 6 1
train:	 Loss = 1.3726,	 Acc = 0.4799
2924 0.243
5676 0.527
2916 0.61
377 0.605
69 0.493
6 0.167
0 0.0
0 0.0
0.556390977443609
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.8558,	 Acc = 0.3953
380 0.35
1718 0.447
815 0.328
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.4019083969465649
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8933,	 Acc1 = 0.3557,	 Acc2 = 0.3725

 ===== Epoch 216	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   2.8946233   2.9660015
 -0.3640846  -0.3725409   1.9284266   2.3146374  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  2.8954394   2.9662137
 -0.3640846  -0.3725409   1.9284266   2.3146374  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.3720,	 Acc = 0.4799
2925 0.245
5681 0.527
2911 0.606
376 0.614
69 0.565
6 0.167
0 0.0
0 0.0
0.5560101736149508
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.9781,	 Acc = 0.4010
380 0.334
1718 0.46
815 0.33
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.41068702290076337
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9716,	 Acc1 = 0.3414,	 Acc2 = 0.3554

 ===== Epoch 217	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.226186    1.5391887
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.4307575   1.6698141
 -0.38555372 -0.3798593 ] 2 6
train:	 Loss = 1.3720,	 Acc = 0.4781
2927 0.235
5679 0.524
2913 0.61
374 0.639
69 0.594
6 0.167
0 0.0
0 0.0
0.5567968145116691
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.9188,	 Acc = 0.4083
380 0.345
1718 0.472
815 0.326
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.41755725190839693
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9137,	 Acc1 = 0.3654,	 Acc2 = 0.3842

 ===== Epoch 218	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  1.8765587   0.78571683
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 3 6
train:	 Loss = 1.3662,	 Acc = 0.4804
2921 0.239
5680 0.528
2915 0.608
377 0.637
69 0.522
6 0.167
0 0.0
0 0.0
0.5581960871006963
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.9602,	 Acc = 0.3993
380 0.353
1718 0.448
815 0.341
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.40610687022900765
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9539,	 Acc1 = 0.3520,	 Acc2 = 0.3680

 ===== Epoch 219	 =====
[-0.36602148 -0.3783333   1.2500825   2.2777724  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.2499363   2.3813772  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.3685,	 Acc = 0.4821
2927 0.241
5681 0.528
2911 0.617
375 0.635
68 0.485
6 0.167
0 0.0
0 0.0
0.5603362459904878
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 2.0421,	 Acc = 0.3773
380 0.205
1718 0.443
815 0.337
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.4022900763358779
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 2.0048,	 Acc1 = 0.3251,	 Acc2 = 0.3680

 ===== Epoch 220	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.2048461   1.7930456
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.2021936   1.7659348
 -0.38555372 -0.3798593 ] 4 3
train:	 Loss = 1.3679,	 Acc = 0.4849
2925 0.244
5678 0.534
2913 0.611
377 0.639
69 0.551
6 0.333
0 0.0
0 0.0
0.562866305429614
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.9148,	 Acc = 0.4053
380 0.345
1718 0.449
815 0.363
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.41412213740458015
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9282,	 Acc1 = 0.3497,	 Acc2 = 0.3653

 ===== Epoch 221	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.5935313   2.512718
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.696196    2.6778479
 -0.38555372 -0.3798593 ] 2 6
train:	 Loss = 1.3711,	 Acc = 0.4845
2924 0.244
5679 0.534
2913 0.612
377 0.621
69 0.478
6 0.167
0 0.0
0 0.0
0.5622512162759841
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.9888,	 Acc = 0.4183
380 0.339
1718 0.455
815 0.4
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.4297709923664122
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9661,	 Acc1 = 0.3464,	 Acc2 = 0.3613

 ===== Epoch 222	 =====
[ 0.28750363  1.2215084  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 0.2405542   1.001277   -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   2.64075     3.342389   -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 0
train:	 Loss = 1.3643,	 Acc = 0.4815
2917 0.243
5683 0.527
2916 0.61
377 0.626
69 0.565
6 0.5
0 0.0
0 0.0
0.5581703679151475
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.9164,	 Acc = 0.3810
380 0.192
1718 0.441
815 0.364
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.4083969465648855
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9082,	 Acc1 = 0.3289,	 Acc2 = 0.3725

 ===== Epoch 223	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   1.0627667   2.5211833  -0.42342317 -0.42019257
  2.60682     1.1396368 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   1.0433146   2.5709534  -0.42342317 -0.42019257
  2.3246496   1.1028959 ] 1 1
train:	 Loss = 1.3673,	 Acc = 0.4835
2928 0.245
5681 0.535
2910 0.604
374 0.639
69 0.493
6 0.167
0 0.0
0 0.0
0.5608407079646017
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.9532,	 Acc = 0.4163
380 0.337
1718 0.463
815 0.378
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.42786259541984734
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9292,	 Acc1 = 0.3571,	 Acc2 = 0.3743

 ===== Epoch 224	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  4.2351565   1.7537283   1.070204    0.9708459   2.8728578  -4.962507
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 2
train:	 Loss = 1.3653,	 Acc = 0.4802
2926 0.237
5684 0.528
2908 0.61
375 0.643
69 0.551
6 0.333
0 0.0
0 0.0
0.5590577305905773
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.9446,	 Acc = 0.4117
380 0.355
1718 0.464
815 0.35
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.4198473282442748
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9392,	 Acc1 = 0.3470,	 Acc2 = 0.3621

 ===== Epoch 225	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 1
train:	 Loss = 1.3669,	 Acc = 0.4815
2922 0.243
5681 0.526
2914 0.61
376 0.652
69 0.536
6 0.167
0 0.0
0 0.0
0.558478885695335
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.9959,	 Acc = 0.3947
380 0.345
1718 0.451
815 0.319
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.4019083969465649
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9669,	 Acc1 = 0.3454,	 Acc2 = 0.3601

 ===== Epoch 226	 =====
[ 2.6216717   1.8923281  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.2016401   2.1316392 ] [ 2.255651    1.6569084  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.3850522   1.8744533 ] 4 5
train:	 Loss = 1.3645,	 Acc = 0.4826
2924 0.241
5682 0.53
2911 0.613
376 0.646
69 0.478
6 0.167
0 0.0
0 0.0
0.5608137992038921
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.9639,	 Acc = 0.3827
380 0.337
1718 0.435
815 0.312
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.3893129770992366
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9607,	 Acc1 = 0.3557,	 Acc2 = 0.3725

 ===== Epoch 227	 =====
[-0.36602148 -0.3783333   1.3638052   2.4589977  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.4696574   2.5512457  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.3747,	 Acc = 0.4820
2923 0.237
5684 0.529
2913 0.616
373 0.625
69 0.565
6 0.167
0 0.0
0 0.0
0.5609729132117192
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.9924,	 Acc = 0.4170
380 0.35
1718 0.473
815 0.355
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.4267175572519084
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9637,	 Acc1 = 0.3608,	 Acc2 = 0.3787

 ===== Epoch 228	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   1.3745872   1.59546    -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   1.3654325   1.7099313  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 4
train:	 Loss = 1.3664,	 Acc = 0.4887
2922 0.248
5683 0.539
2913 0.613
376 0.638
69 0.522
5 0.2
0 0.0
0 0.0
0.5665487508290957
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.9262,	 Acc = 0.4123
380 0.345
1718 0.447
815 0.395
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.4221374045801527
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9147,	 Acc1 = 0.3532,	 Acc2 = 0.3695

 ===== Epoch 229	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   3.5450864   2.4058952
 -0.3640846  -0.3725409   3.8760176   1.7497473  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  3.546041    2.4060862
 -0.3640846  -0.3725409   3.8760176   1.7497473  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.3672,	 Acc = 0.4791
2925 0.246
5679 0.523
2913 0.607
376 0.633
69 0.522
6 0.333
0 0.0
0 0.0
0.5545725975892956
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.9011,	 Acc = 0.4117
380 0.347
1718 0.448
815 0.39
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.4209923664122137
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8903,	 Acc1 = 0.3435,	 Acc2 = 0.3579

 ===== Epoch 230	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   1.7163029   1.9769248
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  1.7115207   1.9837676
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.3690,	 Acc = 0.4823
2926 0.241
5679 0.532
2913 0.607
375 0.632
69 0.609
6 0.167
0 0.0
0 0.0
0.5604954656049547
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.9451,	 Acc = 0.4110
380 0.337
1718 0.464
815 0.355
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.4217557251908397
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9229,	 Acc1 = 0.3646,	 Acc2 = 0.3832

 ===== Epoch 231	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  0.75668305  0.82198125
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  0.8381332   0.80226415
 -0.38555372 -0.3798593 ] 2 3
train:	 Loss = 1.3641,	 Acc = 0.4862
2924 0.242
5680 0.536
2913 0.614
377 0.645
69 0.493
5 0.2
0 0.0
0 0.0
0.565015479876161
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.9839,	 Acc = 0.3923
380 0.347
1718 0.441
815 0.331
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.3988549618320611
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9965,	 Acc1 = 0.3384,	 Acc2 = 0.3516

 ===== Epoch 232	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   1.6830734   2.825975
 -0.3640846  -0.3725409   3.4091444   1.2420926  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  1.607607    2.63725
 -0.3640846  -0.3725409   3.100757    0.9882654  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 5
train:	 Loss = 1.3677,	 Acc = 0.4853
2925 0.24
5675 0.533
2918 0.617
375 0.651
69 0.536
6 0.0
0 0.0
0 0.0
0.564635629768882
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.9577,	 Acc = 0.3987
380 0.337
1718 0.443
815 0.352
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.4076335877862595
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9354,	 Acc1 = 0.3462,	 Acc2 = 0.3611

 ===== Epoch 233	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 3 1
train:	 Loss = 1.3654,	 Acc = 0.4809
2920 0.241
5685 0.531
2911 0.604
377 0.631
69 0.522
6 0.167
0 0.0
0 0.0
0.5582449160035367
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.9296,	 Acc = 0.4230
380 0.35
1718 0.463
815 0.396
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.433587786259542
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9464,	 Acc1 = 0.3530,	 Acc2 = 0.3693

 ===== Epoch 234	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.2227771   2.2909012
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.9897922   2.064155
 -0.38555372 -0.3798593 ] 5 5
train:	 Loss = 1.3676,	 Acc = 0.4838
2926 0.242
5679 0.529
2912 0.62
376 0.628
69 0.551
6 0.167
0 0.0
0 0.0
0.5621543906215439
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.9072,	 Acc = 0.3763
380 0.197
1718 0.448
815 0.33
85 0.176
2 0.0
0 0.0
0 0.0
0 0.0
0.4022900763358779
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9294,	 Acc1 = 0.3264,	 Acc2 = 0.3695

 ===== Epoch 235	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 0
train:	 Loss = 1.3628,	 Acc = 0.4856
2922 0.241
5685 0.534
2910 0.615
376 0.652
69 0.522
6 0.167
0 0.0
0 0.0
0.564669467167809
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.9256,	 Acc = 0.3997
380 0.339
1718 0.449
815 0.342
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.4083969465648855
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9535,	 Acc1 = 0.3445,	 Acc2 = 0.3591

 ===== Epoch 236	 =====
[-0.36602148 -0.3783333   1.1018372   1.4577292  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.1195657   1.513916   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 3 3
train:	 Loss = 1.3634,	 Acc = 0.4852
2924 0.239
5682 0.538
2913 0.612
374 0.615
69 0.58
6 0.167
0 0.0
0 0.0
0.5647943387881469
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.9070,	 Acc = 0.4173
380 0.35
1718 0.449
815 0.404
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.42709923664122135
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9257,	 Acc1 = 0.3470,	 Acc2 = 0.3621

 ===== Epoch 237	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.4529825   2.5077884
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.6037596   2.6285553
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.3662,	 Acc = 0.4855
2925 0.243
5681 0.536
2912 0.607
375 0.653
69 0.58
6 0.167
0 0.0
0 0.0
0.5638615503704523
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.9768,	 Acc = 0.4207
380 0.339
1718 0.471
815 0.377
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.43244274809160305
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9421,	 Acc1 = 0.3629,	 Acc2 = 0.3812

 ===== Epoch 238	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 1
train:	 Loss = 1.3694,	 Acc = 0.4840
2923 0.244
5679 0.534
2915 0.611
377 0.61
68 0.5
6 0.167
0 0.0
0 0.0
0.5615257048092869
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.8465,	 Acc = 0.3937
380 0.203
1718 0.449
815 0.387
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.4213740458015267
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9101,	 Acc1 = 0.3152,	 Acc2 = 0.3561

 ===== Epoch 239	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.3650,	 Acc = 0.4879
2925 0.244
5676 0.537
2917 0.618
375 0.637
69 0.493
6 0.167
0 0.0
0 0.0
0.5668472851929669
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.9512,	 Acc = 0.4040
380 0.345
1718 0.46
815 0.336
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.4125954198473282
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9668,	 Acc1 = 0.3503,	 Acc2 = 0.3661

 ===== Epoch 240	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   2.52222     2.1302872
 -0.3640846  -0.3725409   2.826698    0.9807999  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  2.5218112   2.021554
 -0.3640846  -0.3725409   2.775206    0.85637474 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 5
train:	 Loss = 1.3671,	 Acc = 0.4843
2926 0.251
5680 0.53
2914 0.607
374 0.644
68 0.574
6 0.167
0 0.0
0 0.0
0.5598318955983189
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.9322,	 Acc = 0.3907
380 0.326
1718 0.457
815 0.303
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.4
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9443,	 Acc1 = 0.3437,	 Acc2 = 0.3581

 ===== Epoch 241	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.6311612   1.1004845
  2.699407    3.3677058 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.861114    1.1547064
  2.5279005   3.4201927 ] 2 2
train:	 Loss = 1.3605,	 Acc = 0.4844
2927 0.237
5679 0.531
2911 0.619
376 0.646
69 0.536
6 0.167
0 0.0
0 0.0
0.5643181064041588
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.9886,	 Acc = 0.3953
380 0.337
1718 0.451
815 0.329
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.40381679389312974
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9605,	 Acc1 = 0.3559,	 Acc2 = 0.3728

 ===== Epoch 242	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   4.2856903   2.7015069
 -0.3640846  -0.3725409   4.645556    2.2001662  -0.42342317 -0.42019257
  3.0199363   0.98480034] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  4.3960633   2.7506092
 -0.3640846  -0.3725409   4.76342     2.2449594  -0.42342317 -0.42019257
  2.8925176   1.0714037 ] 2 2
train:	 Loss = 1.3676,	 Acc = 0.4815
2922 0.241
5684 0.533
2910 0.604
377 0.621
69 0.58
6 0.167
0 0.0
0 0.0
0.5592527083793942
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.9524,	 Acc = 0.3957
380 0.35
1718 0.457
815 0.307
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.4022900763358779
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9168,	 Acc1 = 0.3635,	 Acc2 = 0.3820

 ===== Epoch 243	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.18906     1.8620553
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.1947432   1.8867016
 -0.38555372 -0.3798593 ] 3 4
train:	 Loss = 1.3676,	 Acc = 0.4830
2927 0.236
5674 0.533
2919 0.614
373 0.633
69 0.507
6 0.167
0 0.0
0 0.0
0.5628802123658887
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.9129,	 Acc = 0.4217
380 0.337
1718 0.481
815 0.362
85 0.176
2 0.0
0 0.0
0 0.0
0 0.0
0.43396946564885497
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9125,	 Acc1 = 0.3509,	 Acc2 = 0.3668

 ===== Epoch 244	 =====
[-0.36602148 -0.3783333   2.3068943   2.1169353  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   2.239697    2.281721   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.3704,	 Acc = 0.4833
2925 0.236
5680 0.532
2912 0.616
377 0.637
68 0.559
6 0.167
0 0.0
0 0.0
0.5633086365144311
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.9342,	 Acc = 0.3910
380 0.197
1718 0.45
815 0.377
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.41908396946564885
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9271,	 Acc1 = 0.3208,	 Acc2 = 0.3628

 ===== Epoch 245	 =====
[ 0.48293656  1.8746083  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  3.932399    1.7202438  -0.40141198 -0.40778467  4.484803    1.9162773
  4.6071477   3.5120451 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  4.287955    1.6979207  -0.40141198 -0.40778467  3.8729832   1.9064187
  5.1944165   3.498923  ] 2 2
train:	 Loss = 1.3671,	 Acc = 0.4836
2923 0.236
5679 0.535
2915 0.614
377 0.621
68 0.515
6 0.167
0 0.0
0 0.0
0.5635157545605307
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.9554,	 Acc = 0.3900
380 0.339
1718 0.449
815 0.313
85 0.176
2 0.0
0 0.0
0 0.0
0 0.0
0.3973282442748092
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9320,	 Acc1 = 0.3584,	 Acc2 = 0.3757

 ===== Epoch 246	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   1.1685851   2.7059522
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  1.1904275   2.7772818
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 3 1
train:	 Loss = 1.3690,	 Acc = 0.4826
2923 0.235
5678 0.53
2915 0.616
377 0.653
69 0.522
6 0.167
0 0.0
0 0.0
0.5626312880044223
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 2.0030,	 Acc = 0.3950
380 0.342
1718 0.448
815 0.33
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.4026717557251908
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9787,	 Acc1 = 0.3518,	 Acc2 = 0.3678

 ===== Epoch 247	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 3 1
train:	 Loss = 1.3648,	 Acc = 0.4845
2922 0.24
5681 0.535
2913 0.613
377 0.631
69 0.536
6 0.167
0 0.0
0 0.0
0.5635640061905814
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.8939,	 Acc = 0.3917
380 0.195
1718 0.458
815 0.363
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.4202290076335878
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9041,	 Acc1 = 0.3318,	 Acc2 = 0.3760

 ===== Epoch 248	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.7757397   3.4490607 ] [ 2.139739    3.2086535  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.0235205   3.4700556 ] 4 2
train:	 Loss = 1.3697,	 Acc = 0.4814
2927 0.243
5678 0.528
2911 0.613
377 0.605
69 0.565
6 0.167
0 0.0
0 0.0
0.5586771374847915
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.9832,	 Acc = 0.3850
380 0.197
1718 0.453
815 0.35
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.4122137404580153
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9675,	 Acc1 = 0.3258,	 Acc2 = 0.3688

 ===== Epoch 249	 =====
[-0.36602148 -0.3783333   3.0594962   2.3774464  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   2.7831113  -4.7848034  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 2
train:	 Loss = 1.3646,	 Acc = 0.4866
2923 0.244
5683 0.536
2912 0.614
375 0.627
69 0.58
6 0.167
0 0.0
0 0.0
0.5649530127142067
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.9511,	 Acc = 0.4027
380 0.337
1718 0.457
815 0.344
85 0.176
2 0.0
0 0.0
0 0.0
0 0.0
0.4122137404580153
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9470,	 Acc1 = 0.3598,	 Acc2 = 0.3775

 ===== Epoch 250	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 1
train:	 Loss = 1.3643,	 Acc = 0.4821
2924 0.238
5679 0.531
2914 0.617
376 0.606
69 0.493
6 0.167
0 0.0
0 0.0
0.5611455108359134
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.9586,	 Acc = 0.4047
380 0.334
1718 0.468
815 0.326
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.4148854961832061
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9316,	 Acc1 = 0.3569,	 Acc2 = 0.3740

 ===== Epoch 251	 =====
[ 1.8555483   1.3835177  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.6668382   1.6303895 ] [ 1.7279363   1.2265712  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.7122499   1.4545581 ] 5 5
train:	 Loss = 1.3658,	 Acc = 0.4823
2923 0.248
5683 0.526
2911 0.613
376 0.625
69 0.551
6 0.167
0 0.0
0 0.0
0.558098396904367
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.9633,	 Acc = 0.3863
380 0.205
1718 0.458
815 0.34
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.4125954198473282
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9629,	 Acc1 = 0.3208,	 Acc2 = 0.3628

 ===== Epoch 252	 =====
[-0.36602148 -0.3783333   1.1375788   1.1745651  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.1349998   1.2013582  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 3
train:	 Loss = 1.3698,	 Acc = 0.4844
2921 0.238
5680 0.532
2916 0.621
376 0.617
69 0.551
6 0.167
0 0.0
0 0.0
0.5640543826682878
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.8937,	 Acc = 0.3937
380 0.321
1718 0.439
815 0.353
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.40419847328244274
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9083,	 Acc1 = 0.3332,	 Acc2 = 0.3454

 ===== Epoch 253	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.235657    1.5317948
 -0.38555372 -0.3798593 ] [ 2.3669856   3.4466047  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.2223976   1.5761582
 -0.38555372 -0.3798593 ] 2 2
train:	 Loss = 1.3684,	 Acc = 0.4809
2926 0.238
5679 0.53
2912 0.612
376 0.622
69 0.507
6 0.167
0 0.0
0 0.0
0.5595001105950012
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.9640,	 Acc = 0.3957
380 0.337
1718 0.446
815 0.337
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.40419847328244274
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9415,	 Acc1 = 0.3400,	 Acc2 = 0.3536

 ===== Epoch 254	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   2.193331    2.1255112  -0.42342317 -0.42019257
  3.1539676   1.1553828 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   2.2064898   1.991132   -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 2
train:	 Loss = 1.3638,	 Acc = 0.4819
2924 0.244
5680 0.527
2913 0.611
376 0.649
69 0.522
6 0.167
0 0.0
0 0.0
0.5589340999557718
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.9448,	 Acc = 0.4033
380 0.355
1718 0.458
815 0.335
85 0.176
2 0.0
0 0.0
0 0.0
0 0.0
0.41030534351145037
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9635,	 Acc1 = 0.3489,	 Acc2 = 0.3643

 ===== Epoch 255	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   1.921793    2.1747403
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  1.8987163   2.1927042
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 3
train:	 Loss = 1.3649,	 Acc = 0.4825
2922 0.24
5683 0.529
2912 0.616
376 0.628
69 0.493
6 0.167
0 0.0
0 0.0
0.5606898076497899
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.8529,	 Acc = 0.4330
380 0.332
1718 0.49
815 0.385
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.44770992366412216
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8945,	 Acc1 = 0.3549,	 Acc2 = 0.3715

 ===== Epoch 256	 =====
[-0.36602148 -0.3783333   1.9255159   1.0884831  -0.4409929  -0.3635196
  1.465981    3.757221   -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   2.0069802   1.0767881  -0.44088638 -0.36343393
  1.4609263   3.773963   -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 0
train:	 Loss = 1.3700,	 Acc = 0.4825
2925 0.24
5682 0.533
2910 0.605
376 0.641
69 0.522
6 0.167
0 0.0
0 0.0
0.5607652327767334
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.9854,	 Acc = 0.4207
380 0.347
1718 0.478
815 0.355
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.4312977099236641
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9440,	 Acc1 = 0.3561,	 Acc2 = 0.3730

 ===== Epoch 257	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 3 1
train:	 Loss = 1.3662,	 Acc = 0.4849
2923 0.244
5682 0.533
2911 0.618
377 0.605
69 0.522
6 0.167
0 0.0
0 0.0
0.5626312880044223
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.9403,	 Acc = 0.4060
380 0.35
1718 0.455
815 0.353
85 0.176
2 0.0
0 0.0
0 0.0
0 0.0
0.41412213740458015
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9466,	 Acc1 = 0.3489,	 Acc2 = 0.3643

 ===== Epoch 258	 =====
[ 3.3516128   1.6366572  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   2.6201534   3.3249695  -0.42342317 -0.42019257
  3.1962924   2.052909  ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   2.814112    3.123401   -0.42342317 -0.42019257
  3.1164908   1.8429613 ] 5 5
train:	 Loss = 1.3658,	 Acc = 0.4843
2925 0.238
5679 0.532
2913 0.618
376 0.636
69 0.565
6 0.167
0 0.0
0 0.0
0.5639721331416565
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.9393,	 Acc = 0.4127
380 0.347
1718 0.459
815 0.372
85 0.176
2 0.0
0 0.0
0 0.0
0 0.0
0.4221374045801527
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9404,	 Acc1 = 0.3509,	 Acc2 = 0.3668

 ===== Epoch 259	 =====
[ 2.2705994   1.7404444  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.2276533   1.9610568 ] [ 2.2035096   1.5227444  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.214426    1.7222414 ] 5 5
train:	 Loss = 1.3618,	 Acc = 0.4870
2928 0.247
5679 0.535
2911 0.617
375 0.621
69 0.522
6 0.167
0 0.0
0 0.0
0.5646017699115045
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.9855,	 Acc = 0.3987
380 0.347
1718 0.46
815 0.314
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.40610687022900765
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9671,	 Acc1 = 0.3602,	 Acc2 = 0.3780

 ===== Epoch 260	 =====
[ 0.4711559   2.5530224  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 0.583039    2.537834   -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 3 0
train:	 Loss = 1.3624,	 Acc = 0.4848
2924 0.237
5682 0.536
2911 0.615
376 0.638
69 0.507
6 0.167
0 0.0
0 0.0
0.565015479876161
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.9906,	 Acc = 0.3927
380 0.347
1718 0.443
815 0.325
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.39923664122137403
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9572,	 Acc1 = 0.3481,	 Acc2 = 0.3633

 ===== Epoch 261	 =====
[ 3.390513    2.9377942   3.1118906   0.8234415  -0.4409929  -0.3635196
  2.1905715   2.1360104  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   3.223773    0.9227741  -0.44088638 -0.36343393
  2.1534998   2.2420447  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.3651,	 Acc = 0.4848
2926 0.242
5676 0.536
2915 0.609
376 0.638
69 0.507
6 0.167
0 0.0
0 0.0
0.5632603406326034
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.8869,	 Acc = 0.4323
380 0.347
1718 0.477
815 0.405
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.4446564885496183
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9146,	 Acc1 = 0.3435,	 Acc2 = 0.3579

 ===== Epoch 262	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  3.1786008   1.287735   -0.40141198 -0.40778467  2.7163985   2.3894863
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  3.3712637   1.10357    -0.40141198 -0.40778467  2.7258692   2.251467
 -0.38555372 -0.3798593 ] 5 5
train:	 Loss = 1.3722,	 Acc = 0.4815
2924 0.239
5680 0.531
2913 0.614
376 0.62
69 0.391
6 0.167
0 0.0
0 0.0
0.5598186643078283
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.9682,	 Acc = 0.3663
380 0.187
1718 0.445
815 0.304
85 0.176
2 0.0
0 0.0
0 0.0
0 0.0
0.39236641221374047
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9694,	 Acc1 = 0.3291,	 Acc2 = 0.3728

 ===== Epoch 263	 =====
[ 2.3609388   2.704906   -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.9569452   2.955684  ] [ 2.3570883   2.6871862  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.9569452   2.955684  ] 0 0
train:	 Loss = 1.3657,	 Acc = 0.4870
2926 0.241
5679 0.536
2912 0.619
376 0.646
69 0.464
6 0.167
0 0.0
0 0.0
0.5666887856668879
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.9489,	 Acc = 0.3987
380 0.337
1718 0.45
815 0.342
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.4076335877862595
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9538,	 Acc1 = 0.3452,	 Acc2 = 0.3598

 ===== Epoch 264	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  1.6945926   1.089618   -0.40141198 -0.40778467  2.0227518   3.084512
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  1.6945926   1.089618   -0.40141198 -0.40778467  2.036389    3.0771184
 -0.38555372 -0.3798593 ] 3 0
train:	 Loss = 1.3611,	 Acc = 0.4823
2924 0.24
5681 0.529
2912 0.615
376 0.644
69 0.42
6 0.333
0 0.0
0 0.0
0.560703228659885
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 2.0187,	 Acc = 0.3953
380 0.324
1718 0.449
815 0.337
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.40572519083969466
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9524,	 Acc1 = 0.3577,	 Acc2 = 0.3750

 ===== Epoch 265	 =====
[ 2.2718456   1.3531408  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   2.6773672   2.797407   -0.42342317 -0.42019257
  3.2320058   1.5122937 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   2.631024    2.6356544  -0.42342317 -0.42019257
  3.4299653   1.3049705 ] 5 5
train:	 Loss = 1.3640,	 Acc = 0.4845
2922 0.247
5679 0.532
2916 0.612
377 0.626
68 0.5
6 0.333
0 0.0
0 0.0
0.5613530842361265
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.9044,	 Acc = 0.4057
380 0.347
1718 0.448
815 0.363
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.41412213740458015
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9065,	 Acc1 = 0.3569,	 Acc2 = 0.3740

 ===== Epoch 266	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   2.7168462   2.1080916  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   2.715702    2.0359251  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 2
train:	 Loss = 1.3608,	 Acc = 0.4861
2919 0.246
5684 0.533
2914 0.616
376 0.63
69 0.565
6 0.167
0 0.0
0 0.0
0.5637086970936015
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 2.0446,	 Acc = 0.4003
380 0.35
1718 0.453
815 0.334
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.4076335877862595
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9826,	 Acc1 = 0.3544,	 Acc2 = 0.3710

 ===== Epoch 267	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 1
train:	 Loss = 1.3637,	 Acc = 0.4845
2925 0.246
5675 0.533
2916 0.609
377 0.639
69 0.565
6 0.333
0 0.0
0 0.0
0.5617604777175716
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.9840,	 Acc = 0.4127
380 0.345
1718 0.469
815 0.35
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.42251908396946564
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9330,	 Acc1 = 0.3491,	 Acc2 = 0.3646

 ===== Epoch 268	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   1.2647355   3.0039527  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   1.3385413   3.0039527  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 0
train:	 Loss = 1.3636,	 Acc = 0.4865
2922 0.243
5677 0.534
2918 0.618
376 0.644
69 0.536
6 0.167
0 0.0
0 0.0
0.5652221976564227
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.9913,	 Acc = 0.4117
380 0.342
1718 0.483
815 0.318
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.4217557251908397
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9426,	 Acc1 = 0.3691,	 Acc2 = 0.3887

 ===== Epoch 269	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   1.5856757   1.9769248
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  1.5862135   1.9770994
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.3611,	 Acc = 0.4872
2921 0.25
5681 0.536
2915 0.612
376 0.62
69 0.58
6 0.167
0 0.0
0 0.0
0.563943848789654
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.9567,	 Acc = 0.3987
380 0.35
1718 0.444
815 0.35
85 0.176
2 0.0
0 0.0
0 0.0
0 0.0
0.40572519083969466
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9737,	 Acc1 = 0.3503,	 Acc2 = 0.3661

 ===== Epoch 270	 =====
[-0.36602148 -0.3783333   2.276839    2.2913644  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   2.123948    2.211509   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 2
train:	 Loss = 1.3666,	 Acc = 0.4827
2920 0.236
5683 0.532
2915 0.614
376 0.636
68 0.529
6 0.167
0 0.0
0 0.0
0.5622236958443855
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.9497,	 Acc = 0.3967
380 0.353
1718 0.446
815 0.335
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.4030534351145038
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9489,	 Acc1 = 0.3303,	 Acc2 = 0.3419

 ===== Epoch 271	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 0.5619691   2.9884222  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 6
train:	 Loss = 1.3652,	 Acc = 0.4838
2921 0.243
5686 0.531
2910 0.615
376 0.614
69 0.565
6 0.167
0 0.0
0 0.0
0.5614015695810766
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.9251,	 Acc = 0.4123
380 0.342
1718 0.448
815 0.393
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.42251908396946564
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9263,	 Acc1 = 0.3392,	 Acc2 = 0.3526

 ===== Epoch 272	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  3.099963    1.0366008  -0.40141198 -0.40778467  2.6607103   2.2539315
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  3.1864648   1.1956525  -0.40141198 -0.40778467  2.765647    2.4387789
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.3692,	 Acc = 0.4834
2924 0.235
5682 0.533
2913 0.613
374 0.647
69 0.536
6 0.167
0 0.0
0 0.0
0.5636886333480761
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.9957,	 Acc = 0.4040
380 0.342
1718 0.455
815 0.35
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.4129770992366412
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9639,	 Acc1 = 0.3454,	 Acc2 = 0.3601

 ===== Epoch 273	 =====
[ 4.2989874   2.5783362  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  0.73352623  2.5657287  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  0.88181394  2.6271172  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 6
train:	 Loss = 1.3665,	 Acc = 0.4805
2925 0.236
5681 0.528
2913 0.61
374 0.652
69 0.551
6 0.167
0 0.0
0 0.0
0.5595488222934867
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.9418,	 Acc = 0.3930
380 0.179
1718 0.474
815 0.346
85 0.176
2 0.0
0 0.0
0 0.0
0 0.0
0.42404580152671756
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9444,	 Acc1 = 0.3315,	 Acc2 = 0.3757

 ===== Epoch 274	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   1.8137004   2.3414385
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  1.8184897   2.3771906
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 4
train:	 Loss = 1.3697,	 Acc = 0.4839
2925 0.24
5679 0.534
2912 0.61
377 0.631
69 0.565
6 0.333
0 0.0
0 0.0
0.5627557226584098
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.9481,	 Acc = 0.4177
380 0.334
1718 0.467
815 0.374
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.4297709923664122
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9440,	 Acc1 = 0.3534,	 Acc2 = 0.3698

 ===== Epoch 275	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 1
train:	 Loss = 1.3635,	 Acc = 0.4839
2923 0.241
5680 0.533
2915 0.613
375 0.619
69 0.536
6 0.167
0 0.0
0 0.0
0.5624101713653953
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.9719,	 Acc = 0.3893
380 0.35
1718 0.443
815 0.318
85 0.176
2 0.0
0 0.0
0 0.0
0 0.0
0.3950381679389313
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9721,	 Acc1 = 0.3526,	 Acc2 = 0.3688

 ===== Epoch 276	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.2881899   2.1676695
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.2120438   2.3623753
 -0.38555372 -0.3798593 ] 4 6
train:	 Loss = 1.3629,	 Acc = 0.4862
2922 0.248
5682 0.529
2912 0.618
377 0.655
69 0.58
6 0.167
0 0.0
0 0.0
0.5632323678974133
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.9571,	 Acc = 0.4020
380 0.332
1718 0.452
815 0.353
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.4122137404580153
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9560,	 Acc1 = 0.3516,	 Acc2 = 0.3675

 ===== Epoch 277	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.7079396   2.517647
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.7102128   2.5694044
 -0.38555372 -0.3798593 ] 1 4
train:	 Loss = 1.3635,	 Acc = 0.4830
2918 0.236
5684 0.534
2915 0.609
376 0.644
69 0.536
6 0.333
0 0.0
0 0.0
0.5624309392265193
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.9430,	 Acc = 0.4163
380 0.353
1718 0.476
815 0.345
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.4255725190839695
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9520,	 Acc1 = 0.3509,	 Acc2 = 0.3668

 ===== Epoch 278	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 1
train:	 Loss = 1.3653,	 Acc = 0.4819
2927 0.241
5681 0.528
2910 0.612
375 0.651
69 0.478
6 0.167
0 0.0
0 0.0
0.5597832098219223
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.9844,	 Acc = 0.4067
380 0.342
1718 0.458
815 0.352
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.41603053435114506
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9903,	 Acc1 = 0.3419,	 Acc2 = 0.3559

 ===== Epoch 279	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.0276763   2.1307
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 6
train:	 Loss = 1.3631,	 Acc = 0.4890
2921 0.248
5682 0.536
2914 0.616
376 0.662
69 0.522
6 0.167
0 0.0
0 0.0
0.5668177296341329
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 2.0083,	 Acc = 0.4193
380 0.339
1718 0.466
815 0.383
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.4309160305343511
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9953,	 Acc1 = 0.3514,	 Acc2 = 0.3673

 ===== Epoch 280	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 1
train:	 Loss = 1.3648,	 Acc = 0.4857
2923 0.249
5683 0.529
2912 0.616
375 0.659
69 0.522
6 0.333
0 0.0
0 0.0
0.5620784964068546
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.9408,	 Acc = 0.4257
380 0.345
1718 0.48
815 0.377
85 0.176
2 0.0
0 0.0
0 0.0
0 0.0
0.43740458015267175
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9545,	 Acc1 = 0.3412,	 Acc2 = 0.3551

 ===== Epoch 281	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.4398487   1.6205215
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.4815211   1.696925
 -0.38555372 -0.3798593 ] 3 4
train:	 Loss = 1.3670,	 Acc = 0.4808
2925 0.241
5682 0.53
2909 0.605
377 0.639
69 0.507
6 0.167
0 0.0
0 0.0
0.55833241181024
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.9713,	 Acc = 0.4107
380 0.353
1718 0.458
815 0.361
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.41908396946564885
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9575,	 Acc1 = 0.3476,	 Acc2 = 0.3628

 ===== Epoch 282	 =====
[-0.36602148 -0.3783333   3.9103878   1.7612811  -0.4409929  -0.3635196
  3.2915025   2.7694266  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   4.251711    1.5728037  -0.44088638 -0.36343393
  3.458887    2.515502   -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 5
train:	 Loss = 1.3754,	 Acc = 0.4816
2923 0.235
5681 0.528
2912 0.62
377 0.618
69 0.536
6 0.167
0 0.0
0 0.0
0.5614151464897733
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.9253,	 Acc = 0.4187
380 0.345
1718 0.464
815 0.388
85 0.129
2 0.0
0 0.0
0 0.0
0 0.0
0.42938931297709926
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9248,	 Acc1 = 0.3476,	 Acc2 = 0.3628

 ===== Epoch 283	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 1
train:	 Loss = 1.3680,	 Acc = 0.4794
2924 0.238
5678 0.526
2915 0.608
376 0.649
69 0.565
6 0.333
0 0.0
0 0.0
0.5573861123396727
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.9289,	 Acc = 0.4270
380 0.347
1718 0.483
815 0.372
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.4385496183206107
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9176,	 Acc1 = 0.3571,	 Acc2 = 0.3743

 ===== Epoch 284	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.4284844   1.76347
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.2129269   1.8571261
 -0.38555372 -0.3798593 ] 3 4
train:	 Loss = 1.3621,	 Acc = 0.4840
2925 0.243
5680 0.532
2911 0.613
377 0.623
69 0.522
6 0.167
0 0.0
0 0.0
0.5618710604887759
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 2.0790,	 Acc = 0.4077
380 0.342
1718 0.463
815 0.352
85 0.129
2 0.0
0 0.0
0 0.0
0 0.0
0.417175572519084
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9997,	 Acc1 = 0.3641,	 Acc2 = 0.3827

 ===== Epoch 285	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 1
train:	 Loss = 1.3678,	 Acc = 0.4799
2922 0.241
5679 0.527
2915 0.61
377 0.613
69 0.565
6 0.167
0 0.0
0 0.0
0.5571523325226619
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 2.0189,	 Acc = 0.4020
380 0.347
1718 0.456
815 0.335
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.40992366412213743
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9778,	 Acc1 = 0.3569,	 Acc2 = 0.3740

 ===== Epoch 286	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   1.8082759   1.2943513  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  2.8213246   8.654177
 -0.3640846  -0.3725409   1.910117    1.2893744  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 5
train:	 Loss = 1.3693,	 Acc = 0.4819
2927 0.248
5679 0.524
2911 0.612
376 0.641
69 0.565
6 0.167
0 0.0
0 0.0
0.5575710651476606
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.9197,	 Acc = 0.4070
380 0.342
1718 0.458
815 0.352
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.416412213740458
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9497,	 Acc1 = 0.3520,	 Acc2 = 0.3680

 ===== Epoch 287	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   2.5497203   2.312544
 -0.3640846  -0.3725409   2.6882393   1.2172077  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  2.628016    2.3771906
 -0.3640846  -0.3725409   2.572664    1.2396042  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 3
train:	 Loss = 1.3639,	 Acc = 0.4828
2925 0.243
5677 0.529
2915 0.609
376 0.654
69 0.551
6 0.167
0 0.0
0 0.0
0.5602123189207121
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.9759,	 Acc = 0.4033
380 0.337
1718 0.451
815 0.356
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.4129770992366412
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9574,	 Acc1 = 0.3406,	 Acc2 = 0.3544

 ===== Epoch 288	 =====
[-0.36602148 -0.3783333   1.1444831   1.5483416  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0765158   1.5478896  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.3659,	 Acc = 0.4870
2925 0.245
5683 0.532
2908 0.621
377 0.637
69 0.551
6 0.167
0 0.0
0 0.0
0.565077960853699
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.9516,	 Acc = 0.3943
380 0.35
1718 0.448
815 0.324
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.40076335877862596
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9381,	 Acc1 = 0.3499,	 Acc2 = 0.3656

 ===== Epoch 289	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 1
train:	 Loss = 1.3663,	 Acc = 0.4821
2923 0.243
5682 0.529
2911 0.613
377 0.629
69 0.522
6 0.167
0 0.0
0 0.0
0.5594250967385296
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.9754,	 Acc = 0.3930
380 0.197
1718 0.476
815 0.33
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.4213740458015267
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9293,	 Acc1 = 0.3326,	 Acc2 = 0.3770

 ===== Epoch 290	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.6708133   1.8053688
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.8022696   1.9187418
 -0.38555372 -0.3798593 ] 6 4
train:	 Loss = 1.3649,	 Acc = 0.4825
2924 0.239
5677 0.528
2916 0.614
376 0.662
69 0.565
6 0.167
0 0.0
0 0.0
0.5611455108359134
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 2.0146,	 Acc = 0.4040
380 0.339
1718 0.458
815 0.342
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.4133587786259542
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9802,	 Acc1 = 0.3641,	 Acc2 = 0.3827

 ===== Epoch 291	 =====
[ 2.3382242   2.6973116  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.955623    2.955684  ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.955623    2.955684  ] 0 4
train:	 Loss = 1.3672,	 Acc = 0.4839
2922 0.233
5684 0.535
2910 0.611
377 0.663
69 0.536
6 0.167
0 0.0
0 0.0
0.5648905593632545
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.9317,	 Acc = 0.4203
380 0.35
1718 0.474
815 0.366
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.4305343511450382
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9145,	 Acc1 = 0.3431,	 Acc2 = 0.3574

 ===== Epoch 292	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.9920652   1.6229862
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.1973946   1.6525618
 -0.38555372 -0.3798593 ] 2 3
train:	 Loss = 1.3675,	 Acc = 0.4820
2925 0.241
5680 0.529
2911 0.613
377 0.631
69 0.551
6 0.167
0 0.0
0 0.0
0.5599911533783036
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.9664,	 Acc = 0.4090
380 0.353
1718 0.461
815 0.351
85 0.176
2 0.0
0 0.0
0 0.0
0 0.0
0.417175572519084
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9487,	 Acc1 = 0.3456,	 Acc2 = 0.3603

 ===== Epoch 293	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   1.2853318   1.3764719  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   1.3825976   1.525782   -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 4
train:	 Loss = 1.3650,	 Acc = 0.4843
2925 0.246
5677 0.528
2917 0.617
374 0.639
69 0.565
6 0.167
0 0.0
0 0.0
0.5612075638615504
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 2.0145,	 Acc = 0.3907
380 0.342
1718 0.441
815 0.33
85 0.176
2 0.0
0 0.0
0 0.0
0 0.0
0.3977099236641221
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9631,	 Acc1 = 0.3617,	 Acc2 = 0.3797

 ===== Epoch 294	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 1
train:	 Loss = 1.3715,	 Acc = 0.4782
2923 0.237
5682 0.528
2912 0.606
376 0.606
69 0.565
6 0.167
0 0.0
0 0.0
0.5562189054726369
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.9858,	 Acc = 0.4193
380 0.342
1718 0.488
815 0.336
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.4305343511450382
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9806,	 Acc1 = 0.3606,	 Acc2 = 0.3785

 ===== Epoch 295	 =====
[ 2.8427975   2.2745688  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.9159425   2.5436616 ] [ 2.2670882   2.2619119  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.8467228   2.5069208 ] 1 3
train:	 Loss = 1.3696,	 Acc = 0.4850
2922 0.251
5685 0.525
2911 0.623
376 0.633
68 0.515
6 0.167
0 0.0
0 0.0
0.5605792615520672
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.9978,	 Acc = 0.3883
380 0.355
1718 0.437
815 0.324
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.3931297709923664
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9981,	 Acc1 = 0.3439,	 Acc2 = 0.3583

 ===== Epoch 296	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   1.8294452   3.1258893  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   1.8803661   3.2328951  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 4
train:	 Loss = 1.3678,	 Acc = 0.4829
2922 0.24
5681 0.53
2915 0.615
375 0.637
69 0.478
6 0.167
0 0.0
0 0.0
0.5613530842361265
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.8315,	 Acc = 0.4277
380 0.355
1718 0.474
815 0.389
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.4381679389312977
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9026,	 Acc1 = 0.3437,	 Acc2 = 0.3581

 ===== Epoch 297	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.7186716   1.5884813
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.7186716   1.5934107
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.3667,	 Acc = 0.4825
2922 0.243
5680 0.526
2914 0.617
377 0.642
69 0.58
6 0.167
0 0.0
0 0.0
0.5600265310634535
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.9789,	 Acc = 0.4237
380 0.35
1718 0.487
815 0.347
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.43435114503816796
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9478,	 Acc1 = 0.3613,	 Acc2 = 0.3792

 ===== Epoch 298	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   2.3908288   1.0900898
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  2.3881001   1.0946761
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.3705,	 Acc = 0.4781
2925 0.238
5680 0.527
2912 0.601
377 0.65
68 0.544
6 0.167
0 0.0
0 0.0
0.555678425301338
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 2.1073,	 Acc = 0.4047
380 0.326
1718 0.464
815 0.344
85 0.153
2 0.0
0 0.0
0 0.0
0 0.0
0.41603053435114506
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 2.0196,	 Acc1 = 0.3631,	 Acc2 = 0.3815

 ===== Epoch 299	 =====
[-0.36602148 -0.3783333   1.3097876   1.6049745  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.3169502   1.6316915  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 3 4
train:	 Loss = 1.3676,	 Acc = 0.4825
2919 0.243
5684 0.526
2915 0.616
376 0.63
68 0.574
6 0.167
0 0.0
0 0.0
0.5597303569455189
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 2.0511,	 Acc = 0.3963
380 0.342
1718 0.459
815 0.313
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.40419847328244274
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 2.0272,	 Acc1 = 0.3584,	 Acc2 = 0.3757

 ===== Epoch 300	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.4062716   3.026541  ] [ 1.986752    2.6719978  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.4587379   3.215494  ] 6 4
train:	 Loss = 1.3691,	 Acc = 0.4851
2926 0.247
5675 0.529
2916 0.622
376 0.617
69 0.536
6 0.167
0 0.0
0 0.0
0.5622649856226498
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.9283,	 Acc = 0.4067
380 0.347
1718 0.46
815 0.346
85 0.176
2 0.0
0 0.0
0 0.0
0 0.0
0.4152671755725191
0.45190839694656487
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9746,	 Acc1 = 0.3421,	 Acc2 = 0.3561
