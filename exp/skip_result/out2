(0.24300000000005184, array([-1.000000e+00, -1.000000e+00,  1.189783e+03,  1.790000e-01,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 4, array([  0.   ,   0.   , -15.851,   0.091,   0.   ,   0.   ,   0.   ,
         0.   ,   0.   ,   0.   ,   0.   ,   0.   ,   0.   ,   0.   ]), 1)
((0.236, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.364685e+03, -3.300000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), array([0., 0., 0.])), (0.24, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.543573e+03, -3.290000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), array([0., 0., 0.])))
(0.236, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.364685e+03, -3.300000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([0.   , 0.   , 0.   , 0.   , 5.823, 3.129, 0.   , 0.   , 0.   ,
       0.   , 0.   , 0.   , 0.   , 0.   ]))
(0.24, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.543573e+03, -3.290000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([   0.   ,    0.   ,    0.   ,    0.   , -173.065,    3.128,
          0.   ,    0.   ,    0.   ,    0.   ,    0.   ,    0.   ,
          0.   ,    0.   ]))
(0.24, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.543573e+03, -3.290000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([ 0.00000e+00,  0.00000e+00,  0.00000e+00,  0.00000e+00,
       -1.87299e+02, -1.20000e-02,  0.00000e+00,  0.00000e+00,
        0.00000e+00,  0.00000e+00,  0.00000e+00,  0.00000e+00,
        0.00000e+00,  0.00000e+00]))
(0.238, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.370508e+03,  2.799000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([  0.   ,   0.   ,   0.   ,   0.   , -14.234,  -3.14 ,   0.   ,
         0.   ,   0.   ,   0.   ,   0.   ,   0.   ,   0.   ,   0.   ]))
(0.238, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.370508e+03,  2.799000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([ 0.   ,  0.   ,  0.   ,  0.   , 56.61 , -3.132,  0.   ,  0.   ,
        0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ]))
(0.236, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.356274e+03, -3.410000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 7.0844e+01,
       8.0000e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
       0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]))
(0.236, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.356274e+03, -3.410000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([ 0.00000e+00,  0.00000e+00,  0.00000e+00,  0.00000e+00,
       -1.10632e+02,  7.00000e-03,  0.00000e+00,  0.00000e+00,
        0.00000e+00,  0.00000e+00,  0.00000e+00,  0.00000e+00,
        0.00000e+00,  0.00000e+00]))
(0.2350000000000001, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.427118e+03, -3.330000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([ 0.00000e+00,  0.00000e+00,  0.00000e+00,  0.00000e+00,
       -1.81476e+02, -1.00000e-03,  0.00000e+00,  0.00000e+00,
        0.00000e+00,  0.00000e+00,  0.00000e+00,  0.00000e+00,
        0.00000e+00,  0.00000e+00]))
(0.2350000000000001, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.427118e+03, -3.330000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.1969e+01,
       -1.0000e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
        0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]))
(0.248, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.245642e+03, -3.340000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([ 0.00000e+00,  0.00000e+00,  0.00000e+00,  0.00000e+00,
        1.69507e+02, -9.00000e-03,  0.00000e+00,  0.00000e+00,
        0.00000e+00,  0.00000e+00,  0.00000e+00,  0.00000e+00,
        0.00000e+00,  0.00000e+00]))
14 1 14

 ===== Epoch 1	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 6 6
train:	 Loss = 1.8305,	 Acc = 0.2658
5568 0.217
10878 0.269
5609 0.293
721 0.348
124 0.452
12 0.333
0 0.0
0 0.0
0.28165359778597787
0.0
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 2
val:	 Loss = 1.5813,	 Acc = 0.4091
715 0.317
3297 0.443
1558 0.396
160 0.244
4 0.75
0 0.0
0 0.0
0 0.0
0.42219565650527996
0.42219565650527996
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 6
Testing:	 Loss = 1.6316,	 Acc1 = 0.3243,	 Acc2 = 0.3347

 ===== Epoch 2	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 0 1
train:	 Loss = 1.5904,	 Acc = 0.3924
5570 0.25
10874 0.415
5612 0.478
719 0.473
125 0.456
12 0.167
0 0.0
0 0.0
0.438069426825049
0.42219565650527996
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.5048,	 Acc = 0.4070
715 0.333
3297 0.427
1558 0.408
160 0.325
4 0.0
0 0.0
0 0.0
0 0.0
0.4176130703327356
0.42219565650527996
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 6
Testing:	 Loss = 1.5767,	 Acc1 = 0.3427,	 Acc2 = 0.3569

 ===== Epoch 3	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239   1.5989717   2.6211805
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064  -0.19906227  0.37298378
 -0.00148575 -0.00146706] 6 6
train:	 Loss = 1.5588,	 Acc = 0.4120
5571 0.248
10877 0.436
5607 0.512
720 0.529
125 0.48
12 0.083
0 0.0
0 0.0
0.46473675105241913
0.42219565650527996
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4846,	 Acc = 0.4520
715 0.341
3297 0.456
1558 0.507
160 0.331
4 0.0
0 0.0
0 0.0
0 0.0
0.4678222753536561
0.4678222753536561
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 6
Testing:	 Loss = 1.5523,	 Acc1 = 0.3776,	 Acc2 = 0.3989

 ===== Epoch 4	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892  2.5777335   0.8689975  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 1.0109920e-03  2.5630770e-03  2.3875074e-03  1.9614373e-03
  2.8118784e+00  3.7599225e+00  1.5849024e-03  3.2306046e-03
 -5.6345215e+00 -2.3511665e+00  1.5010408e-03  2.7898324e-03
 -1.4857454e-03 -1.4670575e-03] 5 2
train:	 Loss = 1.5384,	 Acc = 0.4198
5568 0.246
10877 0.446
5610 0.527
720 0.533
125 0.456
12 0.25
0 0.0
0 0.0
0.4755535055350554
0.4678222753536561
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 3
val:	 Loss = 1.4967,	 Acc = 0.4196
715 0.312
3297 0.434
1558 0.439
160 0.425
4 0.0
0 0.0
0 0.0
0 0.0
0.4349472006375772
0.4678222753536561
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 6
Testing:	 Loss = 1.5593,	 Acc1 = 0.3852,	 Acc2 = 0.4081

 ===== Epoch 5	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 6 1
train:	 Loss = 1.5241,	 Acc = 0.4216
5566 0.236
10876 0.452
5613 0.535
720 0.524
125 0.416
12 0.083
0 0.0
0 0.0
0.48103309120258275
0.4678222753536561
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 3
val:	 Loss = 1.4694,	 Acc = 0.4541
715 0.344
3297 0.461
1558 0.503
160 0.344
4 0.0
0 0.0
0 0.0
0 0.0
0.4698147041243276
0.4698147041243276
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 4
Testing:	 Loss = 1.5355,	 Acc1 = 0.3846,	 Acc2 = 0.4073

 ===== Epoch 6	 =====
[ 0.9567993   1.9598169  -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
  0.77800775  2.303326  ] [-0.0163915   0.31700134  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
  0.07312567  0.6151313 ] 6 6
train:	 Loss = 1.5101,	 Acc = 0.4303
5564 0.24
10877 0.463
5613 0.545
721 0.524
125 0.424
12 0.083
0 0.0
0 0.0
0.49135347014065023
0.4698147041243276
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 3
val:	 Loss = 1.4908,	 Acc = 0.4299
715 0.338
3297 0.463
1558 0.413
160 0.331
4 0.0
0 0.0
0 0.0
0 0.0
0.442916915720263
0.4698147041243276
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 6
Testing:	 Loss = 1.5497,	 Acc1 = 0.3852,	 Acc2 = 0.4081

 ===== Epoch 7	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 1 1
train:	 Loss = 1.4965,	 Acc = 0.4374
5566 0.248
10877 0.468
5611 0.551
721 0.553
125 0.472
12 0.167
0 0.0
0 0.0
0.4981551942811023
0.4698147041243276
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 3
val:	 Loss = 1.4530,	 Acc = 0.4702
715 0.33
3297 0.495
1558 0.489
160 0.394
4 0.75
0 0.0
0 0.0
0 0.0
0.49013747758517634
0.49013747758517634
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.5169,	 Acc1 = 0.4011,	 Acc2 = 0.4272

 ===== Epoch 8	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 1 1
train:	 Loss = 1.4927,	 Acc = 0.4345
5564 0.244
10882 0.468
5608 0.547
721 0.531
125 0.448
12 0.083
0 0.0
0 0.0
0.4956767350703251
0.49013747758517634
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 3
val:	 Loss = 1.4407,	 Acc = 0.4315
715 0.329
3297 0.446
1558 0.456
160 0.331
4 0.75
0 0.0
0 0.0
0 0.0
0.44610480175333733
0.49013747758517634
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 6
Testing:	 Loss = 1.5208,	 Acc1 = 0.3889,	 Acc2 = 0.4125

 ===== Epoch 9	 =====
[-0.3669651  -0.38051367  1.0701447   1.7532287  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.1077325  -0.2951739   0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 5 5
train:	 Loss = 1.4810,	 Acc = 0.4429
5568 0.246
10877 0.48
5609 0.557
721 0.526
125 0.448
12 0.083
0 0.0
0 0.0
0.5062269372693727
0.49013747758517634
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 3
val:	 Loss = 1.4735,	 Acc = 0.4367
715 0.313
3297 0.473
1558 0.427
160 0.319
4 0.75
0 0.0
0 0.0
0 0.0
0.45427375971309025
0.49013747758517634
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.5290,	 Acc1 = 0.3850,	 Acc2 = 0.4078

 ===== Epoch 10	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438  1.8840256   1.7952027
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.01569946  0.06521839
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 3 2
train:	 Loss = 1.4806,	 Acc = 0.4394
5569 0.24
10877 0.477
5609 0.552
720 0.547
125 0.464
12 0.0
0 0.0
0 0.0
0.503430778988641
0.49013747758517634
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 3
val:	 Loss = 1.4329,	 Acc = 0.4482
715 0.308
3297 0.465
1558 0.483
160 0.381
4 0.75
0 0.0
0 0.0
0 0.0
0.4682207611077904
0.49013747758517634
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 4
Testing:	 Loss = 1.5046,	 Acc1 = 0.3953,	 Acc2 = 0.4202

 ===== Epoch 11	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239   2.0534418   1.3005809
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.06581702  0.26382402
 -0.00148575 -0.00146706] 3 2
train:	 Loss = 1.4740,	 Acc = 0.4421
5571 0.242
10875 0.479
5609 0.554
720 0.549
125 0.488
12 0.083
0 0.0
0 0.0
0.5062568479326451
0.49013747758517634
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 3
val:	 Loss = 1.4144,	 Acc = 0.4782
715 0.324
3297 0.484
1558 0.547
160 0.369
4 0.75
0 0.0
0 0.0
0 0.0
0.5000996214385336
0.5000996214385336
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 6
Testing:	 Loss = 1.4921,	 Acc1 = 0.3951,	 Acc2 = 0.4200

 ===== Epoch 12	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 1 1
train:	 Loss = 1.4746,	 Acc = 0.4422
5567 0.244
10878 0.478
5610 0.555
720 0.56
125 0.456
12 0.083
0 0.0
0 0.0
0.5058518304987027
0.5000996214385336
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4453,	 Acc = 0.4407
715 0.331
3297 0.467
1558 0.451
160 0.281
4 0.75
0 0.0
0 0.0
0 0.0
0.4562661884837617
0.5000996214385336
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 4
Testing:	 Loss = 1.5118,	 Acc1 = 0.3848,	 Acc2 = 0.4076

 ===== Epoch 13	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 1 1
train:	 Loss = 1.4686,	 Acc = 0.4456
5572 0.238
10872 0.482
5611 0.565
720 0.567
125 0.504
12 0.083
0 0.0
0 0.0
0.5122837370242215
0.5000996214385336
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 3
val:	 Loss = 1.4282,	 Acc = 0.4693
715 0.336
3297 0.482
1558 0.515
160 0.35
4 0.75
0 0.0
0 0.0
0 0.0
0.488344291691572
0.5000996214385336
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 4
Testing:	 Loss = 1.4978,	 Acc1 = 0.4050,	 Acc2 = 0.4319

 ===== Epoch 14	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438  2.427371    2.6077046
 -0.36420864 -0.37237892  1.6890361   2.0054708  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.6363401   0.04459199
  0.0015849   0.0032306  -0.17270523  0.20507373  0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 4
train:	 Loss = 1.4734,	 Acc = 0.4419
5570 0.236
10872 0.481
5612 0.555
721 0.564
125 0.504
12 0.083
0 0.0
0 0.0
0.5078998962057433
0.5000996214385336
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 3
val:	 Loss = 1.4385,	 Acc = 0.4644
715 0.317
3297 0.483
1558 0.504
160 0.35
4 0.75
0 0.0
0 0.0
0 0.0
0.48535564853556484
0.5000996214385336
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 4
Testing:	 Loss = 1.5034,	 Acc1 = 0.3939,	 Acc2 = 0.4185

 ===== Epoch 15	 =====
[-0.3669651  -0.38051367  3.1606507   2.6013045   2.6544986   0.7762456
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 1.0109920e-03  2.5630770e-03 -6.0861409e-01 -5.3549872e-04
 -7.4210811e+00 -1.3244854e+00  1.5849024e-03  3.2306046e-03
 -1.4162532e-03  3.5063999e-03  1.5010408e-03  2.7898324e-03
 -1.4857454e-03 -1.4670575e-03] 4 0
train:	 Loss = 1.4701,	 Acc = 0.4418
5566 0.236
10874 0.478
5614 0.561
721 0.556
125 0.464
12 0.083
0 0.0
0 0.0
0.5077251239478843
0.5000996214385336
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 3
val:	 Loss = 1.4414,	 Acc = 0.4416
715 0.324
3297 0.462
1558 0.466
160 0.294
4 0.75
0 0.0
0 0.0
0 0.0
0.4582586172544332
0.5000996214385336
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 6
Testing:	 Loss = 1.5051,	 Acc1 = 0.3854,	 Acc2 = 0.4083

 ===== Epoch 16	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 1 1
train:	 Loss = 1.4667,	 Acc = 0.4446
5569 0.237
10874 0.482
5612 0.562
720 0.585
125 0.424
12 0.083
0 0.0
0 0.0
0.5110995790808972
0.5000996214385336
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 3
val:	 Loss = 1.4224,	 Acc = 0.4608
715 0.319
3297 0.482
1558 0.491
160 0.35
4 0.75
0 0.0
0 0.0
0 0.0
0.48097230524008766
0.5000996214385336
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.5031,	 Acc1 = 0.3800,	 Acc2 = 0.4018

 ===== Epoch 17	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438  2.3811042   1.1647369
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.00011543 -0.00955232
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 2 0
train:	 Loss = 1.4655,	 Acc = 0.4425
5567 0.234
10880 0.485
5610 0.554
718 0.557
125 0.424
12 0.0
0 0.0
0 0.0
0.5094263476506198
0.5000996214385336
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4174,	 Acc = 0.4944
715 0.348
3297 0.517
1558 0.526
160 0.362
4 0.75
0 0.0
0 0.0
0 0.0
0.5152420800956365
0.5152420800956365
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 4
Testing:	 Loss = 1.4921,	 Acc1 = 0.4038,	 Acc2 = 0.4304

 ===== Epoch 18	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892  5.532222   -4.5572267   0.6767969   2.6556094
  3.9117646   2.3059502 ] [ 1.0109920e-03  2.5630770e-03  2.3875074e-03  1.9614373e-03
  1.0307996e-03  7.6088449e-04  1.5849024e-03  3.2306046e-03
 -1.1220910e+01  7.6447406e+00  2.3460300e-03  5.1536608e-01
 -1.0781312e+00  4.2148885e-01] 4 6
train:	 Loss = 1.4652,	 Acc = 0.4448
5567 0.232
10878 0.486
5609 0.562
721 0.566
125 0.416
12 0.167
0 0.0
0 0.0
0.5132314788123379
0.5152420800956365
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 3
val:	 Loss = 1.4076,	 Acc = 0.4587
715 0.169
3297 0.505
1558 0.507
160 0.344
4 0.0
0 0.0
0 0.0
0 0.0
0.4999003785614664
0.5152420800956365
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4870,	 Acc1 = 0.3606,	 Acc2 = 0.4123

 ===== Epoch 19	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892  2.6035328   1.1549809  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306   0.58833784  0.22339803  0.00150104  0.00278983
 -0.00148575 -0.00146706] 2 2
train:	 Loss = 1.4644,	 Acc = 0.4459
5569 0.234
10874 0.485
5612 0.562
720 0.586
125 0.432
12 0.167
0 0.0
0 0.0
0.5138672663322378
0.5152420800956365
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4755,	 Acc = 0.4428
715 0.313
3297 0.484
1558 0.424
160 0.356
4 0.75
0 0.0
0 0.0
0 0.0
0.46124726041044034
0.5152420800956365
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.5404,	 Acc1 = 0.3839,	 Acc2 = 0.4066

 ===== Epoch 20	 =====
[ 1.4040618   3.4064293  -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239   1.8280978   1.0915475
 -0.38672873 -0.38123247] [ 0.1822983   0.03884442  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064  -0.08312608  0.25907794
 -0.00148575 -0.00146706] 6 4
train:	 Loss = 1.4630,	 Acc = 0.4459
5569 0.24
10874 0.485
5613 0.557
719 0.583
125 0.456
12 0.0
0 0.0
0 0.0
0.5119068211958715
0.5152420800956365
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4182,	 Acc = 0.4698
715 0.315
3297 0.491
1558 0.509
160 0.344
4 0.75
0 0.0
0 0.0
0 0.0
0.4919306634787806
0.5152420800956365
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 6
Testing:	 Loss = 1.4994,	 Acc1 = 0.3879,	 Acc2 = 0.4113

 ===== Epoch 21	 =====
[ 2.6080654   2.3536592  -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
  2.1400511   2.5841155 ] [-0.301135   -0.01557759  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.21303754  0.04949148] 0 3
train:	 Loss = 1.4647,	 Acc = 0.4414
5569 0.229
10878 0.484
5608 0.553
720 0.569
125 0.432
12 0.25
0 0.0
0 0.0
0.5095427550020181
0.5152420800956365
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4627,	 Acc = 0.4407
715 0.316
3297 0.484
1558 0.418
160 0.306
4 0.75
0 0.0
0 0.0
0 0.0
0.4584578601315003
0.5152420800956365
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 6
Testing:	 Loss = 1.5259,	 Acc1 = 0.3862,	 Acc2 = 0.4093

 ===== Epoch 22	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 1.0109920e-03  2.5630770e-03  2.3875074e-03  1.9614373e-03
  1.0307996e-03  7.6088449e-04  1.5849024e-03  3.2306046e-03
  4.2494531e+00  5.0014601e+00  1.5010408e-03  2.7898324e-03
 -1.4857454e-03 -1.4670575e-03] 1 2
train:	 Loss = 1.4618,	 Acc = 0.4456
5566 0.234
10876 0.487
5613 0.559
720 0.596
125 0.4
12 0.083
0 0.0
0 0.0
0.5134901418194396
0.5152420800956365
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.3993,	 Acc = 0.4667
715 0.323
3297 0.482
1558 0.513
160 0.344
4 0.75
0 0.0
0 0.0
0 0.0
0.4871488344291692
0.5152420800956365
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 6
Testing:	 Loss = 1.4790,	 Acc1 = 0.3864,	 Acc2 = 0.4095

 ===== Epoch 23	 =====
[ 4.1380606   3.3508873  -0.4202629  -0.33521438 -0.4411929  -0.3648088
  1.2763383   2.1287153  -0.40146217 -0.4092239   2.396753    1.676841
  4.191045    3.5681913 ] [-4.9273033e+00 -4.4660888e+00  2.3875074e-03  1.9614373e-03
  1.0307996e-03  7.6088449e-04  1.5205632e-01  8.9366788e-01
 -1.4162532e-03  3.5063999e-03 -6.9619324e-03  6.2452579e-01
 -9.8160543e+00 -7.6707277e+00] 0 6
train:	 Loss = 1.4631,	 Acc = 0.4467
5568 0.238
10876 0.491
5610 0.553
721 0.573
125 0.456
12 0.083
0 0.0
0 0.0
0.5136070110701108
0.5152420800956365
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 3
val:	 Loss = 1.4480,	 Acc = 0.4552
715 0.315
3297 0.469
1558 0.501
160 0.35
4 0.75
0 0.0
0 0.0
0 0.0
0.47519426180514046
0.5152420800956365
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 4
Testing:	 Loss = 1.5213,	 Acc1 = 0.3848,	 Acc2 = 0.4076

 ===== Epoch 24	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 6 1
train:	 Loss = 1.4615,	 Acc = 0.4468
5567 0.229
10879 0.493
5609 0.558
720 0.579
125 0.376
12 0.25
0 0.0
0 0.0
0.516633035456904
0.5152420800956365
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4669,	 Acc = 0.4424
715 0.322
3297 0.483
1558 0.425
160 0.3
4 0.75
0 0.0
0 0.0
0 0.0
0.4596533173939032
0.5152420800956365
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 6
Testing:	 Loss = 1.5252,	 Acc1 = 0.3914,	 Acc2 = 0.4155

 ===== Epoch 25	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
train:	 Loss = 1.4579,	 Acc = 0.4479
5569 0.243
10876 0.489
5610 0.555
720 0.592
125 0.448
12 0.167
0 0.0
0 0.0
0.5137519460300985
0.5152420800956365
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4500,	 Acc = 0.4304
715 0.179
3297 0.48
1558 0.45
160 0.356
4 0.0
0 0.0
0 0.0
0 0.0
0.46622833233711897
0.5152420800956365
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 4
Testing:	 Loss = 1.5100,	 Acc1 = 0.3639,	 Acc2 = 0.4163

 ===== Epoch 26	 =====
[ 2.4300506   2.5733018  -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
  2.0563557   2.838663  ] [ 0.17351009  0.07512575  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
  0.13734357  0.17688784] 3 4
train:	 Loss = 1.4597,	 Acc = 0.4458
5567 0.236
10879 0.488
5610 0.558
720 0.561
124 0.435
12 0.167
0 0.0
0 0.0
0.5132314788123379
0.5152420800956365
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4397,	 Acc = 0.4506
715 0.316
3297 0.489
1558 0.442
160 0.337
4 0.75
0 0.0
0 0.0
0 0.0
0.4698147041243276
0.5152420800956365
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 4
Testing:	 Loss = 1.5054,	 Acc1 = 0.3928,	 Acc2 = 0.4172

 ===== Epoch 27	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 3 1
train:	 Loss = 1.4572,	 Acc = 0.4480
5571 0.237
10873 0.486
5612 0.565
719 0.595
125 0.432
12 0.333
0 0.0
0 0.0
0.5156565365319186
0.5152420800956365
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4353,	 Acc = 0.4557
715 0.324
3297 0.496
1558 0.447
160 0.3
4 0.75
0 0.0
0 0.0
0 0.0
0.4743972902968719
0.5152420800956365
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 6
Testing:	 Loss = 1.5055,	 Acc1 = 0.3825,	 Acc2 = 0.4048

 ===== Epoch 28	 =====
[ 2.1283367   3.196885   -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
  2.020234    3.4684715 ] [ 0.07505526  0.13559465  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.16864908  0.19727126] 4 4
train:	 Loss = 1.4579,	 Acc = 0.4447
5570 0.232
10877 0.485
5607 0.561
721 0.591
125 0.44
12 0.167
0 0.0
0 0.0
0.513089609041633
0.5152420800956365
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4468,	 Acc = 0.4498
715 0.319
3297 0.488
1558 0.441
160 0.325
4 0.75
0 0.0
0 0.0
0 0.0
0.46842000398485756
0.5152420800956365
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 6
Testing:	 Loss = 1.5102,	 Acc1 = 0.3955,	 Acc2 = 0.4205

 ===== Epoch 29	 =====
[ 2.6683207   2.3132648  -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
  2.0651648   2.5736187 ] [-6.4695224e-02 -5.1142257e-01  2.3875074e-03  1.9614373e-03
  1.0307996e-03  7.6088449e-04  1.5849024e-03  3.2306046e-03
 -1.4162532e-03  3.5063999e-03  1.5010408e-03  2.7898324e-03
  2.8373158e-01 -8.6266643e-01] 4 5
train:	 Loss = 1.4534,	 Acc = 0.4467
5568 0.235
10876 0.486
5610 0.565
721 0.577
125 0.44
12 0.083
0 0.0
0 0.0
0.5145871771217713
0.5152420800956365
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4541,	 Acc = 0.4384
715 0.187
3297 0.5
1558 0.438
160 0.306
4 0.0
0 0.0
0 0.0
0 0.0
0.47419804741980476
0.5152420800956365
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 4
Testing:	 Loss = 1.5120,	 Acc1 = 0.3691,	 Acc2 = 0.4225

 ===== Epoch 30	 =====
[ 2.2094362   1.7073542  -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892  1.8237734   3.3060734  -0.42425194 -0.42087102
  2.8497066   1.8991989 ] [-2.81746650e+00 -2.49782586e+00  2.38750735e-03  1.96143729e-03
  1.03079958e-03  7.60884490e-04  1.58490241e-03  3.23060458e-03
 -7.38608003e-01  7.22225383e-02  1.50104077e-03  2.78983242e-03
 -1.13668811e+00  1.00450024e-01] 1 4
train:	 Loss = 1.4525,	 Acc = 0.4470
5572 0.234
10873 0.489
5610 0.561
720 0.581
125 0.432
12 0.083
0 0.0
0 0.0
0.5155709342560554
0.5152420800956365
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4652,	 Acc = 0.4360
715 0.316
3297 0.476
1558 0.416
160 0.344
4 0.75
0 0.0
0 0.0
0 0.0
0.45307830245068736
0.5152420800956365
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 4
Testing:	 Loss = 1.5216,	 Acc1 = 0.3920,	 Acc2 = 0.4163

 ===== Epoch 31	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
  1.9523962   2.954128  ] [ 2.9469974e+00  3.6760485e+00  2.3875074e-03  1.9614373e-03
  1.0307996e-03  7.6088449e-04  1.5849024e-03  3.2306046e-03
 -1.4162532e-03  3.5063999e-03  1.5010408e-03  2.7898324e-03
 -1.4857454e-03 -1.4670575e-03] 0 0
train:	 Loss = 1.4544,	 Acc = 0.4483
5566 0.233
10876 0.49
5612 0.563
721 0.585
125 0.48
12 0.167
0 0.0
0 0.0
0.5173527037933817
0.5152420800956365
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4163,	 Acc = 0.4731
715 0.326
3297 0.495
1558 0.499
160 0.412
4 0.75
0 0.0
0 0.0
0 0.0
0.4941223351265192
0.5152420800956365
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 4
Testing:	 Loss = 1.4821,	 Acc1 = 0.4017,	 Acc2 = 0.4279

 ===== Epoch 32	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892  1.596725    2.2019289  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00033043 -0.19347987  0.00150104  0.00278983
 -0.00148575 -0.00146706] 3 5
train:	 Loss = 1.4540,	 Acc = 0.4506
5567 0.234
10876 0.496
5611 0.562
721 0.58
125 0.488
12 0.0
0 0.0
0 0.0
0.5202652061112713
0.5152420800956365
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4169,	 Acc = 0.4625
715 0.323
3297 0.492
1558 0.476
160 0.337
4 0.75
0 0.0
0 0.0
0 0.0
0.4823670053795577
0.5152420800956365
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4891,	 Acc1 = 0.4015,	 Acc2 = 0.4277

 ===== Epoch 33	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 4.0401082e+00  3.4976652e+00  2.3875074e-03  1.9614373e-03
  1.0307996e-03  7.6088449e-04  1.5849024e-03  3.2306046e-03
 -1.4162532e-03  3.5063999e-03  1.5010408e-03  2.7898324e-03
 -1.4857454e-03 -1.4670575e-03] 0 1
train:	 Loss = 1.4553,	 Acc = 0.4530
5570 0.239
10876 0.494
5611 0.568
718 0.589
125 0.48
12 0.167
0 0.0
0 0.0
0.5217391304347826
0.5152420800956365
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4424,	 Acc = 0.4524
715 0.316
3297 0.483
1558 0.457
160 0.381
4 0.75
0 0.0
0 0.0
0 0.0
0.471807132894999
0.5152420800956365
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.5071,	 Acc1 = 0.3932,	 Acc2 = 0.4177

 ===== Epoch 34	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239   2.7049003   1.5981461
 -0.38672873 -0.38123247] [ 1.0109920e-03  2.5630770e-03  2.3875074e-03  1.9614373e-03
  1.0307996e-03  7.6088449e-04  4.8084531e+00  3.1412544e+00
 -1.4162532e-03  3.5063999e-03  8.2742974e-02  1.6415642e-01
 -1.4857454e-03 -1.4670575e-03] 2 2
train:	 Loss = 1.4503,	 Acc = 0.4478
5566 0.23
10879 0.49
5610 0.565
720 0.59
125 0.448
12 0.083
0 0.0
0 0.0
0.5177562550443906
0.5152420800956365
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4325,	 Acc = 0.4576
715 0.326
3297 0.485
1558 0.467
160 0.387
4 0.75
0 0.0
0 0.0
0 0.0
0.47638971906754335
0.5152420800956365
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 4
Testing:	 Loss = 1.4933,	 Acc1 = 0.3984,	 Acc2 = 0.4240

 ===== Epoch 35	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239   2.2156453   1.7063516
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.44748014  0.23534754
 -0.00148575 -0.00146706] 2 2
train:	 Loss = 1.4495,	 Acc = 0.4513
5569 0.233
10875 0.497
5610 0.563
721 0.584
125 0.464
12 0.25
0 0.0
0 0.0
0.5214784062734245
0.5152420800956365
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 3
val:	 Loss = 1.4297,	 Acc = 0.4627
715 0.316
3297 0.488
1558 0.484
160 0.375
4 0.75
0 0.0
0 0.0
0 0.0
0.48356246264196057
0.5152420800956365
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4957,	 Acc1 = 0.3978,	 Acc2 = 0.4232

 ===== Epoch 36	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  1.0850587   2.2627022  -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.18471482 -0.02133318 -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
train:	 Loss = 1.4512,	 Acc = 0.4523
5568 0.24
10878 0.493
5610 0.57
720 0.572
124 0.476
12 0.167
0 0.0
0 0.0
0.520410516605166
0.5152420800956365
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 3
val:	 Loss = 1.4328,	 Acc = 0.4597
715 0.324
3297 0.469
1558 0.507
160 0.412
4 0.75
0 0.0
0 0.0
0 0.0
0.4789798764694162
0.5152420800956365
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 4
Testing:	 Loss = 1.5003,	 Acc1 = 0.3926,	 Acc2 = 0.4170

 ===== Epoch 37	 =====
[-0.3669651  -0.38051367  1.7585715   2.218083   -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.2907469   0.01194918  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 0 0
train:	 Loss = 1.4462,	 Acc = 0.4535
5567 0.236
10875 0.494
5613 0.575
721 0.581
124 0.492
12 0.167
0 0.0
0 0.0
0.5234361487460363
0.5152420800956365
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4264,	 Acc = 0.4594
715 0.333
3297 0.477
1558 0.488
160 0.369
4 0.75
0 0.0
0 0.0
0 0.0
0.47738593345287905
0.5152420800956365
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4981,	 Acc1 = 0.3916,	 Acc2 = 0.4158

 ===== Epoch 38	 =====
[-0.3669651  -0.38051367  3.1773324  -4.552917   -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 1.0109920e-03  2.5630770e-03 -1.6524813e-01  7.8748002e+00
  6.2680340e+00  1.3260071e+00  1.5849024e-03  3.2306046e-03
 -1.4162532e-03  3.5063999e-03  1.5010408e-03  2.7898324e-03
 -1.4857454e-03 -1.4670575e-03] 2 5
train:	 Loss = 1.4440,	 Acc = 0.4526
5569 0.232
10874 0.493
5611 0.575
721 0.587
125 0.488
12 0.25
0 0.0
0 0.0
0.5233235311076515
0.5152420800956365
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4176,	 Acc = 0.4547
715 0.327
3297 0.491
1558 0.446
160 0.356
4 0.75
0 0.0
0 0.0
0 0.0
0.47280334728033474
0.5152420800956365
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4883,	 Acc1 = 0.3910,	 Acc2 = 0.4150

 ===== Epoch 39	 =====
[ 4.833306    2.015359   -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892  2.2727091   3.2861788  -0.42425194 -0.42087102
  3.1937401   2.345313  ] [-5.6878729e+00 -2.8666859e+00  2.3875074e-03  1.9614373e-03
  1.0307996e-03  7.6088449e-04  1.5849024e-03  3.2306046e-03
 -2.2149031e-01 -4.5460117e-01  1.5010408e-03  2.7898324e-03
 -2.0453589e-01 -4.6518978e-01] 5 5
train:	 Loss = 1.4473,	 Acc = 0.4527
5568 0.234
10873 0.496
5613 0.568
721 0.594
125 0.456
12 0.083
0 0.0
0 0.0
0.5228321033210332
0.5152420800956365
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4486,	 Acc = 0.4433
715 0.319
3297 0.47
1558 0.452
160 0.356
4 0.75
0 0.0
0 0.0
0 0.0
0.46104801753337316
0.5152420800956365
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 4
Testing:	 Loss = 1.5020,	 Acc1 = 0.4007,	 Acc2 = 0.4267

 ===== Epoch 40	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 0 1
train:	 Loss = 1.4464,	 Acc = 0.4495
5570 0.231
10878 0.491
5606 0.57
721 0.578
125 0.456
12 0.167
0 0.0
0 0.0
0.5196055818244724
0.5152420800956365
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 3
val:	 Loss = 1.4386,	 Acc = 0.4475
715 0.317
3297 0.472
1558 0.463
160 0.356
4 0.75
0 0.0
0 0.0
0 0.0
0.4660290894600518
0.5152420800956365
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.5138,	 Acc1 = 0.3833,	 Acc2 = 0.4058

 ===== Epoch 41	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239   1.80579     2.208032
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064  -0.2574555   0.59604937
 -0.00148575 -0.00146706] 6 6
train:	 Loss = 1.4454,	 Acc = 0.4491
5568 0.232
10873 0.487
5614 0.574
720 0.583
125 0.448
12 0.167
0 0.0
0 0.0
0.5189114391143912
0.5152420800956365
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 3
val:	 Loss = 1.4054,	 Acc = 0.4867
715 0.326
3297 0.494
1558 0.556
160 0.375
4 0.75
0 0.0
0 0.0
0 0.0
0.5096632795377565
0.5152420800956365
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 4
Testing:	 Loss = 1.4754,	 Acc1 = 0.4056,	 Acc2 = 0.4327

 ===== Epoch 42	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239   0.9701986   2.176062
 -0.38672873 -0.38123247] [ 1.01099198e-03  2.56307703e-03  2.38750735e-03  1.96143729e-03
  1.03079958e-03  7.60884490e-04  1.58490241e-03  3.23060458e-03
 -1.41625316e-03  3.50639992e-03  1.09821856e-01  8.47591400e-01
 -1.48574542e-03 -1.46705750e-03] 6 6
train:	 Loss = 1.4443,	 Acc = 0.4488
5568 0.229
10877 0.49
5609 0.571
721 0.574
125 0.488
12 0.167
0 0.0
0 0.0
0.5194880073800738
0.5152420800956365
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4311,	 Acc = 0.4599
715 0.315
3297 0.494
1558 0.468
160 0.312
4 0.75
0 0.0
0 0.0
0 0.0
0.4805738194859534
0.5152420800956365
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 4
Testing:	 Loss = 1.4969,	 Acc1 = 0.4033,	 Acc2 = 0.4299

 ===== Epoch 43	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892  1.2636079   2.122351   -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306   0.15686293 -0.19347987  0.00150104  0.00278983
 -0.00148575 -0.00146706] 2 5
train:	 Loss = 1.4443,	 Acc = 0.4512
5569 0.229
10874 0.49
5611 0.579
721 0.595
125 0.464
12 0.25
0 0.0
0 0.0
0.5227469295969556
0.5152420800956365
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4417,	 Acc = 0.4339
715 0.186
3297 0.492
1558 0.436
160 0.337
4 0.0
0 0.0
0 0.0
0 0.0
0.46921697549312613
0.5152420800956365
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 6
Testing:	 Loss = 1.5033,	 Acc1 = 0.3699,	 Acc2 = 0.4235

 ===== Epoch 44	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  3.8152783   1.1628909  -0.40146217 -0.4092239   2.6939359   2.387555
 -0.38672873 -0.38123247] [ 1.0109920e-03  2.5630770e-03  2.3875074e-03  1.9614373e-03
  1.0307996e-03  7.6088449e-04  9.6771568e-01 -9.5024548e-02
 -1.4162532e-03  3.5063999e-03 -1.0365434e-03 -8.7385610e-02
 -1.4857454e-03 -1.4670575e-03] 2 2
train:	 Loss = 1.4422,	 Acc = 0.4507
5566 0.232
10877 0.493
5613 0.57
719 0.569
125 0.48
12 0.167
0 0.0
0 0.0
0.5209846650524617
0.5152420800956365
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4292,	 Acc = 0.4484
715 0.316
3297 0.484
1558 0.445
160 0.325
4 0.75
0 0.0
0 0.0
0 0.0
0.46722454672245467
0.5152420800956365
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 4
Testing:	 Loss = 1.4972,	 Acc1 = 0.3980,	 Acc2 = 0.4235

 ===== Epoch 45	 =====
[ 2.3891435   3.1817374  -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
  2.5422328   3.4422295 ] [ 0.62893456  0.09326642  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.3197579   0.17179199] 0 4
train:	 Loss = 1.4439,	 Acc = 0.4543
5567 0.234
10877 0.494
5610 0.577
721 0.605
125 0.464
12 0.167
0 0.0
0 0.0
0.5251081003170943
0.5152420800956365
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 3
val:	 Loss = 1.4158,	 Acc = 0.4442
715 0.18
3297 0.498
1558 0.46
160 0.375
4 0.0
0 0.0
0 0.0
0 0.0
0.48176927674835623
0.5152420800956365
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4812,	 Acc1 = 0.3660,	 Acc2 = 0.4187

 ===== Epoch 46	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438  1.2588531   1.9972183
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144 -0.06864249  0.41844553
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 3 6
train:	 Loss = 1.4469,	 Acc = 0.4488
5569 0.227
10875 0.49
5612 0.571
719 0.584
125 0.48
12 0.167
0 0.0
0 0.0
0.5199215821945453
0.5152420800956365
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4504,	 Acc = 0.4597
715 0.32
3297 0.496
1558 0.453
160 0.4
4 0.75
0 0.0
0 0.0
0 0.0
0.47957760510061764
0.5152420800956365
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.5092,	 Acc1 = 0.4029,	 Acc2 = 0.4294

 ===== Epoch 47	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892  1.6001651   2.1894948  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.17920677  0.01266855  0.00150104  0.00278983
 -0.00148575 -0.00146706] 5 0
train:	 Loss = 1.4435,	 Acc = 0.4529
5567 0.234
10876 0.493
5612 0.575
721 0.589
124 0.46
12 0.083
0 0.0
0 0.0
0.5230902277313347
0.5152420800956365
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4375,	 Acc = 0.4463
715 0.316
3297 0.484
1558 0.437
160 0.325
4 0.75
0 0.0
0 0.0
0 0.0
0.46483363219764895
0.5152420800956365
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 4
Testing:	 Loss = 1.5033,	 Acc1 = 0.3945,	 Acc2 = 0.4192

 ===== Epoch 48	 =====
[ 2.709225    2.0380805  -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
  2.018031    2.2980776 ] [-1.4090689e+00  1.0536021e-01  2.3875074e-03  1.9614373e-03
  1.0307996e-03  7.6088449e-04  1.5849024e-03  3.2306046e-03
 -1.4162532e-03  3.5063999e-03  1.5010408e-03  2.7898324e-03
  3.2151026e-01  1.8198368e-01] 1 2
train:	 Loss = 1.4438,	 Acc = 0.4493
5567 0.23
10874 0.488
5613 0.576
721 0.571
125 0.496
12 0.167
0 0.0
0 0.0
0.5198039780916691
0.5152420800956365
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4182,	 Acc = 0.4444
715 0.179
3297 0.496
1558 0.466
160 0.369
4 0.0
0 0.0
0 0.0
0 0.0
0.48216776250249055
0.5152420800956365
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 4
Testing:	 Loss = 1.4856,	 Acc1 = 0.3747,	 Acc2 = 0.4292

 ===== Epoch 49	 =====
[ 0.6076411   1.2175761  -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [-0.002504    0.00860997  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 0 0
train:	 Loss = 1.4439,	 Acc = 0.4537
5568 0.23
10877 0.494
5610 0.58
720 0.592
125 0.456
12 0.25
0 0.0
0 0.0
0.5256572878228782
0.5152420800956365
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4479,	 Acc = 0.4344
715 0.317
3297 0.469
1558 0.425
160 0.337
4 0.75
0 0.0
0 0.0
0 0.0
0.45108587368001596
0.5152420800956365
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 4
Testing:	 Loss = 1.5005,	 Acc1 = 0.3998,	 Acc2 = 0.4257

 ===== Epoch 50	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239   2.7135975   1.5833908
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.7259002   0.01228198
 -0.00148575 -0.00146706] 0 2
train:	 Loss = 1.4416,	 Acc = 0.4510
5566 0.23
10879 0.49
5610 0.577
720 0.592
125 0.456
12 0.25
0 0.0
0 0.0
0.5217917675544794
0.5152420800956365
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4277,	 Acc = 0.4559
715 0.326
3297 0.474
1558 0.483
160 0.387
4 0.75
0 0.0
0 0.0
0 0.0
0.4743972902968719
0.5152420800956365
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4999,	 Acc1 = 0.3951,	 Acc2 = 0.4200

 ===== Epoch 51	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438  0.8015398   2.1637144
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144 -0.13373041 -0.13073243
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 1 5
train:	 Loss = 1.4429,	 Acc = 0.4500
5567 0.233
10876 0.491
5612 0.569
720 0.569
125 0.52
12 0.25
0 0.0
0 0.0
0.5196886710867685
0.5152420800956365
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4156,	 Acc = 0.4737
715 0.316
3297 0.489
1558 0.522
160 0.381
4 0.75
0 0.0
0 0.0
0 0.0
0.49611476389719067
0.5152420800956365
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 4
Testing:	 Loss = 1.4827,	 Acc1 = 0.4068,	 Acc2 = 0.4341

 ===== Epoch 52	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 1 1
train:	 Loss = 1.4414,	 Acc = 0.4525
5567 0.23
10875 0.493
5613 0.574
720 0.601
125 0.52
12 0.25
0 0.0
0 0.0
0.5237244162582877
0.5152420800956365
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 3
val:	 Loss = 1.4252,	 Acc = 0.4688
715 0.327
3297 0.487
1558 0.499
160 0.437
4 0.75
0 0.0
0 0.0
0 0.0
0.48894202032277345
0.5152420800956365
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 4
Testing:	 Loss = 1.4859,	 Acc1 = 0.4097,	 Acc2 = 0.4376

 ===== Epoch 53	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 6 1
train:	 Loss = 1.4478,	 Acc = 0.4530
5566 0.231
10877 0.494
5611 0.576
721 0.598
125 0.464
12 0.167
0 0.0
0 0.0
0.5242707252392482
0.5152420800956365
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4173,	 Acc = 0.4557
715 0.323
3297 0.49
1558 0.455
160 0.344
4 0.75
0 0.0
0 0.0
0 0.0
0.474596533173939
0.5152420800956365
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 6
Testing:	 Loss = 1.4878,	 Acc1 = 0.3972,	 Acc2 = 0.4225

 ===== Epoch 54	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 3 1
train:	 Loss = 1.4408,	 Acc = 0.4528
5569 0.233
10876 0.492
5610 0.577
720 0.594
125 0.464
12 0.083
0 0.0
0 0.0
0.5233811912587211
0.5152420800956365
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4096,	 Acc = 0.4960
715 0.324
3297 0.494
1558 0.593
160 0.344
4 0.75
0 0.0
0 0.0
0 0.0
0.5204223948993824
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4962,	 Acc1 = 0.4025,	 Acc2 = 0.4289

 ===== Epoch 55	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892  1.2739286   3.2961261  -0.42425194 -0.42087102
  2.361185    2.303326  ] [ 3.7834585e+00  2.8748357e+00  2.3875074e-03  1.9614373e-03
  1.0307996e-03  7.6088449e-04  1.5849024e-03  3.2306046e-03
  1.3843071e-01 -8.8115111e-02  1.5010408e-03  2.7898324e-03
  5.0189316e-01 -1.1357585e-01] 2 3
train:	 Loss = 1.4419,	 Acc = 0.4485
5570 0.225
10870 0.486
5615 0.578
720 0.593
125 0.52
12 0.167
0 0.0
0 0.0
0.5201822165840156
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4259,	 Acc = 0.4566
715 0.326
3297 0.492
1558 0.448
160 0.387
4 0.75
0 0.0
0 0.0
0 0.0
0.47519426180514046
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 4
Testing:	 Loss = 1.4807,	 Acc1 = 0.4132,	 Acc2 = 0.4418

 ===== Epoch 56	 =====
[ 4.885073    2.2425754  -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
  2.1620762   3.1666882 ] [-5.7445045e+00 -3.1387963e+00  2.3875074e-03  1.9614373e-03
  1.0307996e-03  7.6088449e-04  1.5849024e-03  3.2306046e-03
 -1.4162532e-03  3.5063999e-03  1.5010408e-03  2.7898324e-03
 -1.4881381e-01 -3.6836854e-01] 2 5
train:	 Loss = 1.4399,	 Acc = 0.4509
5565 0.237
10879 0.488
5610 0.573
721 0.596
125 0.456
12 0.167
0 0.0
0 0.0
0.5196864011068196
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4260,	 Acc = 0.4716
715 0.326
3297 0.492
1558 0.504
160 0.381
4 0.75
0 0.0
0 0.0
0 0.0
0.49232914923291493
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4875,	 Acc1 = 0.4128,	 Acc2 = 0.4414

 ===== Epoch 57	 =====
[ 0.4784495   1.8664057  -0.4202629  -0.33521438 -0.4411929  -0.3648088
  3.9390469   1.7211708  -0.40146217 -0.4092239   4.4743886   1.9104667
  4.601597    3.510459  ] [-9.2383802e-01 -2.6883028e+00  2.3875074e-03  1.9614373e-03
  1.0307996e-03  7.6088449e-04  7.5753039e-01 -4.5896973e-02
 -1.4162532e-03  3.5063999e-03 -1.3652129e+00 -1.6194472e-02
  1.2564914e+00 -2.6946329e-02] 2 3
train:	 Loss = 1.4392,	 Acc = 0.4537
5568 0.235
10875 0.491
5611 0.579
721 0.594
125 0.52
12 0.167
0 0.0
0 0.0
0.523869926199262
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4185,	 Acc = 0.4615
715 0.317
3297 0.487
1558 0.478
160 0.419
4 0.75
0 0.0
0 0.0
0 0.0
0.48196851962542336
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 4
Testing:	 Loss = 1.4790,	 Acc1 = 0.4077,	 Acc2 = 0.4351

 ===== Epoch 58	 =====
[ 2.769631    3.4720697  -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239   2.385032    1.7653729
 -0.38672873 -0.38123247] [-3.4302969e+00 -4.6112137e+00  2.3875074e-03  1.9614373e-03
  1.0307996e-03  7.6088449e-04  1.5849024e-03  3.2306046e-03
 -1.4162532e-03  3.5063999e-03 -4.9102309e-01  9.2965275e-02
 -1.4857454e-03 -1.4670575e-03] 2 3
train:	 Loss = 1.4418,	 Acc = 0.4509
5568 0.239
10876 0.488
5611 0.571
721 0.594
124 0.484
12 0.167
0 0.0
0 0.0
0.5188537822878229
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4102,	 Acc = 0.4836
715 0.323
3297 0.498
1558 0.535
160 0.387
4 0.75
0 0.0
0 0.0
0 0.0
0.5064753935046822
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 4
Testing:	 Loss = 1.4820,	 Acc1 = 0.4145,	 Acc2 = 0.4433

 ===== Epoch 59	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 2 1
train:	 Loss = 1.4384,	 Acc = 0.4541
5564 0.236
10881 0.494
5609 0.576
721 0.594
125 0.456
12 0.25
0 0.0
0 0.0
0.524094996541388
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4435,	 Acc = 0.4374
715 0.323
3297 0.475
1558 0.418
160 0.356
4 0.75
0 0.0
0 0.0
0 0.0
0.4536760310818888
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.5078,	 Acc1 = 0.3965,	 Acc2 = 0.4217

 ===== Epoch 60	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892  1.6890361   3.032524   -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306   0.13409412 -0.1110205   0.00150104  0.00278983
 -0.00148575 -0.00146706] 6 2
train:	 Loss = 1.4426,	 Acc = 0.4524
5568 0.24
10873 0.492
5614 0.571
720 0.579
125 0.464
12 0.25
0 0.0
0 0.0
0.520410516605166
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4281,	 Acc = 0.4534
715 0.319
3297 0.484
1558 0.46
160 0.356
4 0.75
0 0.0
0 0.0
0 0.0
0.4726041044032676
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 4
Testing:	 Loss = 1.4938,	 Acc1 = 0.3957,	 Acc2 = 0.4207

 ===== Epoch 61	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 6 1
train:	 Loss = 1.4417,	 Acc = 0.4519
5568 0.235
10875 0.49
5611 0.577
721 0.585
125 0.488
12 0.0
0 0.0
0 0.0
0.5216789667896679
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 3
val:	 Loss = 1.4366,	 Acc = 0.4656
715 0.327
3297 0.486
1558 0.496
160 0.369
4 0.75
0 0.0
0 0.0
0 0.0
0.48535564853556484
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 4
Testing:	 Loss = 1.4969,	 Acc1 = 0.4031,	 Acc2 = 0.4297

 ===== Epoch 62	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239   1.3112411   2.4096878
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064  -0.1339013   0.49163568
 -0.00148575 -0.00146706] 4 6
train:	 Loss = 1.4384,	 Acc = 0.4532
5567 0.234
10878 0.493
5610 0.575
720 0.599
125 0.456
12 0.167
0 0.0
0 0.0
0.5234938022484866
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4029,	 Acc = 0.4561
715 0.183
3297 0.502
1558 0.494
160 0.362
4 0.0
0 0.0
0 0.0
0 0.0
0.4949193066347878
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4826,	 Acc1 = 0.3745,	 Acc2 = 0.4289

 ===== Epoch 63	 =====
[-0.3669651  -0.38051367  2.456762    2.1387177  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.527999    0.11931741  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 2 3
train:	 Loss = 1.4405,	 Acc = 0.4503
5570 0.224
10871 0.495
5613 0.569
721 0.602
125 0.448
12 0.167
0 0.0
0 0.0
0.5228923999538693
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 3
val:	 Loss = 1.4424,	 Acc = 0.4358
715 0.317
3297 0.468
1558 0.433
160 0.319
4 0.75
0 0.0
0 0.0
0 0.0
0.4526798166965531
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 4
Testing:	 Loss = 1.5060,	 Acc1 = 0.3870,	 Acc2 = 0.4103

 ===== Epoch 64	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 5 1
train:	 Loss = 1.4413,	 Acc = 0.4504
5569 0.227
10873 0.493
5612 0.573
721 0.584
125 0.472
12 0.167
0 0.0
0 0.0
0.5220550077841204
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4103,	 Acc = 0.4700
715 0.315
3297 0.497
1558 0.493
160 0.369
4 0.75
0 0.0
0 0.0
0 0.0
0.4921299063558478
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4768,	 Acc1 = 0.4071,	 Acc2 = 0.4344

 ===== Epoch 65	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 1 1
train:	 Loss = 1.4391,	 Acc = 0.4521
5568 0.233
10879 0.491
5608 0.576
720 0.6
125 0.448
12 0.083
0 0.0
0 0.0
0.5224285055350554
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.3987,	 Acc = 0.4838
715 0.323
3297 0.494
1558 0.546
160 0.381
4 0.75
0 0.0
0 0.0
0 0.0
0.5066746363817494
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 4
Testing:	 Loss = 1.4737,	 Acc1 = 0.4124,	 Acc2 = 0.4409

 ===== Epoch 66	 =====
[ 0.20217434  1.2402978  -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.025917    0.54678315  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 6 6
train:	 Loss = 1.4353,	 Acc = 0.4547
5569 0.241
10878 0.492
5607 0.574
721 0.605
125 0.496
12 0.167
0 0.0
0 0.0
0.5231505506544427
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 3
val:	 Loss = 1.4277,	 Acc = 0.4475
715 0.18
3297 0.496
1558 0.478
160 0.369
4 0.0
0 0.0
0 0.0
0 0.0
0.485554891412632
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 4
Testing:	 Loss = 1.4879,	 Acc1 = 0.3761,	 Acc2 = 0.4309

 ===== Epoch 67	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  1.9204973   1.0205295  -0.40146217 -0.4092239   1.8662853   3.081054
 -0.38672873 -0.38123247] [ 1.0109920e-03  2.5630770e-03  2.3875074e-03  1.9614373e-03
  1.0307996e-03  7.6088449e-04 -4.8482771e+00 -3.0611022e+00
 -1.4162532e-03  3.5063999e-03 -2.9638255e-01 -2.0940548e-02
 -1.4857454e-03 -1.4670575e-03] 3 3
train:	 Loss = 1.4383,	 Acc = 0.4536
5567 0.233
10880 0.494
5607 0.574
721 0.61
125 0.472
12 0.167
0 0.0
0 0.0
0.524416258287691
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4355,	 Acc = 0.4496
715 0.31
3297 0.487
1558 0.445
160 0.35
4 0.75
0 0.0
0 0.0
0 0.0
0.46941621837019326
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.5084,	 Acc1 = 0.3868,	 Acc2 = 0.4100

 ===== Epoch 68	 =====
[ 2.001722    2.1415904  -0.4202629  -0.33521438 -0.4411929  -0.3648088
  0.6687463   1.3080436  -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.52583647  0.03279753  0.00238751  0.00196144  0.0010308   0.00076088
  0.00994668 -0.11958833 -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 3 2
train:	 Loss = 1.4391,	 Acc = 0.4503
5562 0.231
10879 0.49
5613 0.572
721 0.596
125 0.44
12 0.083
0 0.0
0 0.0
0.5204610951008646
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 3
val:	 Loss = 1.4063,	 Acc = 0.4927
715 0.326
3297 0.503
1558 0.556
160 0.406
4 0.75
0 0.0
0 0.0
0 0.0
0.5164375373580394
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4838,	 Acc1 = 0.4073,	 Acc2 = 0.4346

 ===== Epoch 69	 =====
[ 2.0790942   3.2574763  -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
  1.7911711   3.5550702 ] [-0.23791634 -0.17279671  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.33864725 -0.26135564] 5 5
train:	 Loss = 1.4393,	 Acc = 0.4497
5571 0.232
10872 0.489
5611 0.569
721 0.605
125 0.512
12 0.167
0 0.0
0 0.0
0.5197508794187187
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4516,	 Acc = 0.4334
715 0.308
3297 0.468
1558 0.427
160 0.337
4 0.75
0 0.0
0 0.0
0 0.0
0.4512851165570831
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 6
Testing:	 Loss = 1.5128,	 Acc1 = 0.3850,	 Acc2 = 0.4078

 ===== Epoch 70	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 0 1
train:	 Loss = 1.4418,	 Acc = 0.4511
5569 0.233
10877 0.491
5609 0.569
720 0.619
125 0.48
12 0.167
0 0.0
0 0.0
0.5211901055180764
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4106,	 Acc = 0.4733
715 0.308
3297 0.491
1558 0.52
160 0.381
4 0.75
0 0.0
0 0.0
0 0.0
0.49691173540545924
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 4
Testing:	 Loss = 1.4813,	 Acc1 = 0.4091,	 Acc2 = 0.4369

 ===== Epoch 71	 =====
[-0.3669651  -0.38051367  1.7748457   1.7396232  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.37063769 -0.330131    0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 2 5
train:	 Loss = 1.4388,	 Acc = 0.4519
5567 0.235
10878 0.49
5609 0.571
721 0.617
125 0.512
12 0.083
0 0.0
0 0.0
0.5214182761602767
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4369,	 Acc = 0.4513
715 0.312
3297 0.477
1558 0.466
160 0.394
4 0.75
0 0.0
0 0.0
0 0.0
0.4712094042637976
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4953,	 Acc1 = 0.4013,	 Acc2 = 0.4274

 ===== Epoch 72	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239   1.2027272   2.2350833
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064  -0.14828706  0.46790528
 -0.00148575 -0.00146706] 6 6
train:	 Loss = 1.4404,	 Acc = 0.4525
5568 0.233
10878 0.492
5609 0.573
720 0.611
125 0.512
12 0.083
0 0.0
0 0.0
0.5228321033210332
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4122,	 Acc = 0.4756
715 0.319
3297 0.49
1558 0.531
160 0.337
4 0.75
0 0.0
0 0.0
0 0.0
0.497907949790795
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4888,	 Acc1 = 0.4031,	 Acc2 = 0.4297

 ===== Epoch 73	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  1.9475007   2.6981606  -0.40146217 -0.4092239   2.8497117   3.2876282
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.22370921 -0.17485686 -0.00141625  0.0035064  -0.0788959  -0.04467092
 -0.00148575 -0.00146706] 6 3
train:	 Loss = 1.4382,	 Acc = 0.4532
5568 0.233
10877 0.49
5611 0.579
720 0.618
124 0.476
12 0.083
0 0.0
0 0.0
0.523869926199262
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4457,	 Acc = 0.4440
715 0.319
3297 0.47
1558 0.453
160 0.369
4 0.75
0 0.0
0 0.0
0 0.0
0.4618449890416418
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 4
Testing:	 Loss = 1.5002,	 Acc1 = 0.4007,	 Acc2 = 0.4267

 ===== Epoch 74	 =====
[-0.3669651  -0.38051367  1.8676127   0.82125247 -0.4411929  -0.3648088
  1.2313315   3.8370516  -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 1.0109920e-03  2.5630770e-03 -4.5196855e-01  9.1851123e-02
  1.0307996e-03  7.6088449e-04 -3.3853495e+00 -9.2573175e+00
 -1.4162532e-03  3.5063999e-03  1.5010408e-03  2.7898324e-03
 -1.4857454e-03 -1.4670575e-03] 4 4
train:	 Loss = 1.4386,	 Acc = 0.4525
5565 0.232
10878 0.493
5611 0.574
721 0.601
125 0.512
12 0.083
0 0.0
0 0.0
0.5230875655732979
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 3
val:	 Loss = 1.4293,	 Acc = 0.4397
715 0.178
3297 0.48
1558 0.483
160 0.356
4 0.0
0 0.0
0 0.0
0 0.0
0.4769874476987448
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4990,	 Acc1 = 0.3602,	 Acc2 = 0.4118

 ===== Epoch 75	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 6
train:	 Loss = 1.4389,	 Acc = 0.4528
5565 0.232
10875 0.493
5614 0.574
721 0.613
125 0.48
12 0.167
0 0.0
0 0.0
0.5236063872715744
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 3
val:	 Loss = 1.4140,	 Acc = 0.4608
715 0.32
3297 0.481
1558 0.49
160 0.375
4 0.75
0 0.0
0 0.0
0 0.0
0.48077306236302053
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4875,	 Acc1 = 0.3949,	 Acc2 = 0.4197

 ===== Epoch 76	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.978155    1.8802805  -0.40146217 -0.4092239   3.1578593   2.697416
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.51271296  0.14447239 -0.00141625  0.0035064   0.74621135  0.05974275
 -0.00148575 -0.00146706] 2 2
train:	 Loss = 1.4380,	 Acc = 0.4547
5572 0.24
10876 0.492
5608 0.577
720 0.606
125 0.496
11 0.273
0 0.0
0 0.0
0.5238177623990773
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4333,	 Acc = 0.4847
715 0.324
3297 0.5
1558 0.535
160 0.381
4 0.75
0 0.0
0 0.0
0 0.0
0.5074716078900179
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.5026,	 Acc1 = 0.4033,	 Acc2 = 0.4299

 ===== Epoch 77	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438  2.2667768   1.7352641
 -0.36420864 -0.37237892  0.9207424   2.3088617  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144 -0.0970616   0.03943539
  0.0015849   0.0032306   0.04303221  0.17758727  0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 4
train:	 Loss = 1.4439,	 Acc = 0.4517
5568 0.234
10874 0.492
5613 0.573
721 0.596
124 0.427
12 0.083
0 0.0
0 0.0
0.5215059963099631
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4309,	 Acc = 0.4655
715 0.31
3297 0.482
1558 0.51
160 0.375
4 0.75
0 0.0
0 0.0
0 0.0
0.48754732018330343
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4872,	 Acc1 = 0.4149,	 Acc2 = 0.4438

 ===== Epoch 78	 =====
[-0.3669651  -0.38051367  2.0258856   1.653455   -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 1.01099198e-03  2.56307703e-03 -8.54109466e-01 -1.22885354e-01
  1.03079958e-03  7.60884490e-04  1.58490241e-03  3.23060458e-03
 -1.41625316e-03  3.50639992e-03  1.50104077e-03  2.78983242e-03
 -1.48574542e-03 -1.46705750e-03] 2 1
train:	 Loss = 1.4391,	 Acc = 0.4525
5568 0.237
10877 0.49
5609 0.572
721 0.62
125 0.496
12 0.083
0 0.0
0 0.0
0.5216213099630996
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4290,	 Acc = 0.4496
715 0.319
3297 0.484
1558 0.445
160 0.356
4 0.75
0 0.0
0 0.0
0 0.0
0.4682207611077904
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4994,	 Acc1 = 0.3996,	 Acc2 = 0.4254

 ===== Epoch 79	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438  1.8878495   2.0904562
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.3906486   0.11936269
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 6 2
train:	 Loss = 1.4364,	 Acc = 0.4535
5568 0.243
10875 0.49
5613 0.572
719 0.601
125 0.504
12 0.25
0 0.0
0 0.0
0.5211023985239852
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4120,	 Acc = 0.4728
715 0.315
3297 0.492
1558 0.515
160 0.375
4 0.75
0 0.0
0 0.0
0 0.0
0.4953177923889221
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4836,	 Acc1 = 0.4083,	 Acc2 = 0.4359

 ===== Epoch 80	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 3 1
train:	 Loss = 1.4377,	 Acc = 0.4516
5569 0.231
10876 0.492
5612 0.573
718 0.599
125 0.464
12 0.25
0 0.0
0 0.0
0.522400968690538
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4617,	 Acc = 0.4395
715 0.323
3297 0.48
1558 0.418
160 0.325
4 0.75
0 0.0
0 0.0
0 0.0
0.4560669456066946
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.5191,	 Acc1 = 0.4000,	 Acc2 = 0.4259

 ===== Epoch 81	 =====
[ 0.21192986  1.5306302  -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 0 0
train:	 Loss = 1.4402,	 Acc = 0.4510
5570 0.232
10876 0.491
5608 0.572
721 0.603
125 0.536
12 0.0
0 0.0
0 0.0
0.521450813055011
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4450,	 Acc = 0.4512
715 0.33
3297 0.485
1558 0.449
160 0.312
4 0.75
0 0.0
0 0.0
0 0.0
0.46842000398485756
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 4
Testing:	 Loss = 1.5001,	 Acc1 = 0.4058,	 Acc2 = 0.4329

 ===== Epoch 82	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438  4.0436435   2.4012492
 -0.36420864 -0.37237892  3.874084    1.7543026  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 1.0109920e-03  2.5630770e-03  2.3875074e-03  1.9614373e-03
 -1.1843306e+00  7.6088449e-04  1.5849024e-03  3.2306046e-03
 -1.4162532e-03  3.5063999e-03  1.5010408e-03  2.7898324e-03
 -1.4857454e-03 -1.4670575e-03] 0 0
train:	 Loss = 1.4365,	 Acc = 0.4512
5565 0.233
10879 0.488
5611 0.575
720 0.622
125 0.48
12 0.25
0 0.0
0 0.0
0.5213005130570127
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4207,	 Acc = 0.4543
715 0.326
3297 0.486
1558 0.456
160 0.35
4 0.75
0 0.0
0 0.0
0 0.0
0.4726041044032676
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 6
Testing:	 Loss = 1.4907,	 Acc1 = 0.4000,	 Acc2 = 0.4259

 ===== Epoch 83	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 1.0109920e-03  2.5630770e-03  4.3362632e+00  1.3727793e+00
  1.0307996e-03  7.6088449e-04  1.5849024e-03  3.2306046e-03
 -1.4162532e-03  3.5063999e-03  1.5010408e-03  2.7898324e-03
 -1.4857454e-03 -1.4670575e-03] 6 1
train:	 Loss = 1.4411,	 Acc = 0.4534
5566 0.236
10874 0.491
5614 0.573
721 0.628
125 0.504
12 0.25
0 0.0
0 0.0
0.5232330220223683
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4378,	 Acc = 0.4519
715 0.186
3297 0.499
1558 0.488
160 0.325
4 0.0
0 0.0
0 0.0
0 0.0
0.489738991831042
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 6
Testing:	 Loss = 1.5091,	 Acc1 = 0.3668,	 Acc2 = 0.4197

 ===== Epoch 84	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438  2.119182    2.3901494
 -0.36420864 -0.37237892  1.1850578   3.365757   -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 1.0109920e-03  2.5630770e-03  2.3875074e-03  1.9614373e-03
  5.2357906e-01  7.0374988e-02  1.5849024e-03  3.2306046e-03
 -3.0012307e+00 -6.9505663e+00  1.5010408e-03  2.7898324e-03
 -1.4857454e-03 -1.4670575e-03] 4 3
train:	 Loss = 1.4387,	 Acc = 0.4541
5570 0.242
10875 0.488
5610 0.578
720 0.612
125 0.496
12 0.083
0 0.0
0 0.0
0.5222004382424172
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4292,	 Acc = 0.4646
715 0.319
3297 0.494
1558 0.479
160 0.356
4 0.75
0 0.0
0 0.0
0 0.0
0.48535564853556484
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 6
Testing:	 Loss = 1.4920,	 Acc1 = 0.4048,	 Acc2 = 0.4317

 ===== Epoch 85	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 6 1
train:	 Loss = 1.4382,	 Acc = 0.4521
5569 0.231
10875 0.492
5610 0.573
721 0.619
125 0.48
12 0.083
0 0.0
0 0.0
0.5231505506544427
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4305,	 Acc = 0.4604
715 0.327
3297 0.494
1558 0.46
160 0.35
4 0.75
0 0.0
0 0.0
0 0.0
0.4793783622235505
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4969,	 Acc1 = 0.4042,	 Acc2 = 0.4309

 ===== Epoch 86	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 3.2325666e+00  2.3789909e+00  2.3875074e-03  1.9614373e-03
  1.0307996e-03  7.6088449e-04  1.5849024e-03  3.2306046e-03
 -1.4162532e-03  3.5063999e-03  1.5010408e-03  2.7898324e-03
 -1.4857454e-03 -1.4670575e-03] 4 1
train:	 Loss = 1.4371,	 Acc = 0.4512
5567 0.228
10874 0.489
5614 0.576
721 0.62
125 0.504
11 0.182
0 0.0
0 0.0
0.5226866532141827
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4154,	 Acc = 0.4813
715 0.323
3297 0.495
1558 0.535
160 0.375
4 0.75
0 0.0
0 0.0
0 0.0
0.5038852361028093
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4894,	 Acc1 = 0.4102,	 Acc2 = 0.4381

 ===== Epoch 87	 =====
[-0.3669651  -0.38051367  3.7815368  -5.051785   -0.4411929  -0.3648088
  2.7233047   3.3011029  -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 1.0109920e-03  2.5630770e-03 -9.4576597e+00  5.1955876e+00
  1.0307996e-03  7.6088449e-04 -6.5524373e+00 -8.0782557e+00
 -1.4162532e-03  3.5063999e-03  1.5010408e-03  2.7898324e-03
 -1.4857454e-03 -1.4670575e-03] 2 1
train:	 Loss = 1.4354,	 Acc = 0.4550
5569 0.24
10876 0.493
5609 0.575
721 0.613
125 0.488
12 0.167
0 0.0
0 0.0
0.5240731130715562
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4185,	 Acc = 0.4808
715 0.323
3297 0.497
1558 0.531
160 0.369
4 0.75
0 0.0
0 0.0
0 0.0
0.5032875074716079
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 4
Testing:	 Loss = 1.4867,	 Acc1 = 0.4165,	 Acc2 = 0.4458

 ===== Epoch 88	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.2304795   3.7198129  -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 1.0109920e-03  2.5630770e-03  3.3899934e+00  1.2953742e+00
  1.0307996e-03  7.6088449e-04 -5.5062919e+00 -8.9993973e+00
 -1.4162532e-03  3.5063999e-03  1.5010408e-03  2.7898324e-03
 -1.4857454e-03 -1.4670575e-03] 6 1
train:	 Loss = 1.4384,	 Acc = 0.4533
5567 0.239
10879 0.488
5609 0.577
720 0.611
125 0.536
12 0.167
0 0.0
0 0.0
0.52211011818968
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4645,	 Acc = 0.4533
715 0.323
3297 0.478
1558 0.468
160 0.375
4 0.75
0 0.0
0 0.0
0 0.0
0.471807132894999
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 4
Testing:	 Loss = 1.5108,	 Acc1 = 0.4141,	 Acc2 = 0.4428

 ===== Epoch 89	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  0.9050329   2.4999712  -0.40146217 -0.4092239   2.1279268   1.9448957
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.2563677   0.20588185 -0.00141625  0.0035064   0.02265717  0.12144174
 -0.00148575 -0.00146706] 1 4
train:	 Loss = 1.4389,	 Acc = 0.4524
5568 0.233
10877 0.49
5611 0.575
719 0.627
125 0.48
12 0.167
0 0.0
0 0.0
0.5226591328413284
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4265,	 Acc = 0.4717
715 0.327
3297 0.493
1558 0.499
160 0.406
4 0.75
0 0.0
0 0.0
0 0.0
0.49232914923291493
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 4
Testing:	 Loss = 1.4852,	 Acc1 = 0.4099,	 Acc2 = 0.4379

 ===== Epoch 90	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 1.0109920e-03  2.5630770e-03  2.3875074e-03  1.9614373e-03
  1.0307996e-03  7.6088449e-04  1.5849024e-03  3.2306046e-03
 -1.4162532e-03  3.5063999e-03  4.5866222e+00  2.4280345e+00
 -1.4857454e-03 -1.4670575e-03] 2 1
train:	 Loss = 1.4380,	 Acc = 0.4545
5567 0.239
10880 0.49
5609 0.579
720 0.622
124 0.444
12 0.167
0 0.0
0 0.0
0.5237244162582877
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4369,	 Acc = 0.4334
715 0.185
3297 0.486
1558 0.446
160 0.344
4 0.0
0 0.0
0 0.0
0 0.0
0.4688184897389918
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 6
Testing:	 Loss = 1.4958,	 Acc1 = 0.3705,	 Acc2 = 0.4242

 ===== Epoch 91	 =====
[-0.3669651  -0.38051367  1.9331185   3.0933697  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.2542997  -0.06545583  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 2 2
train:	 Loss = 1.4362,	 Acc = 0.4529
5565 0.228
10877 0.492
5613 0.576
720 0.633
125 0.504
12 0.083
0 0.0
0 0.0
0.5249322649449473
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4365,	 Acc = 0.4376
715 0.326
3297 0.469
1558 0.433
160 0.337
4 0.75
0 0.0
0 0.0
0 0.0
0.4534767882048217
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.5014,	 Acc1 = 0.3939,	 Acc2 = 0.4185

 ===== Epoch 92	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 3 1
train:	 Loss = 1.4366,	 Acc = 0.4542
5565 0.238
10877 0.492
5614 0.571
721 0.62
123 0.545
12 0.167
0 0.0
0 0.0
0.5234334467054822
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4146,	 Acc = 0.4505
715 0.182
3297 0.5
1558 0.479
160 0.369
4 0.0
0 0.0
0 0.0
0 0.0
0.4887427774457063
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4842,	 Acc1 = 0.3685,	 Acc2 = 0.4217

 ===== Epoch 93	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438  2.388369    1.0914785
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 1.0109920e-03  2.5630770e-03  6.7711029e+00  3.7498622e+00
  1.3579485e-01 -8.6901329e-02  1.5849024e-03  3.2306046e-03
 -1.4162532e-03  3.5063999e-03  1.5010408e-03  2.7898324e-03
 -1.4857454e-03 -1.4670575e-03] 5 2
train:	 Loss = 1.4369,	 Acc = 0.4534
5569 0.239
10877 0.49
5609 0.573
720 0.617
125 0.504
12 0.083
0 0.0
0 0.0
0.5222279882373292
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4378,	 Acc = 0.4388
715 0.326
3297 0.469
1558 0.436
160 0.344
4 0.75
0 0.0
0 0.0
0 0.0
0.4548714883442917
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 4
Testing:	 Loss = 1.4940,	 Acc1 = 0.4009,	 Acc2 = 0.4269

 ===== Epoch 94	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 3 1
train:	 Loss = 1.4365,	 Acc = 0.4546
5568 0.24
10875 0.493
5611 0.575
721 0.606
125 0.464
12 0.083
0 0.0
0 0.0
0.5233510147601476
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4151,	 Acc = 0.4493
715 0.18
3297 0.482
1558 0.515
160 0.35
4 0.0
0 0.0
0 0.0
0 0.0
0.48754732018330343
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4871,	 Acc1 = 0.3722,	 Acc2 = 0.4262

 ===== Epoch 95	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892  0.6283323   2.6097662  -0.42425194 -0.42087102
  2.6193213   2.6234784 ] [ 1.0109920e-03  2.5630770e-03  2.3875074e-03  1.9614373e-03
  1.0307996e-03  7.6088449e-04  1.5849024e-03  3.2306046e-03
  8.3144091e-02  3.1043848e-01  7.8548861e+00  2.3473513e+00
 -4.7180974e-01  4.2148885e-01] 6 6
train:	 Loss = 1.4386,	 Acc = 0.4492
5567 0.232
10878 0.487
5611 0.571
720 0.612
124 0.476
12 0.167
0 0.0
0 0.0
0.5190544825598155
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4497,	 Acc = 0.4424
715 0.322
3297 0.48
1558 0.429
160 0.337
4 0.75
0 0.0
0 0.0
0 0.0
0.4596533173939032
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4998,	 Acc1 = 0.4064,	 Acc2 = 0.4336

 ===== Epoch 96	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 1 1
train:	 Loss = 1.4392,	 Acc = 0.4531
5563 0.237
10877 0.49
5614 0.572
721 0.624
125 0.512
12 0.167
0 0.0
0 0.0
0.5222779410917056
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4279,	 Acc = 0.4616
715 0.323
3297 0.484
1558 0.486
160 0.375
4 0.75
0 0.0
0 0.0
0 0.0
0.481370790994222
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4929,	 Acc1 = 0.3982,	 Acc2 = 0.4237

 ===== Epoch 97	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892  1.1128157   1.2967292  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306   0.19697481  0.03557393  0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 2
train:	 Loss = 1.4381,	 Acc = 0.4526
5568 0.238
10877 0.489
5610 0.57
720 0.633
125 0.544
12 0.25
0 0.0
0 0.0
0.5215636531365314
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4155,	 Acc = 0.4517
715 0.185
3297 0.501
1558 0.479
160 0.375
4 0.0
0 0.0
0 0.0
0 0.0
0.489738991831042
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4849,	 Acc1 = 0.3745,	 Acc2 = 0.4289

 ===== Epoch 98	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438  2.3738396   1.0936985
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.03495065  0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 0 0
train:	 Loss = 1.4374,	 Acc = 0.4550
5570 0.233
10877 0.495
5609 0.575
720 0.624
124 0.516
12 0.167
0 0.0
0 0.0
0.5262945450351747
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 3
val:	 Loss = 1.4340,	 Acc = 0.4445
715 0.312
3297 0.463
1558 0.471
160 0.381
4 0.75
0 0.0
0 0.0
0 0.0
0.46343893205817893
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4963,	 Acc1 = 0.3994,	 Acc2 = 0.4252

 ===== Epoch 99	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  1.0777456   2.2096658  -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.155641    0.07078102 -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 4
train:	 Loss = 1.4370,	 Acc = 0.4540
5568 0.23
10875 0.494
5612 0.576
720 0.633
125 0.536
12 0.167
0 0.0
0 0.0
0.5259455719557196
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4400,	 Acc = 0.4592
715 0.316
3297 0.479
1558 0.494
160 0.344
4 0.75
0 0.0
0 0.0
0 0.0
0.47957760510061764
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 6
Testing:	 Loss = 1.4996,	 Acc1 = 0.3965,	 Acc2 = 0.4217

 ===== Epoch 100	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 1 1
train:	 Loss = 1.4365,	 Acc = 0.4534
5570 0.228
10876 0.493
5611 0.579
719 0.618
124 0.484
12 0.167
0 0.0
0 0.0
0.5257755737515858
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4436,	 Acc = 0.4433
715 0.319
3297 0.476
1558 0.44
160 0.356
4 0.75
0 0.0
0 0.0
0 0.0
0.46104801753337316
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 6
Testing:	 Loss = 1.5029,	 Acc1 = 0.3982,	 Acc2 = 0.4237

 ===== Epoch 101	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  1.6881499   1.0958972  -0.40146217 -0.4092239   1.8712003   3.0687578
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 0 0
train:	 Loss = 1.4355,	 Acc = 0.4552
5568 0.24
10875 0.492
5612 0.575
720 0.622
125 0.472
12 0.25
0 0.0
0 0.0
0.5242735239852399
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4467,	 Acc = 0.4412
715 0.324
3297 0.482
1558 0.419
160 0.337
4 0.75
0 0.0
0 0.0
0 0.0
0.45786013150029886
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.5120,	 Acc1 = 0.4009,	 Acc2 = 0.4269

 ===== Epoch 102	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 0 1
train:	 Loss = 1.4355,	 Acc = 0.4532
5565 0.228
10881 0.493
5609 0.577
720 0.617
125 0.544
12 0.167
0 0.0
0 0.0
0.5254510866432236
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4144,	 Acc = 0.4768
715 0.309
3297 0.497
1558 0.522
160 0.369
4 0.75
0 0.0
0 0.0
0 0.0
0.500697350069735
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 6
Testing:	 Loss = 1.4816,	 Acc1 = 0.4112,	 Acc2 = 0.4394

 ===== Epoch 103	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 0 1
train:	 Loss = 1.4377,	 Acc = 0.4543
5568 0.236
10877 0.493
5611 0.577
720 0.599
124 0.484
12 0.083
0 0.0
0 0.0
0.524504151291513
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4467,	 Acc = 0.4407
715 0.315
3297 0.466
1558 0.452
160 0.356
4 0.75
0 0.0
0 0.0
0 0.0
0.45865710300856743
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 6
Testing:	 Loss = 1.5136,	 Acc1 = 0.3835,	 Acc2 = 0.4061

 ===== Epoch 104	 =====
[ 1.2269729   2.7601242  -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [-1.7426919e+00 -3.7586021e+00  2.3875074e-03  1.9614373e-03
  1.0307996e-03  7.6088449e-04  1.5849024e-03  3.2306046e-03
 -1.4162532e-03  3.5063999e-03  1.5010408e-03  2.7898324e-03
 -1.4857454e-03 -1.4670575e-03] 6 1
train:	 Loss = 1.4365,	 Acc = 0.4542
5568 0.238
10877 0.493
5611 0.572
719 0.613
125 0.536
12 0.167
0 0.0
0 0.0
0.5235239852398524
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4223,	 Acc = 0.4388
715 0.18
3297 0.492
1558 0.456
160 0.337
4 0.0
0 0.0
0 0.0
0 0.0
0.4755927475592748
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 6
Testing:	 Loss = 1.4956,	 Acc1 = 0.3641,	 Acc2 = 0.4165

 ===== Epoch 105	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892  0.7779776   1.6100675  -0.42425194 -0.42087102
  2.6770277   1.6840143 ] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.04261395 -0.38130394  0.00150104  0.00278983
 -0.43497702 -0.38875195] 5 5
train:	 Loss = 1.4391,	 Acc = 0.4532
5566 0.236
10877 0.492
5611 0.574
721 0.603
125 0.512
12 0.167
0 0.0
0 0.0
0.5229447711287905
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 3
val:	 Loss = 1.4264,	 Acc = 0.4594
715 0.183
3297 0.495
1558 0.519
160 0.381
4 0.0
0 0.0
0 0.0
0 0.0
0.4987049212990636
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 4
Testing:	 Loss = 1.4904,	 Acc1 = 0.3755,	 Acc2 = 0.4302

 ===== Epoch 106	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 5 6
train:	 Loss = 1.4374,	 Acc = 0.4529
5569 0.237
10877 0.491
5609 0.571
721 0.619
124 0.565
12 0.25
0 0.0
0 0.0
0.52211266793519
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4406,	 Acc = 0.4629
715 0.31
3297 0.487
1558 0.487
160 0.394
4 0.75
0 0.0
0 0.0
0 0.0
0.48455867702729627
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4980,	 Acc1 = 0.4025,	 Acc2 = 0.4289

 ===== Epoch 107	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 1 1
train:	 Loss = 1.4375,	 Acc = 0.4526
5567 0.238
10877 0.488
5610 0.576
721 0.612
125 0.496
12 0.083
0 0.0
0 0.0
0.5215335831651773
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4452,	 Acc = 0.4217
715 0.187
3297 0.475
1558 0.427
160 0.331
4 0.0
0 0.0
0 0.0
0 0.0
0.4550707312213588
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 6
Testing:	 Loss = 1.5083,	 Acc1 = 0.3600,	 Acc2 = 0.4115

 ===== Epoch 108	 =====
[-0.3669651  -0.38051367  1.4143575   2.3518703  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.21216284  0.23917033  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 5 6
train:	 Loss = 1.4368,	 Acc = 0.4545
5565 0.238
10878 0.493
5612 0.573
720 0.612
125 0.496
12 0.083
0 0.0
0 0.0
0.5238946215483945
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4180,	 Acc = 0.4679
715 0.317
3297 0.486
1558 0.508
160 0.381
4 0.75
0 0.0
0 0.0
0 0.0
0.48934050607690777
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4826,	 Acc1 = 0.4044,	 Acc2 = 0.4312

 ===== Epoch 109	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438  2.3879871   1.0959184
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.00194617 -0.00439572
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 5 0
train:	 Loss = 1.4378,	 Acc = 0.4520
5570 0.234
10875 0.488
5609 0.574
721 0.637
125 0.496
12 0.083
0 0.0
0 0.0
0.5220851112905086
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4322,	 Acc = 0.4623
715 0.326
3297 0.497
1558 0.465
160 0.325
4 0.75
0 0.0
0 0.0
0 0.0
0.48176927674835623
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 4
Testing:	 Loss = 1.4910,	 Acc1 = 0.4110,	 Acc2 = 0.4391

 ===== Epoch 110	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239   0.7558181   0.96858656
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.18344843  0.14991818
 -0.00148575 -0.00146706] 2 6
train:	 Loss = 1.4369,	 Acc = 0.4519
5564 0.229
10881 0.492
5609 0.575
721 0.613
125 0.488
12 0.167
0 0.0
0 0.0
0.5232879870878487
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4215,	 Acc = 0.4533
715 0.323
3297 0.478
1558 0.469
160 0.362
4 0.75
0 0.0
0 0.0
0 0.0
0.471807132894999
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4905,	 Acc1 = 0.3951,	 Acc2 = 0.4200

 ===== Epoch 111	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892  1.6374339   3.3160205  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 1.0109920e-03  2.5630770e-03  2.3875074e-03  1.9614373e-03
  1.0307996e-03  7.6088449e-04  1.5849024e-03  3.2306046e-03
 -3.8565898e+00 -6.8589449e+00  1.5010408e-03  2.7898324e-03
 -1.4857454e-03 -1.4670575e-03] 6 1
train:	 Loss = 1.4382,	 Acc = 0.4528
5566 0.232
10879 0.493
5609 0.572
721 0.617
125 0.488
12 0.167
0 0.0
0 0.0
0.5237518736308082
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4333,	 Acc = 0.4360
715 0.207
3297 0.484
1558 0.45
160 0.35
4 0.0
0 0.0
0 0.0
0 0.0
0.4686192468619247
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.5008,	 Acc1 = 0.3608,	 Acc2 = 0.4187

 ===== Epoch 112	 =====
[-0.3669651  -0.38051367  1.9310848   0.88247716 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.2495247   0.0693787   0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 1 2
train:	 Loss = 1.4383,	 Acc = 0.4523
5567 0.229
10877 0.491
5611 0.577
720 0.612
125 0.56
12 0.167
0 0.0
0 0.0
0.5239550302680888
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4107,	 Acc = 0.4859
715 0.322
3297 0.5
1558 0.542
160 0.375
4 0.75
0 0.0
0 0.0
0 0.0
0.5092647937836222
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 4
Testing:	 Loss = 1.4813,	 Acc1 = 0.4122,	 Acc2 = 0.4406

 ===== Epoch 113	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239   1.4050081   1.7432399
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.32223594 -0.2107836
 -0.00148575 -0.00146706] 2 5
train:	 Loss = 1.4366,	 Acc = 0.4537
5570 0.239
10874 0.492
5611 0.57
720 0.631
125 0.504
12 0.083
0 0.0
0 0.0
0.5227770730019605
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4242,	 Acc = 0.4512
715 0.313
3297 0.467
1558 0.487
160 0.387
4 0.75
0 0.0
0 0.0
0 0.0
0.4708109185096633
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 6
Testing:	 Loss = 1.4997,	 Acc1 = 0.3891,	 Acc2 = 0.4128

 ===== Epoch 114	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 5 1
train:	 Loss = 1.4339,	 Acc = 0.4547
5568 0.239
10880 0.492
5608 0.577
719 0.618
125 0.472
12 0.167
0 0.0
0 0.0
0.5241582103321033
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4653,	 Acc = 0.4435
715 0.316
3297 0.467
1558 0.462
160 0.337
4 0.75
0 0.0
0 0.0
0 0.0
0.4616457461645746
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 4
Testing:	 Loss = 1.5170,	 Acc1 = 0.3980,	 Acc2 = 0.4235

 ===== Epoch 115	 =====
[-0.3669651  -0.38051367  3.378733    1.2883744  -0.4411929  -0.3648088
  2.1635334   2.7121177  -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.71212405  0.251655    0.0010308   0.00076088
 -0.09395351  0.66031194 -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 6 6
train:	 Loss = 1.4368,	 Acc = 0.4526
5567 0.235
10878 0.493
5609 0.567
721 0.627
125 0.52
12 0.167
0 0.0
0 0.0
0.5223983857019314
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4525,	 Acc = 0.4407
715 0.319
3297 0.465
1558 0.452
160 0.362
4 0.75
0 0.0
0 0.0
0 0.0
0.458059374377366
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 6
Testing:	 Loss = 1.5328,	 Acc1 = 0.3759,	 Acc2 = 0.3969

 ===== Epoch 116	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239   1.4469767   2.5990474
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.29684705  0.1451721
 -0.00148575 -0.00146706] 1 4
train:	 Loss = 1.4385,	 Acc = 0.4540
5566 0.238
10876 0.492
5615 0.573
718 0.621
125 0.512
12 0.167
0 0.0
0 0.0
0.5234636227372305
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4269,	 Acc = 0.4501
715 0.313
3297 0.478
1558 0.461
160 0.369
4 0.75
0 0.0
0 0.0
0 0.0
0.4696154612472604
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.5016,	 Acc1 = 0.3963,	 Acc2 = 0.4215

 ===== Epoch 117	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 5 1
train:	 Loss = 1.4374,	 Acc = 0.4515
5569 0.231
10877 0.491
5608 0.572
721 0.614
125 0.512
12 0.083
0 0.0
0 0.0
0.5223433085394684
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 3
val:	 Loss = 1.4451,	 Acc = 0.4503
715 0.317
3297 0.474
1558 0.469
160 0.362
4 0.75
0 0.0
0 0.0
0 0.0
0.46921697549312613
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 4
Testing:	 Loss = 1.5041,	 Acc1 = 0.4075,	 Acc2 = 0.4349

 ===== Epoch 118	 =====
[-0.3669651  -0.38051367  1.7626405   1.0117294  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00513433  0.18423775  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 2
train:	 Loss = 1.4362,	 Acc = 0.4522
5566 0.231
10879 0.491
5610 0.573
721 0.619
124 0.532
12 0.25
0 0.0
0 0.0
0.5231177216649372
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4443,	 Acc = 0.4515
715 0.33
3297 0.49
1558 0.436
160 0.35
4 0.75
0 0.0
0 0.0
0 0.0
0.4688184897389918
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 4
Testing:	 Loss = 1.5017,	 Acc1 = 0.4064,	 Acc2 = 0.4336

 ===== Epoch 119	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 5 1
train:	 Loss = 1.4356,	 Acc = 0.4545
5561 0.238
10880 0.493
5613 0.572
721 0.621
125 0.52
12 0.167
0 0.0
0 0.0
0.5238891130194225
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4450,	 Acc = 0.4438
715 0.317
3297 0.463
1558 0.469
160 0.356
4 0.75
0 0.0
0 0.0
0 0.0
0.4618449890416418
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 4
Testing:	 Loss = 1.5086,	 Acc1 = 0.3961,	 Acc2 = 0.4212

 ===== Epoch 120	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 5 1
train:	 Loss = 1.4402,	 Acc = 0.4515
5567 0.233
10879 0.488
5609 0.571
720 0.64
125 0.576
12 0.25
0 0.0
0 0.0
0.5217641971749784
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4681,	 Acc = 0.4486
715 0.324
3297 0.49
1558 0.428
160 0.331
4 0.75
0 0.0
0 0.0
0 0.0
0.46622833233711897
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.5324,	 Acc1 = 0.3943,	 Acc2 = 0.4190

 ===== Epoch 121	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
  1.9453477   3.5734396 ] [ 1.0109920e-03  2.5630770e-03  2.3875074e-03  1.9614373e-03
  1.0307996e-03  7.6088449e-04  1.5849024e-03  3.2306046e-03
 -1.4162532e-03  3.5063999e-03  5.6512198e+00  2.3995581e+00
 -5.0013676e+00 -7.6809192e+00] 2 1
train:	 Loss = 1.4360,	 Acc = 0.4544
5569 0.239
10877 0.493
5609 0.569
721 0.632
124 0.54
12 0.25
0 0.0
0 0.0
0.5235541717119299
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4395,	 Acc = 0.4482
715 0.319
3297 0.484
1558 0.442
160 0.35
4 0.75
0 0.0
0 0.0
0 0.0
0.4666268180912532
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4957,	 Acc1 = 0.4019,	 Acc2 = 0.4282

 ===== Epoch 122	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 1 1
train:	 Loss = 1.4361,	 Acc = 0.4540
5570 0.229
10873 0.493
5612 0.579
720 0.625
125 0.528
12 0.25
0 0.0
0 0.0
0.5264098719870833
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4055,	 Acc = 0.4789
715 0.327
3297 0.495
1558 0.524
160 0.375
4 0.75
0 0.0
0 0.0
0 0.0
0.5004981071926679
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4885,	 Acc1 = 0.4054,	 Acc2 = 0.4324

 ===== Epoch 123	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239   2.312816    0.7989006
  2.6202023   3.2874017 ] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064  -0.5494163   0.09296528
  0.14490223  0.10045002] 2 3
train:	 Loss = 1.4349,	 Acc = 0.4523
5570 0.234
10875 0.493
5609 0.568
721 0.624
125 0.496
12 0.167
0 0.0
0 0.0
0.5225464190981433
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4544,	 Acc = 0.4370
715 0.31
3297 0.473
1558 0.429
160 0.337
4 0.75
0 0.0
0 0.0
0 0.0
0.4550707312213588
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 6
Testing:	 Loss = 1.5203,	 Acc1 = 0.3943,	 Acc2 = 0.4190

 ===== Epoch 124	 =====
[-0.3669651  -0.38051367  1.0851989   3.0072017  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00445837  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 6 0
train:	 Loss = 1.4374,	 Acc = 0.4543
5571 0.236
10873 0.491
5610 0.576
721 0.628
125 0.528
12 0.25
0 0.0
0 0.0
0.5245948907214116
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4327,	 Acc = 0.4465
715 0.317
3297 0.478
1558 0.447
160 0.362
4 0.75
0 0.0
0 0.0
0 0.0
0.46483363219764895
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.5014,	 Acc1 = 0.3953,	 Acc2 = 0.4202

 ===== Epoch 125	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 0 1
train:	 Loss = 1.4384,	 Acc = 0.4536
5567 0.236
10877 0.493
5610 0.571
721 0.619
125 0.504
12 0.167
0 0.0
0 0.0
0.5235514557509369
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4112,	 Acc = 0.4752
715 0.327
3297 0.493
1558 0.517
160 0.362
4 0.75
0 0.0
0 0.0
0 0.0
0.4963140067742578
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 6
Testing:	 Loss = 1.4886,	 Acc1 = 0.4027,	 Acc2 = 0.4292

 ===== Epoch 126	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239   1.3135097   2.0826118
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064  -0.2549179   0.20687109
 -0.00148575 -0.00146706] 2 1
train:	 Loss = 1.4351,	 Acc = 0.4539
5570 0.235
10874 0.493
5612 0.576
720 0.608
124 0.532
12 0.167
0 0.0
0 0.0
0.5243339868527275
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4174,	 Acc = 0.4716
715 0.323
3297 0.498
1558 0.495
160 0.356
4 0.75
0 0.0
0 0.0
0 0.0
0.4927276349870492
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4961,	 Acc1 = 0.3990,	 Acc2 = 0.4247

 ===== Epoch 127	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239   2.789594    1.3448467
  3.0479329   3.5078344 ] [ 1.0109920e-03  2.5630770e-03  2.3875074e-03  1.9614373e-03
  1.0307996e-03  7.6088449e-04  1.5849024e-03  3.2306046e-03
 -1.4162532e-03  3.5063999e-03 -1.4574686e-01  3.0653873e-01
 -7.3652668e+00 -7.5535231e+00] 6 6
train:	 Loss = 1.4383,	 Acc = 0.4512
5572 0.234
10874 0.492
5611 0.567
719 0.609
124 0.444
12 0.25
0 0.0
0 0.0
0.5210495963091119
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4459,	 Acc = 0.4498
715 0.312
3297 0.492
1558 0.436
160 0.319
4 0.75
0 0.0
0 0.0
0 0.0
0.46941621837019326
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.5110,	 Acc1 = 0.4075,	 Acc2 = 0.4349

 ===== Epoch 128	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 0 1
train:	 Loss = 1.4380,	 Acc = 0.4537
5571 0.237
10873 0.492
5611 0.572
720 0.628
125 0.552
12 0.083
0 0.0
0 0.0
0.5234992214981835
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4192,	 Acc = 0.4574
715 0.316
3297 0.487
1558 0.47
160 0.356
4 0.75
0 0.0
0 0.0
0 0.0
0.4775851763299462
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 4
Testing:	 Loss = 1.4843,	 Acc1 = 0.4077,	 Acc2 = 0.4351

 ===== Epoch 129	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239   1.6893362   2.525271
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064  -0.14828706  0.35399947
 -0.00148575 -0.00146706] 6 6
train:	 Loss = 1.4381,	 Acc = 0.4542
5568 0.232
10874 0.496
5612 0.57
721 0.641
125 0.472
12 0.083
0 0.0
0 0.0
0.5256572878228782
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4131,	 Acc = 0.4791
715 0.323
3297 0.493
1558 0.531
160 0.369
4 0.75
0 0.0
0 0.0
0 0.0
0.5012950787009365
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4878,	 Acc1 = 0.4106,	 Acc2 = 0.4386

 ===== Epoch 130	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 1 1
train:	 Loss = 1.4335,	 Acc = 0.4539
5565 0.238
10881 0.49
5610 0.574
719 0.644
125 0.52
12 0.167
0 0.0
0 0.0
0.5232605061393901
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 3
val:	 Loss = 1.4444,	 Acc = 0.4491
715 0.316
3297 0.476
1558 0.463
160 0.356
4 0.75
0 0.0
0 0.0
0 0.0
0.46802151823072324
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.5116,	 Acc1 = 0.3895,	 Acc2 = 0.4133

 ===== Epoch 131	 =====
[-0.3669651  -0.38051367  1.9575319   2.5559528  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.31731182  0.02942773  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 5 1
train:	 Loss = 1.4361,	 Acc = 0.4517
5565 0.238
10879 0.49
5610 0.569
721 0.619
125 0.464
12 0.083
0 0.0
0 0.0
0.5203205165158241
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4188,	 Acc = 0.4461
715 0.319
3297 0.478
1558 0.447
160 0.35
4 0.75
0 0.0
0 0.0
0 0.0
0.4642359035664475
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4867,	 Acc1 = 0.3945,	 Acc2 = 0.4192

 ===== Epoch 132	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 0 1
train:	 Loss = 1.4388,	 Acc = 0.4543
5568 0.231
10872 0.495
5614 0.574
721 0.635
125 0.512
12 0.25
0 0.0
0 0.0
0.5261185424354243
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4391,	 Acc = 0.4447
715 0.327
3297 0.465
1558 0.465
160 0.35
4 0.75
0 0.0
0 0.0
0 0.0
0.46144650328750747
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4995,	 Acc1 = 0.3961,	 Acc2 = 0.4212

 ===== Epoch 133	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438  1.1785558   2.6698632
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.198131    0.08326649
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 3 4
train:	 Loss = 1.4358,	 Acc = 0.4564
5564 0.24
10875 0.496
5616 0.574
720 0.612
125 0.496
12 0.167
0 0.0
0 0.0
0.525997233110445
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4424,	 Acc = 0.4452
715 0.319
3297 0.463
1558 0.472
160 0.369
4 0.75
0 0.0
0 0.0
0 0.0
0.4632396891811118
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.5095,	 Acc1 = 0.3891,	 Acc2 = 0.4128

 ===== Epoch 134	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438  1.3670627   1.5576681
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144 -0.21532057  0.06521839
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 1 1
train:	 Loss = 1.4410,	 Acc = 0.4528
5568 0.233
10873 0.494
5613 0.57
721 0.628
125 0.488
12 0.167
0 0.0
0 0.0
0.5235239852398524
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4124,	 Acc = 0.4763
715 0.324
3297 0.492
1558 0.519
160 0.406
4 0.75
0 0.0
0 0.0
0 0.0
0.497907949790795
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4906,	 Acc1 = 0.4007,	 Acc2 = 0.4267

 ===== Epoch 135	 =====
[-0.3669651  -0.38051367  2.3843398   1.1183057  -0.4411929  -0.3648088
  2.1927874   3.329017   -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 1.0109920e-03  2.5630770e-03  6.5827274e-01  7.8098798e+00
  1.0307996e-03  7.6088449e-04  2.6662860e-02 -8.8883594e-02
 -1.4162532e-03  3.5063999e-03  1.5010408e-03  2.7898324e-03
 -1.4857454e-03 -1.4670575e-03] 1 2
train:	 Loss = 1.4388,	 Acc = 0.4526
5571 0.229
10876 0.496
5607 0.569
721 0.623
125 0.488
12 0.167
0 0.0
0 0.0
0.5244795571189667
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4096,	 Acc = 0.4649
715 0.323
3297 0.5
1558 0.467
160 0.35
4 0.75
0 0.0
0 0.0
0 0.0
0.4851564056584977
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4870,	 Acc1 = 0.4118,	 Acc2 = 0.4401

 ===== Epoch 136	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438  2.898067    2.9606764
 -0.36420864 -0.37237892  1.933284    2.3113484  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.22257277  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 0 0
train:	 Loss = 1.4354,	 Acc = 0.4568
5568 0.238
10879 0.495
5608 0.576
720 0.637
125 0.52
12 0.167
0 0.0
0 0.0
0.5269257380073801
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4088,	 Acc = 0.4726
715 0.323
3297 0.495
1558 0.504
160 0.356
4 0.75
0 0.0
0 0.0
0 0.0
0.4939230922494521
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4844,	 Acc1 = 0.4050,	 Acc2 = 0.4319

 ===== Epoch 137	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  3.975054    1.4559877  -0.40146217 -0.4092239   3.3835819   2.048183
 -0.38672873 -0.38123247] [ 1.0109920e-03  2.5630770e-03  2.3875074e-03  1.9614373e-03
  1.0307996e-03  7.6088449e-04  3.5149124e-01  4.0076289e-02
 -1.4162532e-03  3.5063999e-03  1.3385960e+00  9.2965275e-02
 -1.4857454e-03 -1.4670575e-03] 3 2
train:	 Loss = 1.4369,	 Acc = 0.4549
5570 0.236
10877 0.495
5608 0.571
720 0.64
125 0.504
12 0.083
0 0.0
0 0.0
0.5253719294199054
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4096,	 Acc = 0.4615
715 0.323
3297 0.496
1558 0.46
160 0.381
4 0.75
0 0.0
0 0.0
0 0.0
0.4811715481171548
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4821,	 Acc1 = 0.4040,	 Acc2 = 0.4307

 ===== Epoch 138	 =====
[-0.3669651  -0.38051367  1.2878208   2.9595823  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.07933543  0.11682048  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 6 4
train:	 Loss = 1.4376,	 Acc = 0.4513
5571 0.229
10870 0.492
5614 0.569
720 0.622
125 0.552
12 0.167
0 0.0
0 0.0
0.522691886281068
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4381,	 Acc = 0.4431
715 0.326
3297 0.477
1558 0.434
160 0.344
4 0.75
0 0.0
0 0.0
0 0.0
0.4598525602709703
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.5007,	 Acc1 = 0.3943,	 Acc2 = 0.4190

 ===== Epoch 139	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 5 1
train:	 Loss = 1.4359,	 Acc = 0.4513
5569 0.227
10870 0.493
5615 0.57
721 0.617
125 0.52
12 0.167
0 0.0
0 0.0
0.5233235311076515
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4313,	 Acc = 0.4416
715 0.323
3297 0.471
1558 0.442
160 0.35
4 0.75
0 0.0
0 0.0
0 0.0
0.4584578601315003
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.5038,	 Acc1 = 0.3899,	 Acc2 = 0.4138

 ===== Epoch 140	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239   1.4957513   1.3350099
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.07766519 -0.41961092
 -0.00148575 -0.00146706] 5 5
train:	 Loss = 1.4376,	 Acc = 0.4510
5568 0.23
10875 0.493
5612 0.568
720 0.61
125 0.52
12 0.083
0 0.0
0 0.0
0.5220249077490775
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4404,	 Acc = 0.4447
715 0.326
3297 0.48
1558 0.434
160 0.337
4 0.75
0 0.0
0 0.0
0 0.0
0.4616457461645746
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 6
Testing:	 Loss = 1.5019,	 Acc1 = 0.4093,	 Acc2 = 0.4371

 ===== Epoch 141	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 1 1
train:	 Loss = 1.4372,	 Acc = 0.4530
5572 0.237
10872 0.49
5610 0.576
721 0.609
125 0.536
12 0.083
0 0.0
0 0.0
0.5223760092272203
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4376,	 Acc = 0.4615
715 0.319
3297 0.493
1558 0.469
160 0.362
4 0.75
0 0.0
0 0.0
0 0.0
0.48176927674835623
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4974,	 Acc1 = 0.4042,	 Acc2 = 0.4309

 ===== Epoch 142	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892  3.1952333   3.2787185  -0.42425194 -0.42087102
  3.0941873   2.2193515 ] [ 1.0109920e-03  2.5630770e-03  2.3875074e-03  1.9614373e-03
  1.0307996e-03  7.6088449e-04  1.5849024e-03  3.2306046e-03
 -6.8020992e+00 -6.7902288e+00  1.5010408e-03  2.7898324e-03
  7.4083304e-01  3.5524276e-01] 6 6
train:	 Loss = 1.4389,	 Acc = 0.4517
5567 0.232
10877 0.49
5611 0.574
720 0.617
125 0.544
12 0.167
0 0.0
0 0.0
0.5222254251945806
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4662,	 Acc = 0.4290
715 0.18
3297 0.495
1558 0.419
160 0.294
4 0.0
0 0.0
0 0.0
0 0.0
0.46443514644351463
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 4
Testing:	 Loss = 1.5189,	 Acc1 = 0.3745,	 Acc2 = 0.4289

 ===== Epoch 143	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239   2.7117062   1.5833908
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.01228198
 -0.00148575 -0.00146706] 2 0
train:	 Loss = 1.4360,	 Acc = 0.4552
5565 0.235
10874 0.493
5616 0.576
720 0.637
125 0.504
12 0.167
0 0.0
0 0.0
0.5259699083414999
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4707,	 Acc = 0.4182
715 0.207
3297 0.469
1558 0.418
160 0.319
4 0.0
0 0.0
0 0.0
0 0.0
0.4482964734010759
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 4
Testing:	 Loss = 1.5263,	 Acc1 = 0.3621,	 Acc2 = 0.4202

 ===== Epoch 144	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239   1.80579     2.208032
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00996401  0.2448397
 -0.00148575 -0.00146706] 6 6
train:	 Loss = 1.4380,	 Acc = 0.4545
5567 0.24
10878 0.492
5611 0.572
719 0.623
125 0.512
12 0.167
0 0.0
0 0.0
0.5234361487460363
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 3
val:	 Loss = 1.4282,	 Acc = 0.4641
715 0.324
3297 0.478
1558 0.504
160 0.406
4 0.75
0 0.0
0 0.0
0 0.0
0.4839609483960948
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4964,	 Acc1 = 0.3978,	 Acc2 = 0.4232

 ===== Epoch 145	 =====
[-0.3669651  -0.38051367  2.2016537   2.9686525  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 1.0109920e-03  2.5630770e-03 -5.9006677e+00 -3.6360738e+00
  4.9084907e+00  1.3827298e+00  1.5849024e-03  3.2306046e-03
 -1.4162532e-03  3.5063999e-03  1.5010408e-03  2.7898324e-03
 -1.4857454e-03 -1.4670575e-03] 6 1
train:	 Loss = 1.4400,	 Acc = 0.4548
5568 0.233
10880 0.496
5608 0.573
720 0.629
124 0.468
12 0.25
0 0.0
0 0.0
0.5258879151291513
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4076,	 Acc = 0.4747
715 0.317
3297 0.5
1558 0.506
160 0.35
4 0.75
0 0.0
0 0.0
0 0.0
0.4971109782825264
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4779,	 Acc1 = 0.4066,	 Acc2 = 0.4339

 ===== Epoch 146	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438  0.68912345  0.81398475
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144 -0.06772428  0.01107409
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 1 0
train:	 Loss = 1.4381,	 Acc = 0.4522
5569 0.224
10876 0.493
5609 0.576
721 0.62
125 0.56
12 0.167
0 0.0
0 0.0
0.5255722769993657
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 3
val:	 Loss = 1.4301,	 Acc = 0.4644
715 0.316
3297 0.49
1558 0.488
160 0.362
4 0.75
0 0.0
0 0.0
0 0.0
0.485554891412632
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.5012,	 Acc1 = 0.3982,	 Acc2 = 0.4237

 ===== Epoch 147	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 3 1
train:	 Loss = 1.4394,	 Acc = 0.4536
5569 0.234
10878 0.496
5608 0.566
720 0.622
125 0.52
12 0.167
0 0.0
0 0.0
0.5241307732226258
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4221,	 Acc = 0.4506
715 0.31
3297 0.482
1558 0.458
160 0.35
4 0.75
0 0.0
0 0.0
0 0.0
0.47061167563259615
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4944,	 Acc1 = 0.3895,	 Acc2 = 0.4133

 ===== Epoch 148	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438  2.746649    2.7320218
 -0.36420864 -0.37237892  1.9350032   2.2093892  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 1.0109920e-03  2.5630770e-03  2.3875074e-03  1.9614373e-03
  2.7697220e-01 -7.9068856e+00  1.5849024e-03  3.2306046e-03
  2.4575984e-01  2.6462770e-01  1.5010408e-03  2.7898324e-03
 -1.4857454e-03 -1.4670575e-03] 6 6
train:	 Loss = 1.4342,	 Acc = 0.4530
5570 0.232
10875 0.493
5611 0.572
720 0.624
124 0.532
12 0.167
0 0.0
0 0.0
0.5239880059970015
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4440,	 Acc = 0.4506
715 0.316
3297 0.489
1558 0.442
160 0.331
4 0.75
0 0.0
0 0.0
0 0.0
0.4698147041243276
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.5049,	 Acc1 = 0.4000,	 Acc2 = 0.4259

 ===== Epoch 149	 =====
[ 1.8469741   1.9219476  -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
  1.6176113   2.1904852 ] [-0.22290345 -0.2453594   0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.290478   -0.4499022 ] 5 5
train:	 Loss = 1.4349,	 Acc = 0.4578
5569 0.238
10877 0.497
5610 0.579
719 0.62
125 0.504
12 0.167
0 0.0
0 0.0
0.5285129447039151
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4278,	 Acc = 0.4695
715 0.322
3297 0.5
1558 0.486
160 0.344
4 0.75
0 0.0
0 0.0
0 0.0
0.4905359633393106
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 4
Testing:	 Loss = 1.4904,	 Acc1 = 0.4116,	 Acc2 = 0.4399

 ===== Epoch 150	 =====
[-0.3669651  -0.38051367  3.337233    2.864344    2.1501534   1.158077
 -0.36420864 -0.37237892  1.0354124   1.1475204  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00971615  0.05439709  0.04595215  0.08326649
  0.0015849   0.0032306  -0.14343151  0.3241817   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 4
train:	 Loss = 1.4368,	 Acc = 0.4544
5568 0.235
10878 0.496
5608 0.568
721 0.62
125 0.568
12 0.083
0 0.0
0 0.0
0.5249077490774908
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4319,	 Acc = 0.4477
715 0.316
3297 0.483
1558 0.446
160 0.312
4 0.75
0 0.0
0 0.0
0 0.0
0.4664275752141861
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.5067,	 Acc1 = 0.3885,	 Acc2 = 0.4120

 ===== Epoch 151	 =====
[ 2.886764    2.1138194  -0.4202629  -0.33521438 -0.4411929  -0.3648088
  0.46677956  2.1482549  -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [-3.5584357e+00 -2.9846003e+00  2.3875074e-03  1.9614373e-03
  1.0307996e-03  7.6088449e-04 -1.7623962e+00 -5.5420446e+00
 -1.4162532e-03  3.5063999e-03  1.5010408e-03  2.7898324e-03
 -1.4857454e-03 -1.4670575e-03] 1 1
train:	 Loss = 1.4359,	 Acc = 0.4510
5572 0.227
10874 0.491
5609 0.572
720 0.633
125 0.52
12 0.083
0 0.0
0 0.0
0.5230680507497116
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4336,	 Acc = 0.4569
715 0.319
3297 0.488
1558 0.465
160 0.344
4 0.75
0 0.0
0 0.0
0 0.0
0.4765889619446105
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.5001,	 Acc1 = 0.4038,	 Acc2 = 0.4304

 ===== Epoch 152	 =====
[-0.3669651  -0.38051367  1.8855157   0.99132115 -0.4411929  -0.3648088
  0.80208     3.7114384  -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.09196735  0.10183886  0.0010308   0.00076088
  0.00277733  0.1997409  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 6 4
train:	 Loss = 1.4344,	 Acc = 0.4530
5567 0.23
10877 0.49
5612 0.577
719 0.634
125 0.52
12 0.083
0 0.0
0 0.0
0.524416258287691
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4548,	 Acc = 0.4506
715 0.322
3297 0.489
1558 0.44
160 0.337
4 0.75
0 0.0
0 0.0
0 0.0
0.469017732616059
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 6
Testing:	 Loss = 1.5199,	 Acc1 = 0.4009,	 Acc2 = 0.4269

 ===== Epoch 153	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239   1.2923355   1.7702912
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064  -0.20921783 -0.01619447
 -0.00148575 -0.00146706] 4 1
train:	 Loss = 1.4341,	 Acc = 0.4569
5566 0.238
10878 0.495
5611 0.577
720 0.625
125 0.528
12 0.083
0 0.0
0 0.0
0.5270955839963104
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 3
val:	 Loss = 1.4360,	 Acc = 0.4609
715 0.31
3297 0.476
1558 0.508
160 0.369
4 0.75
0 0.0
0 0.0
0 0.0
0.4823670053795577
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.5001,	 Acc1 = 0.4093,	 Acc2 = 0.4371

 ===== Epoch 154	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
  1.1793082   1.3743585 ] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
  0.00795892 -0.11867171] 3 5
train:	 Loss = 1.4360,	 Acc = 0.4540
5571 0.239
10875 0.491
5608 0.575
721 0.619
125 0.488
12 0.25
0 0.0
0 0.0
0.5230955538896257
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4425,	 Acc = 0.4430
715 0.319
3297 0.47
1558 0.452
160 0.356
4 0.75
0 0.0
0 0.0
0 0.0
0.4606495317792389
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.5003,	 Acc1 = 0.3967,	 Acc2 = 0.4220

 ===== Epoch 155	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.0031962   3.0889566  -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0696568   0.40239215 -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 6 6
train:	 Loss = 1.4357,	 Acc = 0.4537
5565 0.236
10881 0.492
5610 0.573
720 0.615
124 0.5
12 0.083
0 0.0
0 0.0
0.5233757998501182
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4409,	 Acc = 0.4670
715 0.327
3297 0.49
1558 0.493
160 0.356
4 0.75
0 0.0
0 0.0
0 0.0
0.486949591552102
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 4
Testing:	 Loss = 1.5007,	 Acc1 = 0.4180,	 Acc2 = 0.4476

 ===== Epoch 156	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 5 1
train:	 Loss = 1.4374,	 Acc = 0.4540
5570 0.24
10873 0.491
5612 0.573
720 0.619
125 0.512
12 0.167
0 0.0
0 0.0
0.5228347364779149
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4244,	 Acc = 0.4761
715 0.317
3297 0.496
1558 0.519
160 0.356
4 0.75
0 0.0
0 0.0
0 0.0
0.4987049212990636
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4884,	 Acc1 = 0.4081,	 Acc2 = 0.4356

 ===== Epoch 157	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
train:	 Loss = 1.4352,	 Acc = 0.4568
5564 0.24
10878 0.493
5612 0.577
721 0.63
125 0.56
12 0.083
0 0.0
0 0.0
0.5262278072400277
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4061,	 Acc = 0.4867
715 0.323
3297 0.504
1558 0.536
160 0.381
4 0.75
0 0.0
0 0.0
0 0.0
0.5100617652918908
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4950,	 Acc1 = 0.4054,	 Acc2 = 0.4324

 ===== Epoch 158	 =====
[ 2.8778622   2.7929444  -0.4202629  -0.33521438 -0.4411929  -0.3648088
  1.2690235   3.4043849  -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00724005  0.06000853  0.00238751  0.00196144  0.0010308   0.00076088
  0.08876523  0.3716874  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 4
train:	 Loss = 1.4341,	 Acc = 0.4540
5564 0.23
10879 0.494
5613 0.574
719 0.644
125 0.544
12 0.167
0 0.0
0 0.0
0.525997233110445
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 3
val:	 Loss = 1.3993,	 Acc = 0.4944
715 0.317
3297 0.509
1558 0.551
160 0.425
4 0.75
0 0.0
0 0.0
0 0.0
0.5196254233911137
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 4
Testing:	 Loss = 1.4735,	 Acc1 = 0.4209,	 Acc2 = 0.4510

 ===== Epoch 159	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892  3.1998205   2.0900223  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 1.0109920e-03  2.5630770e-03  2.3875074e-03  1.9614373e-03
  1.0307996e-03  7.6088449e-04  1.5849024e-03  3.2306046e-03
 -3.9060763e-01  4.0155005e-02  1.5010408e-03  2.7898324e-03
  8.9734898e+00  2.5056932e+00] 3 3
train:	 Loss = 1.4374,	 Acc = 0.4557
5569 0.237
10873 0.492
5612 0.578
721 0.631
125 0.544
12 0.167
0 0.0
0 0.0
0.5258029176036442
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4462,	 Acc = 0.4346
715 0.319
3297 0.466
1558 0.429
160 0.337
4 0.75
0 0.0
0 0.0
0 0.0
0.45108587368001596
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.5107,	 Acc1 = 0.3939,	 Acc2 = 0.4185

 ===== Epoch 160	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
  1.955479    3.5576942 ] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.0222611  -0.00656291] 2 0
train:	 Loss = 1.4367,	 Acc = 0.4509
5571 0.234
10875 0.488
5608 0.572
721 0.626
125 0.448
12 0.083
0 0.0
0 0.0
0.5205005478346116
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4265,	 Acc = 0.4414
715 0.317
3297 0.474
1558 0.44
160 0.337
4 0.75
0 0.0
0 0.0
0 0.0
0.45905558876270175
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4947,	 Acc1 = 0.3914,	 Acc2 = 0.4155

 ===== Epoch 161	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892  1.2681948   3.0001955  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306   0.13843071  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 5 0
train:	 Loss = 1.4347,	 Acc = 0.4574
5565 0.235
10876 0.496
5613 0.58
721 0.638
125 0.464
12 0.083
0 0.0
0 0.0
0.5286216636882458
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 3
val:	 Loss = 1.4169,	 Acc = 0.4545
715 0.324
3297 0.484
1558 0.462
160 0.35
4 0.75
0 0.0
0 0.0
0 0.0
0.47300259015740187
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 4
Testing:	 Loss = 1.4835,	 Acc1 = 0.4056,	 Acc2 = 0.4327

 ===== Epoch 162	 =====
[-0.3669651  -0.38051367  1.6710935   3.0412154   2.2388632   0.75626606
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.21104932  0.03192467 -0.02005398  0.01880899
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 3 4
train:	 Loss = 1.4369,	 Acc = 0.4538
5566 0.236
10879 0.494
5610 0.573
720 0.617
125 0.456
12 0.167
0 0.0
0 0.0
0.5238095238095238
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4234,	 Acc = 0.4587
715 0.32
3297 0.49
1558 0.467
160 0.356
4 0.75
0 0.0
0 0.0
0 0.0
0.4783821478382148
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 4
Testing:	 Loss = 1.4855,	 Acc1 = 0.4058,	 Acc2 = 0.4329

 ===== Epoch 163	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 1.0109920e-03  2.5630770e-03  2.3875074e-03  1.9614373e-03
  1.0307996e-03  7.6088449e-04  1.5849024e-03  3.2306046e-03
  3.2097988e+00  5.9497428e+00  1.5010408e-03  2.7898324e-03
 -1.4857454e-03 -1.4670575e-03] 2 2
train:	 Loss = 1.4365,	 Acc = 0.4529
5568 0.23
10877 0.492
5610 0.577
720 0.626
125 0.48
12 0.167
0 0.0
0 0.0
0.5246194649446494
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4241,	 Acc = 0.4852
715 0.32
3297 0.501
1558 0.539
160 0.375
4 0.75
0 0.0
0 0.0
0 0.0
0.5086670651524208
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 4
Testing:	 Loss = 1.4889,	 Acc1 = 0.4159,	 Acc2 = 0.4451

 ===== Epoch 164	 =====
[ 5.5398655   1.5962704  -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239   1.1127402   3.2409031
 -0.38672873 -0.38123247] [-6.4608197e+00 -2.3647940e+00  2.3875074e-03  1.9614373e-03
  1.0307996e-03  7.6088449e-04  1.5849024e-03  3.2306046e-03
 -1.4162532e-03  3.5063999e-03  1.5636691e-01 -4.5757955e-01
 -1.4857454e-03 -1.4670575e-03] 5 5
train:	 Loss = 1.4382,	 Acc = 0.4519
5568 0.236
10877 0.493
5609 0.566
721 0.62
125 0.424
12 0.083
0 0.0
0 0.0
0.521044741697417
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.3999,	 Acc = 0.4878
715 0.316
3297 0.506
1558 0.54
160 0.362
4 0.75
0 0.0
0 0.0
0 0.0
0.5122534369396294
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4775,	 Acc1 = 0.4170,	 Acc2 = 0.4463

 ===== Epoch 165	 =====
[-0.3669651  -0.38051367  3.4365087   2.4403062   3.6505685   1.0515195
  2.8110664   3.5299978  -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.08391725  0.05190015  0.19538203  0.03170049
  0.43270206  0.05849913 -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 3 3
train:	 Loss = 1.4370,	 Acc = 0.4530
5568 0.235
10875 0.492
5611 0.571
721 0.628
125 0.512
12 0.083
0 0.0
0 0.0
0.5228321033210332
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4349,	 Acc = 0.4430
715 0.319
3297 0.476
1558 0.44
160 0.337
4 0.75
0 0.0
0 0.0
0 0.0
0.4606495317792389
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.5023,	 Acc1 = 0.3984,	 Acc2 = 0.4240

 ===== Epoch 166	 =====
[ 2.3120546   2.116344   -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
  2.1845422   2.35581   ] [-0.05389448  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00053982 -0.00146706] 0 0
train:	 Loss = 1.4369,	 Acc = 0.4533
5565 0.235
10876 0.492
5613 0.572
721 0.623
125 0.48
12 0.167
0 0.0
0 0.0
0.5232028592840261
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 3
val:	 Loss = 1.3938,	 Acc = 0.4902
715 0.316
3297 0.503
1558 0.553
160 0.375
4 0.75
0 0.0
0 0.0
0 0.0
0.5150428372185695
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4716,	 Acc1 = 0.4188,	 Acc2 = 0.4486

 ===== Epoch 167	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438  4.03829     2.4012492
 -0.36420864 -0.37237892  4.2811637   1.7543026  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 1.0109920e-03  2.5630770e-03  2.3875074e-03  1.9614373e-03
 -1.1714954e+00  7.6088449e-04  1.5849024e-03  3.2306046e-03
 -4.8033021e-02  3.5063999e-03  1.5010408e-03  2.7898324e-03
 -1.4857454e-03 -1.4670575e-03] 0 0
train:	 Loss = 1.4363,	 Acc = 0.4540
5566 0.236
10877 0.495
5612 0.571
720 0.611
125 0.48
12 0.0
0 0.0
0 0.0
0.5238671739882393
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4330,	 Acc = 0.4452
715 0.313
3297 0.481
1558 0.441
160 0.337
4 0.75
0 0.0
0 0.0
0 0.0
0.4640366606893804
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.5069,	 Acc1 = 0.3887,	 Acc2 = 0.4123

 ===== Epoch 168	 =====
[ 0.07764265  1.9547678  -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.06170122  0.27769655  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 6 6
train:	 Loss = 1.4342,	 Acc = 0.4533
5569 0.235
10875 0.493
5611 0.573
720 0.607
125 0.512
12 0.083
0 0.0
0 0.0
0.5236118318629995
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4522,	 Acc = 0.4302
715 0.323
3297 0.463
1558 0.42
160 0.331
4 0.75
0 0.0
0 0.0
0 0.0
0.4455070731221359
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.5062,	 Acc1 = 0.3945,	 Acc2 = 0.4192

 ===== Epoch 169	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 5 1
train:	 Loss = 1.4366,	 Acc = 0.4526
5568 0.23
10878 0.492
5608 0.573
721 0.628
125 0.568
12 0.083
0 0.0
0 0.0
0.5239852398523985
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4199,	 Acc = 0.4644
715 0.312
3297 0.493
1558 0.486
160 0.35
4 0.75
0 0.0
0 0.0
0 0.0
0.4861526200438334
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.5011,	 Acc1 = 0.3953,	 Acc2 = 0.4202

 ===== Epoch 170	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239   1.0246444   2.694957
  3.988853    2.5709946 ] [ 1.0109920e-03  2.5630770e-03  2.3875074e-03  1.9614373e-03
  1.0307996e-03  7.6088449e-04  1.5849024e-03  3.2306046e-03
 -1.4162532e-03  3.5063999e-03 -3.2414503e+00 -6.0104880e+00
 -9.3825636e+00 -5.7343030e+00] 4 1
train:	 Loss = 1.4346,	 Acc = 0.4521
5562 0.227
10881 0.492
5612 0.575
720 0.622
125 0.504
12 0.167
0 0.0
0 0.0
0.524207492795389
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4529,	 Acc = 0.4442
715 0.327
3297 0.487
1558 0.42
160 0.319
4 0.75
0 0.0
0 0.0
0 0.0
0.460848774656306
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.5214,	 Acc1 = 0.3976,	 Acc2 = 0.4230

 ===== Epoch 171	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 6 1
train:	 Loss = 1.4353,	 Acc = 0.4573
5571 0.235
10872 0.496
5612 0.578
721 0.639
124 0.532
12 0.167
0 0.0
0 0.0
0.5287469004094343
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4336,	 Acc = 0.4618
715 0.322
3297 0.498
1558 0.465
160 0.306
4 0.75
0 0.0
0 0.0
0 0.0
0.48176927674835623
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4945,	 Acc1 = 0.4091,	 Acc2 = 0.4369

 ===== Epoch 172	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892  2.6516948   3.3458622   3.598341    1.2563149
  4.709521    3.043351  ] [ 1.0109920e-03  2.5630770e-03  2.3875074e-03  1.9614373e-03
  1.0307996e-03  7.6088449e-04  1.5849024e-03  3.2306046e-03
  6.2953216e-01  1.7249627e-02 -2.3291418e-01  1.9737895e-01
 -1.2651298e+00  1.2083345e-01] 2 2
train:	 Loss = 1.4346,	 Acc = 0.4515
5565 0.229
10878 0.493
5613 0.571
719 0.604
125 0.552
12 0.167
0 0.0
0 0.0
0.5229146250072059
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4237,	 Acc = 0.4683
715 0.313
3297 0.487
1558 0.509
160 0.369
4 0.75
0 0.0
0 0.0
0 0.0
0.49033672046224347
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4925,	 Acc1 = 0.4035,	 Acc2 = 0.4302

 ===== Epoch 173	 =====
[ 1.7221271   2.7550752  -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
  1.2656482   3.1142044 ] [-0.16924319 -0.1244216   0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
  0.05234448 -0.28173906] 3 5
train:	 Loss = 1.4348,	 Acc = 0.4539
5568 0.237
10875 0.491
5611 0.573
721 0.638
125 0.528
12 0.167
0 0.0
0 0.0
0.5235816420664207
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4392,	 Acc = 0.4435
715 0.316
3297 0.482
1558 0.431
160 0.331
4 0.75
0 0.0
0 0.0
0 0.0
0.4616457461645746
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.5043,	 Acc1 = 0.3943,	 Acc2 = 0.4190

 ===== Epoch 174	 =====
[ 1.8950422   1.8638811  -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
  2.1836612   2.0776446 ] [-2.4735324e+00 -2.6852794e+00  2.3875074e-03  1.9614373e-03
  1.0307996e-03  7.6088449e-04  1.5849024e-03  3.2306046e-03
 -1.4162532e-03  3.5063999e-03  1.5010408e-03  2.7898324e-03
  7.9735480e-02  2.4822979e-01] 4 6
train:	 Loss = 1.4356,	 Acc = 0.4569
5567 0.243
10878 0.493
5610 0.576
720 0.628
125 0.552
12 0.167
0 0.0
0 0.0
0.525684635341597
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4005,	 Acc = 0.4859
715 0.317
3297 0.497
1558 0.552
160 0.369
4 0.75
0 0.0
0 0.0
0 0.0
0.5098625224148237
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 6
Testing:	 Loss = 1.4774,	 Acc1 = 0.4155,	 Acc2 = 0.4446

 ===== Epoch 175	 =====
[ 1.5797744   0.899473   -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892  1.4098128   3.313534   -0.42425194 -0.42087102
  2.039176    1.0961931 ] [ 4.3331191e-02  5.8004099e-01  2.3875074e-03  1.9614373e-03
  1.0307996e-03  7.6088449e-04  1.5849024e-03  3.2306046e-03
 -3.4262004e+00 -6.8543634e+00  1.5010408e-03  2.7898324e-03
 -3.3014557e-01  9.5145774e-01] 6 6
train:	 Loss = 1.4376,	 Acc = 0.4527
5569 0.23
10875 0.495
5611 0.567
720 0.632
125 0.52
12 0.0
0 0.0
0 0.0
0.5241884333736955
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4394,	 Acc = 0.4451
715 0.323
3297 0.482
1558 0.429
160 0.369
4 0.75
0 0.0
0 0.0
0 0.0
0.4624427176728432
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 4
Testing:	 Loss = 1.4990,	 Acc1 = 0.4033,	 Acc2 = 0.4299

 ===== Epoch 176	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239   2.4065838   1.6596266
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.24014643 -0.72335976
 -0.00148575 -0.00146706] 5 5
train:	 Loss = 1.4357,	 Acc = 0.4567
5570 0.239
10876 0.494
5608 0.579
721 0.613
125 0.528
12 0.25
0 0.0
0 0.0
0.5267558528428093
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4319,	 Acc = 0.4463
715 0.319
3297 0.483
1558 0.439
160 0.325
4 0.75
0 0.0
0 0.0
0 0.0
0.46443514644351463
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4964,	 Acc1 = 0.4011,	 Acc2 = 0.4272

 ===== Epoch 177	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 0 1
train:	 Loss = 1.4366,	 Acc = 0.4516
5567 0.232
10878 0.492
5609 0.569
721 0.617
125 0.552
12 0.167
0 0.0
0 0.0
0.5222254251945806
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4299,	 Acc = 0.4330
715 0.31
3297 0.465
1558 0.427
160 0.369
4 0.75
0 0.0
0 0.0
0 0.0
0.4504881450488145
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 4
Testing:	 Loss = 1.4939,	 Acc1 = 0.4019,	 Acc2 = 0.4282

 ===== Epoch 178	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239   1.7970939   1.912926
 -0.38672873 -0.38123247] [ 1.0109920e-03  2.5630770e-03  2.3875074e-03  1.9614373e-03
  1.0307996e-03  7.6088449e-04  1.5849024e-03  3.2306046e-03
 -1.4162532e-03  3.5063999e-03 -2.9553756e-01  9.2352861e-01
 -1.4857454e-03 -1.4670575e-03] 6 6
train:	 Loss = 1.4339,	 Acc = 0.4558
5570 0.241
10877 0.492
5609 0.576
719 0.637
125 0.512
12 0.167
0 0.0
0 0.0
0.5248529581363165
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4365,	 Acc = 0.4384
715 0.31
3297 0.473
1558 0.432
160 0.35
4 0.75
0 0.0
0 0.0
0 0.0
0.456664674237896
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 6
Testing:	 Loss = 1.4953,	 Acc1 = 0.4002,	 Acc2 = 0.4262

 ===== Epoch 179	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438  1.8607007   1.7263843
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.03128349  0.02912219
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 3 3
train:	 Loss = 1.4409,	 Acc = 0.4519
5569 0.236
10873 0.49
5612 0.57
721 0.623
125 0.544
12 0.083
0 0.0
0 0.0
0.5213630859712852
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4133,	 Acc = 0.4536
715 0.31
3297 0.487
1558 0.458
160 0.35
4 0.75
0 0.0
0 0.0
0 0.0
0.4739988045427376
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4861,	 Acc1 = 0.3998,	 Acc2 = 0.4257

 ===== Epoch 180	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438  2.51914     2.2169933
 -0.36420864 -0.37237892  1.6821555   1.3837677  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.18071336  0.1889768
  0.0015849   0.0032306  -0.241005    0.4203843   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 6
train:	 Loss = 1.4390,	 Acc = 0.4532
5569 0.233
10874 0.494
5611 0.569
721 0.632
125 0.528
12 0.083
0 0.0
0 0.0
0.523957792769417
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4546,	 Acc = 0.4569
715 0.315
3297 0.482
1558 0.48
160 0.337
4 0.75
0 0.0
0 0.0
0 0.0
0.4771866905758119
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.5123,	 Acc1 = 0.4029,	 Acc2 = 0.4294

 ===== Epoch 181	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 0 1
train:	 Loss = 1.4360,	 Acc = 0.4553
5568 0.241
10872 0.492
5615 0.573
720 0.632
125 0.576
12 0.083
0 0.0
0 0.0
0.5242158671586716
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4598,	 Acc = 0.4386
715 0.18
3297 0.497
1558 0.445
160 0.344
4 0.0
0 0.0
0 0.0
0 0.0
0.4753935046822076
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.5285,	 Acc1 = 0.3639,	 Acc2 = 0.4163

 ===== Epoch 182	 =====
[ 2.0771852   3.060555   -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
  1.4132177   3.5734396 ] [-2.6727893e+00 -4.1183925e+00  2.3875074e-03  1.9614373e-03
  1.0307996e-03  7.6088449e-04  1.5849024e-03  3.2306046e-03
 -1.4162532e-03  3.5063999e-03  1.5010408e-03  2.7898324e-03
 -3.8605013e+00 -7.6809192e+00] 6 6
train:	 Loss = 1.4346,	 Acc = 0.4540
5570 0.23
10873 0.493
5611 0.577
721 0.63
125 0.528
12 0.167
0 0.0
0 0.0
0.52583323722754
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4593,	 Acc = 0.4536
715 0.313
3297 0.493
1558 0.444
160 0.362
4 0.75
0 0.0
0 0.0
0 0.0
0.4736003187886033
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 4
Testing:	 Loss = 1.5148,	 Acc1 = 0.4058,	 Acc2 = 0.4329

 ===== Epoch 183	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  0.7835148   2.5920873  -0.40146217 -0.4092239   2.137379    2.006376
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.22173555  0.40239215 -0.00141625  0.0035064  -0.38777742  0.23060147
 -0.00148575 -0.00146706] 1 4
train:	 Loss = 1.4367,	 Acc = 0.4541
5568 0.238
10878 0.492
5610 0.571
719 0.638
125 0.552
12 0.083
0 0.0
0 0.0
0.5234086715867159
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4295,	 Acc = 0.4536
715 0.324
3297 0.494
1558 0.44
160 0.325
4 0.75
0 0.0
0 0.0
0 0.0
0.47200637577206617
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4943,	 Acc1 = 0.4019,	 Acc2 = 0.4282

 ===== Epoch 184	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239   1.2473432   2.9998996
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.23929882 -0.6996294
 -0.00148575 -0.00146706] 5 5
train:	 Loss = 1.4372,	 Acc = 0.4537
5571 0.236
10875 0.49
5608 0.575
721 0.634
125 0.536
12 0.167
0 0.0
0 0.0
0.523556888299406
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4830,	 Acc = 0.4376
715 0.316
3297 0.476
1558 0.422
160 0.319
4 0.75
0 0.0
0 0.0
0 0.0
0.4548714883442917
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 6
Testing:	 Loss = 1.5402,	 Acc1 = 0.3877,	 Acc2 = 0.4110

 ===== Epoch 185	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.3362463   2.3548186  -0.40146217 -0.4092239   2.7075477   3.0736766
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.29816553 -0.5555955  -0.00141625  0.0035064  -0.24984011 -0.41486484
 -0.00148575 -0.00146706] 5 5
train:	 Loss = 1.4363,	 Acc = 0.4556
5569 0.24
10874 0.494
5611 0.574
721 0.616
125 0.48
12 0.083
0 0.0
0 0.0
0.524822695035461
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4314,	 Acc = 0.4423
715 0.175
3297 0.501
1558 0.452
160 0.337
4 0.0
0 0.0
0 0.0
0 0.0
0.4803745766088862
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4965,	 Acc1 = 0.3728,	 Acc2 = 0.4269

 ===== Epoch 186	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 2 6
train:	 Loss = 1.4346,	 Acc = 0.4540
5569 0.23
10876 0.495
5610 0.574
720 0.624
125 0.56
12 0.167
0 0.0
0 0.0
0.5259182379057833
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4479,	 Acc = 0.4414
715 0.313
3297 0.486
1558 0.417
160 0.325
4 0.75
0 0.0
0 0.0
0 0.0
0.4596533173939032
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 6
Testing:	 Loss = 1.5152,	 Acc1 = 0.4062,	 Acc2 = 0.4334

 ===== Epoch 187	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438  3.015454    1.7175045
 -0.36420864 -0.37237892  2.5742931   1.087837   -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.17337902 -0.12041923
  0.0015849   0.0032306   0.09181394 -0.2209663   0.00150104  0.00278983
 -0.00148575 -0.00146706] 3 5
train:	 Loss = 1.4355,	 Acc = 0.4545
5569 0.232
10872 0.493
5614 0.575
720 0.637
125 0.6
12 0.083
0 0.0
0 0.0
0.5259182379057833
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4048,	 Acc = 0.4826
715 0.31
3297 0.507
1558 0.521
160 0.356
4 0.75
0 0.0
0 0.0
0 0.0
0.5070731221358836
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4815,	 Acc1 = 0.4165,	 Acc2 = 0.4458

 ===== Epoch 188	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239   2.1562846   1.9498141
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064  -0.36323875 -0.23451398
 -0.00148575 -0.00146706] 2 5
train:	 Loss = 1.4341,	 Acc = 0.4527
5568 0.229
10873 0.493
5613 0.572
721 0.631
125 0.552
12 0.083
0 0.0
0 0.0
0.5243888376383764
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4312,	 Acc = 0.4519
715 0.326
3297 0.49
1558 0.441
160 0.331
4 0.75
0 0.0
0 0.0
0 0.0
0.4698147041243276
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.5029,	 Acc1 = 0.4002,	 Acc2 = 0.4262

 ===== Epoch 189	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  1.2268306   3.189447   -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.18310751  0.28571415 -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 6 6
train:	 Loss = 1.4361,	 Acc = 0.4519
5568 0.239
10878 0.49
5609 0.569
720 0.607
125 0.52
12 0.25
0 0.0
0 0.0
0.520179889298893
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4345,	 Acc = 0.4503
715 0.316
3297 0.487
1558 0.447
160 0.325
4 0.75
0 0.0
0 0.0
0 0.0
0.46941621837019326
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.5152,	 Acc1 = 0.3972,	 Acc2 = 0.4225

 ===== Epoch 190	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 3 1
train:	 Loss = 1.4380,	 Acc = 0.4553
5568 0.238
10879 0.496
5608 0.567
720 0.64
125 0.52
12 0.167
0 0.0
0 0.0
0.5250230627306273
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4240,	 Acc = 0.4552
715 0.312
3297 0.497
1558 0.44
160 0.381
4 0.75
0 0.0
0 0.0
0 0.0
0.4755927475592748
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4948,	 Acc1 = 0.4017,	 Acc2 = 0.4279

 ===== Epoch 191	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
  1.2700534   1.2588936 ] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
  0.04384574 -0.10847999] 3 2
train:	 Loss = 1.4337,	 Acc = 0.4540
5571 0.234
10875 0.495
5611 0.571
719 0.62
124 0.524
12 0.083
0 0.0
0 0.0
0.5245948907214116
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4328,	 Acc = 0.4745
715 0.32
3297 0.509
1558 0.488
160 0.319
4 0.75
0 0.0
0 0.0
0 0.0
0.496513249651325
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 4
Testing:	 Loss = 1.5005,	 Acc1 = 0.4135,	 Acc2 = 0.4421

 ===== Epoch 192	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 0 1
train:	 Loss = 1.4366,	 Acc = 0.4533
5570 0.231
10876 0.496
5608 0.569
721 0.613
125 0.568
12 0.083
0 0.0
0 0.0
0.5246799677084535
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4402,	 Acc = 0.4609
715 0.32
3297 0.489
1558 0.477
160 0.35
4 0.75
0 0.0
0 0.0
0 0.0
0.48097230524008766
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 4
Testing:	 Loss = 1.4961,	 Acc1 = 0.4085,	 Acc2 = 0.4361

 ===== Epoch 193	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 0 1
train:	 Loss = 1.4367,	 Acc = 0.4545
5566 0.237
10879 0.493
5610 0.574
720 0.611
125 0.528
12 0.167
0 0.0
0 0.0
0.5243860255966794
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4305,	 Acc = 0.4658
715 0.317
3297 0.498
1558 0.474
160 0.381
4 0.75
0 0.0
0 0.0
0 0.0
0.486949591552102
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 6
Testing:	 Loss = 1.5009,	 Acc1 = 0.4062,	 Acc2 = 0.4334

 ===== Epoch 194	 =====
[-0.3669651  -0.38051367  1.7691492   0.7963091  -0.4411929  -0.3648088
  3.0754814   3.189447   -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 1.0109920e-03  2.5630770e-03 -4.9269147e+00 -1.2440096e+00
  1.0307996e-03  7.6088449e-04 -2.0979278e-01 -7.7666974e-01
 -1.4162532e-03  3.5063999e-03  1.5010408e-03  2.7898324e-03
 -1.4857454e-03 -1.4670575e-03] 5 5
train:	 Loss = 1.4382,	 Acc = 0.4532
5569 0.233
10875 0.491
5610 0.575
721 0.626
125 0.504
12 0.167
0 0.0
0 0.0
0.5237271521651387
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4416,	 Acc = 0.4308
715 0.175
3297 0.489
1558 0.435
160 0.344
4 0.0
0 0.0
0 0.0
0 0.0
0.46722454672245467
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.5077,	 Acc1 = 0.3629,	 Acc2 = 0.4150

 ===== Epoch 195	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892  1.3066084   1.6324488  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.05020133  0.18674941  0.00150104  0.00278983
 -0.00148575 -0.00146706] 1 4
train:	 Loss = 1.4357,	 Acc = 0.4528
5568 0.225
10875 0.497
5611 0.569
721 0.631
125 0.52
12 0.25
0 0.0
0 0.0
0.5257726014760148
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4799,	 Acc = 0.4430
715 0.316
3297 0.486
1558 0.42
160 0.331
4 0.75
0 0.0
0 0.0
0 0.0
0.46104801753337316
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 4
Testing:	 Loss = 1.5189,	 Acc1 = 0.4139,	 Acc2 = 0.4426

 ===== Epoch 196	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239   1.1142527   2.1367147
 -0.38672873 -0.38123247] [ 1.0109920e-03  2.5630770e-03  2.3875074e-03  1.9614373e-03
  1.0307996e-03  7.6088449e-04  1.5849024e-03  3.2306046e-03
 -1.4162532e-03  3.5063999e-03 -1.9155401e-04  2.6857010e-01
 -1.4857454e-03 -1.4670575e-03] 6 4
train:	 Loss = 1.4374,	 Acc = 0.4518
5569 0.233
10874 0.492
5612 0.569
720 0.626
125 0.488
12 0.167
0 0.0
0 0.0
0.5221703280862596
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 3
val:	 Loss = 1.3959,	 Acc = 0.4953
715 0.323
3297 0.507
1558 0.562
160 0.381
4 0.75
0 0.0
0 0.0
0 0.0
0.5198246662681809
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 4
Testing:	 Loss = 1.4784,	 Acc1 = 0.4161,	 Acc2 = 0.4453

 ===== Epoch 197	 =====
[-0.3669651  -0.38051367  2.2358313   2.097901   -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 0 0
train:	 Loss = 1.4370,	 Acc = 0.4533
5571 0.232
10870 0.494
5613 0.572
721 0.626
125 0.48
12 0.167
0 0.0
0 0.0
0.524306556715299
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4549,	 Acc = 0.4285
715 0.18
3297 0.486
1558 0.432
160 0.325
4 0.0
0 0.0
0 0.0
0 0.0
0.4638374178123132
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.5172,	 Acc1 = 0.3582,	 Acc2 = 0.4093

 ===== Epoch 198	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438  1.0860224   1.8151823
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144 -0.2959982   0.02654389
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 1 1
train:	 Loss = 1.4338,	 Acc = 0.4534
5569 0.236
10877 0.493
5608 0.569
721 0.631
125 0.52
12 0.083
0 0.0
0 0.0
0.5233235311076515
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4557,	 Acc = 0.4376
715 0.316
3297 0.471
1558 0.431
160 0.344
4 0.75
0 0.0
0 0.0
0 0.0
0.4548714883442917
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.5120,	 Acc1 = 0.3955,	 Acc2 = 0.4205

 ===== Epoch 199	 =====
[ 1.842067    1.3842016  -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [-0.01184174  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 0 0
train:	 Loss = 1.4384,	 Acc = 0.4536
5566 0.236
10879 0.493
5610 0.571
720 0.614
125 0.52
12 0.083
0 0.0
0 0.0
0.5232906722010838
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4958,	 Acc = 0.4315
715 0.316
3297 0.473
1558 0.406
160 0.319
4 0.75
0 0.0
0 0.0
0 0.0
0.4478979876469416
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.5411,	 Acc1 = 0.3974,	 Acc2 = 0.4227

 ===== Epoch 200	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  1.1503193   1.0847317  -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 1.0109920e-03  2.5630770e-03  2.3875074e-03  1.9614373e-03
  1.0307996e-03  7.6088449e-04 -3.2133811e+00 -3.2023439e+00
 -1.4162532e-03  3.5063999e-03  4.2100372e+00  7.0127439e+00
 -1.4857454e-03 -1.4670575e-03] 0 2
train:	 Loss = 1.4389,	 Acc = 0.4515
5570 0.235
10875 0.489
5610 0.569
721 0.635
124 0.524
12 0.083
0 0.0
0 0.0
0.5209318417714219
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4360,	 Acc = 0.4583
715 0.32
3297 0.491
1558 0.462
160 0.362
4 0.75
0 0.0
0 0.0
0 0.0
0.4779836620840805
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4919,	 Acc1 = 0.4040,	 Acc2 = 0.4307

 ===== Epoch 201	 =====
[ 1.7628031   1.3665292  -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.01339604 -0.00046037  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 6 0
train:	 Loss = 1.4375,	 Acc = 0.4547
5568 0.234
10877 0.496
5610 0.574
720 0.604
125 0.504
12 0.167
0 0.0
0 0.0
0.5253690036900369
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 3
val:	 Loss = 1.4400,	 Acc = 0.4513
715 0.31
3297 0.478
1558 0.466
160 0.375
4 0.75
0 0.0
0 0.0
0 0.0
0.4714086471408647
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.5025,	 Acc1 = 0.3951,	 Acc2 = 0.4200

 ===== Epoch 202	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 5 1
train:	 Loss = 1.4362,	 Acc = 0.4533
5564 0.233
10877 0.491
5613 0.576
721 0.619
125 0.528
12 0.167
0 0.0
0 0.0
0.5240373530089923
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4098,	 Acc = 0.4444
715 0.183
3297 0.488
1558 0.483
160 0.344
4 0.0
0 0.0
0 0.0
0 0.0
0.4815700338712891
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 6
Testing:	 Loss = 1.4877,	 Acc1 = 0.3672,	 Acc2 = 0.4202

 ===== Epoch 203	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 1 6
train:	 Loss = 1.4334,	 Acc = 0.4558
5565 0.232
10877 0.495
5613 0.576
720 0.64
125 0.544
12 0.167
0 0.0
0 0.0
0.5276993140024212
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 3
val:	 Loss = 1.4283,	 Acc = 0.4794
715 0.317
3297 0.502
1558 0.517
160 0.369
4 0.75
0 0.0
0 0.0
0 0.0
0.5024905359633393
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 4
Testing:	 Loss = 1.4910,	 Acc1 = 0.4081,	 Acc2 = 0.4356

 ===== Epoch 204	 =====
[-0.3669651  -0.38051367  2.9747095   1.6081034  -0.4411929  -0.3648088
  2.0161352   3.141993   -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 1.0109920e-03  2.5630770e-03  2.8361425e-01 -3.3762184e-01
  1.0307996e-03  7.6088449e-04  1.3534015e-01 -8.5650200e-01
 -1.4162532e-03  3.5063999e-03  1.5010408e-03  2.7898324e-03
 -1.4857454e-03 -1.4670575e-03] 5 5
train:	 Loss = 1.4349,	 Acc = 0.4526
5569 0.235
10874 0.491
5614 0.573
718 0.613
125 0.488
12 0.167
0 0.0
0 0.0
0.522400968690538
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4473,	 Acc = 0.4336
715 0.316
3297 0.463
1558 0.434
160 0.35
4 0.75
0 0.0
0 0.0
0 0.0
0.4502889021717474
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 4
Testing:	 Loss = 1.5165,	 Acc1 = 0.3912,	 Acc2 = 0.4153

 ===== Epoch 205	 =====
[ 1.4854485   2.6136959  -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
  1.2066206   2.9357584 ] [-0.15883267 -0.02767137  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00337468 -0.07280902] 3 2
train:	 Loss = 1.4369,	 Acc = 0.4513
5569 0.228
10877 0.491
5608 0.575
721 0.614
125 0.488
12 0.167
0 0.0
0 0.0
0.5230352303523035
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4252,	 Acc = 0.4810
715 0.323
3297 0.5
1558 0.524
160 0.375
4 0.75
0 0.0
0 0.0
0 0.0
0.5034867503486751
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4876,	 Acc1 = 0.4089,	 Acc2 = 0.4366

 ===== Epoch 206	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438  3.6207438   2.2902517
 -0.36420864 -0.37237892  2.7583392   2.002984   -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144 -0.1676531  -0.00697402
  0.0015849   0.0032306  -0.05779205 -0.02856113  0.00150104  0.00278983
 -0.00148575 -0.00146706] 3 0
train:	 Loss = 1.4322,	 Acc = 0.4564
5569 0.242
10877 0.495
5608 0.573
721 0.624
125 0.52
12 0.167
0 0.0
0 0.0
0.5253992965461569
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4542,	 Acc = 0.4398
715 0.31
3297 0.472
1558 0.438
160 0.362
4 0.75
0 0.0
0 0.0
0 0.0
0.4582586172544332
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 4
Testing:	 Loss = 1.5078,	 Acc1 = 0.4002,	 Acc2 = 0.4262

 ===== Epoch 207	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 1.0109920e-03  2.5630770e-03  2.3875074e-03  1.9614373e-03
  1.0307996e-03  7.6088449e-04  1.5849024e-03  3.2306046e-03
 -1.4162532e-03  3.5063999e-03  4.6382451e+00  2.9406109e+00
 -1.4857454e-03 -1.4670575e-03] 6 1
train:	 Loss = 1.4365,	 Acc = 0.4546
5568 0.238
10876 0.492
5610 0.572
721 0.635
125 0.56
12 0.25
0 0.0
0 0.0
0.5242158671586716
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4195,	 Acc = 0.4779
715 0.316
3297 0.497
1558 0.525
160 0.35
4 0.75
0 0.0
0 0.0
0 0.0
0.5008965929468021
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4812,	 Acc1 = 0.4141,	 Acc2 = 0.4428

 ===== Epoch 208	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239   1.1940312   2.013754
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.20291194  0.28755438
 -0.00148575 -0.00146706] 4 6
train:	 Loss = 1.4356,	 Acc = 0.4552
5571 0.238
10873 0.492
5611 0.575
720 0.629
125 0.552
12 0.167
0 0.0
0 0.0
0.5249408915287469
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4011,	 Acc = 0.4873
715 0.315
3297 0.499
1558 0.553
160 0.375
4 0.75
0 0.0
0 0.0
0 0.0
0.5118549511854951
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4839,	 Acc1 = 0.4126,	 Acc2 = 0.4411

 ===== Epoch 209	 =====
[ 2.2287147   2.346085   -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
  2.2233071   2.5972364 ] [-2.8385565e+00 -3.2627575e+00  2.3875074e-03  1.9614373e-03
  1.0307996e-03  7.6088449e-04  1.5849024e-03  3.2306046e-03
 -1.4162532e-03  3.5063999e-03  1.5010408e-03  2.7898324e-03
 -1.4857454e-03  3.6287969e-03] 0 0
train:	 Loss = 1.4356,	 Acc = 0.4542
5568 0.237
10874 0.491
5612 0.573
721 0.634
125 0.552
12 0.167
0 0.0
0 0.0
0.5238122693726938
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4357,	 Acc = 0.4581
715 0.313
3297 0.499
1558 0.449
160 0.35
4 0.75
0 0.0
0 0.0
0 0.0
0.47878063359234907
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4915,	 Acc1 = 0.4077,	 Acc2 = 0.4351

 ===== Epoch 210	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  3.8096542   2.1733775  -0.40146217 -0.4092239   3.2262945   2.8179176
 -0.38672873 -0.38123247] [ 1.0109920e-03  2.5630770e-03  2.3875074e-03  1.9614373e-03
  1.0307996e-03  7.6088449e-04 -8.8584881e+00 -5.5973129e+00
 -1.4162532e-03  3.5063999e-03 -8.1692314e+00 -6.2477922e+00
 -1.4857454e-03 -1.4670575e-03] 2 1
train:	 Loss = 1.4389,	 Acc = 0.4571
5563 0.24
10880 0.494
5611 0.575
721 0.646
125 0.528
12 0.167
0 0.0
0 0.0
0.5268891578765347
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 3
val:	 Loss = 1.4132,	 Acc = 0.4749
715 0.19
3297 0.512
1558 0.541
160 0.344
4 0.0
0 0.0
0 0.0
0 0.0
0.5154413229727037
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4771,	 Acc1 = 0.3842,	 Acc2 = 0.4406

 ===== Epoch 211	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239   1.4647473   3.2851691
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.23760884  0.00278983
 -0.00148575 -0.00146706] 0 0
train:	 Loss = 1.4370,	 Acc = 0.4527
5568 0.23
10876 0.494
5611 0.57
720 0.642
125 0.472
12 0.167
0 0.0
0 0.0
0.5242735239852399
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4230,	 Acc = 0.4730
715 0.326
3297 0.487
1558 0.521
160 0.381
4 0.75
0 0.0
0 0.0
0 0.0
0.4939230922494521
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 4
Testing:	 Loss = 1.4894,	 Acc1 = 0.4157,	 Acc2 = 0.4448

 ===== Epoch 212	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 6 1
train:	 Loss = 1.4344,	 Acc = 0.4566
5569 0.234
10873 0.495
5613 0.579
720 0.625
125 0.56
12 0.167
0 0.0
0 0.0
0.5281669837974976
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4253,	 Acc = 0.4438
715 0.189
3297 0.493
1558 0.465
160 0.375
4 0.0
0 0.0
0 0.0
0 0.0
0.4801753337318191
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 6
Testing:	 Loss = 1.4886,	 Acc1 = 0.3738,	 Acc2 = 0.4282

 ===== Epoch 213	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 2 6
train:	 Loss = 1.4386,	 Acc = 0.4513
5569 0.229
10873 0.491
5613 0.573
720 0.606
125 0.592
12 0.083
0 0.0
0 0.0
0.5225162889926772
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4033,	 Acc = 0.4902
715 0.316
3297 0.507
1558 0.547
160 0.362
4 0.75
0 0.0
0 0.0
0 0.0
0.5150428372185695
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 4
Testing:	 Loss = 1.4788,	 Acc1 = 0.4108,	 Acc2 = 0.4389

 ===== Epoch 214	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239   1.432231    1.7481585
 -0.38672873 -0.38123247] [ 1.0109920e-03  2.5630770e-03  2.3875074e-03  1.9614373e-03
  1.0307996e-03  7.6088449e-04  1.5849024e-03  3.2306046e-03
 -1.4162532e-03  3.5063999e-03  1.7837064e-01  8.0013061e-01
 -1.4857454e-03 -1.4670575e-03] 6 6
train:	 Loss = 1.4359,	 Acc = 0.4520
5570 0.237
10876 0.488
5608 0.574
721 0.607
125 0.528
12 0.167
0 0.0
0 0.0
0.5212201591511937
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4383,	 Acc = 0.4763
715 0.316
3297 0.504
1558 0.506
160 0.325
4 0.75
0 0.0
0 0.0
0 0.0
0.49910340705319783
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 4
Testing:	 Loss = 1.5099,	 Acc1 = 0.4159,	 Acc2 = 0.4451

 ===== Epoch 215	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
train:	 Loss = 1.4370,	 Acc = 0.4554
5563 0.242
10880 0.493
5611 0.573
721 0.621
125 0.512
12 0.083
0 0.0
0 0.0
0.5237189463369647
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4219,	 Acc = 0.4590
715 0.313
3297 0.486
1558 0.48
160 0.344
4 0.75
0 0.0
0 0.0
0 0.0
0.4797768479776848
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4997,	 Acc1 = 0.3901,	 Acc2 = 0.4140

 ===== Epoch 216	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 5 1
train:	 Loss = 1.4404,	 Acc = 0.4520
5567 0.235
10877 0.491
5611 0.571
720 0.612
125 0.512
12 0.167
0 0.0
0 0.0
0.5217641971749784
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4317,	 Acc = 0.4506
715 0.312
3297 0.483
1558 0.456
160 0.356
4 0.75
0 0.0
0 0.0
0 0.0
0.47041243275552896
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4920,	 Acc1 = 0.4027,	 Acc2 = 0.4292

 ===== Epoch 217	 =====
[-0.3669651  -0.38051367  2.661013    1.567287   -0.4411929  -0.3648088
  1.8079792   3.7756407  -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.07108929  0.07437257  0.0010308   0.00076088
  0.2237129   0.00937155 -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 2 0
train:	 Loss = 1.4353,	 Acc = 0.4537
5566 0.236
10877 0.492
5612 0.569
720 0.639
125 0.56
12 0.167
0 0.0
0 0.0
0.5235212729159461
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4401,	 Acc = 0.4438
715 0.323
3297 0.48
1558 0.436
160 0.319
4 0.75
0 0.0
0 0.0
0 0.0
0.46104801753337316
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.5046,	 Acc1 = 0.4000,	 Acc2 = 0.4259

 ===== Epoch 218	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438  2.5309932   2.7764208
 -0.36420864 -0.37237892  1.6087667   2.3237827  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.14129275  0.09615799
  0.0015849   0.0032306   0.05278789  0.3058574   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 4
train:	 Loss = 1.4375,	 Acc = 0.4556
5569 0.233
10873 0.495
5612 0.576
721 0.627
125 0.528
12 0.167
0 0.0
0 0.0
0.5270137807761056
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4415,	 Acc = 0.4573
715 0.323
3297 0.494
1558 0.448
160 0.381
4 0.75
0 0.0
0 0.0
0 0.0
0.47638971906754335
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4962,	 Acc1 = 0.4068,	 Acc2 = 0.4341

 ===== Epoch 219	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438  1.8618484   1.0359799
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144 -0.3427531   0.03685709
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 3 1
train:	 Loss = 1.4368,	 Acc = 0.4521
5569 0.231
10874 0.493
5613 0.571
720 0.607
124 0.565
12 0.083
0 0.0
0 0.0
0.5230928905033732
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4348,	 Acc = 0.4447
715 0.317
3297 0.476
1558 0.448
160 0.325
4 0.75
0 0.0
0 0.0
0 0.0
0.4628412034269775
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.5027,	 Acc1 = 0.3914,	 Acc2 = 0.4155

 ===== Epoch 220	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  1.2650862   2.3771498  -0.40146217 -0.4092239   2.3581867   1.8539047
 -0.38672873 -0.38123247] [ 1.0109920e-03  2.5630770e-03  2.3875074e-03  1.9614373e-03
  1.0307996e-03  7.6088449e-04 -8.4273338e-01  1.5675429e-01
 -1.4162532e-03  3.5063999e-03 -7.4066895e-01  6.9234900e-02
 -1.4857454e-03 -1.4670575e-03] 1 1
train:	 Loss = 1.4377,	 Acc = 0.4553
5570 0.238
10874 0.494
5613 0.573
720 0.615
123 0.553
12 0.167
0 0.0
0 0.0
0.5249682850882251
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.5287,	 Acc = 0.4105
715 0.176
3297 0.465
1558 0.413
160 0.319
4 0.0
0 0.0
0 0.0
0 0.0
0.44391313010559874
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 4
Testing:	 Loss = 1.5781,	 Acc1 = 0.3528,	 Acc2 = 0.4028

 ===== Epoch 221	 =====
[-0.3669651  -0.38051367  1.6751626   1.4539078  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.2980727   0.05190015  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 2 2
train:	 Loss = 1.4372,	 Acc = 0.4536
5566 0.23
10877 0.492
5611 0.58
721 0.616
125 0.52
12 0.083
0 0.0
0 0.0
0.5253084284561282
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4425,	 Acc = 0.4498
715 0.323
3297 0.485
1558 0.441
160 0.362
4 0.75
0 0.0
0 0.0
0 0.0
0.4678222753536561
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 4
Testing:	 Loss = 1.4975,	 Acc1 = 0.4108,	 Acc2 = 0.4389

 ===== Epoch 222	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892  2.0066729   2.637121   -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [4.0360217e+00 1.4749806e+00 2.3875074e-03 1.9614373e-03 1.0307996e-03
 7.6088449e-04 1.5849024e-03 3.2306046e-03 1.6445366e-01 5.1200581e-01
 1.5010408e-03 2.7898324e-03 6.3045397e+00 2.7044318e+00] 6 6
train:	 Loss = 1.4365,	 Acc = 0.4537
5567 0.234
10877 0.494
5610 0.57
721 0.62
125 0.512
12 0.167
0 0.0
0 0.0
0.5240703372729894
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4381,	 Acc = 0.4330
715 0.186
3297 0.494
1558 0.431
160 0.319
4 0.0
0 0.0
0 0.0
0 0.0
0.4682207611077904
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 6
Testing:	 Loss = 1.5066,	 Acc1 = 0.3664,	 Acc2 = 0.4192

 ===== Epoch 223	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 1.0109920e-03  2.5630770e-03  2.4180732e+00  1.2778956e+00
  1.0307996e-03  7.6088449e-04  1.5849024e-03  3.2306046e-03
 -1.4162532e-03  3.5063999e-03  1.5010408e-03  2.7898324e-03
 -1.4857454e-03 -1.4670575e-03] 6 6
train:	 Loss = 1.4331,	 Acc = 0.4548
5570 0.233
10870 0.496
5614 0.57
721 0.641
125 0.536
12 0.25
0 0.0
0 0.0
0.5259485641794487
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4551,	 Acc = 0.4405
715 0.323
3297 0.48
1558 0.418
160 0.356
4 0.75
0 0.0
0 0.0
0 0.0
0.4572624028690974
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.5078,	 Acc1 = 0.4035,	 Acc2 = 0.4302

 ===== Epoch 224	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 6 1
train:	 Loss = 1.4370,	 Acc = 0.4524
5568 0.232
10874 0.494
5612 0.567
721 0.62
125 0.552
12 0.167
0 0.0
0 0.0
0.5230627306273062
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4149,	 Acc = 0.4728
715 0.308
3297 0.494
1558 0.512
160 0.387
4 0.75
0 0.0
0 0.0
0 0.0
0.4963140067742578
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.5018,	 Acc1 = 0.4019,	 Acc2 = 0.4282

 ===== Epoch 225	 =====
[-0.3669651  -0.38051367  1.701202    2.673867   -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.25155285 -0.01052324  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 2 0
train:	 Loss = 1.4370,	 Acc = 0.4526
5571 0.229
10873 0.494
5610 0.568
721 0.648
125 0.528
12 0.25
0 0.0
0 0.0
0.5243642235165216
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4337,	 Acc = 0.4590
715 0.313
3297 0.483
1558 0.487
160 0.35
4 0.75
0 0.0
0 0.0
0 0.0
0.4797768479776848
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.5041,	 Acc1 = 0.3976,	 Acc2 = 0.4230

 ===== Epoch 226	 =====
[-0.3669651  -0.38051367  1.7789148   3.095637   -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 1.0109920e-03  2.5630770e-03  3.3491194e-01 -7.8109503e+00
  1.0307996e-03  7.6088449e-04  1.5849024e-03  3.2306046e-03
 -1.4162532e-03  3.5063999e-03  1.5010408e-03  2.7898324e-03
 -1.4857454e-03 -1.4670575e-03] 1 1
train:	 Loss = 1.4375,	 Acc = 0.4535
5567 0.234
10874 0.492
5613 0.573
721 0.623
125 0.504
12 0.25
0 0.0
0 0.0
0.5238397232631883
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4491,	 Acc = 0.4506
715 0.324
3297 0.482
1558 0.451
160 0.356
4 0.75
0 0.0
0 0.0
0 0.0
0.4686192468619247
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.5081,	 Acc1 = 0.4005,	 Acc2 = 0.4264

 ===== Epoch 227	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239   2.389947    1.8243941
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064  -0.39793298  0.08347313
 -0.00148575 -0.00146706] 4 3
train:	 Loss = 1.4355,	 Acc = 0.4511
5568 0.233
10875 0.492
5611 0.565
721 0.627
125 0.536
12 0.167
0 0.0
0 0.0
0.5212177121771218
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 3
val:	 Loss = 1.4315,	 Acc = 0.4547
715 0.319
3297 0.474
1558 0.484
160 0.375
4 0.75
0 0.0
0 0.0
0 0.0
0.4739988045427376
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 4
Testing:	 Loss = 1.5010,	 Acc1 = 0.3953,	 Acc2 = 0.4202

 ===== Epoch 228	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239   2.1320868   1.0767921
  2.662491    3.5865607 ] [ 1.0109920e-03  2.5630770e-03  2.3875074e-03  1.9614373e-03
  1.0307996e-03  7.6088449e-04  1.5849024e-03  3.2306046e-03
 -1.4162532e-03  3.5063999e-03  1.8767861e-01  6.5774828e-01
 -6.5388947e+00 -7.7063985e+00] 6 6
train:	 Loss = 1.4372,	 Acc = 0.4497
5572 0.234
10870 0.486
5612 0.573
721 0.603
125 0.52
12 0.167
0 0.0
0 0.0
0.5189734717416379
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4810,	 Acc = 0.4238
715 0.186
3297 0.483
1558 0.418
160 0.344
4 0.0
0 0.0
0 0.0
0 0.0
0.45766088862323173
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.5258,	 Acc1 = 0.3689,	 Acc2 = 0.4222

 ===== Epoch 229	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892  1.2934216   2.9106703  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.07188097  0.17300619  0.00150104  0.00278983
 -0.00148575 -0.00146706] 0 1
train:	 Loss = 1.4357,	 Acc = 0.4537
5569 0.233
10874 0.495
5613 0.572
719 0.608
125 0.536
12 0.167
0 0.0
0 0.0
0.5245920544311826
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.3977,	 Acc = 0.4927
715 0.31
3297 0.503
1558 0.565
160 0.375
4 0.75
0 0.0
0 0.0
0 0.0
0.518629209005778
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4788,	 Acc1 = 0.4099,	 Acc2 = 0.4379

 ===== Epoch 230	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  3.2133145   1.2940867  -0.40146217 -0.4092239   3.352578    1.9399773
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.18907705  0.32870078 -0.00141625  0.0035064   0.03789052  0.20687109
 -0.00148575 -0.00146706] 2 2
train:	 Loss = 1.4357,	 Acc = 0.4555
5569 0.235
10876 0.497
5611 0.574
719 0.611
125 0.528
12 0.083
0 0.0
0 0.0
0.5262065386611313
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4407,	 Acc = 0.4454
715 0.317
3297 0.469
1558 0.463
160 0.356
4 0.75
0 0.0
0 0.0
0 0.0
0.46363817493524606
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.5044,	 Acc1 = 0.3939,	 Acc2 = 0.4185

 ===== Epoch 231	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892  1.281954    1.1102183  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306   0.05604202  0.09512791  0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 2
train:	 Loss = 1.4350,	 Acc = 0.4546
5566 0.235
10882 0.494
5607 0.574
720 0.624
125 0.512
12 0.083
0 0.0
0 0.0
0.5250201775625505
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4526,	 Acc = 0.4430
715 0.317
3297 0.484
1558 0.426
160 0.319
4 0.75
0 0.0
0 0.0
0 0.0
0.460848774656306
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.5175,	 Acc1 = 0.3908,	 Acc2 = 0.4148

 ===== Epoch 232	 =====
[-0.3669651  -0.38051367  2.2757049   1.0162644  -0.4411929  -0.3648088
  1.4344232   3.217361   -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 3.9235260e+00  4.3170190e+00 -6.0673885e+00 -1.4862124e+00
  1.0307996e-03  7.6088449e-04  1.3527665e-02 -1.0284485e+00
 -1.4162532e-03  3.5063999e-03  1.5010408e-03  2.7898324e-03
 -1.4857454e-03 -1.4670575e-03] 5 5
train:	 Loss = 1.4385,	 Acc = 0.4494
5568 0.221
10875 0.493
5611 0.568
721 0.627
125 0.536
12 0.083
0 0.0
0 0.0
0.5227167896678967
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4556,	 Acc = 0.4332
715 0.31
3297 0.473
1558 0.416
160 0.325
4 0.75
0 0.0
0 0.0
0 0.0
0.45068738792588164
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 4
Testing:	 Loss = 1.5077,	 Acc1 = 0.4027,	 Acc2 = 0.4292

 ===== Epoch 233	 =====
[ 0.845434    3.0706537  -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00091161 -0.00650726  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 0 0
train:	 Loss = 1.4372,	 Acc = 0.4537
5567 0.239
10877 0.492
5611 0.571
721 0.616
124 0.508
12 0.083
0 0.0
0 0.0
0.5228019602190833
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4158,	 Acc = 0.4498
715 0.31
3297 0.475
1558 0.469
160 0.369
4 0.75
0 0.0
0 0.0
0 0.0
0.4696154612472604
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4929,	 Acc1 = 0.3945,	 Acc2 = 0.4192

 ===== Epoch 234	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239   1.4046305   2.220328
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.0404281   0.26382402
 -0.00148575 -0.00146706] 2 6
train:	 Loss = 1.4362,	 Acc = 0.4539
5566 0.231
10875 0.493
5615 0.574
719 0.638
125 0.536
12 0.167
0 0.0
0 0.0
0.5253084284561282
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4350,	 Acc = 0.4449
715 0.316
3297 0.478
1558 0.442
160 0.35
4 0.75
0 0.0
0 0.0
0 0.0
0.4632396891811118
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4996,	 Acc1 = 0.3936,	 Acc2 = 0.4182

 ===== Epoch 235	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239   2.3888133   1.0202302
  2.8933163   3.3556309 ] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.48217702  0.10720351
  0.3488954   0.11573759] 2 2
train:	 Loss = 1.4341,	 Acc = 0.4535
5563 0.233
10881 0.493
5611 0.577
720 0.599
125 0.496
12 0.25
0 0.0
0 0.0
0.5241224278056372
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4625,	 Acc = 0.4402
715 0.327
3297 0.479
1558 0.417
160 0.362
4 0.75
0 0.0
0 0.0
0 0.0
0.4562661884837617
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.5117,	 Acc1 = 0.4046,	 Acc2 = 0.4314

 ===== Epoch 236	 =====
[-0.3669651  -0.38051367  3.1691947  -4.1175413   3.4375894   1.5576681
  1.8147305   3.608157    4.3442335   0.85158986 -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 1.0109920e-03  2.5630770e-03 -8.0790157e+00  4.1668506e+00
  1.1102999e+00  2.4569941e-01  2.5117943e-01  4.6380165e-01
  4.8751357e-01  4.1580322e-01  1.5010408e-03  2.7898324e-03
 -1.4857454e-03 -1.4670575e-03] 2 6
train:	 Loss = 1.4325,	 Acc = 0.4568
5567 0.239
10877 0.494
5611 0.576
720 0.626
125 0.584
12 0.25
0 0.0
0 0.0
0.5266070913808014
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4173,	 Acc = 0.4881
715 0.327
3297 0.504
1558 0.538
160 0.387
4 0.75
0 0.0
0 0.0
0 0.0
0.5110579796772265
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 4
Testing:	 Loss = 1.4882,	 Acc1 = 0.4143,	 Acc2 = 0.4431

 ===== Epoch 237	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 0 1
train:	 Loss = 1.4378,	 Acc = 0.4530
5567 0.232
10876 0.493
5611 0.573
721 0.613
125 0.512
12 0.25
0 0.0
0 0.0
0.5237244162582877
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4311,	 Acc = 0.4494
715 0.313
3297 0.478
1558 0.459
160 0.362
4 0.75
0 0.0
0 0.0
0 0.0
0.4688184897389918
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4955,	 Acc1 = 0.3951,	 Acc2 = 0.4200

 ===== Epoch 238	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438  3.549241    2.4012492
 -0.36420864 -0.37237892  4.2811637   1.7543026  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308  -0.00181742
  0.0015849   0.0032306  -0.00141625 -0.00107468  0.00150104  0.00278983
 -0.00148575 -0.00146706] 0 0
train:	 Loss = 1.4357,	 Acc = 0.4556
5569 0.238
10876 0.493
5609 0.575
721 0.635
125 0.488
12 0.25
0 0.0
0 0.0
0.5255146168482961
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4223,	 Acc = 0.4566
715 0.324
3297 0.484
1558 0.469
160 0.35
4 0.75
0 0.0
0 0.0
0 0.0
0.4753935046822076
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 6
Testing:	 Loss = 1.4970,	 Acc1 = 0.3961,	 Acc2 = 0.4212

 ===== Epoch 239	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 1 1
train:	 Loss = 1.4355,	 Acc = 0.4539
5569 0.232
10876 0.494
5611 0.574
719 0.627
125 0.488
12 0.083
0 0.0
0 0.0
0.5252263160929481
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4211,	 Acc = 0.4672
715 0.317
3297 0.483
1558 0.512
160 0.369
4 0.75
0 0.0
0 0.0
0 0.0
0.4885435345686392
0.5204223948993824
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4849,	 Acc1 = 0.4054,	 Acc2 = 0.4324

 ===== Epoch 240	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239   1.6658944   1.799802
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.03196513  0.7431777
 -0.00148575 -0.00146706] 6 6
train:	 Loss = 1.4358,	 Acc = 0.4553
5567 0.239
10876 0.492
5612 0.573
720 0.64
125 0.504
12 0.167
0 0.0
0 0.0
0.5245315652925915
0.5204223948993824
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 3
val:	 Loss = 1.3940,	 Acc = 0.4806
715 0.183
3297 0.508
1558 0.567
160 0.412
4 0.0
0 0.0
0 0.0
0 0.0
0.5230125523012552
0.5230125523012552
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4773,	 Acc1 = 0.3769,	 Acc2 = 0.4319

 ===== Epoch 241	 =====
[-0.3669651  -0.38051367  2.4596112   1.4675134  -0.4411929  -0.3648088
  1.5570666   3.8035548  -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.34296206 -0.04797728  0.0010308   0.00076088
  0.14728291 -0.07046076 -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 2 3
train:	 Loss = 1.4364,	 Acc = 0.4539
5569 0.228
10876 0.496
5610 0.574
721 0.627
124 0.5
12 0.25
0 0.0
0 0.0
0.5265524995675489
0.5230125523012552
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4175,	 Acc = 0.4693
715 0.317
3297 0.501
1558 0.484
160 0.344
4 0.75
0 0.0
0 0.0
0 0.0
0.4909344490934449
0.5230125523012552
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4877,	 Acc1 = 0.4083,	 Acc2 = 0.4359

 ===== Epoch 242	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438  1.3567388   1.5732076
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144 -0.28132954  0.04459199
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 1 1
train:	 Loss = 1.4339,	 Acc = 0.4553
5565 0.238
10878 0.495
5611 0.572
721 0.612
125 0.544
12 0.167
0 0.0
0 0.0
0.5250475586556753
0.5230125523012552
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4340,	 Acc = 0.4691
715 0.316
3297 0.49
1558 0.505
160 0.369
4 0.75
0 0.0
0 0.0
0 0.0
0.4909344490934449
0.5230125523012552
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4960,	 Acc1 = 0.4002,	 Acc2 = 0.4262

 ===== Epoch 243	 =====
[ 2.8013773   3.2726238  -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
  2.6078691   3.536701  ] [ 0.01330834  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 0 0
train:	 Loss = 1.4342,	 Acc = 0.4558
5566 0.232
10877 0.494
5611 0.58
721 0.639
125 0.544
12 0.25
0 0.0
0 0.0
0.5277297359621815
0.5230125523012552
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 3
val:	 Loss = 1.4217,	 Acc = 0.4773
715 0.327
3297 0.5
1558 0.51
160 0.362
4 0.75
0 0.0
0 0.0
0 0.0
0.4987049212990636
0.5230125523012552
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 4
Testing:	 Loss = 1.4923,	 Acc1 = 0.4079,	 Acc2 = 0.4354

 ===== Epoch 244	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892  1.081281    3.1369703  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.26593542  0.18674941  0.00150104  0.00278983
 -0.00148575 -0.00146706] 1 1
train:	 Loss = 1.4386,	 Acc = 0.4543
5570 0.228
10878 0.498
5610 0.57
718 0.634
124 0.524
12 0.167
0 0.0
0 0.0
0.527044170222581
0.5230125523012552
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.5226,	 Acc = 0.4170
715 0.179
3297 0.475
1558 0.414
160 0.325
4 0.0
0 0.0
0 0.0
0 0.0
0.4508866308029488
0.5230125523012552
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.5704,	 Acc1 = 0.3697,	 Acc2 = 0.4232

 ===== Epoch 245	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438  2.7061186   2.054937
 -0.36420864 -0.37237892  3.6619427   0.9187338  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 1.0109920e-03  2.5630770e-03  2.3875074e-03  1.9614373e-03
  1.0241244e+00 -7.9068856e+00  1.5849024e-03  3.2306046e-03
  2.6635703e-01  2.8295201e-01  1.5010408e-03  2.7898324e-03
 -1.4857454e-03 -1.4670575e-03] 6 2
train:	 Loss = 1.4353,	 Acc = 0.4544
5568 0.235
10875 0.494
5611 0.572
721 0.627
125 0.528
12 0.167
0 0.0
0 0.0
0.5249077490774908
0.5230125523012552
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4192,	 Acc = 0.4606
715 0.189
3297 0.505
1558 0.504
160 0.337
4 0.0
0 0.0
0 0.0
0 0.0
0.499302649930265
0.5230125523012552
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 6
Testing:	 Loss = 1.4926,	 Acc1 = 0.3806,	 Acc2 = 0.4364

 ===== Epoch 246	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  1.3444113   2.5418422  -0.40146217 -0.4092239   2.6629317   2.016213
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064  -0.03404083  0.00278983
 -0.00148575 -0.00146706] 0 0
train:	 Loss = 1.4368,	 Acc = 0.4526
5567 0.23
10877 0.495
5610 0.568
721 0.637
125 0.52
12 0.167
0 0.0
0 0.0
0.5241279907754396
0.5230125523012552
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4325,	 Acc = 0.4440
715 0.182
3297 0.493
1558 0.47
160 0.375
4 0.0
0 0.0
0 0.0
0 0.0
0.481370790994222
0.5230125523012552
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 4
Testing:	 Loss = 1.5033,	 Acc1 = 0.3672,	 Acc2 = 0.4202

 ===== Epoch 247	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438  4.2906537   2.6965024
 -0.36420864 -0.37237892  4.6561365   2.1969552  -0.42425194 -0.42087102
  3.0157766   0.9833525 ] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.26322174  0.05748349
  0.0015849   0.0032306   0.2219119   0.08596575  0.00150104  0.00278983
 -0.27442643  0.16669612] 2 3
train:	 Loss = 1.4357,	 Acc = 0.4547
5569 0.238
10872 0.49
5613 0.578
721 0.628
125 0.568
12 0.083
0 0.0
0 0.0
0.5243614138269043
0.5230125523012552
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4495,	 Acc = 0.4365
715 0.317
3297 0.469
1558 0.432
160 0.337
4 0.75
0 0.0
0 0.0
0 0.0
0.4534767882048217
0.5230125523012552
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 4
Testing:	 Loss = 1.5091,	 Acc1 = 0.3967,	 Acc2 = 0.4220

 ===== Epoch 248	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239   2.9601157   1.959651
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.09289592  0.27331614
 -0.00148575 -0.00146706] 2 6
train:	 Loss = 1.4400,	 Acc = 0.4533
5569 0.236
10875 0.49
5611 0.575
720 0.618
125 0.544
12 0.333
0 0.0
0 0.0
0.5231505506544427
0.5230125523012552
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4488,	 Acc = 0.4472
715 0.319
3297 0.489
1558 0.429
160 0.337
4 0.75
0 0.0
0 0.0
0 0.0
0.4654313608288504
0.5230125523012552
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 4
Testing:	 Loss = 1.5068,	 Acc1 = 0.4097,	 Acc2 = 0.4376

 ===== Epoch 249	 =====
[ 3.0668788   2.5884495  -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
  2.3105266   2.8596568 ] [-0.41291755 -0.41769576  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.21020268 -0.70979077] 0 5
train:	 Loss = 1.4368,	 Acc = 0.4549
5568 0.242
10876 0.493
5611 0.569
720 0.618
125 0.52
12 0.25
0 0.0
0 0.0
0.5231780442804428
0.5230125523012552
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4286,	 Acc = 0.4608
715 0.319
3297 0.485
1558 0.485
160 0.362
4 0.75
0 0.0
0 0.0
0 0.0
0.48097230524008766
0.5230125523012552
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4944,	 Acc1 = 0.3988,	 Acc2 = 0.4245

 ===== Epoch 250	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 6 1
train:	 Loss = 1.4367,	 Acc = 0.4551
5570 0.237
10877 0.495
5607 0.574
721 0.617
125 0.496
12 0.25
0 0.0
0 0.0
0.5252566024679968
0.5230125523012552
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4122,	 Acc = 0.4808
715 0.308
3297 0.499
1558 0.533
160 0.369
4 0.75
0 0.0
0 0.0
0 0.0
0.5054791791193465
0.5230125523012552
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4739,	 Acc1 = 0.4108,	 Acc2 = 0.4389

 ===== Epoch 251	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  1.6881499   1.0958972  -0.40146217 -0.4092239   1.8632604   3.0761354
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.15963687 -0.01519224 -0.00141625  0.0035064   0.37639377 -0.0114484
 -0.00148575 -0.00146706] 0 3
train:	 Loss = 1.4357,	 Acc = 0.4561
5566 0.237
10876 0.494
5613 0.577
720 0.628
125 0.496
12 0.167
0 0.0
0 0.0
0.5263461316730081
0.5230125523012552
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4428,	 Acc = 0.4376
715 0.322
3297 0.465
1558 0.442
160 0.35
4 0.75
0 0.0
0 0.0
0 0.0
0.4540745168360231
0.5230125523012552
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 4
Testing:	 Loss = 1.5049,	 Acc1 = 0.3897,	 Acc2 = 0.4135

 ===== Epoch 252	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438  1.2990017   2.2502925
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144 -0.00080278  0.11678439
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 1 1
train:	 Loss = 1.4328,	 Acc = 0.4555
5569 0.235
10873 0.494
5612 0.574
721 0.646
125 0.512
12 0.167
0 0.0
0 0.0
0.5263218589632704
0.5230125523012552
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 3
val:	 Loss = 1.4394,	 Acc = 0.4723
715 0.326
3297 0.5
1558 0.492
160 0.356
4 0.75
0 0.0
0 0.0
0 0.0
0.4931261207411835
0.5230125523012552
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 4
Testing:	 Loss = 1.4977,	 Acc1 = 0.4110,	 Acc2 = 0.4391

 ===== Epoch 253	 =====
[ 3.9370897   2.9747179  -0.4202629  -0.33521438 -0.4411929  -0.3648088
  1.9323112   3.5997827  -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [-4.7074490e+00 -4.0155950e+00  2.3875074e-03  1.9614373e-03
  1.0307996e-03  7.6088449e-04  1.0786996e-01 -3.7750810e-01
 -1.4162532e-03  3.5063999e-03  1.5010408e-03  2.7898324e-03
 -1.4857454e-03 -1.4670575e-03] 5 5
train:	 Loss = 1.4357,	 Acc = 0.4543
5571 0.231
10877 0.494
5607 0.575
720 0.624
125 0.544
12 0.167
0 0.0
0 0.0
0.5258635603483075
0.5230125523012552
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4255,	 Acc = 0.4623
715 0.32
3297 0.485
1558 0.488
160 0.369
4 0.75
0 0.0
0 0.0
0 0.0
0.4825662482566248
0.5230125523012552
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.5019,	 Acc1 = 0.3986,	 Acc2 = 0.4242

 ===== Epoch 254	 =====
[-0.3669651  -0.38051367  0.8248015   0.88701236 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 1.0109920e-03  2.5630770e-03 -2.8007843e+00 -1.3438869e+00
  1.0307996e-03  7.6088449e-04  1.5849024e-03  3.2306046e-03
 -1.4162532e-03  3.5063999e-03  1.5010408e-03  2.7898324e-03
 -1.4857454e-03 -1.4670575e-03] 5 1
train:	 Loss = 1.4330,	 Acc = 0.4549
5565 0.24
10876 0.49
5613 0.578
721 0.631
125 0.52
12 0.083
0 0.0
0 0.0
0.5239522684037585
0.5230125523012552
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4692,	 Acc = 0.4381
715 0.316
3297 0.48
1558 0.417
160 0.306
4 0.75
0 0.0
0 0.0
0 0.0
0.45546921697549314
0.5230125523012552
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 4
Testing:	 Loss = 1.5247,	 Acc1 = 0.4068,	 Acc2 = 0.4341

 ===== Epoch 255	 =====
[-0.3669651  -0.38051367  1.5217723   0.9935887  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.26620734  0.05689402  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 2
train:	 Loss = 1.4372,	 Acc = 0.4542
5571 0.23
10875 0.498
5609 0.57
720 0.624
125 0.504
12 0.083
0 0.0
0 0.0
0.5261518943544201
0.5230125523012552
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4774,	 Acc = 0.4316
715 0.319
3297 0.468
1558 0.414
160 0.344
4 0.75
0 0.0
0 0.0
0 0.0
0.4476987447698745
0.5230125523012552
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 4
Testing:	 Loss = 1.5251,	 Acc1 = 0.4007,	 Acc2 = 0.4267

 ===== Epoch 256	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239   0.8696256   2.5301893
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.21221991 -0.7470902
 -0.00148575 -0.00146706] 5 5
train:	 Loss = 1.4371,	 Acc = 0.4517
5566 0.229
10874 0.493
5615 0.569
720 0.632
125 0.504
12 0.083
0 0.0
0 0.0
0.5230600714862216
0.5230125523012552
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4073,	 Acc = 0.4660
715 0.317
3297 0.482
1558 0.507
160 0.387
4 0.75
0 0.0
0 0.0
0 0.0
0.4871488344291692
0.5230125523012552
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4875,	 Acc1 = 0.4005,	 Acc2 = 0.4264

 ===== Epoch 257	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438  0.5870308   0.8850232
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144 -0.21715415  0.2534343
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 6 6
train:	 Loss = 1.4328,	 Acc = 0.4554
5570 0.236
10873 0.492
5612 0.577
720 0.635
125 0.544
12 0.167
0 0.0
0 0.0
0.5257755737515858
0.5230125523012552
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4420,	 Acc = 0.4438
715 0.31
3297 0.476
1558 0.443
160 0.381
4 0.75
0 0.0
0 0.0
0 0.0
0.4628412034269775
0.5230125523012552
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.5080,	 Acc1 = 0.3953,	 Acc2 = 0.4202

 ===== Epoch 258	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  3.2189405   1.9444827  -0.40146217 -0.4092239   3.4905832   2.7269268
 -0.38672873 -0.38123247] [ 1.0109920e-03  2.5630770e-03  2.3875074e-03  1.9614373e-03
  1.0307996e-03  7.6088449e-04  5.6526119e-01 -2.9103428e-03
 -1.4162532e-03  3.5063999e-03 -7.9229182e-01 -1.1448396e-02
 -1.4857454e-03 -1.4670575e-03] 2 0
train:	 Loss = 1.4324,	 Acc = 0.4551
5566 0.229
10880 0.496
5608 0.576
721 0.632
125 0.568
12 0.167
0 0.0
0 0.0
0.5276144356047504
0.5230125523012552
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4177,	 Acc = 0.4503
715 0.323
3297 0.483
1558 0.449
160 0.356
4 0.75
0 0.0
0 0.0
0 0.0
0.46842000398485756
0.5230125523012552
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4997,	 Acc1 = 0.3932,	 Acc2 = 0.4177

 ===== Epoch 259	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
  0.89386076  1.0751995 ] [ 1.0109920e-03  2.5630770e-03  2.3875074e-03  1.9614373e-03
  1.0307996e-03  7.6088449e-04  1.5849024e-03  3.2306046e-03
 -1.4162532e-03  3.5063999e-03  1.5010408e-03  2.7898324e-03
 -1.7540220e-02  8.6992407e-01] 5 6
train:	 Loss = 1.4346,	 Acc = 0.4548
5567 0.232
10875 0.496
5612 0.573
721 0.626
125 0.504
12 0.25
0 0.0
0 0.0
0.5261458633611992
0.5230125523012552
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 3
val:	 Loss = 1.4298,	 Acc = 0.4573
715 0.31
3297 0.472
1558 0.501
160 0.369
4 0.75
0 0.0
0 0.0
0 0.0
0.4781829049611476
0.5230125523012552
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4982,	 Acc1 = 0.4040,	 Acc2 = 0.4307

 ===== Epoch 260	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438  0.7518322   1.2135758
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.1770462  -0.13588902
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 5 5
train:	 Loss = 1.4358,	 Acc = 0.4578
5569 0.235
10877 0.498
5609 0.579
720 0.631
125 0.52
12 0.083
0 0.0
0 0.0
0.5294931672720983
0.5230125523012552
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4672,	 Acc = 0.4419
715 0.323
3297 0.486
1558 0.417
160 0.306
4 0.75
0 0.0
0 0.0
0 0.0
0.45885634588563456
0.5230125523012552
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.5306,	 Acc1 = 0.3924,	 Acc2 = 0.4167

 ===== Epoch 261	 =====
[-0.3669651  -0.38051367  3.5093389  -4.70938     3.7549553   0.9871409
  2.822882    3.524415   -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 1.01099198e-03  2.56307703e-03  9.05605972e-01  7.90226650e+00
  7.89537951e-02  8.06881860e-02  2.66628601e-02  1.13767646e-01
 -1.41625316e-03  3.50639992e-03  1.50104077e-03  2.78983242e-03
 -1.48574542e-03 -1.46705750e-03] 3 2
train:	 Loss = 1.4341,	 Acc = 0.4557
5568 0.24
10878 0.491
5609 0.577
720 0.637
125 0.536
12 0.167
0 0.0
0 0.0
0.5249077490774908
0.5230125523012552
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4463,	 Acc = 0.4486
715 0.319
3297 0.47
1558 0.473
160 0.331
4 0.75
0 0.0
0 0.0
0 0.0
0.46702530384538754
0.5230125523012552
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.5163,	 Acc1 = 0.3945,	 Acc2 = 0.4192

 ===== Epoch 262	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 5 1
train:	 Loss = 1.4370,	 Acc = 0.4552
5572 0.238
10872 0.494
5610 0.571
721 0.638
125 0.496
12 0.333
0 0.0
0 0.0
0.5249711649365628
0.5230125523012552
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4049,	 Acc = 0.4723
715 0.323
3297 0.491
1558 0.513
160 0.35
4 0.75
0 0.0
0 0.0
0 0.0
0.4935246064953178
0.5230125523012552
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4820,	 Acc1 = 0.4060,	 Acc2 = 0.4332

 ===== Epoch 263	 =====
[ 2.9540784   1.7931916  -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892  2.0542605   3.3458622  -0.42425194 -0.42087102
  2.8708503   1.9936701 ] [ 0.0314723  -0.00348381  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.22148696 -0.01939898  0.00150104  0.00278983
 -0.00148575 -0.00656291] 0 0
train:	 Loss = 1.4339,	 Acc = 0.4536
5569 0.236
10874 0.493
5613 0.572
719 0.622
125 0.512
12 0.167
0 0.0
0 0.0
0.5234388514097907
0.5230125523012552
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 3
val:	 Loss = 1.3939,	 Acc = 0.4969
715 0.316
3297 0.5
1558 0.585
160 0.375
4 0.75
0 0.0
0 0.0
0 0.0
0.522614066547121
0.5230125523012552
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4873,	 Acc1 = 0.4035,	 Acc2 = 0.4302

 ===== Epoch 264	 =====
[ 1.7693268   2.502612   -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 8.3594042e-01  4.8026735e-01  2.3875074e-03  1.9614373e-03
  1.0307996e-03  7.6088449e-04  1.5849024e-03  3.2306046e-03
 -1.4162532e-03  3.5063999e-03  1.5010408e-03  2.7898324e-03
 -1.4857454e-03 -1.4670575e-03] 6 6
train:	 Loss = 1.4363,	 Acc = 0.4542
5570 0.233
10879 0.494
5607 0.575
719 0.627
125 0.48
12 0.083
0 0.0
0 0.0
0.5252566024679968
0.5230125523012552
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4160,	 Acc = 0.4779
715 0.323
3297 0.497
1558 0.52
160 0.356
4 0.75
0 0.0
0 0.0
0 0.0
0.4999003785614664
0.5230125523012552
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4903,	 Acc1 = 0.4130,	 Acc2 = 0.4416

 ===== Epoch 265	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 1 1
train:	 Loss = 1.4362,	 Acc = 0.4536
5570 0.231
10871 0.495
5613 0.573
721 0.619
125 0.512
12 0.167
0 0.0
0 0.0
0.5252566024679968
0.5230125523012552
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4267,	 Acc = 0.4695
715 0.324
3297 0.501
1558 0.477
160 0.381
4 0.75
0 0.0
0 0.0
0 0.0
0.49013747758517634
0.5230125523012552
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 4
Testing:	 Loss = 1.4883,	 Acc1 = 0.4120,	 Acc2 = 0.4404

 ===== Epoch 266	 =====
[-0.3669651  -0.38051367  1.6405787   1.0797569  -0.4411929  -0.3648088
  3.2909508   3.5997827  -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 1.0109920e-03  2.5630770e-03  2.9185474e-01 -3.1015554e-01
  1.0307996e-03  7.6088449e-04 -4.5580259e-01 -8.9948863e-01
 -1.4162532e-03  3.5063999e-03  1.5010408e-03  2.7898324e-03
 -1.4857454e-03 -1.4670575e-03] 5 5
train:	 Loss = 1.4339,	 Acc = 0.4562
5571 0.235
10877 0.494
5607 0.579
720 0.633
125 0.528
12 0.25
0 0.0
0 0.0
0.5271322299752033
0.5230125523012552
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4403,	 Acc = 0.4463
715 0.326
3297 0.485
1558 0.431
160 0.331
4 0.75
0 0.0
0 0.0
0 0.0
0.46343893205817893
0.5230125523012552
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.5089,	 Acc1 = 0.3951,	 Acc2 = 0.4200

 ===== Epoch 267	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239   1.5967029   2.6334765
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064  -0.0162699  -0.24400613
 -0.00148575 -0.00146706] 2 5
train:	 Loss = 1.4341,	 Acc = 0.4525
5568 0.234
10877 0.491
5611 0.571
720 0.624
124 0.548
12 0.167
0 0.0
0 0.0
0.5227744464944649
0.5230125523012552
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 3
val:	 Loss = 1.4340,	 Acc = 0.4438
715 0.172
3297 0.498
1558 0.461
160 0.387
4 0.0
0 0.0
0 0.0
0 0.0
0.4825662482566248
0.5230125523012552
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4985,	 Acc1 = 0.3685,	 Acc2 = 0.4217

 ===== Epoch 268	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.9078314   2.254328   -0.40146217 -0.4092239   3.0792155   3.2212293
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.34354803 -0.27925298 -0.00141625  0.0035064  -0.3065381  -0.24875219
 -0.00148575 -0.00146706] 5 5
train:	 Loss = 1.4371,	 Acc = 0.4539
5570 0.234
10878 0.494
5608 0.57
720 0.632
124 0.524
12 0.25
0 0.0
0 0.0
0.5246223042324991
0.5230125523012552
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 3
val:	 Loss = 1.4086,	 Acc = 0.4733
715 0.317
3297 0.489
1558 0.521
160 0.369
4 0.75
0 0.0
0 0.0
0 0.0
0.4955170352659892
0.5230125523012552
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4914,	 Acc1 = 0.3969,	 Acc2 = 0.4222

 ===== Epoch 269	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
train:	 Loss = 1.4368,	 Acc = 0.4546
5569 0.235
10871 0.498
5614 0.567
721 0.624
125 0.48
12 0.25
0 0.0
0 0.0
0.5252263160929481
0.5230125523012552
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4183,	 Acc = 0.4651
715 0.31
3297 0.496
1558 0.479
160 0.375
4 0.75
0 0.0
0 0.0
0 0.0
0.4871488344291692
0.5230125523012552
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4856,	 Acc1 = 0.4056,	 Acc2 = 0.4327

 ===== Epoch 270	 =====
[-0.3669651  -0.38051367  1.7593852   1.5037947  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.15170154 -0.2951739   0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 5 5
train:	 Loss = 1.4353,	 Acc = 0.4536
5568 0.229
10877 0.495
5610 0.574
720 0.631
125 0.488
12 0.167
0 0.0
0 0.0
0.525830258302583
0.5230125523012552
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 3
val:	 Loss = 1.4523,	 Acc = 0.4419
715 0.32
3297 0.463
1558 0.46
160 0.369
4 0.75
0 0.0
0 0.0
0 0.0
0.4592548316397689
0.5230125523012552
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.5150,	 Acc1 = 0.3976,	 Acc2 = 0.4230

 ===== Epoch 271	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239   1.4000932   1.9793248
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00573384  0.15941033
 -0.00148575 -0.00146706] 4 4
train:	 Loss = 1.4337,	 Acc = 0.4552
5569 0.233
10873 0.497
5614 0.573
719 0.62
125 0.512
12 0.167
0 0.0
0 0.0
0.5264371792654097
0.5230125523012552
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4699,	 Acc = 0.4370
715 0.324
3297 0.463
1558 0.442
160 0.344
4 0.75
0 0.0
0 0.0
0 0.0
0.45307830245068736
0.5230125523012552
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 4
Testing:	 Loss = 1.5381,	 Acc1 = 0.3986,	 Acc2 = 0.4242

 ===== Epoch 272	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892  3.3328385   1.4558852  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306   0.55906415  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 0 0
train:	 Loss = 1.4346,	 Acc = 0.4562
5569 0.234
10876 0.498
5609 0.572
721 0.632
125 0.52
12 0.25
0 0.0
0 0.0
0.527532722135732
0.5230125523012552
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4322,	 Acc = 0.4545
715 0.316
3297 0.49
1558 0.454
160 0.35
4 0.75
0 0.0
0 0.0
0 0.0
0.47419804741980476
0.5230125523012552
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4900,	 Acc1 = 0.4110,	 Acc2 = 0.4391

 ===== Epoch 273	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892  5.611344    2.8310924   0.6306686   2.176062
  3.5144281   1.8598357 ] [0.00101099 0.00256308 0.00238751 0.00196144 0.0010308  0.00076088
 0.0015849  0.0032306  0.05387037 0.282952   0.1005139  0.43942884
 0.7304454  0.35524276] 4 4
train:	 Loss = 1.4348,	 Acc = 0.4543
5567 0.236
10877 0.495
5611 0.572
720 0.614
125 0.48
12 0.333
0 0.0
0 0.0
0.5243586047852407
0.5230125523012552
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4351,	 Acc = 0.4540
715 0.316
3297 0.488
1558 0.453
160 0.362
4 0.75
0 0.0
0 0.0
0 0.0
0.4736003187886033
0.5230125523012552
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4919,	 Acc1 = 0.4040,	 Acc2 = 0.4307

 ===== Epoch 274	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239   0.715362    3.1302385
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.07343501 -0.5762314
 -0.00148575 -0.00146706] 5 5
train:	 Loss = 1.4343,	 Acc = 0.4529
5572 0.236
10875 0.489
5609 0.573
719 0.637
125 0.528
12 0.167
0 0.0
0 0.0
0.5226066897347175
0.5230125523012552
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4115,	 Acc = 0.4526
715 0.176
3297 0.495
1558 0.498
160 0.375
4 0.0
0 0.0
0 0.0
0 0.0
0.4919306634787806
0.5230125523012552
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4857,	 Acc1 = 0.3718,	 Acc2 = 0.4257

 ===== Epoch 275	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 0 6
train:	 Loss = 1.4347,	 Acc = 0.4538
5567 0.231
10877 0.494
5610 0.574
721 0.627
125 0.512
12 0.167
0 0.0
0 0.0
0.5252234073219948
0.5230125523012552
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4833,	 Acc = 0.4384
715 0.315
3297 0.478
1558 0.421
160 0.337
4 0.75
0 0.0
0 0.0
0 0.0
0.4560669456066946
0.5230125523012552
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 4
Testing:	 Loss = 1.5237,	 Acc1 = 0.4143,	 Acc2 = 0.4431

 ===== Epoch 276	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239   1.1955436   1.8784969
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064  -0.18213895 -0.08263953
 -0.00148575 -0.00146706] 2 1
train:	 Loss = 1.4338,	 Acc = 0.4558
5565 0.232
10877 0.497
5613 0.574
721 0.632
124 0.524
12 0.167
0 0.0
0 0.0
0.5276416671470572
0.5230125523012552
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4238,	 Acc = 0.4608
715 0.308
3297 0.485
1558 0.489
160 0.362
4 0.75
0 0.0
0 0.0
0 0.0
0.4825662482566248
0.5230125523012552
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4900,	 Acc1 = 0.4025,	 Acc2 = 0.4289

 ===== Epoch 277	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438  3.073956    2.0793564
 -0.36420864 -0.37237892  3.4136822   1.2768348  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.9581211  -0.04307022
  0.0015849   0.0032306   0.03219072 -0.01023683  0.00150104  0.00278983
 -0.00148575 -0.00146706] 2 3
train:	 Loss = 1.4345,	 Acc = 0.4558
5569 0.236
10873 0.496
5612 0.572
721 0.639
125 0.528
12 0.167
0 0.0
0 0.0
0.5262641988122009
0.5230125523012552
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4143,	 Acc = 0.4629
715 0.171
3297 0.516
1558 0.496
160 0.356
4 0.0
0 0.0
0 0.0
0 0.0
0.5044829647340108
0.5230125523012552
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4957,	 Acc1 = 0.3732,	 Acc2 = 0.4274

 ===== Epoch 278	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 3 6
train:	 Loss = 1.4352,	 Acc = 0.4543
5566 0.23
10880 0.494
5608 0.575
721 0.644
125 0.512
12 0.167
0 0.0
0 0.0
0.5261731811368615
0.5230125523012552
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4088,	 Acc = 0.4827
715 0.316
3297 0.506
1558 0.519
160 0.381
4 0.75
0 0.0
0 0.0
0 0.0
0.5064753935046822
0.5230125523012552
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4790,	 Acc1 = 0.4091,	 Acc2 = 0.4369

 ===== Epoch 279	 =====
[-0.3669651  -0.38051367  2.702107    2.1886044  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 1 0
train:	 Loss = 1.4337,	 Acc = 0.4552
5569 0.234
10875 0.493
5611 0.579
721 0.634
124 0.468
12 0.167
0 0.0
0 0.0
0.5262065386611313
0.5230125523012552
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4548,	 Acc = 0.4348
715 0.18
3297 0.487
1558 0.449
160 0.362
4 0.0
0 0.0
0 0.0
0 0.0
0.4710101613867304
0.5230125523012552
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.5283,	 Acc1 = 0.3575,	 Acc2 = 0.4085

 ===== Epoch 280	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 3 6
train:	 Loss = 1.4357,	 Acc = 0.4519
5567 0.236
10876 0.49
5612 0.569
720 0.615
125 0.552
12 0.25
0 0.0
0 0.0
0.5213606226578265
0.5230125523012552
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4422,	 Acc = 0.4458
715 0.32
3297 0.482
1558 0.436
160 0.344
4 0.75
0 0.0
0 0.0
0 0.0
0.46363817493524606
0.5230125523012552
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.5129,	 Acc1 = 0.3943,	 Acc2 = 0.4190

 ===== Epoch 281	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438  0.34231526  2.1814742
  4.844807    1.0149466  -0.40146217 -0.4092239   4.682718    1.3620613
 -0.38672873 -0.38123247] [ 1.0109920e-03  2.5630770e-03  2.3875074e-03  1.9614373e-03
  2.3033788e-02 -3.6020112e-01 -1.1055860e+01 -3.0488200e+00
 -1.4162532e-03  3.5063999e-03 -9.9708533e-01 -5.9996182e-01
 -1.4857454e-03 -1.4670575e-03] 5 5
train:	 Loss = 1.4339,	 Acc = 0.4565
5565 0.237
10879 0.494
5611 0.579
720 0.64
125 0.496
12 0.167
0 0.0
0 0.0
0.5270651985934167
0.5230125523012552
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4327,	 Acc = 0.4578
715 0.322
3297 0.494
1558 0.456
160 0.331
4 0.75
0 0.0
0 0.0
0 0.0
0.4771866905758119
0.5230125523012552
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4985,	 Acc1 = 0.4040,	 Acc2 = 0.4307

 ===== Epoch 282	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 0 1
train:	 Loss = 1.4342,	 Acc = 0.4575
5567 0.238
10873 0.498
5614 0.574
721 0.621
125 0.544
12 0.167
0 0.0
0 0.0
0.5278754684347075
0.5230125523012552
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4364,	 Acc = 0.4601
715 0.31
3297 0.47
1558 0.517
160 0.362
4 0.75
0 0.0
0 0.0
0 0.0
0.481370790994222
0.5230125523012552
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.5078,	 Acc1 = 0.3951,	 Acc2 = 0.4200

 ===== Epoch 283	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239   1.1981899   1.325173
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.32054338  0.05025059
 -0.00148575 -0.00146706] 1 2
train:	 Loss = 1.4353,	 Acc = 0.4571
5569 0.234
10876 0.497
5610 0.578
720 0.637
125 0.488
12 0.083
0 0.0
0 0.0
0.5288012454592631
0.5230125523012552
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4354,	 Acc = 0.4642
715 0.317
3297 0.49
1558 0.486
160 0.362
4 0.75
0 0.0
0 0.0
0 0.0
0.4851564056584977
0.5230125523012552
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.5163,	 Acc1 = 0.3910,	 Acc2 = 0.4150

 ===== Epoch 284	 =====
[-0.3669651  -0.38051367  0.60020775  2.4811227  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.11048216  0.06188789  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 5 4
train:	 Loss = 1.4361,	 Acc = 0.4510
5570 0.232
10874 0.491
5611 0.568
720 0.621
125 0.528
12 0.167
0 0.0
0 0.0
0.5212201591511937
0.5230125523012552
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4470,	 Acc = 0.4578
715 0.317
3297 0.487
1558 0.469
160 0.356
4 0.75
0 0.0
0 0.0
0 0.0
0.47778441920701337
0.5230125523012552
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 4
Testing:	 Loss = 1.5069,	 Acc1 = 0.4015,	 Acc2 = 0.4277

 ===== Epoch 285	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438  1.5299526   2.9695563
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 1.0109920e-03  2.5630770e-03  2.3875074e-03  1.9614373e-03
 -4.7249126e+00 -3.8718457e+00  1.5849024e-03  3.2306046e-03
  3.9903531e+00  2.6971788e+00  1.5010408e-03  2.7898324e-03
 -1.4857454e-03 -1.4670575e-03] 6 1
train:	 Loss = 1.4351,	 Acc = 0.4540
5566 0.233
10878 0.493
5610 0.575
721 0.616
125 0.56
12 0.167
0 0.0
0 0.0
0.5250201775625505
0.5230125523012552
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 3
val:	 Loss = 1.4211,	 Acc = 0.4878
715 0.324
3297 0.5
1558 0.544
160 0.419
4 0.75
0 0.0
0 0.0
0 0.0
0.5110579796772265
0.5230125523012552
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4886,	 Acc1 = 0.4054,	 Acc2 = 0.4324

 ===== Epoch 286	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 1 1
train:	 Loss = 1.4382,	 Acc = 0.4529
5567 0.234
10877 0.491
5610 0.571
721 0.639
125 0.504
12 0.167
0 0.0
0 0.0
0.5230325742288844
0.5230125523012552
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4209,	 Acc = 0.4550
715 0.31
3297 0.487
1558 0.462
160 0.369
4 0.75
0 0.0
0 0.0
0 0.0
0.4755927475592748
0.5230125523012552
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 4
Testing:	 Loss = 1.4817,	 Acc1 = 0.4064,	 Acc2 = 0.4336

 ===== Epoch 287	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 5 1
train:	 Loss = 1.4348,	 Acc = 0.4547
5567 0.239
10874 0.493
5613 0.571
721 0.617
125 0.536
12 0.167
0 0.0
0 0.0
0.5238397232631883
0.5230125523012552
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4201,	 Acc = 0.4522
715 0.183
3297 0.505
1558 0.472
160 0.387
4 0.0
0 0.0
0 0.0
0 0.0
0.4905359633393106
0.5230125523012552
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4858,	 Acc1 = 0.3747,	 Acc2 = 0.4292

 ===== Epoch 288	 =====
[-0.3669651  -0.38051367  1.5177035   2.712416   -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00696649  0.25664887  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 5 6
train:	 Loss = 1.4355,	 Acc = 0.4515
5570 0.226
10873 0.494
5611 0.567
721 0.646
125 0.504
12 0.25
0 0.0
0 0.0
0.5239303425210472
0.5230125523012552
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4397,	 Acc = 0.4480
715 0.31
3297 0.483
1558 0.446
160 0.362
4 0.75
0 0.0
0 0.0
0 0.0
0.467623032476589
0.5230125523012552
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4999,	 Acc1 = 0.4025,	 Acc2 = 0.4289

 ===== Epoch 289	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 3 1
train:	 Loss = 1.4331,	 Acc = 0.4557
5567 0.236
10879 0.496
5610 0.572
719 0.637
125 0.512
12 0.333
0 0.0
0 0.0
0.5262611703660998
0.5230125523012552
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4225,	 Acc = 0.4627
715 0.187
3297 0.507
1558 0.505
160 0.387
4 0.0
0 0.0
0 0.0
0 0.0
0.5018928073321379
0.5230125523012552
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4912,	 Acc1 = 0.3769,	 Acc2 = 0.4319

 ===== Epoch 290	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438  2.259129    1.7086247
 -0.36420864 -0.37237892  0.8754482   2.2765331  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.01844841  0.10131459
  0.0015849   0.0032306   0.08856317  0.2921142   0.00150104  0.00278983
 -0.00148575 -0.00146706] 5 4
train:	 Loss = 1.4349,	 Acc = 0.4527
5568 0.235
10880 0.49
5606 0.573
721 0.635
125 0.528
12 0.167
0 0.0
0 0.0
0.5227167896678967
0.5230125523012552
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4461,	 Acc = 0.4271
715 0.207
3297 0.48
1558 0.427
160 0.331
4 0.0
0 0.0
0 0.0
0 0.0
0.4584578601315003
0.5230125523012552
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 6
Testing:	 Loss = 1.5077,	 Acc1 = 0.3705,	 Acc2 = 0.4304

 ===== Epoch 291	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239   0.77321124  1.426001
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.35693023  0.03601237
 -0.00148575 -0.00146706] 2 4
train:	 Loss = 1.4403,	 Acc = 0.4501
5567 0.224
10877 0.495
5611 0.563
720 0.632
125 0.496
12 0.167
0 0.0
0 0.0
0.5225136927068319
0.5230125523012552
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.3951,	 Acc = 0.4894
715 0.323
3297 0.501
1558 0.552
160 0.375
4 0.75
0 0.0
0 0.0
0 0.0
0.513050408447898
0.5230125523012552
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 4
Testing:	 Loss = 1.4738,	 Acc1 = 0.4196,	 Acc2 = 0.4496

 ===== Epoch 292	 =====
[-0.3669651  -0.38051367  1.5559498   2.385884   -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.15536585 -0.3151494   0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 5 5
train:	 Loss = 1.4357,	 Acc = 0.4512
5568 0.233
10877 0.494
5609 0.564
721 0.616
125 0.456
12 0.083
0 0.0
0 0.0
0.5212177121771218
0.5230125523012552
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4173,	 Acc = 0.4627
715 0.323
3297 0.492
1558 0.474
160 0.356
4 0.75
0 0.0
0 0.0
0 0.0
0.4825662482566248
0.5230125523012552
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4824,	 Acc1 = 0.4058,	 Acc2 = 0.4329

 ===== Epoch 293	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438  0.5514708   2.3901494
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144 -0.19057146  0.09357969
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 1 1
train:	 Loss = 1.4365,	 Acc = 0.4500
5571 0.227
10875 0.491
5608 0.566
721 0.637
125 0.544
12 0.167
0 0.0
0 0.0
0.5217115506602849
0.5230125523012552
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4125,	 Acc = 0.4709
715 0.315
3297 0.495
1558 0.502
160 0.362
4 0.75
0 0.0
0 0.0
0 0.0
0.4931261207411835
0.5230125523012552
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4895,	 Acc1 = 0.3992,	 Acc2 = 0.4250

 ===== Epoch 294	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892  1.2831008   3.032524   -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306   0.04086392  0.07222254  0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
train:	 Loss = 1.4345,	 Acc = 0.4536
5567 0.228
10879 0.497
5609 0.57
720 0.612
125 0.576
12 0.25
0 0.0
0 0.0
0.5259729028538483
0.5230125523012552
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4483,	 Acc = 0.4398
715 0.319
3297 0.473
1558 0.431
160 0.375
4 0.75
0 0.0
0 0.0
0 0.0
0.4570631599920303
0.5230125523012552
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 4
Testing:	 Loss = 1.5089,	 Acc1 = 0.3922,	 Acc2 = 0.4165

 ===== Epoch 295	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 0 1
train:	 Loss = 1.4353,	 Acc = 0.4517
5572 0.224
10871 0.491
5612 0.576
720 0.637
125 0.56
12 0.167
0 0.0
0 0.0
0.5247404844290657
0.5230125523012552
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4201,	 Acc = 0.4625
715 0.316
3297 0.499
1558 0.461
160 0.375
4 0.75
0 0.0
0 0.0
0 0.0
0.4833632197648934
0.5230125523012552
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4833,	 Acc1 = 0.4054,	 Acc2 = 0.4324

 ===== Epoch 296	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  3.4428496   2.7958596  -0.40146217 -0.4092239   3.348798    3.1056461
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.21415502  0.47608352 -0.00141625  0.0035064   0.54226017  0.387222
 -0.00148575 -0.00146706] 6 6
train:	 Loss = 1.4344,	 Acc = 0.4555
5567 0.239
10875 0.494
5612 0.569
721 0.639
125 0.568
12 0.083
0 0.0
0 0.0
0.5248198328048429
0.5230125523012552
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4288,	 Acc = 0.4520
715 0.323
3297 0.487
1558 0.446
160 0.356
4 0.75
0 0.0
0 0.0
0 0.0
0.47041243275552896
0.5230125523012552
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.5046,	 Acc1 = 0.3955,	 Acc2 = 0.4205

 ===== Epoch 297	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
  2.2136152   1.2037854 ] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.39153448 -0.03713804] 1 1
train:	 Loss = 1.4374,	 Acc = 0.4511
5565 0.232
10877 0.491
5612 0.57
721 0.607
125 0.504
12 0.25
0 0.0
0 0.0
0.5213581599123768
0.5230125523012552
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4646,	 Acc = 0.4360
715 0.315
3297 0.476
1558 0.418
160 0.319
4 0.75
0 0.0
0 0.0
0 0.0
0.45327754532775455
0.5230125523012552
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.5294,	 Acc1 = 0.3963,	 Acc2 = 0.4215

 ===== Epoch 298	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892  1.6001651   2.1845212  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306   0.18613331  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 0 0
train:	 Loss = 1.4345,	 Acc = 0.4540
5567 0.233
10880 0.495
5607 0.571
721 0.631
125 0.512
12 0.167
0 0.0
0 0.0
0.5248774863072931
0.5230125523012552
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4157,	 Acc = 0.4595
715 0.315
3297 0.489
1558 0.471
160 0.375
4 0.75
0 0.0
0 0.0
0 0.0
0.4801753337318191
0.5230125523012552
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 6
Testing:	 Loss = 1.4884,	 Acc1 = 0.3941,	 Acc2 = 0.4187

 ===== Epoch 299	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 2 1
train:	 Loss = 1.4333,	 Acc = 0.4539
5568 0.234
10872 0.49
5614 0.576
721 0.645
125 0.536
12 0.167
0 0.0
0 0.0
0.5246194649446494
0.5230125523012552
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4175,	 Acc = 0.4599
715 0.323
3297 0.486
1558 0.478
160 0.356
4 0.75
0 0.0
0 0.0
0 0.0
0.4793783622235505
0.5230125523012552
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 1
Testing:	 Loss = 1.4898,	 Acc1 = 0.4027,	 Acc2 = 0.4292

 ===== Epoch 300	 =====
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239   1.5702364   1.3891127
 -0.38672873 -0.38123247] [ 1.0109920e-03  2.5630770e-03  2.3875074e-03  1.9614373e-03
  1.0307996e-03  7.6088449e-04  1.5849024e-03  3.2306046e-03
 -1.4162532e-03  3.5063999e-03 -1.4236428e-01  9.5200503e-01
 -1.4857454e-03 -1.4670575e-03] 0 6
train:	 Loss = 1.4353,	 Acc = 0.4530
5569 0.226
10874 0.497
5611 0.57
721 0.619
125 0.528
12 0.167
0 0.0
0 0.0
0.5257452574525745
0.5230125523012552
[-0.3669651  -0.38051367 -0.4202629  -0.33521438 -0.4411929  -0.3648088
  2.7384944   1.7658333  -0.40146217 -0.4092239   2.652346    2.9925222
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308  0.00238751  0.00196144  0.0010308   0.00076088
 -0.06290602  0.0032306  -0.00141625  0.0035064   0.02434976  0.05974275
 -0.00148575 -0.00146706] 3 0
val:	 Loss = 1.4246,	 Acc = 0.4616
715 0.323
3297 0.504
1558 0.445
160 0.369
4 0.75
0 0.0
0 0.0
0 0.0
0.481370790994222
0.5230125523012552
[-0.3669651  -0.38051367  1.0774686   2.3382647  -0.4411929  -0.3648088
 -0.36420864 -0.37237892 -0.40146217 -0.4092239  -0.42425194 -0.42087102
 -0.38672873 -0.38123247] [ 0.00101099  0.00256308 -0.04249902  0.22918259  0.0010308   0.00076088
  0.0015849   0.0032306  -0.00141625  0.0035064   0.00150104  0.00278983
 -0.00148575 -0.00146706] 4 4
Testing:	 Loss = 1.4951,	 Acc1 = 0.4044,	 Acc2 = 0.4312
