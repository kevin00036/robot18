(0.24300000000005184, array([-1.000000e+00, -1.000000e+00,  1.189783e+03,  1.790000e-01,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 4, array([-1.000000e+00, -1.000000e+00,  1.173932e+03,  2.700000e-01,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 1)
(0.24, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.543573e+03, -3.290000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.370508e+03,  2.799000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]))
(0.238, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.370508e+03,  2.799000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.356274e+03, -3.410000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]))
(0.236, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.356274e+03, -3.410000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.427118e+03, -3.330000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]))
(0.2350000000000001, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.427118e+03, -3.330000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.245642e+03, -3.340000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]))
(0.248, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.245642e+03, -3.340000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.415149e+03, -3.430000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]))
(0.256, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.415149e+03, -3.430000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.211029e+03, -3.400000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]))
(0.256, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.211029e+03, -3.400000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([-1.000000e+00, -1.000000e+00,  2.398328e+03,  4.980000e-01,
        2.350128e+03, -3.360000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]))
(0.2670000000000001, array([-1.000000e+00, -1.000000e+00,  2.398328e+03,  4.980000e-01,
        2.350128e+03, -3.360000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.383771e+03, -3.390000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]))
(0.2609999999999997, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.383771e+03, -3.390000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.435205e+03, -3.500000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]))
(0.28500000000000014, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.435205e+03, -3.500000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.406091e+03, -3.470000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]))
14 1 14

 ===== Epoch 1	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.5317802   2.0690842
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.5427674   1.9359941
 -0.38555372 -0.3798593 ] 5 1
train:	 Loss = 1.9279,	 Acc = 0.1827,	 Loss Con1 = 0.3956,	 Loss Con2 = 0.3843
2925 0.125
5677 0.195
2914 0.201
377 0.257
69 0.391
6 0.167
0 0.0
0 0.0
0.20115006082052417
0.0
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.9375,	 Acc = 0.2033,	 Loss Con1 = 0.2815,	 Loss Con2 = 0.2684
380 0.15
1718 0.241
815 0.153
85 0.165
2 0.0
0 0.0
0 0.0
0 0.0
0.21106870229007635
0.21106870229007635

 ===== Epoch 2	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.139686    2.4831421
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.1802214   2.690171
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.8933,	 Acc = 0.2370,	 Loss Con1 = 0.3121,	 Loss Con2 = 0.2889
2925 0.136
5679 0.265
2912 0.274
377 0.281
69 0.362
6 0.333
0 0.0
0 0.0
0.2696007961959527
0.21106870229007635
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.8520,	 Acc = 0.3293,	 Loss Con1 = 0.2760,	 Loss Con2 = 0.2485
380 0.195
1718 0.389
815 0.293
85 0.082
2 0.0
0 0.0
0 0.0
0 0.0
0.34885496183206105
0.34885496183206105

 ===== Epoch 3	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.9738816   2.2095683
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.1701183   2.1307
 -0.38555372 -0.3798593 ] 2 5
train:	 Loss = 1.7921,	 Acc = 0.2857,	 Loss Con1 = 0.3114,	 Loss Con2 = 0.2742
2925 0.147
5678 0.334
2914 0.332
376 0.269
69 0.333
6 0.167
0 0.0
0 0.0
0.33053190312949243
0.34885496183206105
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.7737,	 Acc = 0.3460,	 Loss Con1 = 0.2743,	 Loss Con2 = 0.2297
380 0.197
1718 0.413
815 0.301
85 0.094
2 0.0
0 0.0
0 0.0
0 0.0
0.36755725190839694
0.36755725190839694

 ===== Epoch 4	 =====
[ 0.21533065  1.5354013  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 0.21533065  1.5354013  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.7523,	 Acc = 0.3285,	 Loss Con1 = 0.3111,	 Loss Con2 = 0.2656
2924 0.149
5678 0.389
2914 0.391
377 0.342
69 0.319
6 0.167
0 0.0
0 0.0
0.38666519239274655
0.36755725190839694
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.7294,	 Acc = 0.3540,	 Loss Con1 = 0.2735,	 Loss Con2 = 0.2254
380 0.192
1718 0.424
815 0.31
85 0.094
2 0.0
0 0.0
0 0.0
0 0.0
0.37748091603053435
0.37748091603053435

 ===== Epoch 5	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 3 0
train:	 Loss = 1.7275,	 Acc = 0.3444,	 Loss Con1 = 0.3099,	 Loss Con2 = 0.2619
2926 0.161
5677 0.403
2913 0.413
377 0.358
69 0.362
6 0.167
0 0.0
0 0.0
0.40367175403671757
0.37748091603053435
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 0
val:	 Loss = 1.7218,	 Acc = 0.3490,	 Loss Con1 = 0.2709,	 Loss Con2 = 0.2191
380 0.187
1718 0.415
815 0.312
85 0.106
2 0.0
0 0.0
0 0.0
0 0.0
0.37251908396946565
0.37748091603053435

 ===== Epoch 6	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.5567844   2.4067388
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.5575422   2.3229413
 -0.38555372 -0.3798593 ] 5 5
train:	 Loss = 1.7141,	 Acc = 0.3443,	 Loss Con1 = 0.3100,	 Loss Con2 = 0.2592
2922 0.162
5682 0.402
2912 0.414
377 0.363
69 0.304
6 0.167
0 0.0
0 0.0
0.4032721644925934
0.37748091603053435
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6935,	 Acc = 0.3643,	 Loss Con1 = 0.2742,	 Loss Con2 = 0.2216
380 0.192
1718 0.429
815 0.331
85 0.153
2 0.0
0 0.0
0 0.0
0 0.0
0.3893129770992366
0.3893129770992366

 ===== Epoch 7	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  1.1716505   3.2689044  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   2.421242    1.6294265  -0.44088638 -0.36343393
  1.2463571   3.2856467  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 6
train:	 Loss = 1.7008,	 Acc = 0.3533,	 Loss Con1 = 0.3081,	 Loss Con2 = 0.2602
2920 0.162
5682 0.41
2915 0.429
376 0.394
69 0.333
6 0.333
0 0.0
0 0.0
0.41500884173297964
0.3893129770992366
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6951,	 Acc = 0.3627,	 Loss Con1 = 0.2717,	 Loss Con2 = 0.2113
380 0.197
1718 0.424
815 0.336
85 0.118
2 0.0
0 0.0
0 0.0
0 0.0
0.38664122137404583
0.3893129770992366

 ===== Epoch 8	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  0.98057544  1.610663
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.0184594   1.4652498
 -0.38555372 -0.3798593 ] 5 5
train:	 Loss = 1.6889,	 Acc = 0.3510,	 Loss Con1 = 0.3070,	 Loss Con2 = 0.2637
2924 0.158
5680 0.414
2913 0.421
376 0.372
69 0.304
6 0.333
0 0.0
0 0.0
0.4134232640424591
0.3893129770992366
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.6959,	 Acc = 0.3713,	 Loss Con1 = 0.2740,	 Loss Con2 = 0.2198
380 0.203
1718 0.441
815 0.328
85 0.153
2 0.0
0 0.0
0 0.0
0 0.0
0.39580152671755725
0.39580152671755725

 ===== Epoch 9	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 3 0
train:	 Loss = 1.6778,	 Acc = 0.3585,	 Loss Con1 = 0.3084,	 Loss Con2 = 0.2633
2923 0.161
5677 0.418
2917 0.438
376 0.375
69 0.333
6 0.333
0 0.0
0 0.0
0.4222222222222222
0.39580152671755725
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6760,	 Acc = 0.3947,	 Loss Con1 = 0.2718,	 Loss Con2 = 0.2172
380 0.174
1718 0.462
815 0.379
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.4267175572519084
0.4267175572519084

 ===== Epoch 10	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 0
train:	 Loss = 1.6734,	 Acc = 0.3628,	 Loss Con1 = 0.3069,	 Loss Con2 = 0.2626
2922 0.163
5682 0.425
2914 0.439
375 0.408
69 0.29
6 0.167
0 0.0
0 0.0
0.427371213796153
0.4267175572519084
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6567,	 Acc = 0.3960,	 Loss Con1 = 0.2646,	 Loss Con2 = 0.2132
380 0.203
1718 0.46
815 0.374
85 0.176
2 0.0
0 0.0
0 0.0
0 0.0
0.42404580152671756
0.4267175572519084

 ===== Epoch 11	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   0.68171597  2.2399824  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   0.66684127  2.3295684  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 6
train:	 Loss = 1.6686,	 Acc = 0.3604,	 Loss Con1 = 0.3081,	 Loss Con2 = 0.2641
2924 0.16
5678 0.42
2915 0.442
376 0.399
69 0.29
6 0.333
0 0.0
0 0.0
0.4251437417072092
0.4267175572519084
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6567,	 Acc = 0.3840,	 Loss Con1 = 0.2719,	 Loss Con2 = 0.2140
380 0.195
1718 0.449
815 0.368
85 0.082
2 0.0
0 0.0
0 0.0
0 0.0
0.4114503816793893
0.4267175572519084

 ===== Epoch 12	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   1.667032    1.1567692
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  1.7279471   1.0680034
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 5
train:	 Loss = 1.6632,	 Acc = 0.3650,	 Loss Con1 = 0.3073,	 Loss Con2 = 0.2638
2926 0.162
5674 0.425
2916 0.447
377 0.406
69 0.304
6 0.333
0 0.0
0 0.0
0.4305463393054634
0.4267175572519084
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 0
val:	 Loss = 1.6626,	 Acc = 0.3643,	 Loss Con1 = 0.2674,	 Loss Con2 = 0.2176
380 0.195
1718 0.431
815 0.33
85 0.106
2 0.0
0 0.0
0 0.0
0 0.0
0.3889312977099237
0.4267175572519084

 ===== Epoch 13	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   1.6830734   0.94339526
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  1.7737917   0.90574425
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 5
train:	 Loss = 1.6629,	 Acc = 0.3631,	 Loss Con1 = 0.3067,	 Loss Con2 = 0.2661
2923 0.16
5681 0.421
2914 0.448
375 0.413
69 0.377
6 0.333
0 0.0
0 0.0
0.4286346047540077
0.4267175572519084
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.6612,	 Acc = 0.3797,	 Loss Con1 = 0.2650,	 Loss Con2 = 0.2160
380 0.203
1718 0.441
815 0.361
85 0.118
2 0.0
0 0.0
0 0.0
0 0.0
0.40534351145038167
0.4267175572519084

 ===== Epoch 14	 =====
[-0.36602148 -0.3783333   1.6590793   2.8123865  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.6589187   2.8117106  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.6602,	 Acc = 0.3676,	 Loss Con1 = 0.3057,	 Loss Con2 = 0.2654
2925 0.163
5676 0.43
2916 0.451
376 0.391
69 0.29
6 0.333
0 0.0
0 0.0
0.4337056286630543
0.4267175572519084
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.6477,	 Acc = 0.3767,	 Loss Con1 = 0.2684,	 Loss Con2 = 0.2175
380 0.197
1718 0.44
815 0.35
85 0.165
2 0.0
0 0.0
0 0.0
0 0.0
0.4026717557251908
0.4267175572519084

 ===== Epoch 15	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.1223855   0.87620306
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 5
train:	 Loss = 1.6544,	 Acc = 0.3679,	 Loss Con1 = 0.3065,	 Loss Con2 = 0.2672
2925 0.162
5682 0.429
2909 0.453
377 0.406
69 0.304
6 0.333
0 0.0
0 0.0
0.434479708061484
0.4267175572519084
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6441,	 Acc = 0.3747,	 Loss Con1 = 0.2662,	 Loss Con2 = 0.2159
380 0.195
1718 0.445
815 0.337
85 0.118
2 0.0
0 0.0
0 0.0
0 0.0
0.40076335877862596
0.4267175572519084

 ===== Epoch 16	 =====
[-0.36602148 -0.3783333   2.4689496   2.3865075  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   2.8078866   2.5014174  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 6
train:	 Loss = 1.6506,	 Acc = 0.3686,	 Loss Con1 = 0.3063,	 Loss Con2 = 0.2673
2923 0.159
5679 0.43
2914 0.454
377 0.406
69 0.348
6 0.333
0 0.0
0 0.0
0.4362631288004422
0.4267175572519084
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6356,	 Acc = 0.3733,	 Loss Con1 = 0.2632,	 Loss Con2 = 0.2107
380 0.187
1718 0.434
815 0.358
85 0.129
2 0.0
0 0.0
0 0.0
0 0.0
0.40038167938931296
0.4267175572519084

 ===== Epoch 17	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.223913    0.9550713
  2.9917192   3.4228172 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.9780483   0.8564861
  2.7117527   3.3152192 ] 2 5
train:	 Loss = 1.6504,	 Acc = 0.3688,	 Loss Con1 = 0.3060,	 Loss Con2 = 0.2678
2925 0.158
5679 0.431
2912 0.455
377 0.414
69 0.333
6 0.333
0 0.0
0 0.0
0.4370231117991817
0.4267175572519084
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.6451,	 Acc = 0.3710,	 Loss Con1 = 0.2700,	 Loss Con2 = 0.2183
380 0.2
1718 0.439
815 0.328
85 0.176
2 0.0
0 0.0
0 0.0
0 0.0
0.39580152671755725
0.4267175572519084

 ===== Epoch 18	 =====
[ 2.2099035   1.8011979  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  1.4715981   1.1482159  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 3.4008324   1.96827    -0.4209566  -0.33425197 -0.44088638 -0.36343393
  1.4412669   1.3016869  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.6477,	 Acc = 0.3727,	 Loss Con1 = 0.3054,	 Loss Con2 = 0.2678
2925 0.163
5678 0.434
2913 0.459
377 0.403
69 0.377
6 0.333
0 0.0
0 0.0
0.4403405949353091
0.4267175572519084
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.6317,	 Acc = 0.4023,	 Loss Con1 = 0.2656,	 Loss Con2 = 0.2160
380 0.192
1718 0.466
815 0.39
85 0.176
2 0.0
0 0.0
0 0.0
0 0.0
0.43282442748091604
0.43282442748091604

 ===== Epoch 19	 =====
[ 3.7976482   2.1150908  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  1.8254677   2.0802028  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 2.3542416   2.3353221  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  1.7900817   2.2978525  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.6441,	 Acc = 0.3739,	 Loss Con1 = 0.3057,	 Loss Con2 = 0.2666
2916 0.158
5687 0.436
2913 0.461
377 0.43
69 0.406
6 0.333
0 0.0
0 0.0
0.44332744144940345
0.43282442748091604
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6539,	 Acc = 0.3807,	 Loss Con1 = 0.2661,	 Loss Con2 = 0.2170
380 0.189
1718 0.448
815 0.352
85 0.165
2 0.0
0 0.0
0 0.0
0 0.0
0.4083969465648855
0.43282442748091604

 ===== Epoch 20	 =====
[ 1.3291603   1.0291224  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   3.255809    2.9292974  -0.42342317 -0.42019257
  1.6372993   1.1002716 ] [ 1.7624302   1.1531607  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   3.2884223   3.0810962  -0.42342317 -0.42019257
  1.464029    1.2656053 ] 4 6
train:	 Loss = 1.6462,	 Acc = 0.3724,	 Loss Con1 = 0.3063,	 Loss Con2 = 0.2697
2926 0.164
5676 0.431
2914 0.465
377 0.393
69 0.42
6 0.333
0 0.0
0 0.0
0.43994691439946915
0.43282442748091604
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6383,	 Acc = 0.3923,	 Loss Con1 = 0.2662,	 Loss Con2 = 0.2157
380 0.187
1718 0.46
815 0.366
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.4221374045801527
0.43282442748091604

 ===== Epoch 21	 =====
[-0.36602148 -0.3783333   3.3665488  -4.2350025   2.4019055   1.1056483
 -0.3640846  -0.3725409   1.109683    0.9310298  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   3.3687637   2.9136314   2.335761    1.1835854
 -0.3640846  -0.3725409   1.00212     1.0231044  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 2
train:	 Loss = 1.6429,	 Acc = 0.3690,	 Loss Con1 = 0.3067,	 Loss Con2 = 0.2696
2922 0.159
5680 0.431
2914 0.456
377 0.393
69 0.406
6 0.167
0 0.0
0 0.0
0.43687817820030955
0.43282442748091604
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6182,	 Acc = 0.3933,	 Loss Con1 = 0.2658,	 Loss Con2 = 0.2236
380 0.203
1718 0.455
815 0.378
85 0.153
2 0.0
0 0.0
0 0.0
0 0.0
0.4209923664122137
0.43282442748091604

 ===== Epoch 22	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 0
train:	 Loss = 1.6389,	 Acc = 0.3725,	 Loss Con1 = 0.3059,	 Loss Con2 = 0.2698
2921 0.161
5680 0.435
2915 0.458
377 0.403
69 0.362
6 0.333
0 0.0
0 0.0
0.44069857411296565
0.43282442748091604
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6224,	 Acc = 0.4000,	 Loss Con1 = 0.2685,	 Loss Con2 = 0.2212
380 0.187
1718 0.466
815 0.385
85 0.165
2 0.0
0 0.0
0 0.0
0 0.0
0.4309160305343511
0.43282442748091604

 ===== Epoch 23	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.580272    2.5003946
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.5829234   2.5348995
 -0.38555372 -0.3798593 ] 3 0
train:	 Loss = 1.6418,	 Acc = 0.3756,	 Loss Con1 = 0.3056,	 Loss Con2 = 0.2683
2923 0.16
5681 0.437
2915 0.467
374 0.404
69 0.406
6 0.333
0 0.0
0 0.0
0.4451077943615257
0.43282442748091604
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6346,	 Acc = 0.3873,	 Loss Con1 = 0.2664,	 Loss Con2 = 0.2187
380 0.184
1718 0.462
815 0.352
85 0.141
2 0.0
0 0.0
0 0.0
0 0.0
0.416793893129771
0.43282442748091604

 ===== Epoch 24	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   1.2647355   3.0014641  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   1.2647355   2.9989755  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.6385,	 Acc = 0.3744,	 Loss Con1 = 0.3070,	 Loss Con2 = 0.2694
2924 0.16
5680 0.437
2912 0.46
377 0.419
69 0.406
6 0.333
0 0.0
0 0.0
0.44360902255639095
0.43282442748091604
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6294,	 Acc = 0.3880,	 Loss Con1 = 0.2679,	 Loss Con2 = 0.2213
380 0.184
1718 0.459
815 0.358
85 0.165
2 0.0
0 0.0
0 0.0
0 0.0
0.41755725190839693
0.43282442748091604

 ===== Epoch 25	 =====
[ 2.2798414   2.2492547  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.9922165   2.4833016 ] [ 2.3941865   2.3935442  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.9044802   2.6565084 ] 4 6
train:	 Loss = 1.6365,	 Acc = 0.3748,	 Loss Con1 = 0.3054,	 Loss Con2 = 0.2676
2923 0.164
5678 0.439
2915 0.456
377 0.403
69 0.406
6 0.333
0 0.0
0 0.0
0.44289662797125484
0.43282442748091604
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6201,	 Acc = 0.3873,	 Loss Con1 = 0.2675,	 Loss Con2 = 0.2203
380 0.192
1718 0.441
815 0.39
85 0.165
2 0.0
0 0.0
0 0.0
0 0.0
0.41564885496183207
0.43282442748091604

 ===== Epoch 26	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.055705    2.6040213 ] [ 2.5821667   2.3555734  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.2263312   2.6013973 ] 3 5
train:	 Loss = 1.6367,	 Acc = 0.3736,	 Loss Con1 = 0.3056,	 Loss Con2 = 0.2685
2923 0.161
5681 0.434
2912 0.461
377 0.424
69 0.391
6 0.333
0 0.0
0 0.0
0.44212271973466005
0.43282442748091604
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6270,	 Acc = 0.3947,	 Loss Con1 = 0.2684,	 Loss Con2 = 0.2240
380 0.197
1718 0.464
815 0.368
85 0.129
2 0.0
0 0.0
0 0.0
0 0.0
0.4232824427480916
0.43282442748091604

 ===== Epoch 27	 =====
[-0.36602148 -0.3783333   3.0205052   2.2460582  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   2.9431307   2.0710843  -0.44088638 -0.36343393
  2.1214843   3.7348979  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 2
train:	 Loss = 1.6347,	 Acc = 0.3784,	 Loss Con1 = 0.3061,	 Loss Con2 = 0.2704
2923 0.16
5680 0.442
2913 0.467
377 0.427
69 0.42
6 0.333
0 0.0
0 0.0
0.4488667772249862
0.43282442748091604
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6194,	 Acc = 0.4037,	 Loss Con1 = 0.2651,	 Loss Con2 = 0.2176
380 0.171
1718 0.463
815 0.41
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.43740458015267175
0.43740458015267175

 ===== Epoch 28	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   1.4483947   2.9566712  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   1.3082187   2.9591594  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.6324,	 Acc = 0.3790,	 Loss Con1 = 0.3056,	 Loss Con2 = 0.2682
2923 0.162
5678 0.443
2916 0.467
377 0.416
68 0.382
6 0.333
0 0.0
0 0.0
0.44919845218352683
0.43740458015267175
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6241,	 Acc = 0.3843,	 Loss Con1 = 0.2703,	 Loss Con2 = 0.2257
380 0.192
1718 0.445
815 0.369
85 0.165
2 0.0
0 0.0
0 0.0
0 0.0
0.4122137404580153
0.43740458015267175

 ===== Epoch 29	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 3 0
train:	 Loss = 1.6350,	 Acc = 0.3753,	 Loss Con1 = 0.3066,	 Loss Con2 = 0.2700
2925 0.163
5677 0.436
2914 0.463
377 0.43
69 0.362
6 0.333
0 0.0
0 0.0
0.44387924361384495
0.43740458015267175
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6242,	 Acc = 0.3837,	 Loss Con1 = 0.2658,	 Loss Con2 = 0.2238
380 0.179
1718 0.451
815 0.362
85 0.153
2 0.0
0 0.0
0 0.0
0 0.0
0.4133587786259542
0.43740458015267175

 ===== Epoch 30	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 0
train:	 Loss = 1.6314,	 Acc = 0.3811,	 Loss Con1 = 0.3060,	 Loss Con2 = 0.2708
2922 0.159
5682 0.447
2912 0.475
377 0.39
69 0.377
6 0.333
0 0.0
0 0.0
0.4527968162723856
0.43740458015267175
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6248,	 Acc = 0.3870,	 Loss Con1 = 0.2683,	 Loss Con2 = 0.2274
380 0.205
1718 0.453
815 0.357
85 0.153
2 0.0
0 0.0
0 0.0
0 0.0
0.4133587786259542
0.43740458015267175

 ===== Epoch 31	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   1.1685851   2.7059522
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  1.1904275   2.7772818
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 3 6
train:	 Loss = 1.6351,	 Acc = 0.3782,	 Loss Con1 = 0.3057,	 Loss Con2 = 0.2704
2925 0.16
5682 0.445
2911 0.467
375 0.387
69 0.348
6 0.167
0 0.0
0 0.0
0.4487448855468318
0.43740458015267175
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6235,	 Acc = 0.3947,	 Loss Con1 = 0.2680,	 Loss Con2 = 0.2258
380 0.2
1718 0.458
815 0.375
85 0.176
2 0.0
0 0.0
0 0.0
0 0.0
0.42290076335877863
0.43740458015267175

 ===== Epoch 32	 =====
[-0.36602148 -0.3783333   1.9336393   1.4373413  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.955807    1.230802   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 5
train:	 Loss = 1.6320,	 Acc = 0.3792,	 Loss Con1 = 0.3062,	 Loss Con2 = 0.2694
2926 0.159
5682 0.443
2910 0.47
376 0.415
68 0.441
6 0.0
0 0.0
0 0.0
0.45045343950453437
0.43740458015267175
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.6134,	 Acc = 0.3873,	 Loss Con1 = 0.2656,	 Loss Con2 = 0.2236
380 0.179
1718 0.451
815 0.373
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.41755725190839693
0.43740458015267175

 ===== Epoch 33	 =====
[ 0.5682464   1.9885211  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 0.6689661   1.8341061  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 5
train:	 Loss = 1.6298,	 Acc = 0.3784,	 Loss Con1 = 0.3068,	 Loss Con2 = 0.2694
2925 0.165
5677 0.441
2914 0.467
377 0.406
69 0.391
6 0.0
0 0.0
0 0.0
0.4474178922923808
0.43740458015267175
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.6279,	 Acc = 0.3750,	 Loss Con1 = 0.2644,	 Loss Con2 = 0.2243
380 0.179
1718 0.431
815 0.371
85 0.176
2 0.0
0 0.0
0 0.0
0 0.0
0.4034351145038168
0.43740458015267175

 ===== Epoch 34	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   1.8162863   0.8290012  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   1.7052884   1.055455   -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.6313,	 Acc = 0.3819,	 Loss Con1 = 0.3054,	 Loss Con2 = 0.2699
2922 0.164
5683 0.44
2911 0.479
377 0.43
69 0.449
6 0.333
0 0.0
0 0.0
0.45224408578377184
0.43740458015267175
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6136,	 Acc = 0.3840,	 Loss Con1 = 0.2653,	 Loss Con2 = 0.2199
380 0.205
1718 0.445
815 0.36
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.40992366412213743
0.43740458015267175

 ===== Epoch 35	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   2.0537262   1.0255929  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   2.111513    1.08034    -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 2
train:	 Loss = 1.6288,	 Acc = 0.3807,	 Loss Con1 = 0.3048,	 Loss Con2 = 0.2685
2925 0.158
5679 0.444
2913 0.476
376 0.415
69 0.377
6 0.333
0 0.0
0 0.0
0.4526152825389804
0.43740458015267175
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6209,	 Acc = 0.3903,	 Loss Con1 = 0.2670,	 Loss Con2 = 0.2189
380 0.189
1718 0.453
815 0.375
85 0.165
2 0.0
0 0.0
0 0.0
0 0.0
0.41946564885496185
0.43740458015267175

 ===== Epoch 36	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.7842872   1.3016869  -0.40141198 -0.40778467  1.5867119   3.2767532
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  3.0151455   1.0756661  -0.40141198 -0.40778467  1.5272352   3.119017
 -0.38555372 -0.3798593 ] 5 5
train:	 Loss = 1.6291,	 Acc = 0.3795,	 Loss Con1 = 0.3056,	 Loss Con2 = 0.2673
2926 0.16
5679 0.444
2914 0.472
374 0.396
69 0.406
6 0.167
0 0.0
0 0.0
0.4506746295067463
0.43740458015267175
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6166,	 Acc = 0.4013,	 Loss Con1 = 0.2700,	 Loss Con2 = 0.2305
380 0.203
1718 0.467
815 0.38
85 0.165
2 0.0
0 0.0
0 0.0
0 0.0
0.4301526717557252
0.43740458015267175

 ===== Epoch 37	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.8196955   2.0296502
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.8231046   1.810298
 -0.38555372 -0.3798593 ] 5 5
train:	 Loss = 1.6265,	 Acc = 0.3808,	 Loss Con1 = 0.3068,	 Loss Con2 = 0.2702
2922 0.16
5683 0.441
2915 0.479
373 0.416
69 0.406
6 0.167
0 0.0
0 0.0
0.4521335396860491
0.43740458015267175
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6260,	 Acc = 0.3917,	 Loss Con1 = 0.2670,	 Loss Con2 = 0.2249
380 0.192
1718 0.451
815 0.382
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.4206106870229008
0.43740458015267175

 ===== Epoch 38	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   0.47075936  2.474797
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  0.44202533  2.390527
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 5
train:	 Loss = 1.6285,	 Acc = 0.3806,	 Loss Con1 = 0.3056,	 Loss Con2 = 0.2708
2927 0.16
5682 0.443
2910 0.474
374 0.42
69 0.449
6 0.333
0 0.0
0 0.0
0.4521623714190908
0.43740458015267175
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6126,	 Acc = 0.3983,	 Loss Con1 = 0.2655,	 Loss Con2 = 0.2271
380 0.192
1718 0.459
815 0.394
85 0.153
2 0.0
0 0.0
0 0.0
0 0.0
0.42824427480916033
0.43740458015267175

 ===== Epoch 39	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   1.7287468   3.302573   -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.6280,	 Acc = 0.3805,	 Loss Con1 = 0.3066,	 Loss Con2 = 0.2723
2924 0.161
5681 0.447
2914 0.469
374 0.401
69 0.406
6 0.167
0 0.0
0 0.0
0.4515701017249005
0.43740458015267175
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6365,	 Acc = 0.3853,	 Loss Con1 = 0.2673,	 Loss Con2 = 0.2324
380 0.184
1718 0.449
815 0.368
85 0.176
2 0.0
0 0.0
0 0.0
0 0.0
0.41450381679389314
0.43740458015267175

 ===== Epoch 40	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 3 0
train:	 Loss = 1.6304,	 Acc = 0.3758,	 Loss Con1 = 0.3047,	 Loss Con2 = 0.2701
2923 0.167
5679 0.435
2916 0.467
375 0.403
69 0.406
6 0.167
0 0.0
0 0.0
0.443338861249309
0.43740458015267175
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6310,	 Acc = 0.3947,	 Loss Con1 = 0.2720,	 Loss Con2 = 0.2302
380 0.187
1718 0.46
815 0.379
85 0.153
2 0.0
0 0.0
0 0.0
0 0.0
0.4248091603053435
0.43740458015267175

 ===== Epoch 41	 =====
[ 3.5227978   1.6518456  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.478521    1.840337  ] [ 2.6352446   1.6467828  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.612993    1.8219665 ] 3 0
train:	 Loss = 1.6264,	 Acc = 0.3808,	 Loss Con1 = 0.3057,	 Loss Con2 = 0.2701
2919 0.161
5688 0.442
2912 0.477
374 0.417
69 0.406
6 0.167
0 0.0
0 0.0
0.4518731351530556
0.43740458015267175
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6139,	 Acc = 0.3930,	 Loss Con1 = 0.2700,	 Loss Con2 = 0.2346
380 0.192
1718 0.457
815 0.378
85 0.153
2 0.0
0 0.0
0 0.0
0 0.0
0.4221374045801527
0.43740458015267175

 ===== Epoch 42	 =====
[-0.36602148 -0.3783333   1.5063654   1.2108102  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.5049924   1.0314898  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 5
train:	 Loss = 1.6270,	 Acc = 0.3804,	 Loss Con1 = 0.3056,	 Loss Con2 = 0.2712
2926 0.16
5681 0.443
2912 0.475
374 0.42
69 0.42
6 0.333
0 0.0
0 0.0
0.4516699845166998
0.43740458015267175
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6211,	 Acc = 0.3863,	 Loss Con1 = 0.2639,	 Loss Con2 = 0.2237
380 0.192
1718 0.444
815 0.382
85 0.153
2 0.0
0 0.0
0 0.0
0 0.0
0.41450381679389314
0.43740458015267175

 ===== Epoch 43	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.6548774   2.7247412 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.6209292   2.7536092 ] 4 0
train:	 Loss = 1.6282,	 Acc = 0.3805,	 Loss Con1 = 0.3057,	 Loss Con2 = 0.2719
2921 0.164
5681 0.442
2914 0.472
377 0.422
69 0.42
6 0.167
0 0.0
0 0.0
0.45053608931137396
0.43740458015267175
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6145,	 Acc = 0.3890,	 Loss Con1 = 0.2675,	 Loss Con2 = 0.2317
380 0.197
1718 0.451
815 0.373
85 0.153
2 0.0
0 0.0
0 0.0
0 0.0
0.416793893129771
0.43740458015267175

 ===== Epoch 44	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  1.1840085   3.8604653  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 6
train:	 Loss = 1.6246,	 Acc = 0.3802,	 Loss Con1 = 0.3063,	 Loss Con2 = 0.2709
2926 0.159
5676 0.444
2914 0.474
377 0.401
69 0.449
6 0.333
0 0.0
0 0.0
0.4516699845166998
0.43740458015267175
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6237,	 Acc = 0.3847,	 Loss Con1 = 0.2663,	 Loss Con2 = 0.2284
380 0.197
1718 0.448
815 0.36
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.4118320610687023
0.43740458015267175

 ===== Epoch 45	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 0
train:	 Loss = 1.6250,	 Acc = 0.3838,	 Loss Con1 = 0.3056,	 Loss Con2 = 0.2707
2926 0.161
5683 0.451
2911 0.473
373 0.416
69 0.391
6 0.333
0 0.0
0 0.0
0.4558725945587259
0.43740458015267175
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6397,	 Acc = 0.3933,	 Loss Con1 = 0.2699,	 Loss Con2 = 0.2294
380 0.197
1718 0.46
815 0.367
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.4217557251908397
0.43740458015267175

 ===== Epoch 46	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.0420375   2.6853762 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.9644413   2.7667308 ] 3 4
train:	 Loss = 1.6235,	 Acc = 0.3820,	 Loss Con1 = 0.3050,	 Loss Con2 = 0.2695
2924 0.169
5678 0.438
2915 0.482
376 0.41
69 0.406
6 0.167
0 0.0
0 0.0
0.45079610791685093
0.43740458015267175
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6266,	 Acc = 0.3907,	 Loss Con1 = 0.2685,	 Loss Con2 = 0.2255
380 0.189
1718 0.456
815 0.369
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.4198473282442748
0.43740458015267175

 ===== Epoch 47	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 0
train:	 Loss = 1.6239,	 Acc = 0.3849,	 Loss Con1 = 0.3066,	 Loss Con2 = 0.2734
2926 0.163
5681 0.447
2911 0.484
375 0.4
69 0.449
6 0.167
0 0.0
0 0.0
0.45675735456757355
0.43740458015267175
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6284,	 Acc = 0.3860,	 Loss Con1 = 0.2667,	 Loss Con2 = 0.2242
380 0.203
1718 0.448
815 0.364
85 0.165
2 0.0
0 0.0
0 0.0
0 0.0
0.4125954198473282
0.43740458015267175

 ===== Epoch 48	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.18262     2.0419734
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.4137096   2.0321147
 -0.38555372 -0.3798593 ] 4 2
train:	 Loss = 1.6209,	 Acc = 0.3857,	 Loss Con1 = 0.3061,	 Loss Con2 = 0.2713
2919 0.161
5684 0.448
2913 0.483
377 0.424
69 0.464
6 0.167
0 0.0
0 0.0
0.458282683169411
0.43740458015267175
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6180,	 Acc = 0.3887,	 Loss Con1 = 0.2686,	 Loss Con2 = 0.2308
380 0.203
1718 0.452
815 0.363
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.41564885496183207
0.43740458015267175

 ===== Epoch 49	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 0
train:	 Loss = 1.6234,	 Acc = 0.3802,	 Loss Con1 = 0.3054,	 Loss Con2 = 0.2728
2921 0.161
5682 0.444
2916 0.473
374 0.388
69 0.478
6 0.167
0 0.0
0 0.0
0.45086769094727536
0.43740458015267175
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6244,	 Acc = 0.3953,	 Loss Con1 = 0.2700,	 Loss Con2 = 0.2393
380 0.197
1718 0.457
815 0.378
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.42404580152671756
0.43740458015267175

 ===== Epoch 50	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  4.0818124   1.265412   -0.40141198 -0.40778467  2.7357194   2.4609606
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  3.836349    1.2123947  -0.40141198 -0.40778467  2.449699    2.4018095
 -0.38555372 -0.3798593 ] 2 5
train:	 Loss = 1.6216,	 Acc = 0.3854,	 Loss Con1 = 0.3066,	 Loss Con2 = 0.2711
2924 0.161
5678 0.448
2914 0.484
377 0.419
69 0.406
6 0.167
0 0.0
0 0.0
0.4579831932773109
0.43740458015267175
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.6107,	 Acc = 0.3990,	 Loss Con1 = 0.2678,	 Loss Con2 = 0.2272
380 0.2
1718 0.46
815 0.384
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.42786259541984734
0.43740458015267175

 ===== Epoch 51	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.8898873   1.03102    -0.40141198 -0.40778467  1.9488786   2.8380492
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.8168669   1.1482159  -0.40141198 -0.40778467  1.9219807   2.8922708
 -0.38555372 -0.3798593 ] 2 4
train:	 Loss = 1.6193,	 Acc = 0.3832,	 Loss Con1 = 0.3063,	 Loss Con2 = 0.2724
2923 0.159
5681 0.445
2913 0.483
376 0.412
69 0.42
6 0.5
0 0.0
0 0.0
0.45550027639579876
0.43740458015267175
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6247,	 Acc = 0.3880,	 Loss Con1 = 0.2716,	 Loss Con2 = 0.2403
380 0.197
1718 0.453
815 0.363
85 0.165
2 0.0
0 0.0
0 0.0
0 0.0
0.41564885496183207
0.43740458015267175

 ===== Epoch 52	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 0
train:	 Loss = 1.6187,	 Acc = 0.3857,	 Loss Con1 = 0.3055,	 Loss Con2 = 0.2719
2922 0.161
5685 0.45
2910 0.486
376 0.399
69 0.333
6 0.333
0 0.0
0 0.0
0.4583241211585231
0.43740458015267175
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6242,	 Acc = 0.3910,	 Loss Con1 = 0.2692,	 Loss Con2 = 0.2385
380 0.203
1718 0.451
815 0.378
85 0.165
2 0.0
0 0.0
0 0.0
0 0.0
0.4183206106870229
0.43740458015267175

 ===== Epoch 53	 =====
[-0.36602148 -0.3783333   1.5790668   1.8564243  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.4859036   1.9351896  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 2
train:	 Loss = 1.6204,	 Acc = 0.3868,	 Loss Con1 = 0.3068,	 Loss Con2 = 0.2752
2928 0.163
5678 0.453
2911 0.481
376 0.396
69 0.406
6 0.167
0 0.0
0 0.0
0.4594026548672566
0.43740458015267175
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6072,	 Acc = 0.4000,	 Loss Con1 = 0.2657,	 Loss Con2 = 0.2302
380 0.197
1718 0.459
815 0.394
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.42938931297709926
0.43740458015267175

 ===== Epoch 54	 =====
[ 2.8267894   3.2845953  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.611671    3.5382886 ] [ 2.8156004   3.2845953  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.611671    3.5382886 ] 0 0
train:	 Loss = 1.6214,	 Acc = 0.3832,	 Loss Con1 = 0.3041,	 Loss Con2 = 0.2702
2926 0.162
5674 0.443
2919 0.486
375 0.4
68 0.368
6 0.333
0 0.0
0 0.0
0.45465604954656047
0.43740458015267175
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6048,	 Acc = 0.3883,	 Loss Con1 = 0.2713,	 Loss Con2 = 0.2431
380 0.192
1718 0.46
815 0.353
85 0.165
2 0.0
0 0.0
0 0.0
0 0.0
0.416793893129771
0.43740458015267175

 ===== Epoch 55	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 0
train:	 Loss = 1.6208,	 Acc = 0.3824,	 Loss Con1 = 0.3065,	 Loss Con2 = 0.2735
2927 0.161
5680 0.446
2909 0.48
377 0.398
69 0.391
6 0.333
0 0.0
0 0.0
0.45404269439221323
0.43740458015267175
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6037,	 Acc = 0.3933,	 Loss Con1 = 0.2673,	 Loss Con2 = 0.2371
380 0.192
1718 0.456
815 0.38
85 0.153
2 0.0
0 0.0
0 0.0
0 0.0
0.42251908396946564
0.43740458015267175

 ===== Epoch 56	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.390536    1.0840373  -0.40141198 -0.40778467  1.7151372   3.1559863
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.4012096   1.1426352  -0.40141198 -0.40778467  1.7988592   3.1855621
 -0.38555372 -0.3798593 ] 2 4
train:	 Loss = 1.6183,	 Acc = 0.3833,	 Loss Con1 = 0.3058,	 Loss Con2 = 0.2738
2923 0.16
5678 0.447
2916 0.483
376 0.386
69 0.362
6 0.167
0 0.0
0 0.0
0.45538971807628525
0.43740458015267175
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6129,	 Acc = 0.3930,	 Loss Con1 = 0.2678,	 Loss Con2 = 0.2261
380 0.195
1718 0.456
815 0.375
85 0.176
2 0.0
0 0.0
0 0.0
0 0.0
0.4217557251908397
0.43740458015267175

 ===== Epoch 57	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  5.4377527   1.6979207   3.940099    3.123401    5.3095274   1.7905811
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  4.8709984   1.7035015   4.691901    3.123401    4.548826    1.7856517
  5.6282525   3.1682558 ] 1 2
train:	 Loss = 1.6189,	 Acc = 0.3844,	 Loss Con1 = 0.3057,	 Loss Con2 = 0.2730
2920 0.163
5682 0.444
2915 0.488
376 0.394
69 0.435
6 0.167
0 0.0
0 0.0
0.4557913351016799
0.43740458015267175
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6033,	 Acc = 0.4007,	 Loss Con1 = 0.2617,	 Loss Con2 = 0.2217
380 0.197
1718 0.456
815 0.401
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.4301526717557252
0.43740458015267175

 ===== Epoch 58	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.1920905   1.8226211
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.4284844   1.76347
 -0.38555372 -0.3798593 ] 3 3
train:	 Loss = 1.6146,	 Acc = 0.3839,	 Loss Con1 = 0.3050,	 Loss Con2 = 0.2717
2925 0.161
5681 0.445
2914 0.484
374 0.42
68 0.397
6 0.167
0 0.0
0 0.0
0.4559327656751078
0.43740458015267175
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6202,	 Acc = 0.3853,	 Loss Con1 = 0.2681,	 Loss Con2 = 0.2321
380 0.187
1718 0.455
815 0.355
85 0.176
2 0.0
0 0.0
0 0.0
0 0.0
0.41412213740458015
0.43740458015267175

 ===== Epoch 59	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  0.56006795  1.7092482
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  0.48164868  1.7683994
 -0.38555372 -0.3798593 ] 1 6
train:	 Loss = 1.6166,	 Acc = 0.3831,	 Loss Con1 = 0.3062,	 Loss Con2 = 0.2722
2923 0.16
5679 0.443
2915 0.485
376 0.418
69 0.406
6 0.333
0 0.0
0 0.0
0.4552791597567717
0.43740458015267175
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5959,	 Acc = 0.4107,	 Loss Con1 = 0.2664,	 Loss Con2 = 0.2357
380 0.197
1718 0.467
815 0.413
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.4416030534351145
0.4416030534351145

 ===== Epoch 60	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   1.6376212   0.93228203
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  1.4326358   0.8346169
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 5
train:	 Loss = 1.6166,	 Acc = 0.3877,	 Loss Con1 = 0.3048,	 Loss Con2 = 0.2708
2925 0.162
5681 0.452
2910 0.489
377 0.393
69 0.362
6 0.167
0 0.0
0 0.0
0.46079840760809465
0.4416030534351145
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6058,	 Acc = 0.3923,	 Loss Con1 = 0.2710,	 Loss Con2 = 0.2356
380 0.205
1718 0.46
815 0.361
85 0.176
2 0.0
0 0.0
0 0.0
0 0.0
0.41946564885496185
0.4416030534351145

 ===== Epoch 61	 =====
[ 0.11592446  2.2922885  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 0.11627328  2.365699   -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 1
train:	 Loss = 1.6141,	 Acc = 0.3854,	 Loss Con1 = 0.3061,	 Loss Con2 = 0.2724
2921 0.161
5681 0.447
2915 0.487
376 0.412
69 0.377
6 0.167
0 0.0
0 0.0
0.4580523930584724
0.4416030534351145
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6001,	 Acc = 0.4057,	 Loss Con1 = 0.2647,	 Loss Con2 = 0.2304
380 0.2
1718 0.466
815 0.396
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.4354961832061069
0.4416030534351145

 ===== Epoch 62	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   1.3259486   1.0234104
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  1.2259563   1.2480444
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.6116,	 Acc = 0.3868,	 Loss Con1 = 0.3046,	 Loss Con2 = 0.2732
2918 0.162
5685 0.448
2915 0.485
375 0.437
69 0.391
6 0.333
0 0.0
0 0.0
0.4591160220994475
0.4416030534351145
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6207,	 Acc = 0.3857,	 Loss Con1 = 0.2638,	 Loss Con2 = 0.2407
380 0.184
1718 0.449
815 0.371
85 0.165
2 0.0
0 0.0
0 0.0
0 0.0
0.4148854961832061
0.4416030534351145

 ===== Epoch 63	 =====
[ 4.264669    3.1301804  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  1.2193961   2.861509   -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  1.3176931   2.9563818  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 2
train:	 Loss = 1.6135,	 Acc = 0.3863,	 Loss Con1 = 0.3058,	 Loss Con2 = 0.2733
2920 0.161
5678 0.448
2918 0.491
377 0.416
69 0.304
6 0.167
0 0.0
0 0.0
0.45910698496905394
0.4416030534351145
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5976,	 Acc = 0.3980,	 Loss Con1 = 0.2714,	 Loss Con2 = 0.2351
380 0.179
1718 0.457
815 0.4
85 0.176
2 0.0
0 0.0
0 0.0
0 0.0
0.4297709923664122
0.4416030534351145

 ===== Epoch 64	 =====
[ 2.0214527   3.3883827  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.9587088   3.559283  ] 3 2
train:	 Loss = 1.6118,	 Acc = 0.3895,	 Loss Con1 = 0.3057,	 Loss Con2 = 0.2710
2922 0.16
5684 0.451
2911 0.495
377 0.424
69 0.377
5 0.4
0 0.0
0 0.0
0.4635197877514924
0.4416030534351145
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6070,	 Acc = 0.3910,	 Loss Con1 = 0.2646,	 Loss Con2 = 0.2316
380 0.213
1718 0.454
815 0.367
85 0.153
2 0.0
0 0.0
0 0.0
0 0.0
0.416793893129771
0.4416030534351145

 ===== Epoch 65	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 0
train:	 Loss = 1.6144,	 Acc = 0.3834,	 Loss Con1 = 0.3053,	 Loss Con2 = 0.2736
2924 0.158
5679 0.446
2915 0.488
376 0.396
68 0.338
6 0.333
0 0.0
0 0.0
0.4562140645731977
0.4416030534351145
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.6193,	 Acc = 0.3873,	 Loss Con1 = 0.2662,	 Loss Con2 = 0.2286
380 0.187
1718 0.453
815 0.364
85 0.176
2 0.0
0 0.0
0 0.0
0 0.0
0.416412213740458
0.4416030534351145

 ===== Epoch 66	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  0.30814293  2.6532016
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  0.31306744  2.717282
 -0.38555372 -0.3798593 ] 1 6
train:	 Loss = 1.6180,	 Acc = 0.3850,	 Loss Con1 = 0.3067,	 Loss Con2 = 0.2775
2926 0.159
5674 0.447
2917 0.49
376 0.399
69 0.377
6 0.333
0 0.0
0 0.0
0.45830568458305687
0.4416030534351145
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6039,	 Acc = 0.3977,	 Loss Con1 = 0.2651,	 Loss Con2 = 0.2288
380 0.189
1718 0.462
815 0.378
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.42786259541984734
0.4416030534351145

 ===== Epoch 67	 =====
[-0.36602148 -0.3783333   0.7801638   0.8257068  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   0.7914054   0.8706811  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 4
train:	 Loss = 1.6109,	 Acc = 0.3867,	 Loss Con1 = 0.3054,	 Loss Con2 = 0.2738
2921 0.163
5682 0.446
2914 0.491
376 0.418
69 0.362
6 0.333
0 0.0
0 0.0
0.45893666408754286
0.4416030534351145
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.5969,	 Acc = 0.3927,	 Loss Con1 = 0.2676,	 Loss Con2 = 0.2315
380 0.174
1718 0.456
815 0.385
85 0.165
2 0.0
0 0.0
0 0.0
0 0.0
0.42442748091603055
0.4416030534351145

 ===== Epoch 68	 =====
[-0.36602148 -0.3783333   3.5070775   2.5178957   3.7421734   1.1034256
  2.8179903   3.5702655  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   3.4885752   2.5580406   3.6644714   1.1569124
  2.6517274   3.612121   -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 3 4
train:	 Loss = 1.6095,	 Acc = 0.3892,	 Loss Con1 = 0.3047,	 Loss Con2 = 0.2731
2925 0.162
5680 0.449
2913 0.502
376 0.391
68 0.338
6 0.5
0 0.0
0 0.0
0.4625677319473626
0.4416030534351145
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6094,	 Acc = 0.3927,	 Loss Con1 = 0.2644,	 Loss Con2 = 0.2292
380 0.2
1718 0.459
815 0.367
85 0.165
2 0.0
0 0.0
0 0.0
0 0.0
0.4206106870229008
0.4416030534351145

 ===== Epoch 69	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   2.5161095   2.2214158
 -0.3640846  -0.3725409   1.677826    1.3864257  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  2.352953   -4.6933093
 -0.3640846  -0.3725409   1.6446419   1.4561039  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 2
train:	 Loss = 1.6104,	 Acc = 0.3907,	 Loss Con1 = 0.3048,	 Loss Con2 = 0.2724
2924 0.16
5683 0.453
2910 0.5
376 0.399
69 0.348
6 0.333
0 0.0
0 0.0
0.4651702786377709
0.4416030534351145
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5900,	 Acc = 0.4020,	 Loss Con1 = 0.2706,	 Loss Con2 = 0.2404
380 0.184
1718 0.46
815 0.407
85 0.165
2 0.0
0 0.0
0 0.0
0 0.0
0.433587786259542
0.4416030534351145

 ===== Epoch 70	 =====
[-0.36602148 -0.3783333   1.6070919   0.8755437  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.6552639   0.84123725 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 3
train:	 Loss = 1.6105,	 Acc = 0.3876,	 Loss Con1 = 0.3056,	 Loss Con2 = 0.2750
2924 0.16
5676 0.453
2916 0.487
377 0.416
69 0.304
6 0.333
0 0.0
0 0.0
0.4610791685095091
0.4416030534351145
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5992,	 Acc = 0.3933,	 Loss Con1 = 0.2665,	 Loss Con2 = 0.2315
380 0.184
1718 0.452
815 0.39
85 0.176
2 0.0
0 0.0
0 0.0
0 0.0
0.42366412213740456
0.4416030534351145

 ===== Epoch 71	 =====
[-0.36602148 -0.3783333   1.5490115   1.3761779  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.6357683   1.4437037  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 4
train:	 Loss = 1.6095,	 Acc = 0.3890,	 Loss Con1 = 0.3054,	 Loss Con2 = 0.2730
2929 0.161
5679 0.451
2909 0.494
376 0.418
69 0.362
6 0.167
0 0.0
0 0.0
0.46277243057860384
0.4416030534351145
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6077,	 Acc = 0.3903,	 Loss Con1 = 0.2637,	 Loss Con2 = 0.2289
380 0.184
1718 0.455
815 0.375
85 0.165
2 0.0
0 0.0
0 0.0
0 0.0
0.4202290076335878
0.4416030534351145

 ===== Epoch 72	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   1.6009532   1.1100935
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  1.562145    1.161358
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 6
train:	 Loss = 1.6079,	 Acc = 0.3885,	 Loss Con1 = 0.3058,	 Loss Con2 = 0.2782
2928 0.162
5676 0.451
2914 0.492
376 0.412
68 0.382
6 0.167
0 0.0
0 0.0
0.4618362831858407
0.4416030534351145
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6023,	 Acc = 0.3900,	 Loss Con1 = 0.2645,	 Loss Con2 = 0.2309
380 0.189
1718 0.453
815 0.375
85 0.153
2 0.0
0 0.0
0 0.0
0 0.0
0.41908396946564885
0.4416030534351145

 ===== Epoch 73	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  0.6331833   2.0271854
  3.2439096   1.9085698 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   5.33099    -4.809946    0.7805501   2.170134
  3.495659    2.076528  ] 2 2
train:	 Loss = 1.6090,	 Acc = 0.3848,	 Loss Con1 = 0.3054,	 Loss Con2 = 0.2738
2928 0.157
5679 0.448
2912 0.487
374 0.414
69 0.348
6 0.333
0 0.0
0 0.0
0.4584070796460177
0.4416030534351145
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6095,	 Acc = 0.3913,	 Loss Con1 = 0.2646,	 Loss Con2 = 0.2244
380 0.192
1718 0.455
815 0.374
85 0.176
2 0.0
0 0.0
0 0.0
0 0.0
0.4202290076335878
0.4416030534351145

 ===== Epoch 74	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 0
train:	 Loss = 1.6068,	 Acc = 0.3925,	 Loss Con1 = 0.3050,	 Loss Con2 = 0.2746
2923 0.159
5680 0.458
2914 0.498
376 0.412
69 0.362
6 0.333
0 0.0
0 0.0
0.4679933665008292
0.4416030534351145
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5983,	 Acc = 0.4030,	 Loss Con1 = 0.2688,	 Loss Con2 = 0.2312
380 0.189
1718 0.473
815 0.379
85 0.176
2 0.0
0 0.0
0 0.0
0 0.0
0.43396946564885497
0.4416030534351145

 ===== Epoch 75	 =====
[ 1.7427286   2.9352627  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.296048    3.252235  ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.2687129   3.3834522 ] 4 2
train:	 Loss = 1.6080,	 Acc = 0.3900,	 Loss Con1 = 0.3059,	 Loss Con2 = 0.2756
2924 0.158
5685 0.455
2909 0.497
375 0.381
69 0.406
6 0.333
0 0.0
0 0.0
0.46505970809376385
0.4416030534351145
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5994,	 Acc = 0.4023,	 Loss Con1 = 0.2662,	 Loss Con2 = 0.2307
380 0.192
1718 0.471
815 0.38
85 0.176
2 0.0
0 0.0
0 0.0
0 0.0
0.43282442748091604
0.4416030534351145

 ===== Epoch 76	 =====
[ 2.8598359   3.2820642  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.4242911   3.535664  ] [ 2.9520297   3.2845953  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.4190004   3.5382886 ] 0 0
train:	 Loss = 1.6054,	 Acc = 0.3920,	 Loss Con1 = 0.3067,	 Loss Con2 = 0.2756
2925 0.159
5680 0.458
2912 0.496
376 0.418
69 0.333
6 0.333
0 0.0
0 0.0
0.46743337388034945
0.4416030534351145
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6117,	 Acc = 0.4067,	 Loss Con1 = 0.2655,	 Loss Con2 = 0.2308
380 0.192
1718 0.473
815 0.39
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.43778625954198475
0.4416030534351145

 ===== Epoch 77	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  1.5636733   0.71903497
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
train:	 Loss = 1.6066,	 Acc = 0.3921,	 Loss Con1 = 0.3059,	 Loss Con2 = 0.2726
2925 0.157
5680 0.458
2911 0.498
377 0.401
69 0.362
6 0.5
0 0.0
0 0.0
0.4680968705075749
0.4416030534351145
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6121,	 Acc = 0.3933,	 Loss Con1 = 0.2665,	 Loss Con2 = 0.2368
380 0.184
1718 0.462
815 0.371
85 0.165
2 0.0
0 0.0
0 0.0
0 0.0
0.42366412213740456
0.4416030534351145

 ===== Epoch 78	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   1.934016    1.6124111
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  2.041214    1.7459357
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 6
train:	 Loss = 1.6042,	 Acc = 0.3894,	 Loss Con1 = 0.3054,	 Loss Con2 = 0.2755
2925 0.16
5680 0.453
2913 0.496
376 0.399
68 0.338
6 0.167
0 0.0
0 0.0
0.4635629768882008
0.4416030534351145
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6020,	 Acc = 0.4043,	 Loss Con1 = 0.2635,	 Loss Con2 = 0.2248
380 0.184
1718 0.47
815 0.391
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.4362595419847328
0.4416030534351145

 ===== Epoch 79	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.9663047   0.7751533
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.6291409   0.83923364
 -0.38555372 -0.3798593 ] 2 4
train:	 Loss = 1.6078,	 Acc = 0.3939,	 Loss Con1 = 0.3068,	 Loss Con2 = 0.2747
2926 0.158
5680 0.458
2914 0.504
374 0.417
68 0.353
6 0.167
0 0.0
0 0.0
0.4702499447024994
0.4416030534351145
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6003,	 Acc = 0.3977,	 Loss Con1 = 0.2662,	 Loss Con2 = 0.2268
380 0.189
1718 0.459
815 0.388
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.42786259541984734
0.4416030534351145

 ===== Epoch 80	 =====
[-0.36602148 -0.3783333   2.4177732   3.0819588   2.08756     0.8455989
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   2.1905553   3.0404665   2.0049212   0.79905325
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 5
train:	 Loss = 1.6062,	 Acc = 0.3912,	 Loss Con1 = 0.3060,	 Loss Con2 = 0.2741
2923 0.16
5685 0.457
2910 0.495
375 0.395
69 0.377
6 0.167
0 0.0
0 0.0
0.4660033167495854
0.4416030534351145
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6103,	 Acc = 0.4013,	 Loss Con1 = 0.2714,	 Loss Con2 = 0.2402
380 0.187
1718 0.466
815 0.388
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.43244274809160305
0.4416030534351145

 ===== Epoch 81	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.6056,	 Acc = 0.3915,	 Loss Con1 = 0.3057,	 Loss Con2 = 0.2741
2921 0.159
5682 0.457
2914 0.495
376 0.428
69 0.304
6 0.333
0 0.0
0 0.0
0.4664529678346413
0.4416030534351145
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5872,	 Acc = 0.4103,	 Loss Con1 = 0.2658,	 Loss Con2 = 0.2314
380 0.176
1718 0.477
815 0.401
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.4442748091603053
0.4442748091603053

 ===== Epoch 82	 =====
[-0.36602148 -0.3783333   1.422698    1.4305454  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.3189803   1.7245528  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.6035,	 Acc = 0.3931,	 Loss Con1 = 0.3062,	 Loss Con2 = 0.2747
2924 0.159
5676 0.46
2916 0.497
377 0.411
69 0.348
6 0.167
0 0.0
0 0.0
0.4688191065900044
0.4442748091603053
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6020,	 Acc = 0.4103,	 Loss Con1 = 0.2665,	 Loss Con2 = 0.2351
380 0.192
1718 0.474
815 0.401
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.44198473282442746
0.4442748091603053

 ===== Epoch 83	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.0248713   3.229839   -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.0361059   3.5451522  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.6054,	 Acc = 0.3921,	 Loss Con1 = 0.3065,	 Loss Con2 = 0.2742
2923 0.163
5678 0.458
2915 0.496
377 0.387
69 0.319
6 0.333
0 0.0
0 0.0
0.46611387506909896
0.4442748091603053
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6075,	 Acc = 0.3920,	 Loss Con1 = 0.2683,	 Loss Con2 = 0.2368
380 0.187
1718 0.453
815 0.379
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.4217557251908397
0.4442748091603053

 ===== Epoch 84	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.6068,	 Acc = 0.3902,	 Loss Con1 = 0.3056,	 Loss Con2 = 0.2718
2923 0.16
5679 0.456
2916 0.492
375 0.408
69 0.348
6 0.167
0 0.0
0 0.0
0.4646766169154229
0.4442748091603053
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6097,	 Acc = 0.4043,	 Loss Con1 = 0.2671,	 Loss Con2 = 0.2295
380 0.184
1718 0.474
815 0.384
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.4362595419847328
0.4442748091603053

 ===== Epoch 85	 =====
[-0.36602148 -0.3783333   0.13722287  1.2040142  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 3.2852654   2.6036503  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 5
train:	 Loss = 1.6030,	 Acc = 0.3895,	 Loss Con1 = 0.3062,	 Loss Con2 = 0.2732
2922 0.157
5680 0.459
2915 0.491
376 0.375
69 0.333
6 0.333
0 0.0
0 0.0
0.46462524872871985
0.4442748091603053
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6078,	 Acc = 0.3870,	 Loss Con1 = 0.2657,	 Loss Con2 = 0.2284
380 0.174
1718 0.448
815 0.379
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.4179389312977099
0.4442748091603053

 ===== Epoch 86	 =====
[ 1.326182    1.7834783  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.4878366   1.8665804 ] [ 1.8794463   1.8619514  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.4988587   1.9479352 ] 4 4
train:	 Loss = 1.6062,	 Acc = 0.3932,	 Loss Con1 = 0.3054,	 Loss Con2 = 0.2727
2924 0.16
5678 0.457
2915 0.502
376 0.404
69 0.377
6 0.167
0 0.0
0 0.0
0.4685979655019903
0.4442748091603053
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6044,	 Acc = 0.3927,	 Loss Con1 = 0.2691,	 Loss Con2 = 0.2357
380 0.184
1718 0.461
815 0.367
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.42290076335877863
0.4442748091603053

 ===== Epoch 87	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   1.7813848   1.4386843  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   1.8386      1.6452302  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.6015,	 Acc = 0.3895,	 Loss Con1 = 0.3049,	 Loss Con2 = 0.2735
2923 0.159
5683 0.457
2913 0.49
374 0.39
69 0.333
6 0.333
0 0.0
0 0.0
0.4639027086788281
0.4442748091603053
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6094,	 Acc = 0.3970,	 Loss Con1 = 0.2660,	 Loss Con2 = 0.2301
380 0.182
1718 0.467
815 0.367
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.42824427480916033
0.4442748091603053

 ===== Epoch 88	 =====
[-0.36602148 -0.3783333   1.8556571   2.0535066  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.9074763   2.1277072  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 4
train:	 Loss = 1.6009,	 Acc = 0.3925,	 Loss Con1 = 0.3062,	 Loss Con2 = 0.2775
2925 0.157
5680 0.464
2911 0.492
377 0.401
69 0.29
6 0.333
0 0.0
0 0.0
0.46864978436359617
0.4442748091603053
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5864,	 Acc = 0.4120,	 Loss Con1 = 0.2722,	 Loss Con2 = 0.2422
380 0.187
1718 0.472
815 0.415
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.4446564885496183
0.4446564885496183

 ===== Epoch 89	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   1.8105648   2.2797983   4.215071    0.9821822
  2.9405751   2.6591327 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   1.8117089   2.2350054   3.8862424   0.90577865
  3.2611039   2.5830266 ] 5 5
train:	 Loss = 1.5992,	 Acc = 0.3920,	 Loss Con1 = 0.3059,	 Loss Con2 = 0.2728
2923 0.157
5683 0.458
2911 0.497
376 0.42
69 0.333
6 0.333
0 0.0
0 0.0
0.46788280818131567
0.4446564885496183
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5999,	 Acc = 0.3877,	 Loss Con1 = 0.2648,	 Loss Con2 = 0.2271
380 0.179
1718 0.455
815 0.363
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.4179389312977099
0.4446564885496183

 ===== Epoch 90	 =====
[-0.36602148 -0.3783333   2.3775644   2.9641623   2.2846467   0.8500442
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   2.3144271   2.9476051   2.1267893   0.8901851
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 1
train:	 Loss = 1.6043,	 Acc = 0.3914,	 Loss Con1 = 0.3056,	 Loss Con2 = 0.2733
2928 0.162
5678 0.454
2914 0.498
374 0.409
68 0.353
6 0.333
0 0.0
0 0.0
0.46559734513274337
0.4446564885496183
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6077,	 Acc = 0.3977,	 Loss Con1 = 0.2664,	 Loss Con2 = 0.2375
380 0.184
1718 0.465
815 0.377
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.42862595419847327
0.4446564885496183

 ===== Epoch 91	 =====
[-0.36602148 -0.3783333   1.042132    2.3638544  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.1073823   2.3179598  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 2
train:	 Loss = 1.5996,	 Acc = 0.3938,	 Loss Con1 = 0.3045,	 Loss Con2 = 0.2742
2926 0.157
5681 0.463
2910 0.495
376 0.402
69 0.391
6 0.333
0 0.0
0 0.0
0.4703605397036054
0.4446564885496183
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5948,	 Acc = 0.4087,	 Loss Con1 = 0.2669,	 Loss Con2 = 0.2308
380 0.187
1718 0.476
815 0.391
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.44083969465648853
0.4446564885496183

 ===== Epoch 92	 =====
[-0.36602148 -0.3783333   1.5957192   2.236997   -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.7226821   2.3111649  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 2
train:	 Loss = 1.5999,	 Acc = 0.3907,	 Loss Con1 = 0.3063,	 Loss Con2 = 0.2729
2928 0.165
5678 0.457
2911 0.491
376 0.394
69 0.319
6 0.333
0 0.0
0 0.0
0.4638274336283186
0.4446564885496183
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5800,	 Acc = 0.4017,	 Loss Con1 = 0.2632,	 Loss Con2 = 0.2264
380 0.166
1718 0.467
815 0.398
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.43587786259541983
0.4446564885496183

 ===== Epoch 93	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   2.7227452   1.3701429
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   2.8623087   3.0970893   2.5615432   1.3236172
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 2
train:	 Loss = 1.5992,	 Acc = 0.3925,	 Loss Con1 = 0.3039,	 Loss Con2 = 0.2707
2921 0.154
5684 0.461
2912 0.497
376 0.42
69 0.348
6 0.167
0 0.0
0 0.0
0.46965845031502157
0.4446564885496183
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5696,	 Acc = 0.4183,	 Loss Con1 = 0.2666,	 Loss Con2 = 0.2388
380 0.179
1718 0.485
815 0.41
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.4530534351145038
0.4530534351145038

 ===== Epoch 94	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  1.3418465   2.5406156  -0.40141198 -0.40778467  2.6698015   2.0222564
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  1.2693869   2.5406156  -0.40141198 -0.40778467  2.3943892   2.0222564
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.5989,	 Acc = 0.3961,	 Loss Con1 = 0.3054,	 Loss Con2 = 0.2727
2924 0.159
5678 0.464
2917 0.5
374 0.43
69 0.319
6 0.333
0 0.0
0 0.0
0.4726890756302521
0.4530534351145038
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5950,	 Acc = 0.3907,	 Loss Con1 = 0.2639,	 Loss Con2 = 0.2279
380 0.176
1718 0.46
815 0.363
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.4217557251908397
0.4530534351145038

 ===== Epoch 95	 =====
[-0.36602148 -0.3783333   1.1911908   1.5098314  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.2511554   1.5184458  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 3 0
train:	 Loss = 1.5991,	 Acc = 0.3929,	 Loss Con1 = 0.3059,	 Loss Con2 = 0.2750
2924 0.159
5676 0.463
2916 0.493
377 0.387
69 0.348
6 0.333
0 0.0
0 0.0
0.4685979655019903
0.4530534351145038
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5943,	 Acc = 0.4013,	 Loss Con1 = 0.2665,	 Loss Con2 = 0.2339
380 0.182
1718 0.469
815 0.383
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.43320610687022904
0.4530534351145038

 ===== Epoch 96	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  1.8406473   0.7568214
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 2
train:	 Loss = 1.5962,	 Acc = 0.3970,	 Loss Con1 = 0.3052,	 Loss Con2 = 0.2728
2926 0.16
5677 0.466
2914 0.5
376 0.402
69 0.348
6 0.333
0 0.0
0 0.0
0.47356779473567795
0.4530534351145038
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5953,	 Acc = 0.4180,	 Loss Con1 = 0.2677,	 Loss Con2 = 0.2318
380 0.171
1718 0.486
815 0.409
85 0.247
2 0.0
0 0.0
0 0.0
0 0.0
0.4538167938931298
0.4538167938931298

 ===== Epoch 97	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 0
train:	 Loss = 1.5996,	 Acc = 0.3961,	 Loss Con1 = 0.3052,	 Loss Con2 = 0.2747
2926 0.16
5683 0.465
2909 0.502
375 0.384
69 0.319
6 0.333
0 0.0
0 0.0
0.4723512497235125
0.4538167938931298
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5947,	 Acc = 0.4127,	 Loss Con1 = 0.2650,	 Loss Con2 = 0.2329
380 0.179
1718 0.483
815 0.396
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.44656488549618323
0.4538167938931298

 ===== Epoch 98	 =====
[ 2.4990475   1.1000015  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.1998765   1.2813513 ] [ 2.1124816   1.0594991  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   1.8305897   3.3349235  -0.42342317 -0.42019257
  2.1703377   1.2314888 ] 2 2
train:	 Loss = 1.6006,	 Acc = 0.3934,	 Loss Con1 = 0.3053,	 Loss Con2 = 0.2727
2920 0.158
5686 0.46
2910 0.5
377 0.401
69 0.362
6 0.167
0 0.0
0 0.0
0.4692749778956676
0.4538167938931298
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5924,	 Acc = 0.4073,	 Loss Con1 = 0.2671,	 Loss Con2 = 0.2339
380 0.195
1718 0.471
815 0.391
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.4381679389312977
0.4538167938931298

 ===== Epoch 99	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.5947,	 Acc = 0.3934,	 Loss Con1 = 0.3050,	 Loss Con2 = 0.2713
2924 0.157
5679 0.462
2914 0.498
376 0.402
69 0.29
6 0.167
0 0.0
0 0.0
0.4698142414860681
0.4538167938931298
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5789,	 Acc = 0.3930,	 Loss Con1 = 0.2636,	 Loss Con2 = 0.2256
380 0.166
1718 0.466
815 0.373
85 0.129
2 0.0
0 0.0
0 0.0
0 0.0
0.4259541984732824
0.4538167938931298

 ===== Epoch 100	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   2.0333228   1.1323199
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  1.8968055   1.3236172
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.5956,	 Acc = 0.3931,	 Loss Con1 = 0.3039,	 Loss Con2 = 0.2703
2925 0.158
5681 0.461
2910 0.499
377 0.39
69 0.333
6 0.333
0 0.0
0 0.0
0.4692026982196174
0.4538167938931298
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6223,	 Acc = 0.3973,	 Loss Con1 = 0.2659,	 Loss Con2 = 0.2287
380 0.184
1718 0.464
815 0.374
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.42824427480916033
0.4538167938931298

 ===== Epoch 101	 =====
[-0.36602148 -0.3783333   4.170731    0.9729521  -0.4409929  -0.3635196
  4.2121263   2.4820175  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   4.2439947   0.79140913 -0.44088638 -0.36343393
  4.113267    2.2029793  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 5
train:	 Loss = 1.5976,	 Acc = 0.3964,	 Loss Con1 = 0.3046,	 Loss Con2 = 0.2710
2925 0.158
5682 0.469
2911 0.494
376 0.404
68 0.368
6 0.333
0 0.0
0 0.0
0.473515426296583
0.4538167938931298
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5956,	 Acc = 0.4080,	 Loss Con1 = 0.2706,	 Loss Con2 = 0.2379
380 0.176
1718 0.479
815 0.378
85 0.306
2 0.0
0 0.0
0 0.0
0 0.0
0.4416030534351145
0.4538167938931298

 ===== Epoch 102	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   5.6531096   2.6953783   0.62977433  1.9064187
  3.5388658   1.6986222 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   5.0105877  -5.066262    0.6335617   2.0197916
  3.4784641   1.7668554 ] 4 2
train:	 Loss = 1.5981,	 Acc = 0.3925,	 Loss Con1 = 0.3050,	 Loss Con2 = 0.2699
2924 0.169
5681 0.457
2913 0.491
375 0.4
69 0.406
6 0.167
0 0.0
0 0.0
0.46483856700574966
0.4538167938931298
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5897,	 Acc = 0.3977,	 Loss Con1 = 0.2673,	 Loss Con2 = 0.2274
380 0.179
1718 0.468
815 0.375
85 0.176
2 0.0
0 0.0
0 0.0
0 0.0
0.42938931297709926
0.4538167938931298

 ===== Epoch 103	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 0
train:	 Loss = 1.5967,	 Acc = 0.3925,	 Loss Con1 = 0.3054,	 Loss Con2 = 0.2729
2921 0.157
5682 0.462
2914 0.493
376 0.394
69 0.333
6 0.333
0 0.0
0 0.0
0.46844257765004976
0.4538167938931298
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5909,	 Acc = 0.4173,	 Loss Con1 = 0.2635,	 Loss Con2 = 0.2302
380 0.171
1718 0.492
815 0.4
85 0.176
2 0.0
0 0.0
0 0.0
0 0.0
0.4530534351145038
0.4538167938931298

 ===== Epoch 104	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   1.1680429   2.5609992  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   1.2389886   2.5833957  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 2
train:	 Loss = 1.5966,	 Acc = 0.3958,	 Loss Con1 = 0.3058,	 Loss Con2 = 0.2734
2926 0.157
5681 0.466
2910 0.5
376 0.386
69 0.362
6 0.333
0 0.0
0 0.0
0.4730148197301482
0.4538167938931298
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5834,	 Acc = 0.3953,	 Loss Con1 = 0.2646,	 Loss Con2 = 0.2294
380 0.182
1718 0.46
815 0.383
85 0.176
2 0.0
0 0.0
0 0.0
0 0.0
0.4263358778625954
0.4538167938931298

 ===== Epoch 105	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.7281425   1.5884813
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  3.0486376   1.5884813
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.5940,	 Acc = 0.3958,	 Loss Con1 = 0.3055,	 Loss Con2 = 0.2730
2923 0.16
5678 0.465
2915 0.499
377 0.398
69 0.377
6 0.167
0 0.0
0 0.0
0.4720840243228303
0.4538167938931298
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5815,	 Acc = 0.4187,	 Loss Con1 = 0.2642,	 Loss Con2 = 0.2314
380 0.166
1718 0.491
815 0.405
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.45534351145038165
0.45534351145038165

 ===== Epoch 106	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   2.7605581   1.607966
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  2.5191376   1.628131
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 3 4
train:	 Loss = 1.5940,	 Acc = 0.3970,	 Loss Con1 = 0.3059,	 Loss Con2 = 0.2729
2922 0.157
5681 0.468
2916 0.5
375 0.405
68 0.338
6 0.167
0 0.0
0 0.0
0.47446385142604464
0.45534351145038165
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5771,	 Acc = 0.4170,	 Loss Con1 = 0.2637,	 Loss Con2 = 0.2288
380 0.176
1718 0.482
815 0.412
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.45190839694656487
0.45534351145038165

 ===== Epoch 107	 =====
[-0.36602148 -0.3783333   2.9315581   1.8677509  -0.4409929  -0.3635196
  2.2647173   3.7823343  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   2.723003    1.8355335  -0.44088638 -0.36343393
  2.2725797   3.773963   -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 3 3
train:	 Loss = 1.5977,	 Acc = 0.3949,	 Loss Con1 = 0.3055,	 Loss Con2 = 0.2724
2925 0.157
5680 0.463
2912 0.501
376 0.399
69 0.362
6 0.333
0 0.0
0 0.0
0.47196726749972356
0.45534351145038165
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5826,	 Acc = 0.4013,	 Loss Con1 = 0.2644,	 Loss Con2 = 0.2270
380 0.182
1718 0.471
815 0.38
85 0.176
2 0.0
0 0.0
0 0.0
0 0.0
0.43320610687022904
0.45534351145038165

 ===== Epoch 108	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   1.5367863   1.2434522
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  1.6542158   1.2435989
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.5961,	 Acc = 0.3961,	 Loss Con1 = 0.3052,	 Loss Con2 = 0.2714
2922 0.156
5681 0.468
2914 0.496
376 0.407
69 0.333
6 0.333
0 0.0
0 0.0
0.47346893654653993
0.45534351145038165
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5896,	 Acc = 0.3983,	 Loss Con1 = 0.2645,	 Loss Con2 = 0.2338
380 0.168
1718 0.467
815 0.379
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.4316793893129771
0.45534351145038165

 ===== Epoch 109	 =====
[-0.36602148 -0.3783333   2.412088    2.2981606  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   2.4054024   2.080144   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 5
train:	 Loss = 1.5934,	 Acc = 0.3964,	 Loss Con1 = 0.3057,	 Loss Con2 = 0.2733
2926 0.155
5680 0.467
2912 0.504
375 0.387
69 0.333
6 0.333
0 0.0
0 0.0
0.4744525547445255
0.45534351145038165
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5811,	 Acc = 0.4103,	 Loss Con1 = 0.2668,	 Loss Con2 = 0.2290
380 0.174
1718 0.478
815 0.4
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.4446564885496183
0.45534351145038165

 ===== Epoch 110	 =====
[ 3.1044962  1.3227642 -0.4208693 -0.3341335 -0.4409929 -0.3635196
 -0.3640846 -0.3725409  4.997429   2.7003553  0.6358347  1.8127627
  3.503594   1.7143683] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   5.429971    2.668005    0.64227575  1.86452
  4.065733    1.6645057 ] 4 2
train:	 Loss = 1.5947,	 Acc = 0.3958,	 Loss Con1 = 0.3054,	 Loss Con2 = 0.2724
2925 0.161
5682 0.463
2912 0.502
374 0.396
69 0.348
6 0.167
0 0.0
0 0.0
0.4716355191861108
0.45534351145038165
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5777,	 Acc = 0.4173,	 Loss Con1 = 0.2624,	 Loss Con2 = 0.2265
380 0.174
1718 0.485
815 0.411
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.45267175572519086
0.45534351145038165

 ===== Epoch 111	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   4.132528   -4.017547
 -0.3640846  -0.3725409   5.491191    2.516206    0.43656826  1.5564411
  3.6177852   1.4598068 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  4.54009     2.8906412
 -0.3640846  -0.3725409   5.445992    2.4589705   0.4373248   1.5490474
  3.720514    1.3941982 ] 2 2
train:	 Loss = 1.5924,	 Acc = 0.3950,	 Loss Con1 = 0.3055,	 Loss Con2 = 0.2725
2926 0.161
5680 0.462
2913 0.505
374 0.369
69 0.319
6 0.167
0 0.0
0 0.0
0.4705817297058173
0.45534351145038165
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5635,	 Acc = 0.4197,	 Loss Con1 = 0.2637,	 Loss Con2 = 0.2282
380 0.182
1718 0.485
815 0.416
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.4541984732824427
0.45534351145038165

 ===== Epoch 112	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   3.5556152   2.3395224  -0.42342317 -0.42019257
  4.275156    1.3915739 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   3.9280841   2.5560222  -0.42342317 -0.42019257
  4.0538273   1.635638  ] 6 6
train:	 Loss = 1.5950,	 Acc = 0.3963,	 Loss Con1 = 0.3061,	 Loss Con2 = 0.2719
2925 0.159
5676 0.466
2915 0.502
377 0.385
69 0.377
6 0.333
0 0.0
0 0.0
0.47318367798297023
0.45534351145038165
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5902,	 Acc = 0.4013,	 Loss Con1 = 0.2685,	 Loss Con2 = 0.2352
380 0.187
1718 0.468
815 0.379
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.43244274809160305
0.45534351145038165

 ===== Epoch 113	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  1.2222039   2.7247806  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  1.1800756   2.9926567  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.5905,	 Acc = 0.3964,	 Loss Con1 = 0.3057,	 Loss Con2 = 0.2731
2926 0.164
5680 0.466
2912 0.498
376 0.38
69 0.377
5 0.2
0 0.0
0 0.0
0.47157708471577087
0.45534351145038165
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5946,	 Acc = 0.4100,	 Loss Con1 = 0.2634,	 Loss Con2 = 0.2281
380 0.182
1718 0.481
815 0.39
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.4431297709923664
0.45534351145038165

 ===== Epoch 114	 =====
[-0.36602148 -0.3783333   0.6042989   2.9664278  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   0.6110795   2.723379   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 5
train:	 Loss = 1.5937,	 Acc = 0.3962,	 Loss Con1 = 0.3053,	 Loss Con2 = 0.2732
2926 0.159
5678 0.467
2913 0.498
376 0.386
69 0.377
6 0.333
0 0.0
0 0.0
0.47290422472904226
0.45534351145038165
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5963,	 Acc = 0.4033,	 Loss Con1 = 0.2652,	 Loss Con2 = 0.2469
380 0.174
1718 0.473
815 0.387
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.4366412213740458
0.45534351145038165

 ===== Epoch 115	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 0
train:	 Loss = 1.5960,	 Acc = 0.3910,	 Loss Con1 = 0.3054,	 Loss Con2 = 0.2739
2925 0.163
5678 0.456
2914 0.494
376 0.407
69 0.319
6 0.167
0 0.0
0 0.0
0.46488997014265176
0.45534351145038165
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5703,	 Acc = 0.4073,	 Loss Con1 = 0.2644,	 Loss Con2 = 0.2278
380 0.195
1718 0.473
815 0.389
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.4381679389312977
0.45534351145038165

 ===== Epoch 116	 =====
[-0.36602148 -0.3783333   2.1310294   2.4771202  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   2.1271975   2.4221456  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 5
train:	 Loss = 1.5888,	 Acc = 0.3962,	 Loss Con1 = 0.3060,	 Loss Con2 = 0.2736
2922 0.161
5682 0.464
2915 0.499
374 0.409
69 0.348
6 0.167
0 0.0
0 0.0
0.4721423833738669
0.45534351145038165
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5659,	 Acc = 0.4157,	 Loss Con1 = 0.2645,	 Loss Con2 = 0.2401
380 0.176
1718 0.488
815 0.398
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.45038167938931295
0.45534351145038165

 ===== Epoch 117	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 0
train:	 Loss = 1.5905,	 Acc = 0.3941,	 Loss Con1 = 0.3056,	 Loss Con2 = 0.2748
2924 0.163
5682 0.463
2914 0.493
373 0.397
69 0.362
6 0.167
0 0.0
0 0.0
0.4687085360459973
0.45534351145038165
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5574,	 Acc = 0.4287,	 Loss Con1 = 0.2631,	 Loss Con2 = 0.2260
380 0.187
1718 0.495
815 0.422
85 0.247
2 0.0
0 0.0
0 0.0
0 0.0
0.4637404580152672
0.4637404580152672

 ===== Epoch 118	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  4.2581487   2.6239138
 -0.3640846  -0.3725409   4.2450533   2.055833   -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 2
train:	 Loss = 1.5952,	 Acc = 0.3935,	 Loss Con1 = 0.3061,	 Loss Con2 = 0.2723
2924 0.159
5679 0.456
2915 0.508
375 0.395
69 0.333
6 0.333
0 0.0
0 0.0
0.46915081822202565
0.4637404580152672
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5700,	 Acc = 0.4010,	 Loss Con1 = 0.2644,	 Loss Con2 = 0.2285
380 0.176
1718 0.473
815 0.375
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.433587786259542
0.4637404580152672

 ===== Epoch 119	 =====
[-0.36602148 -0.3783333   2.5107832   1.5732602  -0.4409929  -0.3635196
  0.86440146  3.3247118  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   2.4447978   1.6384861  -0.44088638 -0.36343393
  1.0840254   3.374939   -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 4
train:	 Loss = 1.5882,	 Acc = 0.3951,	 Loss Con1 = 0.3062,	 Loss Con2 = 0.2733
2922 0.163
5677 0.461
2918 0.499
376 0.412
69 0.319
6 0.167
0 0.0
0 0.0
0.47004200751713465
0.4637404580152672
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5711,	 Acc = 0.4067,	 Loss Con1 = 0.2658,	 Loss Con2 = 0.2315
380 0.182
1718 0.476
815 0.383
85 0.247
2 0.0
0 0.0
0 0.0
0 0.0
0.43931297709923667
0.4637404580152672

 ===== Epoch 120	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   0.5884564   3.0786078   2.969461    1.0043639
  2.3660936   3.123642  ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  3.1626666   1.2311099
  2.442368    3.3309653 ] 6 6
train:	 Loss = 1.5933,	 Acc = 0.3929,	 Loss Con1 = 0.3050,	 Loss Con2 = 0.2721
2921 0.162
5686 0.46
2912 0.496
376 0.386
68 0.338
5 0.4
0 0.0
0 0.0
0.4675583066209793
0.4637404580152672
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5542,	 Acc = 0.4267,	 Loss Con1 = 0.2646,	 Loss Con2 = 0.2315
380 0.182
1718 0.49
815 0.428
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.4622137404580153
0.4637404580152672

 ===== Epoch 121	 =====
[ 3.7004356   2.3327909  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  1.2339993   2.925688   -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  1.2244507   3.1879835  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.5929,	 Acc = 0.3929,	 Loss Con1 = 0.3052,	 Loss Con2 = 0.2708
2926 0.158
5677 0.462
2914 0.495
376 0.396
69 0.29
6 0.333
0 0.0
0 0.0
0.4688122096881221
0.4637404580152672
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5526,	 Acc = 0.4293,	 Loss Con1 = 0.2631,	 Loss Con2 = 0.2250
380 0.168
1718 0.494
815 0.433
85 0.271
2 0.0
0 0.0
0 0.0
0 0.0
0.467175572519084
0.467175572519084

 ===== Epoch 122	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.9428372   3.0081706 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.0133803   3.131515  ] 4 4
train:	 Loss = 1.5910,	 Acc = 0.3942,	 Loss Con1 = 0.3046,	 Loss Con2 = 0.2698
2924 0.16
5682 0.464
2911 0.497
376 0.378
69 0.333
6 0.0
0 0.0
0 0.0
0.4698142414860681
0.467175572519084
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5918,	 Acc = 0.4087,	 Loss Con1 = 0.2668,	 Loss Con2 = 0.2307
380 0.187
1718 0.481
815 0.378
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.44083969465648853
0.467175572519084

 ===== Epoch 123	 =====
[ 3.7613955   2.8162873  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  1.2182727   3.464231   -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 2.898124    2.8517268  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  1.3075821   3.5702655  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 4
train:	 Loss = 1.5874,	 Acc = 0.3947,	 Loss Con1 = 0.3058,	 Loss Con2 = 0.2722
2924 0.16
5678 0.463
2914 0.496
377 0.422
69 0.362
6 0.333
0 0.0
0 0.0
0.47069880583812473
0.467175572519084
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5844,	 Acc = 0.3913,	 Loss Con1 = 0.2630,	 Loss Con2 = 0.2355
380 0.184
1718 0.451
815 0.384
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.4213740458015267
0.467175572519084

 ===== Epoch 124	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.5045898   1.074028  ] [ 1.913931    1.1303781  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.41553     1.0399116 ] 3 2
train:	 Loss = 1.5875,	 Acc = 0.3975,	 Loss Con1 = 0.3062,	 Loss Con2 = 0.2727
2922 0.16
5680 0.467
2915 0.502
376 0.41
69 0.319
6 0.333
0 0.0
0 0.0
0.47424275923059916
0.467175572519084
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5727,	 Acc = 0.4147,	 Loss Con1 = 0.2633,	 Loss Con2 = 0.2293
380 0.184
1718 0.488
815 0.388
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.4480916030534351
0.467175572519084

 ===== Epoch 125	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   2.3851      1.0967577
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  2.3858078   1.0968989
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.5885,	 Acc = 0.3961,	 Loss Con1 = 0.3056,	 Loss Con2 = 0.2721
2923 0.157
5683 0.466
2911 0.504
376 0.391
69 0.246
6 0.333
0 0.0
0 0.0
0.47330016583747925
0.467175572519084
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5639,	 Acc = 0.4083,	 Loss Con1 = 0.2618,	 Loss Con2 = 0.2293
380 0.179
1718 0.48
815 0.387
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.4416030534351145
0.467175572519084

 ===== Epoch 126	 =====
[ 6.06544     2.1125596  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.8174279   1.6644362  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6966636   1.5137557  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 5
train:	 Loss = 1.5900,	 Acc = 0.3973,	 Loss Con1 = 0.3049,	 Loss Con2 = 0.2724
2925 0.16
5680 0.464
2915 0.505
374 0.409
69 0.333
5 0.4
0 0.0
0 0.0
0.4741789229238085
0.467175572519084
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5641,	 Acc = 0.4143,	 Loss Con1 = 0.2658,	 Loss Con2 = 0.2298
380 0.168
1718 0.483
815 0.401
85 0.271
2 0.0
0 0.0
0 0.0
0 0.0
0.45
0.467175572519084

 ===== Epoch 127	 =====
[-0.36602148 -0.3783333   1.6265869   1.0431769  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.6345506   1.0428144  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 0
train:	 Loss = 1.5897,	 Acc = 0.3942,	 Loss Con1 = 0.3063,	 Loss Con2 = 0.2725
2920 0.153
5678 0.462
2918 0.503
377 0.406
69 0.377
6 0.333
0 0.0
0 0.0
0.47203801945181256
0.467175572519084
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5735,	 Acc = 0.4037,	 Loss Con1 = 0.2641,	 Loss Con2 = 0.2307
380 0.176
1718 0.47
815 0.391
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.4366412213740458
0.467175572519084

 ===== Epoch 128	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.4484364   1.4652498
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.5003374   1.3395536
 -0.38555372 -0.3798593 ] 5 5
train:	 Loss = 1.5875,	 Acc = 0.3956,	 Loss Con1 = 0.3054,	 Loss Con2 = 0.2711
2927 0.166
5677 0.457
2912 0.502
377 0.43
69 0.391
6 0.167
0 0.0
0 0.0
0.46997013604689747
0.467175572519084
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5646,	 Acc = 0.3987,	 Loss Con1 = 0.2680,	 Loss Con2 = 0.2328
380 0.179
1718 0.456
815 0.401
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.4305343511450382
0.467175572519084

 ===== Epoch 129	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  3.9169276   2.3993447
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  3.5774064   1.854182   -0.40141198 -0.40778467  3.7994895   2.3623753
 -0.38555372 -0.3798593 ] 3 2
train:	 Loss = 1.5876,	 Acc = 0.3966,	 Loss Con1 = 0.3046,	 Loss Con2 = 0.2707
2923 0.157
5681 0.464
2915 0.501
374 0.428
69 0.42
6 0.333
0 0.0
0 0.0
0.4740740740740741
0.467175572519084
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5643,	 Acc = 0.4193,	 Loss Con1 = 0.2652,	 Loss Con2 = 0.2319
380 0.187
1718 0.49
815 0.398
85 0.247
2 0.0
0 0.0
0 0.0
0 0.0
0.4530534351145038
0.467175572519084

 ===== Epoch 130	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   0.955074    1.0545275
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 0.11511011  3.3175035  -0.4209566  -0.33425197  0.9531853   1.1457988
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 2
train:	 Loss = 1.5881,	 Acc = 0.3970,	 Loss Con1 = 0.3057,	 Loss Con2 = 0.2717
2917 0.159
5683 0.463
2916 0.502
377 0.44
69 0.391
6 0.167
0 0.0
0 0.0
0.4736493205170699
0.467175572519084
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5608,	 Acc = 0.4147,	 Loss Con1 = 0.2701,	 Loss Con2 = 0.2456
380 0.189
1718 0.472
815 0.417
85 0.247
2 0.0
0 0.0
0 0.0
0 0.0
0.44732824427480916
0.467175572519084

 ===== Epoch 131	 =====
[ 2.7252703   2.2441921  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.0345426   2.4754286 ] [ 2.6575272   2.2593803  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.9953036   2.485926  ] 1 1
train:	 Loss = 1.5884,	 Acc = 0.3962,	 Loss Con1 = 0.3061,	 Loss Con2 = 0.2746
2921 0.157
5682 0.462
2916 0.505
375 0.413
68 0.412
6 0.333
0 0.0
0 0.0
0.4735271360672046
0.467175572519084
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5629,	 Acc = 0.4063,	 Loss Con1 = 0.2651,	 Loss Con2 = 0.2362
380 0.171
1718 0.467
815 0.407
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.4404580152671756
0.467175572519084

 ===== Epoch 132	 =====
[ 1.1281464   3.3934453  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 0.96933126  3.4136965  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 0
train:	 Loss = 1.5851,	 Acc = 0.3940,	 Loss Con1 = 0.3058,	 Loss Con2 = 0.2713
2924 0.155
5680 0.462
2913 0.495
376 0.441
69 0.391
6 0.333
0 0.0
0 0.0
0.4713622291021672
0.467175572519084
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5701,	 Acc = 0.4007,	 Loss Con1 = 0.2655,	 Loss Con2 = 0.2302
380 0.187
1718 0.466
815 0.387
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.4316793893129771
0.467175572519084

 ===== Epoch 133	 =====
[-0.36602148 -0.3783333   2.9583642   1.6185664  -0.4409929  -0.3635196
  1.7395284   3.0038185  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   2.6852322   1.695109   -0.44088638 -0.36343393
  1.7726693   3.08753    -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 4
train:	 Loss = 1.5878,	 Acc = 0.3942,	 Loss Con1 = 0.3062,	 Loss Con2 = 0.2730
2924 0.157
5678 0.462
2915 0.502
376 0.388
69 0.333
6 0.333
0 0.0
0 0.0
0.47091994692613887
0.467175572519084
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5555,	 Acc = 0.4210,	 Loss Con1 = 0.2646,	 Loss Con2 = 0.2281
380 0.179
1718 0.484
815 0.415
85 0.294
2 0.0
0 0.0
0 0.0
0 0.0
0.45610687022900764
0.467175572519084

 ===== Epoch 134	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   1.917592    1.6368603
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  1.7218355   1.8504039
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
train:	 Loss = 1.5912,	 Acc = 0.3991,	 Loss Con1 = 0.3033,	 Loss Con2 = 0.2703
2925 0.156
5681 0.467
2912 0.507
375 0.413
69 0.493
6 0.333
0 0.0
0 0.0
0.4778281543735486
0.467175572519084
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5597,	 Acc = 0.4237,	 Loss Con1 = 0.2644,	 Loss Con2 = 0.2293
380 0.179
1718 0.482
815 0.432
85 0.271
2 0.0
0 0.0
0 0.0
0 0.0
0.45916030534351143
0.467175572519084

 ===== Epoch 135	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   1.9370707   1.4946109
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  1.8555466   1.621463
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.5884,	 Acc = 0.4027,	 Loss Con1 = 0.3050,	 Loss Con2 = 0.2713
2928 0.16
5679 0.471
2909 0.506
377 0.44
69 0.536
6 0.333
0 0.0
0 0.0
0.4813053097345133
0.467175572519084
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5666,	 Acc = 0.4217,	 Loss Con1 = 0.2632,	 Loss Con2 = 0.2244
380 0.192
1718 0.488
815 0.411
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.4549618320610687
0.467175572519084

 ===== Epoch 136	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 0
train:	 Loss = 1.5847,	 Acc = 0.3984,	 Loss Con1 = 0.3051,	 Loss Con2 = 0.2730
2922 0.167
5682 0.459
2914 0.506
375 0.435
69 0.493
6 0.167
0 0.0
0 0.0
0.47302675215564893
0.467175572519084
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5626,	 Acc = 0.4203,	 Loss Con1 = 0.2670,	 Loss Con2 = 0.2352
380 0.192
1718 0.486
815 0.401
85 0.306
2 0.0
0 0.0
0 0.0
0 0.0
0.4534351145038168
0.467175572519084

 ===== Epoch 137	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  1.2272586   1.926732   -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  1.246918    1.7062919  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 5
train:	 Loss = 1.5876,	 Acc = 0.3950,	 Loss Con1 = 0.3044,	 Loss Con2 = 0.2719
2928 0.162
5680 0.458
2910 0.501
375 0.424
69 0.507
6 0.333
0 0.0
0 0.0
0.4704646017699115
0.467175572519084
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5794,	 Acc = 0.4097,	 Loss Con1 = 0.2605,	 Loss Con2 = 0.2257
380 0.187
1718 0.462
815 0.416
85 0.294
2 0.0
0 0.0
0 0.0
0 0.0
0.44198473282442746
0.467175572519084

 ===== Epoch 138	 =====
[ 1.1748757   1.3050445  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   3.3130248   3.0736306  -0.42342317 -0.42019257
  1.2867895   1.3390869 ] [ 1.3246768   1.3278269  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   3.045832    3.0835848  -0.42342317 -0.42019257
  1.3088351   1.3994468 ] 2 4
train:	 Loss = 1.5885,	 Acc = 0.3956,	 Loss Con1 = 0.3050,	 Loss Con2 = 0.2703
2926 0.158
5679 0.461
2911 0.502
377 0.419
69 0.478
6 0.167
0 0.0
0 0.0
0.4725724397257244
0.467175572519084
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5622,	 Acc = 0.4180,	 Loss Con1 = 0.2658,	 Loss Con2 = 0.2321
380 0.184
1718 0.476
815 0.422
85 0.271
2 0.0
0 0.0
0 0.0
0 0.0
0.45190839694656487
0.467175572519084

 ===== Epoch 139	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.4423759   1.6402385
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.5476921   1.5712289
 -0.38555372 -0.3798593 ] 3 2
train:	 Loss = 1.5839,	 Acc = 0.3991,	 Loss Con1 = 0.3053,	 Loss Con2 = 0.2728
2921 0.155
5680 0.463
2915 0.511
377 0.448
69 0.478
6 0.333
0 0.0
0 0.0
0.47794849121255667
0.467175572519084
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5535,	 Acc = 0.4303,	 Loss Con1 = 0.2695,	 Loss Con2 = 0.2314
380 0.187
1718 0.491
815 0.436
85 0.247
2 0.0
0 0.0
0 0.0
0 0.0
0.46564885496183206
0.467175572519084

 ===== Epoch 140	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  0.96326107  1.6225806  -0.40141198 -0.40778467  2.1845136   1.8768431
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  1.1300849   1.54445    -0.40141198 -0.40778467  2.6660142   1.778258
 -0.38555372 -0.3798593 ] 2 5
train:	 Loss = 1.5848,	 Acc = 0.3997,	 Loss Con1 = 0.3051,	 Loss Con2 = 0.2706
2925 0.16
5679 0.468
2915 0.505
374 0.406
69 0.42
6 0.333
0 0.0
0 0.0
0.47727524051752734
0.467175572519084
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5797,	 Acc = 0.4023,	 Loss Con1 = 0.2643,	 Loss Con2 = 0.2359
380 0.189
1718 0.464
815 0.384
85 0.294
2 0.0
0 0.0
0 0.0
0 0.0
0.43320610687022904
0.467175572519084

 ===== Epoch 141	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   1.5856757   1.9769248
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  1.5862135   1.9770994
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.5882,	 Acc = 0.3984,	 Loss Con1 = 0.3063,	 Loss Con2 = 0.2797
2923 0.158
5683 0.466
2910 0.501
377 0.432
69 0.478
6 0.333
0 0.0
0 0.0
0.4759535655058043
0.467175572519084
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5631,	 Acc = 0.4147,	 Loss Con1 = 0.2663,	 Loss Con2 = 0.2311
380 0.182
1718 0.476
815 0.407
85 0.294
2 0.0
0 0.0
0 0.0
0 0.0
0.4484732824427481
0.467175572519084

 ===== Epoch 142	 =====
[ 2.3047001   2.1252165  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.1879728   2.357333  ] [ 2.3047001   2.1252165  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.1879728   2.357333  ] 0 0
train:	 Loss = 1.5872,	 Acc = 0.3980,	 Loss Con1 = 0.3056,	 Loss Con2 = 0.2723
2927 0.156
5683 0.464
2909 0.504
374 0.439
69 0.493
6 0.333
0 0.0
0 0.0
0.47616414113483024
0.467175572519084
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5905,	 Acc = 0.4010,	 Loss Con1 = 0.2679,	 Loss Con2 = 0.2302
380 0.192
1718 0.463
815 0.377
85 0.329
2 0.0
0 0.0
0 0.0
0 0.0
0.4312977099236641
0.467175572519084

 ===== Epoch 143	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.2553558   1.5194716
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.2208824   1.5022192
 -0.38555372 -0.3798593 ] 3 0
train:	 Loss = 1.5851,	 Acc = 0.3974,	 Loss Con1 = 0.3054,	 Loss Con2 = 0.2743
2924 0.157
5680 0.464
2912 0.502
377 0.427
69 0.478
6 0.333
0 0.0
0 0.0
0.4750110570544007
0.467175572519084
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5629,	 Acc = 0.3980,	 Loss Con1 = 0.2626,	 Loss Con2 = 0.2244
380 0.182
1718 0.463
815 0.384
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.42938931297709926
0.467175572519084

 ===== Epoch 144	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 0
train:	 Loss = 1.5874,	 Acc = 0.3970,	 Loss Con1 = 0.3060,	 Loss Con2 = 0.2724
2923 0.16
5679 0.462
2914 0.503
377 0.432
69 0.464
6 0.333
0 0.0
0 0.0
0.4736318407960199
0.467175572519084
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5863,	 Acc = 0.3997,	 Loss Con1 = 0.2629,	 Loss Con2 = 0.2276
380 0.332
1718 0.425
815 0.395
85 0.247
2 0.0
0 0.0
0 0.0
0 0.0
0.40954198473282444
0.467175572519084

 ===== Epoch 145	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  4.7558494   1.9071994  -0.40141198 -0.40778467  4.4757104   2.0469027
  5.3090477   3.564532  ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  4.6856384   1.9713781  -0.40141198 -0.40778467  4.5166254   2.091266
  5.2689266   3.5802782 ] 2 4
train:	 Loss = 1.5861,	 Acc = 0.3981,	 Loss Con1 = 0.3060,	 Loss Con2 = 0.2758
2922 0.157
5686 0.462
2909 0.509
376 0.428
69 0.522
6 0.333
0 0.0
0 0.0
0.47601149679416316
0.467175572519084
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5738,	 Acc = 0.4050,	 Loss Con1 = 0.2662,	 Loss Con2 = 0.2319
380 0.179
1718 0.47
815 0.393
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.43778625954198475
0.467175572519084

 ===== Epoch 146	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   1.80759     1.8057812
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  1.7241278   1.8237313
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 1
train:	 Loss = 1.5872,	 Acc = 0.3994,	 Loss Con1 = 0.3053,	 Loss Con2 = 0.2739
2925 0.159
5678 0.464
2913 0.506
377 0.448
69 0.493
6 0.167
0 0.0
0 0.0
0.4771646577463231
0.467175572519084
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5806,	 Acc = 0.4100,	 Loss Con1 = 0.2637,	 Loss Con2 = 0.2277
380 0.179
1718 0.471
815 0.404
85 0.271
2 0.0
0 0.0
0 0.0
0 0.0
0.4435114503816794
0.467175572519084

 ===== Epoch 147	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   2.9228194   2.8073611  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   2.9365516   2.8247805  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 4
train:	 Loss = 1.5867,	 Acc = 0.3991,	 Loss Con1 = 0.3060,	 Loss Con2 = 0.2734
2921 0.155
5682 0.462
2914 0.514
376 0.444
69 0.464
6 0.333
0 0.0
0 0.0
0.47783795733392287
0.467175572519084
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5551,	 Acc = 0.4160,	 Loss Con1 = 0.2661,	 Loss Con2 = 0.2328
380 0.189
1718 0.478
815 0.42
85 0.153
2 0.0
0 0.0
0 0.0
0 0.0
0.4488549618320611
0.467175572519084

 ===== Epoch 148	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 0
train:	 Loss = 1.5825,	 Acc = 0.4007,	 Loss Con1 = 0.3054,	 Loss Con2 = 0.2711
2925 0.157
5684 0.469
2907 0.506
377 0.43
69 0.522
6 0.333
0 0.0
0 0.0
0.4793763131704081
0.467175572519084
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5506,	 Acc = 0.4223,	 Loss Con1 = 0.2637,	 Loss Con2 = 0.2263
380 0.184
1718 0.483
815 0.431
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.4568702290076336
0.467175572519084

 ===== Epoch 149	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   2.399304    1.1923226  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   2.5732372   1.1102021  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 2
train:	 Loss = 1.5796,	 Acc = 0.4014,	 Loss Con1 = 0.3049,	 Loss Con2 = 0.2708
2922 0.157
5679 0.468
2915 0.51
377 0.435
69 0.493
6 0.167
0 0.0
0 0.0
0.48021224850762767
0.467175572519084
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5454,	 Acc = 0.4263,	 Loss Con1 = 0.2682,	 Loss Con2 = 0.2341
380 0.197
1718 0.483
815 0.432
85 0.259
2 0.0
0 0.0
0 0.0
0 0.0
0.4595419847328244
0.467175572519084

 ===== Epoch 150	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 0
train:	 Loss = 1.5847,	 Acc = 0.3999,	 Loss Con1 = 0.3056,	 Loss Con2 = 0.2729
2923 0.159
5678 0.464
2915 0.51
377 0.443
69 0.449
6 0.333
0 0.0
0 0.0
0.47783305693753453
0.467175572519084
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5675,	 Acc = 0.4157,	 Loss Con1 = 0.2694,	 Loss Con2 = 0.2329
380 0.176
1718 0.478
815 0.413
85 0.247
2 0.0
0 0.0
0 0.0
0 0.0
0.45038167938931295
0.467175572519084

 ===== Epoch 151	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   1.6089754   1.439045
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  1.6614741   1.5036582
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 4
train:	 Loss = 1.5838,	 Acc = 0.3996,	 Loss Con1 = 0.3054,	 Loss Con2 = 0.2726
2924 0.157
5677 0.464
2916 0.511
377 0.44
68 0.5
6 0.333
0 0.0
0 0.0
0.47821760283060594
0.467175572519084
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5482,	 Acc = 0.4203,	 Loss Con1 = 0.2625,	 Loss Con2 = 0.2242
380 0.187
1718 0.477
815 0.422
85 0.306
2 0.0
0 0.0
0 0.0
0 0.0
0.4541984732824427
0.467175572519084

 ===== Epoch 152	 =====
[-0.36602148 -0.3783333   0.6501943   0.8234415  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   0.46080795  1.0065758  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.5873,	 Acc = 0.4012,	 Loss Con1 = 0.3059,	 Loss Con2 = 0.2729
2921 0.16
5683 0.464
2913 0.515
376 0.434
69 0.464
6 0.333
0 0.0
0 0.0
0.4790538299988947
0.467175572519084
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5624,	 Acc = 0.4027,	 Loss Con1 = 0.2669,	 Loss Con2 = 0.2290
380 0.171
1718 0.466
815 0.4
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.4362595419847328
0.467175572519084

 ===== Epoch 153	 =====
[ 0.99946505  1.9201734  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.2149246   2.1683803 ] [ 1.6122108   2.1201537  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.1474687   2.3809521 ] 6 6
train:	 Loss = 1.5818,	 Acc = 0.4025,	 Loss Con1 = 0.3057,	 Loss Con2 = 0.2719
2923 0.161
5680 0.467
2913 0.515
377 0.422
69 0.478
6 0.333
0 0.0
0 0.0
0.4804864566058596
0.467175572519084
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5704,	 Acc = 0.4143,	 Loss Con1 = 0.2639,	 Loss Con2 = 0.2293
380 0.189
1718 0.478
815 0.4
85 0.271
2 0.0
0 0.0
0 0.0
0 0.0
0.44694656488549617
0.467175572519084

 ===== Epoch 154	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   1.7140111   1.9769248
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  1.714576    1.9770994
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 0
train:	 Loss = 1.5812,	 Acc = 0.4012,	 Loss Con1 = 0.3056,	 Loss Con2 = 0.2742
2924 0.159
5681 0.466
2911 0.512
377 0.446
69 0.478
6 0.167
0 0.0
0 0.0
0.47943387881468374
0.467175572519084
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5755,	 Acc = 0.4083,	 Loss Con1 = 0.2621,	 Loss Con2 = 0.2214
380 0.179
1718 0.476
815 0.395
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.4416030534351145
0.467175572519084

 ===== Epoch 155	 =====
[-0.36602148 -0.3783333   3.4599628   3.0139995   1.948911    1.270124
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  1.744757    1.4303082
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 2
train:	 Loss = 1.5816,	 Acc = 0.4010,	 Loss Con1 = 0.3051,	 Loss Con2 = 0.2707
2926 0.158
5677 0.465
2913 0.513
377 0.432
69 0.507
6 0.333
0 0.0
0 0.0
0.47953992479539925
0.467175572519084
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5655,	 Acc = 0.4133,	 Loss Con1 = 0.2637,	 Loss Con2 = 0.2247
380 0.192
1718 0.478
815 0.398
85 0.247
2 0.0
0 0.0
0 0.0
0 0.0
0.4454198473282443
0.467175572519084

 ===== Epoch 156	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  0.8044172   1.6057339
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  0.8229804   1.4578558
 -0.38555372 -0.3798593 ] 2 5
train:	 Loss = 1.5831,	 Acc = 0.4019,	 Loss Con1 = 0.3053,	 Loss Con2 = 0.2759
2923 0.157
5684 0.468
2909 0.512
377 0.44
69 0.507
6 0.333
0 0.0
0 0.0
0.48114980652294087
0.467175572519084
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5715,	 Acc = 0.4067,	 Loss Con1 = 0.2649,	 Loss Con2 = 0.2425
380 0.176
1718 0.473
815 0.395
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.4400763358778626
0.467175572519084

 ===== Epoch 157	 =====
[-0.36602148 -0.3783333   3.790572    2.289099    4.2860713   0.93672734
  2.638246    3.1517086  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   3.7720597   2.1730053   4.002188    0.8034987
  2.7938373   3.0149798  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 5
train:	 Loss = 1.5836,	 Acc = 0.3987,	 Loss Con1 = 0.3058,	 Loss Con2 = 0.2754
2927 0.16
5674 0.462
2915 0.509
377 0.443
69 0.478
6 0.333
0 0.0
0 0.0
0.47616414113483024
0.467175572519084
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5557,	 Acc = 0.4320,	 Loss Con1 = 0.2657,	 Loss Con2 = 0.2343
380 0.184
1718 0.49
815 0.447
85 0.247
2 0.0
0 0.0
0 0.0
0 0.0
0.4679389312977099
0.4679389312977099

 ===== Epoch 158	 =====
[-0.36602148 -0.3783333   1.2123104   1.2108102  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.2812097   1.3734915  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.5818,	 Acc = 0.4007,	 Loss Con1 = 0.3054,	 Loss Con2 = 0.2757
2923 0.158
5680 0.467
2913 0.508
377 0.44
69 0.493
6 0.333
0 0.0
0 0.0
0.4791597567716971
0.4679389312977099
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5497,	 Acc = 0.4210,	 Loss Con1 = 0.2655,	 Loss Con2 = 0.2478
380 0.184
1718 0.478
815 0.437
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.45534351145038165
0.4679389312977099

 ===== Epoch 159	 =====
[ 2.4950106   1.3809863  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.448099    1.5595319 ] [ 2.558816    1.3151699  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   2.1687284   3.3249695  -0.42342317 -0.42019257
  2.4419265   1.4781773 ] 2 2
train:	 Loss = 1.5824,	 Acc = 0.3990,	 Loss Con1 = 0.3053,	 Loss Con2 = 0.2763
2923 0.17
5679 0.457
2915 0.507
376 0.428
69 0.551
6 0.333
0 0.0
0 0.0
0.4728579325594251
0.4679389312977099
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5442,	 Acc = 0.4253,	 Loss Con1 = 0.2638,	 Loss Con2 = 0.2253
380 0.187
1718 0.485
815 0.433
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.4599236641221374
0.4679389312977099

 ===== Epoch 160	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  0.8263895   3.1732388
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  0.8207072   2.8996649
 -0.38555372 -0.3798593 ] 5 5
train:	 Loss = 1.5808,	 Acc = 0.4007,	 Loss Con1 = 0.3056,	 Loss Con2 = 0.2749
2925 0.157
5680 0.467
2911 0.508
377 0.44
69 0.522
6 0.333
0 0.0
0 0.0
0.4794868959416123
0.4679389312977099
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5427,	 Acc = 0.4340,	 Loss Con1 = 0.2642,	 Loss Con2 = 0.2289
380 0.184
1718 0.484
815 0.465
85 0.247
2 0.0
0 0.0
0 0.0
0 0.0
0.47022900763358777
0.47022900763358777

 ===== Epoch 161	 =====
[-0.36602148 -0.3783333   1.8215413   3.0275912  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.7994425   3.0155525  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.5816,	 Acc = 0.3991,	 Loss Con1 = 0.3058,	 Loss Con2 = 0.2733
2924 0.155
5683 0.463
2910 0.513
376 0.434
69 0.493
6 0.333
0 0.0
0 0.0
0.47821760283060594
0.47022900763358777
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5738,	 Acc = 0.4017,	 Loss Con1 = 0.2639,	 Loss Con2 = 0.2259
380 0.179
1718 0.461
815 0.401
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.43396946564885497
0.47022900763358777

 ===== Epoch 162	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   2.8021908   2.1102834
 -0.3640846  -0.3725409   2.1281068   1.4635694  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  3.1414683   2.039336
 -0.3640846  -0.3725409   2.1841762   1.3914028  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 2
train:	 Loss = 1.5841,	 Acc = 0.4008,	 Loss Con1 = 0.3054,	 Loss Con2 = 0.2730
2924 0.155
5681 0.466
2911 0.515
377 0.422
69 0.507
6 0.333
0 0.0
0 0.0
0.4802078726227333
0.47022900763358777
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5833,	 Acc = 0.3957,	 Loss Con1 = 0.2667,	 Loss Con2 = 0.2337
380 0.174
1718 0.458
815 0.391
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.42786259541984734
0.47022900763358777

 ===== Epoch 163	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 0
train:	 Loss = 1.5820,	 Acc = 0.3987,	 Loss Con1 = 0.3053,	 Loss Con2 = 0.2742
2925 0.157
5679 0.462
2914 0.513
375 0.419
69 0.536
6 0.333
0 0.0
0 0.0
0.47694349220391463
0.47022900763358777
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5598,	 Acc = 0.4153,	 Loss Con1 = 0.2690,	 Loss Con2 = 0.2316
380 0.187
1718 0.48
815 0.411
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.4484732824427481
0.47022900763358777

 ===== Epoch 164	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  0.8642734   1.5589057
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  0.7911579   1.8300151
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.5803,	 Acc = 0.3994,	 Loss Con1 = 0.3056,	 Loss Con2 = 0.2766
2923 0.157
5678 0.465
2917 0.509
375 0.419
69 0.493
6 0.333
0 0.0
0 0.0
0.477722498618021
0.47022900763358777
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5542,	 Acc = 0.4060,	 Loss Con1 = 0.2676,	 Loss Con2 = 0.2310
380 0.184
1718 0.465
815 0.405
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.4381679389312977
0.47022900763358777

 ===== Epoch 165	 =====
[ 1.959065    1.0468421  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.0530596   1.2498592 ] [ 1.9599372   1.1658177  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.9697322   1.34696   ] 2 4
train:	 Loss = 1.5779,	 Acc = 0.4035,	 Loss Con1 = 0.3055,	 Loss Con2 = 0.2741
2921 0.16
5683 0.47
2912 0.512
377 0.443
69 0.478
6 0.333
0 0.0
0 0.0
0.4822593124792749
0.47022900763358777
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5852,	 Acc = 0.3993,	 Loss Con1 = 0.2644,	 Loss Con2 = 0.2295
380 0.176
1718 0.46
815 0.401
85 0.165
2 0.0
0 0.0
0 0.0
0 0.0
0.4316793893129771
0.47022900763358777

 ===== Epoch 166	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  1.3233087   1.0728757  -0.40141198 -0.40778467  1.7147578   2.9809976
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.9314526   2.8084733
 -0.38555372 -0.3798593 ] 5 5
train:	 Loss = 1.5820,	 Acc = 0.4021,	 Loss Con1 = 0.3052,	 Loss Con2 = 0.2756
2923 0.156
5680 0.471
2914 0.51
376 0.418
69 0.507
6 0.333
0 0.0
0 0.0
0.48148148148148145
0.47022900763358777
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5576,	 Acc = 0.4227,	 Loss Con1 = 0.2667,	 Loss Con2 = 0.2329
380 0.174
1718 0.483
815 0.44
85 0.165
2 0.0
0 0.0
0 0.0
0 0.0
0.4587786259541985
0.47022900763358777

 ===== Epoch 167	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  3.2656631   1.6504843  -0.40141198 -0.40778467  3.5214243   2.5915859
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  3.3431776   1.7537283  -0.40141198 -0.40778467  3.1088722   2.670454
 -0.38555372 -0.3798593 ] 4 4
train:	 Loss = 1.5822,	 Acc = 0.4002,	 Loss Con1 = 0.3057,	 Loss Con2 = 0.2769
2924 0.157
5680 0.465
2913 0.512
376 0.444
69 0.464
6 0.333
0 0.0
0 0.0
0.4788810260946484
0.47022900763358777
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5535,	 Acc = 0.4233,	 Loss Con1 = 0.2642,	 Loss Con2 = 0.2263
380 0.171
1718 0.484
815 0.439
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.4599236641221374
0.47022900763358777

 ===== Epoch 168	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   1.5803281   1.4457129
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  1.4773337   1.5281084
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 2
train:	 Loss = 1.5827,	 Acc = 0.3984,	 Loss Con1 = 0.3062,	 Loss Con2 = 0.2773
2926 0.159
5680 0.461
2910 0.511
377 0.424
69 0.507
6 0.333
0 0.0
0 0.0
0.47600088476000885
0.47022900763358777
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5564,	 Acc = 0.4257,	 Loss Con1 = 0.2659,	 Loss Con2 = 0.2307
380 0.174
1718 0.479
815 0.45
85 0.247
2 0.0
0 0.0
0 0.0
0 0.0
0.4622137404580153
0.47022900763358777

 ===== Epoch 169	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 0
train:	 Loss = 1.5853,	 Acc = 0.3992,	 Loss Con1 = 0.3051,	 Loss Con2 = 0.2773
2921 0.156
5684 0.461
2912 0.516
377 0.443
68 0.471
6 0.333
0 0.0
0 0.0
0.47783795733392287
0.47022900763358777
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5665,	 Acc = 0.4057,	 Loss Con1 = 0.2700,	 Loss Con2 = 0.2526
380 0.182
1718 0.464
815 0.405
85 0.247
2 0.0
0 0.0
0 0.0
0 0.0
0.4381679389312977
0.47022900763358777

 ===== Epoch 170	 =====
[-0.36602148 -0.3783333   1.081123    0.8392987  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.1581488   0.9023899  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 4
train:	 Loss = 1.5815,	 Acc = 0.4027,	 Loss Con1 = 0.3049,	 Loss Con2 = 0.2760
2925 0.158
5680 0.468
2911 0.514
377 0.443
69 0.493
6 0.333
0 0.0
0 0.0
0.4819197169081057
0.47022900763358777
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5787,	 Acc = 0.4100,	 Loss Con1 = 0.2645,	 Loss Con2 = 0.2304
380 0.187
1718 0.472
815 0.402
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.44236641221374046
0.47022900763358777

 ===== Epoch 171	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  1.6563971   1.4133021  -0.40141198 -0.40778467  2.4387128   2.150417
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  1.675494    1.1789101  -0.40141198 -0.40778467  2.6637414   1.9212065
 -0.38555372 -0.3798593 ] 5 5
train:	 Loss = 1.5821,	 Acc = 0.4022,	 Loss Con1 = 0.3058,	 Loss Con2 = 0.2756
2924 0.157
5679 0.47
2914 0.509
376 0.441
69 0.478
6 0.333
0 0.0
0 0.0
0.48153471915081825
0.47022900763358777
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5558,	 Acc = 0.4140,	 Loss Con1 = 0.2649,	 Loss Con2 = 0.2289
380 0.187
1718 0.477
815 0.404
85 0.259
2 0.0
0 0.0
0 0.0
0 0.0
0.44694656488549617
0.47022900763358777

 ===== Epoch 172	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 0
train:	 Loss = 1.5834,	 Acc = 0.3995,	 Loss Con1 = 0.3053,	 Loss Con2 = 0.2784
2925 0.157
5681 0.465
2911 0.507
376 0.449
69 0.493
6 0.333
0 0.0
0 0.0
0.4780493199159571
0.47022900763358777
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5737,	 Acc = 0.3980,	 Loss Con1 = 0.2641,	 Loss Con2 = 0.2347
380 0.171
1718 0.458
815 0.398
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.4309160305343511
0.47022900763358777

 ===== Epoch 173	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.9753013   2.1141176  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 3 2
train:	 Loss = 1.5829,	 Acc = 0.3974,	 Loss Con1 = 0.3050,	 Loss Con2 = 0.2751
2923 0.154
5681 0.463
2912 0.505
377 0.443
69 0.522
6 0.333
0 0.0
0 0.0
0.4761746821448314
0.47022900763358777
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5641,	 Acc = 0.4213,	 Loss Con1 = 0.2661,	 Loss Con2 = 0.2344
380 0.174
1718 0.48
815 0.434
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.45725190839694657
0.47022900763358777

 ===== Epoch 174	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 0
train:	 Loss = 1.5778,	 Acc = 0.4013,	 Loss Con1 = 0.3055,	 Loss Con2 = 0.2776
2924 0.156
5677 0.465
2916 0.512
376 0.468
69 0.551
6 0.333
0 0.0
0 0.0
0.4805395842547545
0.47022900763358777
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5545,	 Acc = 0.4167,	 Loss Con1 = 0.2631,	 Loss Con2 = 0.2321
380 0.182
1718 0.47
815 0.434
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.45076335877862594
0.47022900763358777

 ===== Epoch 175	 =====
[ 1.9886448   0.93799216 -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   1.1720471   2.4564822  -0.42342317 -0.42019257
  2.847106    1.118642  ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   1.1051056   2.466436   -0.42342317 -0.42019257
  2.279678    1.0897741 ] 1 5
train:	 Loss = 1.5816,	 Acc = 0.4002,	 Loss Con1 = 0.3064,	 Loss Con2 = 0.2779
2926 0.159
5678 0.465
2912 0.507
377 0.443
69 0.551
6 0.333
0 0.0
0 0.0
0.4781021897810219
0.47022900763358777
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5618,	 Acc = 0.4013,	 Loss Con1 = 0.2661,	 Loss Con2 = 0.2347
380 0.166
1718 0.462
815 0.402
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.4354961832061069
0.47022900763358777

 ===== Epoch 176	 =====
[ 2.3177996   1.2721363  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.4791228   3.3235815
 -0.38555372 -0.3798593 ] 5 2
train:	 Loss = 1.5847,	 Acc = 0.4010,	 Loss Con1 = 0.3058,	 Loss Con2 = 0.2783
2926 0.157
5681 0.467
2911 0.509
375 0.451
69 0.522
6 0.333
0 0.0
0 0.0
0.480092899800929
0.47022900763358777
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5620,	 Acc = 0.4383,	 Loss Con1 = 0.2613,	 Loss Con2 = 0.2298
380 0.176
1718 0.502
815 0.444
85 0.271
2 0.0
0 0.0
0 0.0
0 0.0
0.4763358778625954
0.4763358778625954

 ===== Epoch 177	 =====
[ 2.3782468   2.6973116  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.8207098   2.955684  ] [ 2.4042294   2.704906   -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.955623    2.955684  ] 0 0
train:	 Loss = 1.5809,	 Acc = 0.4025,	 Loss Con1 = 0.3058,	 Loss Con2 = 0.2773
2922 0.157
5685 0.468
2910 0.514
377 0.432
68 0.529
6 0.333
0 0.0
0 0.0
0.48164934777802343
0.4763358778625954
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5699,	 Acc = 0.4050,	 Loss Con1 = 0.2649,	 Loss Con2 = 0.2422
380 0.179
1718 0.469
815 0.4
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.43778625954198475
0.4763358778625954

 ===== Epoch 178	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.8278722   3.103884   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 2
train:	 Loss = 1.5781,	 Acc = 0.4007,	 Loss Con1 = 0.3052,	 Loss Con2 = 0.2789
2926 0.158
5678 0.466
2912 0.511
377 0.44
69 0.507
6 0.333
0 0.0
0 0.0
0.4793187347931874
0.4763358778625954
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5570,	 Acc = 0.4227,	 Loss Con1 = 0.2646,	 Loss Con2 = 0.2341
380 0.179
1718 0.486
815 0.426
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.4580152671755725
0.4763358778625954

 ===== Epoch 179	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.2394452   2.1898513
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.1958792   2.2071035
 -0.38555372 -0.3798593 ] 5 4
train:	 Loss = 1.5793,	 Acc = 0.4025,	 Loss Con1 = 0.3052,	 Loss Con2 = 0.2774
2923 0.159
5679 0.468
2914 0.514
377 0.43
69 0.522
6 0.333
0 0.0
0 0.0
0.4812603648424544
0.4763358778625954
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5689,	 Acc = 0.4190,	 Loss Con1 = 0.2680,	 Loss Con2 = 0.2308
380 0.184
1718 0.481
815 0.412
85 0.282
2 0.0
0 0.0
0 0.0
0 0.0
0.4530534351145038
0.4763358778625954

 ===== Epoch 180	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.6639941   2.0814073
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.9185717   2.1528819
 -0.38555372 -0.3798593 ] 1 2
train:	 Loss = 1.5779,	 Acc = 0.4036,	 Loss Con1 = 0.3048,	 Loss Con2 = 0.2768
2921 0.16
5682 0.47
2914 0.511
376 0.434
69 0.522
6 0.333
0 0.0
0 0.0
0.4822593124792749
0.4763358778625954
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5633,	 Acc = 0.4287,	 Loss Con1 = 0.2656,	 Loss Con2 = 0.2357
380 0.189
1718 0.485
815 0.442
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.4633587786259542
0.4763358778625954

 ===== Epoch 181	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 3 0
train:	 Loss = 1.5823,	 Acc = 0.3978,	 Loss Con1 = 0.3058,	 Loss Con2 = 0.2789
2924 0.155
5679 0.462
2914 0.507
376 0.447
69 0.522
6 0.333
0 0.0
0 0.0
0.47622733303847853
0.4763358778625954
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5560,	 Acc = 0.4170,	 Loss Con1 = 0.2665,	 Loss Con2 = 0.2401
380 0.189
1718 0.481
815 0.413
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.45
0.4763358778625954

 ===== Epoch 182	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   1.0450302   1.3441213  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   1.0913736   1.4735234  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 6
train:	 Loss = 1.5796,	 Acc = 0.4029,	 Loss Con1 = 0.3051,	 Loss Con2 = 0.2801
2926 0.156
5678 0.47
2914 0.512
376 0.444
68 0.544
6 0.333
0 0.0
0 0.0
0.4828577748285777
0.4763358778625954
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5737,	 Acc = 0.4087,	 Loss Con1 = 0.2667,	 Loss Con2 = 0.2373
380 0.184
1718 0.464
815 0.413
85 0.247
2 0.0
0 0.0
0 0.0
0 0.0
0.44122137404580153
0.4763358778625954

 ===== Epoch 183	 =====
[ 2.8503802   1.9859898  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 1.8474222   2.1834385  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.5797,	 Acc = 0.4020,	 Loss Con1 = 0.3053,	 Loss Con2 = 0.2782
2923 0.159
5681 0.467
2912 0.513
377 0.432
69 0.507
6 0.333
0 0.0
0 0.0
0.480375898286346
0.4763358778625954
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5562,	 Acc = 0.4280,	 Loss Con1 = 0.2645,	 Loss Con2 = 0.2339
380 0.179
1718 0.487
815 0.44
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.46412213740458014
0.4763358778625954

 ===== Epoch 184	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   2.2063465   2.0680532
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  2.247512    2.0748997
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 0
train:	 Loss = 1.5784,	 Acc = 0.3996,	 Loss Con1 = 0.3058,	 Loss Con2 = 0.2782
2922 0.158
5684 0.462
2911 0.512
377 0.446
68 0.515
6 0.333
0 0.0
0 0.0
0.47766968826000444
0.4763358778625954
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5512,	 Acc = 0.4310,	 Loss Con1 = 0.2622,	 Loss Con2 = 0.2286
380 0.179
1718 0.487
815 0.452
85 0.247
2 0.0
0 0.0
0 0.0
0 0.0
0.4675572519083969
0.4763358778625954

 ===== Epoch 185	 =====
[-0.36602148 -0.3783333   1.9255159   1.0884831  -0.4409929  -0.3635196
  1.465981    3.757221   -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   2.0069802   1.0767881  -0.44088638 -0.36343393
  1.4609263   3.773963   -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 0
train:	 Loss = 1.5797,	 Acc = 0.4009,	 Loss Con1 = 0.3058,	 Loss Con2 = 0.2800
2927 0.159
5679 0.463
2913 0.517
374 0.433
69 0.522
6 0.333
0 0.0
0 0.0
0.4792611436787966
0.4763358778625954
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5814,	 Acc = 0.4053,	 Loss Con1 = 0.2647,	 Loss Con2 = 0.2320
380 0.174
1718 0.469
815 0.399
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.4389312977099237
0.4763358778625954

 ===== Epoch 186	 =====
[ 2.8833742   2.8567898  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  1.1800756   2.925688   -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 2.768918    2.9757652  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  1.1233442   3.0038185  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 4
train:	 Loss = 1.5801,	 Acc = 0.4033,	 Loss Con1 = 0.3062,	 Loss Con2 = 0.2824
2923 0.155
5682 0.471
2912 0.514
376 0.439
69 0.507
6 0.333
0 0.0
0 0.0
0.48347153123272524
0.4763358778625954
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5668,	 Acc = 0.4177,	 Loss Con1 = 0.2648,	 Loss Con2 = 0.2340
380 0.171
1718 0.484
815 0.412
85 0.247
2 0.0
0 0.0
0 0.0
0 0.0
0.4534351145038168
0.4763358778625954

 ===== Epoch 187	 =====
[ 1.8211735   2.16825    -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.1390338   2.3993225 ] [ 1.42869     2.1328106  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.1721013   2.3940737 ] 1 0
train:	 Loss = 1.5776,	 Acc = 0.4031,	 Loss Con1 = 0.3057,	 Loss Con2 = 0.2795
2924 0.16
5679 0.467
2913 0.517
377 0.432
69 0.507
6 0.333
0 0.0
0 0.0
0.4817558602388324
0.4763358778625954
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5610,	 Acc = 0.4173,	 Loss Con1 = 0.2632,	 Loss Con2 = 0.2271
380 0.184
1718 0.487
815 0.4
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.45114503816793894
0.4763358778625954

 ===== Epoch 188	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   3.1852882   2.2636461
 -0.3640846  -0.3725409   2.5383358   2.1852353  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  3.1773798  -4.8800187
 -0.3640846  -0.3725409   2.562938    1.9961091  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 5
train:	 Loss = 1.5769,	 Acc = 0.4018,	 Loss Con1 = 0.3061,	 Loss Con2 = 0.2801
2924 0.16
5678 0.465
2916 0.514
375 0.437
69 0.536
6 0.333
0 0.0
0 0.0
0.48009730207872625
0.4763358778625954
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5683,	 Acc = 0.4233,	 Loss Con1 = 0.2661,	 Loss Con2 = 0.2341
380 0.174
1718 0.485
815 0.432
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.4595419847328244
0.4763358778625954

 ===== Epoch 189	 =====
[ 2.468228    2.775785   -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.9679686   3.0212924 ] [ 2.5285347   3.0289245  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.8396684   3.3152192 ] 6 6
train:	 Loss = 1.5787,	 Acc = 0.4031,	 Loss Con1 = 0.3059,	 Loss Con2 = 0.2805
2923 0.159
5679 0.468
2915 0.515
376 0.436
69 0.507
6 0.333
0 0.0
0 0.0
0.4820342730790492
0.4763358778625954
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5622,	 Acc = 0.4167,	 Loss Con1 = 0.2678,	 Loss Con2 = 0.2436
380 0.179
1718 0.468
815 0.436
85 0.271
2 0.0
0 0.0
0 0.0
0 0.0
0.45114503816793894
0.4763358778625954

 ===== Epoch 190	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   1.3290042   2.7281787
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  1.3585218   2.8084002
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 4
train:	 Loss = 1.5806,	 Acc = 0.4013,	 Loss Con1 = 0.3055,	 Loss Con2 = 0.2798
2925 0.154
5678 0.464
2914 0.518
376 0.452
69 0.478
6 0.333
0 0.0
0 0.0
0.4812562202808802
0.4763358778625954
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5727,	 Acc = 0.4063,	 Loss Con1 = 0.2641,	 Loss Con2 = 0.2336
380 0.171
1718 0.469
815 0.405
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.4404580152671756
0.4763358778625954

 ===== Epoch 191	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   2.2013412   3.362297    2.8273978   0.80472875
  3.2549314   2.7483606 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   2.226515    3.2378721  -0.42342317 -0.42019257
  3.6094086   2.5462859 ] 5 5
train:	 Loss = 1.5757,	 Acc = 0.4033,	 Loss Con1 = 0.3051,	 Loss Con2 = 0.2775
2925 0.155
5677 0.473
2914 0.511
377 0.43
69 0.536
6 0.333
0 0.0
0 0.0
0.48368904124737366
0.4763358778625954
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5446,	 Acc = 0.4260,	 Loss Con1 = 0.2632,	 Loss Con2 = 0.2330
380 0.174
1718 0.487
815 0.444
85 0.153
2 0.0
0 0.0
0 0.0
0 0.0
0.46259541984732827
0.4763358778625954

 ===== Epoch 192	 =====
[-0.36602148 -0.3783333   2.3946233   1.8518937  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   2.6665504   1.8898914  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 2
train:	 Loss = 1.5788,	 Acc = 0.3995,	 Loss Con1 = 0.3064,	 Loss Con2 = 0.2806
2923 0.155
5681 0.466
2913 0.512
376 0.42
69 0.507
6 0.167
0 0.0
0 0.0
0.4786069651741294
0.4763358778625954
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5694,	 Acc = 0.4167,	 Loss Con1 = 0.2668,	 Loss Con2 = 0.2399
380 0.168
1718 0.481
815 0.421
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.45267175572519086
0.4763358778625954

 ===== Epoch 193	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.5800,	 Acc = 0.4004,	 Loss Con1 = 0.3060,	 Loss Con2 = 0.2802
2922 0.155
5682 0.467
2913 0.513
376 0.42
69 0.464
6 0.333
0 0.0
0 0.0
0.47977006411673667
0.4763358778625954
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5630,	 Acc = 0.4137,	 Loss Con1 = 0.2665,	 Loss Con2 = 0.2369
380 0.179
1718 0.48
815 0.409
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.44770992366412216
0.4763358778625954

 ===== Epoch 194	 =====
[ 3.511539    2.4517663  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.0451245   2.6801276 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.4075365   2.614519  ] 4 2
train:	 Loss = 1.5784,	 Acc = 0.4030,	 Loss Con1 = 0.3063,	 Loss Con2 = 0.2813
2924 0.158
5681 0.468
2911 0.513
377 0.451
69 0.536
6 0.333
0 0.0
0 0.0
0.4820875718708536
0.4763358778625954
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5713,	 Acc = 0.4180,	 Loss Con1 = 0.2629,	 Loss Con2 = 0.2348
380 0.189
1718 0.482
815 0.41
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.45114503816793894
0.4763358778625954

 ===== Epoch 195	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   3.4633489   2.0925024
 -0.3640846  -0.3725409   3.6809146   1.3043052  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  3.4700162   2.046004
 -0.3640846  -0.3725409   3.4234493   1.2719548  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 2
train:	 Loss = 1.5808,	 Acc = 0.4009,	 Loss Con1 = 0.3056,	 Loss Con2 = 0.2806
2923 0.154
5678 0.467
2916 0.51
376 0.452
69 0.522
6 0.333
0 0.0
0 0.0
0.48059701492537316
0.4763358778625954
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5736,	 Acc = 0.4157,	 Loss Con1 = 0.2627,	 Loss Con2 = 0.2316
380 0.184
1718 0.479
815 0.41
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.449236641221374
0.4763358778625954

 ===== Epoch 196	 =====
[ 3.108207    2.0619316  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 2
train:	 Loss = 1.5781,	 Acc = 0.4029,	 Loss Con1 = 0.3045,	 Loss Con2 = 0.2791
2924 0.158
5686 0.465
2906 0.518
377 0.456
69 0.551
6 0.333
0 0.0
0 0.0
0.4819770013268465
0.4763358778625954
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5567,	 Acc = 0.4167,	 Loss Con1 = 0.2610,	 Loss Con2 = 0.2325
380 0.179
1718 0.483
815 0.404
85 0.271
2 0.0
0 0.0
0 0.0
0 0.0
0.45114503816793894
0.4763358778625954

 ===== Epoch 197	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 0
train:	 Loss = 1.5792,	 Acc = 0.3997,	 Loss Con1 = 0.3058,	 Loss Con2 = 0.2810
2927 0.154
5679 0.464
2913 0.514
375 0.429
68 0.544
6 0.333
0 0.0
0 0.0
0.4792611436787966
0.4763358778625954
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5709,	 Acc = 0.4067,	 Loss Con1 = 0.2672,	 Loss Con2 = 0.2391
380 0.182
1718 0.464
815 0.409
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.43931297709923667
0.4763358778625954

 ===== Epoch 198	 =====
[-0.36602148 -0.3783333   1.0819353   3.004938   -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0817951   3.0064929  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 0
train:	 Loss = 1.5791,	 Acc = 0.4017,	 Loss Con1 = 0.3064,	 Loss Con2 = 0.2830
2924 0.155
5679 0.467
2914 0.514
376 0.441
69 0.522
6 0.333
0 0.0
0 0.0
0.48131357806280406
0.4763358778625954
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5631,	 Acc = 0.3940,	 Loss Con1 = 0.2641,	 Loss Con2 = 0.2384
380 0.182
1718 0.438
815 0.417
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.4248091603053435
0.4763358778625954

 ===== Epoch 199	 =====
[ 2.826073    3.2845953  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.4163547   3.5382886 ] [ 2.8503723   3.2845953  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.6090252   3.5382886 ] 0 0
train:	 Loss = 1.5792,	 Acc = 0.4018,	 Loss Con1 = 0.3055,	 Loss Con2 = 0.2799
2923 0.159
5684 0.468
2912 0.511
374 0.43
69 0.478
6 0.333
0 0.0
0 0.0
0.48015478164731895
0.4763358778625954
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5787,	 Acc = 0.4037,	 Loss Con1 = 0.2633,	 Loss Con2 = 0.2369
380 0.176
1718 0.467
815 0.396
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.4366412213740458
0.4763358778625954

 ===== Epoch 200	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  4.0818124   1.265412   -0.40141198 -0.40778467  2.7357194   2.4609606
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  3.836349    1.2123947  -0.40141198 -0.40778467  2.449699    2.4018095
 -0.38555372 -0.3798593 ] 2 5
train:	 Loss = 1.5795,	 Acc = 0.4010,	 Loss Con1 = 0.3064,	 Loss Con2 = 0.2812
2921 0.16
5682 0.465
2913 0.513
377 0.422
69 0.536
6 0.333
0 0.0
0 0.0
0.4789432961202609
0.4763358778625954
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5443,	 Acc = 0.4313,	 Loss Con1 = 0.2650,	 Loss Con2 = 0.2360
380 0.187
1718 0.489
815 0.448
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.466793893129771
0.4763358778625954

 ===== Epoch 201	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.5777,	 Acc = 0.3972,	 Loss Con1 = 0.3057,	 Loss Con2 = 0.2812
2928 0.157
5680 0.461
2911 0.508
375 0.432
68 0.515
6 0.333
0 0.0
0 0.0
0.47511061946902655
0.4763358778625954
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5588,	 Acc = 0.4327,	 Loss Con1 = 0.2657,	 Loss Con2 = 0.2404
380 0.174
1718 0.493
815 0.449
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.47022900763358777
0.4763358778625954

 ===== Epoch 202	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   1.7893952   2.7600794   4.040049    1.4060987
  3.3867593   3.147261  ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   1.7482007   2.5933497   4.118848    1.2508268
  3.3104832   2.9688056 ] 5 5
train:	 Loss = 1.5807,	 Acc = 0.4009,	 Loss Con1 = 0.3048,	 Loss Con2 = 0.2803
2922 0.163
5681 0.466
2915 0.506
376 0.434
68 0.5
6 0.333
0 0.0
0 0.0
0.4778907804554499
0.4763358778625954
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5646,	 Acc = 0.4143,	 Loss Con1 = 0.2634,	 Loss Con2 = 0.2350
380 0.189
1718 0.48
815 0.405
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.44694656488549617
0.4763358778625954

 ===== Epoch 203	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   1.8824522   0.96562165
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  1.999955    0.9613124
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 0
train:	 Loss = 1.5783,	 Acc = 0.4012,	 Loss Con1 = 0.3050,	 Loss Con2 = 0.2818
2926 0.158
5676 0.47
2915 0.508
376 0.42
69 0.493
6 0.333
0 0.0
0 0.0
0.47998230479982307
0.4763358778625954
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5738,	 Acc = 0.4107,	 Loss Con1 = 0.2637,	 Loss Con2 = 0.2353
380 0.176
1718 0.47
815 0.409
85 0.282
2 0.0
0 0.0
0 0.0
0 0.0
0.4446564885496183
0.4763358778625954

 ===== Epoch 204	 =====
[ 2.533135    3.0289245  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.0878906   3.2915998 ] [ 2.873193    2.9074175  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.0636413   3.1761289 ] 5 5
train:	 Loss = 1.5761,	 Acc = 0.4028,	 Loss Con1 = 0.3055,	 Loss Con2 = 0.2831
2924 0.159
5683 0.468
2909 0.512
377 0.44
69 0.536
6 0.333
0 0.0
0 0.0
0.48153471915081825
0.4763358778625954
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5665,	 Acc = 0.4097,	 Loss Con1 = 0.2655,	 Loss Con2 = 0.2374
380 0.176
1718 0.469
815 0.409
85 0.271
2 0.0
0 0.0
0 0.0
0 0.0
0.4435114503816794
0.4763358778625954

 ===== Epoch 205	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   1.2658799   1.956293   -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   1.2693129   2.2076318  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 6
train:	 Loss = 1.5780,	 Acc = 0.4028,	 Loss Con1 = 0.3052,	 Loss Con2 = 0.2841
2924 0.162
5682 0.466
2911 0.514
376 0.444
69 0.507
6 0.333
0 0.0
0 0.0
0.4806501547987616
0.4763358778625954
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5669,	 Acc = 0.4153,	 Loss Con1 = 0.2688,	 Loss Con2 = 0.2331
380 0.179
1718 0.472
815 0.426
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.449618320610687
0.4763358778625954

 ===== Epoch 206	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 3 0
train:	 Loss = 1.5822,	 Acc = 0.4000,	 Loss Con1 = 0.3055,	 Loss Con2 = 0.2814
2927 0.159
5684 0.466
2906 0.507
376 0.441
69 0.507
6 0.333
0 0.0
0 0.0
0.47804446410795265
0.4763358778625954
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5873,	 Acc = 0.4100,	 Loss Con1 = 0.2638,	 Loss Con2 = 0.2336
380 0.176
1718 0.474
815 0.404
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.4438931297709924
0.4763358778625954

 ===== Epoch 207	 =====
[-0.36602148 -0.3783333   2.1281867   1.3082186  -0.4409929  -0.3635196
  1.4715981   3.6037502  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   2.196647    1.513916   -0.44088638 -0.36343393
  1.4710356   3.7823343  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 6
train:	 Loss = 1.5781,	 Acc = 0.4025,	 Loss Con1 = 0.3053,	 Loss Con2 = 0.2796
2926 0.157
5681 0.474
2911 0.505
375 0.421
69 0.522
6 0.333
0 0.0
0 0.0
0.4820836098208361
0.4763358778625954
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5624,	 Acc = 0.4087,	 Loss Con1 = 0.2662,	 Loss Con2 = 0.2362
380 0.184
1718 0.466
815 0.41
85 0.259
2 0.0
0 0.0
0 0.0
0 0.0
0.44122137404580153
0.4763358778625954

 ===== Epoch 208	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.1771905   1.4726437
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.1980268   1.2483623
 -0.38555372 -0.3798593 ] 5 5
train:	 Loss = 1.5776,	 Acc = 0.4022,	 Loss Con1 = 0.3052,	 Loss Con2 = 0.2804
2925 0.157
5681 0.468
2914 0.511
373 0.442
69 0.58
6 0.333
0 0.0
0 0.0
0.4816985513656972
0.4763358778625954
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5634,	 Acc = 0.4273,	 Loss Con1 = 0.2670,	 Loss Con2 = 0.2395
380 0.179
1718 0.494
815 0.42
85 0.282
2 0.0
0 0.0
0 0.0
0 0.0
0.4633587786259542
0.4763358778625954

 ===== Epoch 209	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   1.2813276   2.391781   -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   1.2092375   2.346988   -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 2
train:	 Loss = 1.5767,	 Acc = 0.4052,	 Loss Con1 = 0.3058,	 Loss Con2 = 0.2832
2923 0.158
5683 0.469
2910 0.52
377 0.451
69 0.522
6 0.333
0 0.0
0 0.0
0.48524046434494195
0.4763358778625954
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5483,	 Acc = 0.4147,	 Loss Con1 = 0.2624,	 Loss Con2 = 0.2323
380 0.176
1718 0.475
815 0.423
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.449236641221374
0.4763358778625954

 ===== Epoch 210	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.227576    2.379628
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.1215024   2.1676695
 -0.38555372 -0.3798593 ] 5 5
train:	 Loss = 1.5747,	 Acc = 0.4038,	 Loss Con1 = 0.3056,	 Loss Con2 = 0.2812
2922 0.158
5681 0.469
2915 0.516
375 0.443
69 0.551
6 0.333
0 0.0
0 0.0
0.4833075392438647
0.4763358778625954
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5507,	 Acc = 0.4440,	 Loss Con1 = 0.2636,	 Loss Con2 = 0.2403
380 0.192
1718 0.503
815 0.46
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.4805343511450382
0.4805343511450382

 ===== Epoch 211	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 0
train:	 Loss = 1.5740,	 Acc = 0.4037,	 Loss Con1 = 0.3056,	 Loss Con2 = 0.2828
2924 0.154
5680 0.472
2913 0.517
376 0.423
69 0.507
6 0.333
0 0.0
0 0.0
0.4845201238390093
0.4805343511450382
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5651,	 Acc = 0.4163,	 Loss Con1 = 0.2650,	 Loss Con2 = 0.2431
380 0.179
1718 0.478
815 0.415
85 0.247
2 0.0
0 0.0
0 0.0
0 0.0
0.45076335877862594
0.4805343511450382

 ===== Epoch 212	 =====
[ 2.2926404   0.88483286 -0.4208693  -0.3341335   3.821619   -4.2553697
 -0.3640846  -0.3725409   4.318861    2.1802583  -0.42342317 -0.42019257
  2.743497    0.95593256] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 2
train:	 Loss = 1.5775,	 Acc = 0.4012,	 Loss Con1 = 0.3056,	 Loss Con2 = 0.2823
2924 0.158
5678 0.468
2916 0.507
376 0.449
68 0.529
6 0.333
0 0.0
0 0.0
0.47998673153471916
0.4805343511450382
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5704,	 Acc = 0.4063,	 Loss Con1 = 0.2614,	 Loss Con2 = 0.2340
380 0.179
1718 0.466
815 0.404
85 0.247
2 0.0
0 0.0
0 0.0
0 0.0
0.43931297709923667
0.4805343511450382

 ===== Epoch 213	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   1.8121727   1.4746072
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  1.9182001   1.6370219
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.5765,	 Acc = 0.4039,	 Loss Con1 = 0.3055,	 Loss Con2 = 0.2813
2924 0.158
5679 0.469
2913 0.517
377 0.44
69 0.536
6 0.333
0 0.0
0 0.0
0.4834144183989385
0.4805343511450382
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5584,	 Acc = 0.4237,	 Loss Con1 = 0.2662,	 Loss Con2 = 0.2355
380 0.176
1718 0.48
815 0.439
85 0.247
2 0.0
0 0.0
0 0.0
0 0.0
0.4595419847328244
0.4805343511450382

 ===== Epoch 214	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.0636413   2.6748788 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.215308    2.6066458 ] 3 2
train:	 Loss = 1.5791,	 Acc = 0.4009,	 Loss Con1 = 0.3049,	 Loss Con2 = 0.2810
2922 0.157
5684 0.469
2910 0.508
377 0.422
69 0.522
6 0.333
0 0.0
0 0.0
0.4795489719212912
0.4805343511450382
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5599,	 Acc = 0.4287,	 Loss Con1 = 0.2628,	 Loss Con2 = 0.2326
380 0.179
1718 0.487
815 0.442
85 0.259
2 0.0
0 0.0
0 0.0
0 0.0
0.4648854961832061
0.4805343511450382

 ===== Epoch 215	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   0.9895322   3.2901306  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   0.8075886   3.2403603  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 3
train:	 Loss = 1.5784,	 Acc = 0.4040,	 Loss Con1 = 0.3059,	 Loss Con2 = 0.2805
2919 0.156
5682 0.47
2916 0.516
376 0.447
69 0.522
6 0.333
0 0.0
0 0.0
0.4840313846833904
0.4805343511450382
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5696,	 Acc = 0.4247,	 Loss Con1 = 0.2638,	 Loss Con2 = 0.2354
380 0.184
1718 0.489
815 0.422
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.4595419847328244
0.4805343511450382

 ===== Epoch 216	 =====
[-0.36602148 -0.3783333   1.8507842   1.7952608  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   2.1864936   1.7313476  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 2
train:	 Loss = 1.5775,	 Acc = 0.4051,	 Loss Con1 = 0.3045,	 Loss Con2 = 0.2800
2923 0.157
5681 0.472
2912 0.515
377 0.446
69 0.522
6 0.333
0 0.0
0 0.0
0.48524046434494195
0.4805343511450382
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5600,	 Acc = 0.4250,	 Loss Con1 = 0.2693,	 Loss Con2 = 0.2371
380 0.184
1718 0.494
815 0.417
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.4599236641221374
0.4805343511450382

 ===== Epoch 217	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.15484     1.5096132
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.0665706   1.563835
 -0.38555372 -0.3798593 ] 2 4
train:	 Loss = 1.5763,	 Acc = 0.4020,	 Loss Con1 = 0.3049,	 Loss Con2 = 0.2789
2923 0.156
5679 0.465
2915 0.519
376 0.441
69 0.507
6 0.333
0 0.0
0 0.0
0.481592039800995
0.4805343511450382
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5584,	 Acc = 0.4173,	 Loss Con1 = 0.2658,	 Loss Con2 = 0.2429
380 0.168
1718 0.483
815 0.417
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.4534351145038168
0.4805343511450382

 ===== Epoch 218	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   1.3551351   2.7998955  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   1.2916267   3.105981   -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.5781,	 Acc = 0.4049,	 Loss Con1 = 0.3037,	 Loss Con2 = 0.2806
2924 0.157
5680 0.47
2913 0.516
377 0.459
68 0.544
6 0.333
0 0.0
0 0.0
0.4849624060150376
0.4805343511450382
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5579,	 Acc = 0.4293,	 Loss Con1 = 0.2664,	 Loss Con2 = 0.2376
380 0.174
1718 0.488
815 0.449
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.466412213740458
0.4805343511450382

 ===== Epoch 219	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.5795,	 Acc = 0.4038,	 Loss Con1 = 0.3052,	 Loss Con2 = 0.2791
2924 0.159
5681 0.473
2913 0.508
376 0.441
68 0.529
6 0.333
0 0.0
0 0.0
0.4830827067669173
0.4805343511450382
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5630,	 Acc = 0.4220,	 Loss Con1 = 0.2641,	 Loss Con2 = 0.2363
380 0.182
1718 0.481
815 0.429
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.4568702290076336
0.4805343511450382

 ===== Epoch 220	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   1.8666339   3.2204523  -0.42342317 -0.42019257
  2.435755    1.1711289 ] [ 3.107818    0.95571196 -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   1.9421588   3.1856134  -0.42342317 -0.42019257
  2.3171546   1.118642  ] 2 4
train:	 Loss = 1.5772,	 Acc = 0.4051,	 Loss Con1 = 0.3059,	 Loss Con2 = 0.2810
2924 0.157
5681 0.47
2912 0.521
376 0.436
69 0.507
6 0.333
0 0.0
0 0.0
0.4851835471030517
0.4805343511450382
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5811,	 Acc = 0.4130,	 Loss Con1 = 0.2681,	 Loss Con2 = 0.2355
380 0.182
1718 0.474
815 0.411
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.44656488549618323
0.4805343511450382

 ===== Epoch 221	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 0
train:	 Loss = 1.5781,	 Acc = 0.4002,	 Loss Con1 = 0.3055,	 Loss Con2 = 0.2819
2925 0.155
5681 0.466
2913 0.509
375 0.445
68 0.5
6 0.333
0 0.0
0 0.0
0.4794868959416123
0.4805343511450382
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5559,	 Acc = 0.4283,	 Loss Con1 = 0.2716,	 Loss Con2 = 0.2433
380 0.192
1718 0.485
815 0.438
85 0.247
2 0.0
0 0.0
0 0.0
0 0.0
0.46259541984732827
0.4805343511450382

 ===== Epoch 222	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  3.3926075   1.0393912   2.9760287   3.3473663   3.4998305   1.4011693
  3.6953828   3.115769  ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  3.255552    1.1063603  -0.40141198 -0.40778467  3.9407947   1.4775729
  4.2222486   3.1892505 ] 2 2
train:	 Loss = 1.5762,	 Acc = 0.4022,	 Loss Con1 = 0.3058,	 Loss Con2 = 0.2815
2922 0.158
5683 0.468
2912 0.511
376 0.457
69 0.478
6 0.333
0 0.0
0 0.0
0.48120716338713243
0.4805343511450382
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5687,	 Acc = 0.4227,	 Loss Con1 = 0.2677,	 Loss Con2 = 0.2426
380 0.182
1718 0.484
815 0.426
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.45763358778625957
0.4805343511450382

 ===== Epoch 223	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  0.57256943  0.80226415
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  0.5308982   0.8490921
 -0.38555372 -0.3798593 ] 4 4
train:	 Loss = 1.5736,	 Acc = 0.4025,	 Loss Con1 = 0.3054,	 Loss Con2 = 0.2820
2919 0.159
5688 0.468
2911 0.513
375 0.427
69 0.551
6 0.333
0 0.0
0 0.0
0.48104762957232844
0.4805343511450382
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5572,	 Acc = 0.4387,	 Loss Con1 = 0.2655,	 Loss Con2 = 0.2447
380 0.189
1718 0.497
815 0.45
85 0.271
2 0.0
0 0.0
0 0.0
0 0.0
0.47480916030534354
0.4805343511450382

 ===== Epoch 224	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 0
train:	 Loss = 1.5743,	 Acc = 0.4037,	 Loss Con1 = 0.3054,	 Loss Con2 = 0.2827
2925 0.157
5679 0.469
2915 0.517
374 0.433
69 0.536
6 0.333
0 0.0
0 0.0
0.4834678757049652
0.4805343511450382
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5627,	 Acc = 0.4303,	 Loss Con1 = 0.2668,	 Loss Con2 = 0.2386
380 0.192
1718 0.485
815 0.448
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.4648854961832061
0.4805343511450382

 ===== Epoch 225	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   0.8244471   0.9278367
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  0.811069    0.8190578
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 5
train:	 Loss = 1.5753,	 Acc = 0.4028,	 Loss Con1 = 0.3044,	 Loss Con2 = 0.2789
2921 0.159
5682 0.467
2915 0.514
375 0.453
69 0.522
6 0.333
0 0.0
0 0.0
0.4817066430861059
0.4805343511450382
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5737,	 Acc = 0.4060,	 Loss Con1 = 0.2686,	 Loss Con2 = 0.2415
380 0.182
1718 0.466
815 0.404
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.4385496183206107
0.4805343511450382

 ===== Epoch 226	 =====
[ 1.2468503   1.7227246  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   3.7341256   3.3324351  -0.42342317 -0.42019257
  1.5182588   1.8088448 ] [ 1.4085377   1.7151304  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   3.7661655   3.3224812  -0.42342317 -0.42019257
  1.5971785   1.7904744 ] 2 2
train:	 Loss = 1.5750,	 Acc = 0.4030,	 Loss Con1 = 0.3048,	 Loss Con2 = 0.2818
2922 0.158
5683 0.47
2914 0.51
374 0.447
69 0.522
6 0.333
0 0.0
0 0.0
0.4820915321689144
0.4805343511450382
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5699,	 Acc = 0.4160,	 Loss Con1 = 0.2651,	 Loss Con2 = 0.2441
380 0.184
1718 0.476
815 0.411
85 0.306
2 0.0
0 0.0
0 0.0
0 0.0
0.449618320610687
0.4805343511450382

 ===== Epoch 227	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 0
train:	 Loss = 1.5755,	 Acc = 0.4024,	 Loss Con1 = 0.3065,	 Loss Con2 = 0.2832
2924 0.157
5678 0.465
2915 0.519
376 0.449
69 0.507
6 0.333
0 0.0
0 0.0
0.48186643078283947
0.4805343511450382
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5598,	 Acc = 0.4247,	 Loss Con1 = 0.2615,	 Loss Con2 = 0.2334
380 0.184
1718 0.488
815 0.422
85 0.259
2 0.0
0 0.0
0 0.0
0 0.0
0.4595419847328244
0.4805343511450382

 ===== Epoch 228	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.315466    2.4165974
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.2548522   2.670454
 -0.38555372 -0.3798593 ] 4 6
train:	 Loss = 1.5776,	 Acc = 0.4046,	 Loss Con1 = 0.3048,	 Loss Con2 = 0.2803
2924 0.157
5682 0.474
2911 0.513
376 0.431
69 0.493
6 0.333
0 0.0
0 0.0
0.4845201238390093
0.4805343511450382
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5647,	 Acc = 0.4137,	 Loss Con1 = 0.2648,	 Loss Con2 = 0.2356
380 0.197
1718 0.477
815 0.4
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.4450381679389313
0.4805343511450382

 ===== Epoch 229	 =====
[-0.36602148 -0.3783333   3.1354473  -4.6518197  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   3.0901532   2.5331264   2.2081628   0.74793047
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 2
train:	 Loss = 1.5749,	 Acc = 0.4013,	 Loss Con1 = 0.3041,	 Loss Con2 = 0.2815
2927 0.159
5682 0.464
2909 0.514
377 0.446
67 0.567
6 0.333
0 0.0
0 0.0
0.4799247870810751
0.4805343511450382
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5615,	 Acc = 0.4183,	 Loss Con1 = 0.2680,	 Loss Con2 = 0.2470
380 0.189
1718 0.483
815 0.41
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.45152671755725193
0.4805343511450382

 ===== Epoch 230	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.17757     0.84416294
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.111274    1.0610504
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.5756,	 Acc = 0.4001,	 Loss Con1 = 0.3047,	 Loss Con2 = 0.2820
2923 0.159
5684 0.465
2912 0.508
374 0.439
69 0.536
6 0.333
0 0.0
0 0.0
0.47783305693753453
0.4805343511450382
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5628,	 Acc = 0.4210,	 Loss Con1 = 0.2654,	 Loss Con2 = 0.2343
380 0.168
1718 0.476
815 0.44
85 0.271
2 0.0
0 0.0
0 0.0
0 0.0
0.45763358778625957
0.4805343511450382

 ===== Epoch 231	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.6276464   3.0472612  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 2
train:	 Loss = 1.5765,	 Acc = 0.4061,	 Loss Con1 = 0.3059,	 Loss Con2 = 0.2831
2926 0.156
5681 0.471
2909 0.522
377 0.44
69 0.551
6 0.333
0 0.0
0 0.0
0.4869497898694979
0.4805343511450382
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5705,	 Acc = 0.4220,	 Loss Con1 = 0.2651,	 Loss Con2 = 0.2374
380 0.182
1718 0.489
815 0.41
85 0.271
2 0.0
0 0.0
0 0.0
0 0.0
0.4568702290076336
0.4805343511450382

 ===== Epoch 232	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 0
train:	 Loss = 1.5778,	 Acc = 0.4030,	 Loss Con1 = 0.3058,	 Loss Con2 = 0.2836
2923 0.159
5684 0.469
2911 0.512
375 0.443
69 0.493
6 0.333
0 0.0
0 0.0
0.48192371475953566
0.4805343511450382
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5608,	 Acc = 0.4243,	 Loss Con1 = 0.2633,	 Loss Con2 = 0.2464
380 0.182
1718 0.481
815 0.443
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.4595419847328244
0.4805343511450382

 ===== Epoch 233	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 0
train:	 Loss = 1.5776,	 Acc = 0.4036,	 Loss Con1 = 0.3062,	 Loss Con2 = 0.2815
2925 0.156
5678 0.468
2916 0.518
374 0.452
69 0.478
6 0.333
0 0.0
0 0.0
0.4835784584761694
0.4805343511450382
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5677,	 Acc = 0.4287,	 Loss Con1 = 0.2662,	 Loss Con2 = 0.2400
380 0.195
1718 0.491
815 0.423
85 0.271
2 0.0
0 0.0
0 0.0
0 0.0
0.46259541984732827
0.4805343511450382

 ===== Epoch 234	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.4950334   0.91810185
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.6522503   1.1054137
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.5776,	 Acc = 0.4052,	 Loss Con1 = 0.3061,	 Loss Con2 = 0.2823
2922 0.156
5679 0.47
2915 0.522
377 0.443
69 0.478
6 0.333
0 0.0
0 0.0
0.48562900729604247
0.4805343511450382
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5624,	 Acc = 0.4127,	 Loss Con1 = 0.2651,	 Loss Con2 = 0.2439
380 0.179
1718 0.478
815 0.406
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.44656488549618323
0.4805343511450382

 ===== Epoch 235	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 0
train:	 Loss = 1.5778,	 Acc = 0.4040,	 Loss Con1 = 0.3046,	 Loss Con2 = 0.2807
2920 0.158
5681 0.471
2915 0.513
377 0.435
69 0.536
6 0.333
0 0.0
0 0.0
0.48353227232537577
0.4805343511450382
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5655,	 Acc = 0.4247,	 Loss Con1 = 0.2639,	 Loss Con2 = 0.2365
380 0.179
1718 0.487
815 0.429
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.46030534351145036
0.4805343511450382

 ===== Epoch 236	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 0
train:	 Loss = 1.5738,	 Acc = 0.4045,	 Loss Con1 = 0.3055,	 Loss Con2 = 0.2818
2922 0.158
5678 0.473
2916 0.508
377 0.456
69 0.536
6 0.333
0 0.0
0 0.0
0.4839708158302012
0.4805343511450382
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5519,	 Acc = 0.4327,	 Loss Con1 = 0.2638,	 Loss Con2 = 0.2323
380 0.182
1718 0.497
815 0.432
85 0.282
2 0.0
0 0.0
0 0.0
0 0.0
0.46908396946564884
0.4805343511450382

 ===== Epoch 237	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.590121    3.0869768
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.5632243   3.040149
 -0.38555372 -0.3798593 ] 4 3
train:	 Loss = 1.5749,	 Acc = 0.4048,	 Loss Con1 = 0.3052,	 Loss Con2 = 0.2823
2924 0.158
5677 0.468
2916 0.52
376 0.457
69 0.507
6 0.333
0 0.0
0 0.0
0.48463069438301637
0.4805343511450382
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5522,	 Acc = 0.4377,	 Loss Con1 = 0.2700,	 Loss Con2 = 0.2463
380 0.179
1718 0.49
815 0.467
85 0.271
2 0.0
0 0.0
0 0.0
0 0.0
0.4751908396946565
0.4805343511450382

 ===== Epoch 238	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   1.5868211   1.9924834
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  1.7168682   1.9770994
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 3
train:	 Loss = 1.5759,	 Acc = 0.4022,	 Loss Con1 = 0.3056,	 Loss Con2 = 0.2822
2922 0.155
5678 0.467
2917 0.516
377 0.44
68 0.529
6 0.333
0 0.0
0 0.0
0.48187043997346896
0.4805343511450382
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5598,	 Acc = 0.4277,	 Loss Con1 = 0.2663,	 Loss Con2 = 0.2434
380 0.187
1718 0.486
815 0.431
85 0.306
2 0.0
0 0.0
0 0.0
0 0.0
0.46259541984732827
0.4805343511450382

 ===== Epoch 239	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   3.3244681   1.4585924  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   3.3256109   1.4585924  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.5751,	 Acc = 0.4033,	 Loss Con1 = 0.3064,	 Loss Con2 = 0.2830
2922 0.154
5684 0.47
2911 0.515
376 0.444
69 0.507
6 0.333
0 0.0
0 0.0
0.4837497236347557
0.4805343511450382
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5783,	 Acc = 0.4050,	 Loss Con1 = 0.2658,	 Loss Con2 = 0.2387
380 0.189
1718 0.464
815 0.4
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.4362595419847328
0.4805343511450382

 ===== Epoch 240	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   0.08797286  2.3134298  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 2
train:	 Loss = 1.5761,	 Acc = 0.4057,	 Loss Con1 = 0.3047,	 Loss Con2 = 0.2834
2920 0.158
5687 0.472
2911 0.518
376 0.436
68 0.515
6 0.333
0 0.0
0 0.0
0.48574270557029176
0.4805343511450382
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5967,	 Acc = 0.4020,	 Loss Con1 = 0.2669,	 Loss Con2 = 0.2387
380 0.174
1718 0.46
815 0.407
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.4351145038167939
0.4805343511450382

 ===== Epoch 241	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 2.069085    2.4973314  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 2
train:	 Loss = 1.5797,	 Acc = 0.4009,	 Loss Con1 = 0.3039,	 Loss Con2 = 0.2813
2925 0.154
5681 0.471
2911 0.506
377 0.44
68 0.515
6 0.333
0 0.0
0 0.0
0.48081388919606327
0.4805343511450382
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5695,	 Acc = 0.4310,	 Loss Con1 = 0.2702,	 Loss Con2 = 0.2421
380 0.189
1718 0.49
815 0.438
85 0.259
2 0.0
0 0.0
0 0.0
0 0.0
0.46603053435114505
0.4805343511450382

 ===== Epoch 242	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  3.4864106   2.4680655  -0.40141198 -0.40778467  3.1619089   3.2028143
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  3.2274675   2.5573578  -0.40141198 -0.40778467  3.3380673   3.2397838
 -0.38555372 -0.3798593 ] 4 4
train:	 Loss = 1.5802,	 Acc = 0.4022,	 Loss Con1 = 0.3056,	 Loss Con2 = 0.2822
2928 0.155
5675 0.466
2915 0.517
375 0.453
69 0.551
6 0.333
0 0.0
0 0.0
0.48219026548672567
0.4805343511450382
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5655,	 Acc = 0.4257,	 Loss Con1 = 0.2673,	 Loss Con2 = 0.2406
380 0.182
1718 0.491
815 0.422
85 0.247
2 0.0
0 0.0
0 0.0
0 0.0
0.46106870229007635
0.4805343511450382

 ===== Epoch 243	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   1.47128     2.237494   -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 2
train:	 Loss = 1.5747,	 Acc = 0.4029,	 Loss Con1 = 0.3059,	 Loss Con2 = 0.2826
2923 0.159
5681 0.466
2913 0.516
377 0.448
68 0.544
6 0.333
0 0.0
0 0.0
0.4818131564400221
0.4805343511450382
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5633,	 Acc = 0.4257,	 Loss Con1 = 0.2641,	 Loss Con2 = 0.2385
380 0.174
1718 0.488
815 0.432
85 0.247
2 0.0
0 0.0
0 0.0
0 0.0
0.4622137404580153
0.4805343511450382

 ===== Epoch 244	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.5768,	 Acc = 0.4020,	 Loss Con1 = 0.3056,	 Loss Con2 = 0.2841
2922 0.158
5685 0.466
2910 0.513
376 0.455
69 0.522
6 0.333
0 0.0
0 0.0
0.4808755250939642
0.4805343511450382
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5656,	 Acc = 0.4157,	 Loss Con1 = 0.2658,	 Loss Con2 = 0.2399
380 0.182
1718 0.484
815 0.401
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.449618320610687
0.4805343511450382

 ===== Epoch 245	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.4406064   2.0666196
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.168603    2.1479526
 -0.38555372 -0.3798593 ] 2 4
train:	 Loss = 1.5759,	 Acc = 0.4019,	 Loss Con1 = 0.3061,	 Loss Con2 = 0.2830
2918 0.156
5683 0.468
2917 0.511
375 0.44
69 0.522
6 0.333
0 0.0
0 0.0
0.4811049723756906
0.4805343511450382
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5738,	 Acc = 0.4220,	 Loss Con1 = 0.2656,	 Loss Con2 = 0.2344
380 0.163
1718 0.477
815 0.447
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.4595419847328244
0.4805343511450382

 ===== Epoch 246	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 0
train:	 Loss = 1.5762,	 Acc = 0.4046,	 Loss Con1 = 0.3055,	 Loss Con2 = 0.2838
2924 0.157
5681 0.471
2913 0.518
375 0.435
69 0.522
6 0.333
0 0.0
0 0.0
0.48474126492702346
0.4805343511450382
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5548,	 Acc = 0.4273,	 Loss Con1 = 0.2685,	 Loss Con2 = 0.2371
380 0.184
1718 0.495
815 0.415
85 0.271
2 0.0
0 0.0
0 0.0
0 0.0
0.46259541984732827
0.4805343511450382

 ===== Epoch 247	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6202712   1.6811785  -0.40141198 -0.40778467  2.8001218   2.8380492
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.5680337   1.7230341  -0.40141198 -0.40778467  2.5940347   2.9021297
 -0.38555372 -0.3798593 ] 3 4
train:	 Loss = 1.5741,	 Acc = 0.4035,	 Loss Con1 = 0.3050,	 Loss Con2 = 0.2809
2923 0.157
5681 0.47
2912 0.513
377 0.438
69 0.536
6 0.333
0 0.0
0 0.0
0.48325041459369816
0.4805343511450382
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5698,	 Acc = 0.4200,	 Loss Con1 = 0.2629,	 Loss Con2 = 0.2357
380 0.187
1718 0.485
815 0.409
85 0.259
2 0.0
0 0.0
0 0.0
0 0.0
0.4538167938931298
0.4805343511450382

 ===== Epoch 248	 =====
[-0.36602148 -0.3783333   1.1229566   2.0172615  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.134593    2.0733492  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 4
train:	 Loss = 1.5771,	 Acc = 0.4022,	 Loss Con1 = 0.3058,	 Loss Con2 = 0.2825
2925 0.16
5681 0.468
2912 0.511
375 0.437
69 0.507
6 0.333
0 0.0
0 0.0
0.48059272365365474
0.4805343511450382
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5547,	 Acc = 0.4103,	 Loss Con1 = 0.2665,	 Loss Con2 = 0.2466
380 0.179
1718 0.469
815 0.413
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.4438931297709924
0.4805343511450382

 ===== Epoch 249	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.7436748   2.071549
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  3.099963    1.0366008  -0.40141198 -0.40778467  2.6607103   2.2539315
 -0.38555372 -0.3798593 ] 6 2
train:	 Loss = 1.5775,	 Acc = 0.4027,	 Loss Con1 = 0.3060,	 Loss Con2 = 0.2831
2926 0.157
5678 0.467
2913 0.517
376 0.434
69 0.536
6 0.333
0 0.0
0 0.0
0.482304799823048
0.4805343511450382
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5625,	 Acc = 0.4083,	 Loss Con1 = 0.2639,	 Loss Con2 = 0.2404
380 0.168
1718 0.473
815 0.399
85 0.282
2 0.0
0 0.0
0 0.0
0 0.0
0.4431297709923664
0.4805343511450382

 ===== Epoch 250	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   3.5324821   2.4036725
 -0.3640846  -0.3725409   3.87144     1.7447703  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  3.6514816   2.4349816
 -0.3640846  -0.3725409   3.887461    1.7970288  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 4
train:	 Loss = 1.5748,	 Acc = 0.4038,	 Loss Con1 = 0.3048,	 Loss Con2 = 0.2829
2922 0.158
5686 0.466
2910 0.519
375 0.469
69 0.551
6 0.333
0 0.0
0 0.0
0.4833075392438647
0.4805343511450382
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5619,	 Acc = 0.4183,	 Loss Con1 = 0.2654,	 Loss Con2 = 0.2362
380 0.182
1718 0.481
815 0.418
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.45267175572519086
0.4805343511450382

 ===== Epoch 251	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   0.33593133  2.3770008
  5.359115    1.1537967  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  0.3419333   2.186036
  4.836734    1.0142777  -0.40141198 -0.40778467  4.69354     1.3666646
 -0.38555372 -0.3798593 ] 5 5
train:	 Loss = 1.5747,	 Acc = 0.4027,	 Loss Con1 = 0.3050,	 Loss Con2 = 0.2826
2927 0.156
5680 0.47
2909 0.512
377 0.438
69 0.536
6 0.333
0 0.0
0 0.0
0.48246875345647605
0.4805343511450382
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5680,	 Acc = 0.4127,	 Loss Con1 = 0.2678,	 Loss Con2 = 0.2408
380 0.187
1718 0.473
815 0.409
85 0.247
2 0.0
0 0.0
0 0.0
0 0.0
0.4454198473282443
0.4805343511450382

 ===== Epoch 252	 =====
[-0.36602148 -0.3783333   2.457171    2.3525279  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   2.4829752   2.438      -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 4
train:	 Loss = 1.5759,	 Acc = 0.4059,	 Loss Con1 = 0.3058,	 Loss Con2 = 0.2837
2925 0.158
5681 0.471
2910 0.521
377 0.448
69 0.536
6 0.333
0 0.0
0 0.0
0.48623244498507134
0.4805343511450382
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5728,	 Acc = 0.4157,	 Loss Con1 = 0.2635,	 Loss Con2 = 0.2379
380 0.182
1718 0.467
815 0.433
85 0.259
2 0.0
0 0.0
0 0.0
0 0.0
0.449618320610687
0.4805343511450382

 ===== Epoch 253	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 0
train:	 Loss = 1.5771,	 Acc = 0.4042,	 Loss Con1 = 0.3046,	 Loss Con2 = 0.2820
2925 0.157
5682 0.469
2910 0.519
376 0.447
69 0.464
6 0.333
0 0.0
0 0.0
0.48413137233219067
0.4805343511450382
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5698,	 Acc = 0.4163,	 Loss Con1 = 0.2674,	 Loss Con2 = 0.2524
380 0.184
1718 0.481
815 0.402
85 0.282
2 0.0
0 0.0
0 0.0
0 0.0
0.45
0.4805343511450382

 ===== Epoch 254	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.077309    2.7142441 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.1015584   2.72999   ] 3 0
train:	 Loss = 1.5775,	 Acc = 0.4050,	 Loss Con1 = 0.3053,	 Loss Con2 = 0.2828
2927 0.158
5679 0.472
2911 0.516
376 0.444
69 0.493
6 0.333
0 0.0
0 0.0
0.4849021125981639
0.4805343511450382
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5584,	 Acc = 0.4290,	 Loss Con1 = 0.2628,	 Loss Con2 = 0.2333
380 0.187
1718 0.494
815 0.416
85 0.329
2 0.0
0 0.0
0 0.0
0 0.0
0.46412213740458014
0.4805343511450382

 ===== Epoch 255	 =====
[ 1.9534854   2.8821037  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.027488    3.1367636 ] [ 3.0114627   2.9403257  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.8872854   3.2102454 ] 4 4
train:	 Loss = 1.5782,	 Acc = 0.4027,	 Loss Con1 = 0.3048,	 Loss Con2 = 0.2805
2923 0.159
5681 0.469
2915 0.51
374 0.439
69 0.493
6 0.333
0 0.0
0 0.0
0.48148148148148145
0.4805343511450382
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5443,	 Acc = 0.4360,	 Loss Con1 = 0.2675,	 Loss Con2 = 0.2426
380 0.176
1718 0.49
815 0.463
85 0.259
2 0.0
0 0.0
0 0.0
0 0.0
0.47366412213740455
0.4805343511450382

 ===== Epoch 256	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   0.93861157  3.2403603  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   0.9918209   3.287642   -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 4
train:	 Loss = 1.5770,	 Acc = 0.4040,	 Loss Con1 = 0.3054,	 Loss Con2 = 0.2823
2925 0.155
5680 0.47
2913 0.518
375 0.443
69 0.522
6 0.333
0 0.0
0 0.0
0.4845737034170076
0.4805343511450382
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5533,	 Acc = 0.4277,	 Loss Con1 = 0.2655,	 Loss Con2 = 0.2393
380 0.182
1718 0.49
815 0.433
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.4633587786259542
0.4805343511450382

 ===== Epoch 257	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   1.2773215   1.7348163  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   1.3379699   2.0458791  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 6
train:	 Loss = 1.5756,	 Acc = 0.4022,	 Loss Con1 = 0.3060,	 Loss Con2 = 0.2840
2927 0.155
5680 0.469
2909 0.512
377 0.448
69 0.507
6 0.333
0 0.0
0 0.0
0.48235814622276296
0.4805343511450382
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5642,	 Acc = 0.4220,	 Loss Con1 = 0.2664,	 Loss Con2 = 0.2459
380 0.189
1718 0.483
815 0.425
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.45572519083969465
0.4805343511450382

 ===== Epoch 258	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   0.69474393  3.0517912  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 5
train:	 Loss = 1.5746,	 Acc = 0.4017,	 Loss Con1 = 0.3056,	 Loss Con2 = 0.2842
2926 0.156
5679 0.467
2912 0.514
376 0.441
69 0.551
6 0.333
0 0.0
0 0.0
0.48130944481309446
0.4805343511450382
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5575,	 Acc = 0.4163,	 Loss Con1 = 0.2679,	 Loss Con2 = 0.2493
380 0.184
1718 0.482
815 0.402
85 0.271
2 0.0
0 0.0
0 0.0
0 0.0
0.45
0.4805343511450382

 ===== Epoch 259	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 0
train:	 Loss = 1.5744,	 Acc = 0.4043,	 Loss Con1 = 0.3046,	 Loss Con2 = 0.2820
2923 0.155
5675 0.469
2918 0.52
377 0.454
69 0.507
6 0.333
0 0.0
0 0.0
0.4847982310668878
0.4805343511450382
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5599,	 Acc = 0.4147,	 Loss Con1 = 0.2609,	 Loss Con2 = 0.2351
380 0.182
1718 0.475
815 0.413
85 0.259
2 0.0
0 0.0
0 0.0
0 0.0
0.4484732824427481
0.4805343511450382

 ===== Epoch 260	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   1.2224406   2.256978
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  1.2286313   2.3616316
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 3 6
train:	 Loss = 1.5753,	 Acc = 0.4033,	 Loss Con1 = 0.3054,	 Loss Con2 = 0.2842
2924 0.157
5682 0.471
2911 0.513
376 0.439
69 0.493
6 0.333
0 0.0
0 0.0
0.48286156567890315
0.4805343511450382
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5634,	 Acc = 0.4297,	 Loss Con1 = 0.2659,	 Loss Con2 = 0.2461
380 0.187
1718 0.493
815 0.429
85 0.247
2 0.0
0 0.0
0 0.0
0 0.0
0.4648854961832061
0.4805343511450382

 ===== Epoch 261	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 0
train:	 Loss = 1.5779,	 Acc = 0.4052,	 Loss Con1 = 0.3048,	 Loss Con2 = 0.2828
2925 0.158
5679 0.473
2913 0.512
376 0.449
69 0.536
6 0.333
0 0.0
0 0.0
0.48534778281543733
0.4805343511450382
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5595,	 Acc = 0.4333,	 Loss Con1 = 0.2695,	 Loss Con2 = 0.2501
380 0.168
1718 0.497
815 0.444
85 0.247
2 0.0
0 0.0
0 0.0
0 0.0
0.4717557251908397
0.4805343511450382

 ===== Epoch 262	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.107731    3.3204677 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.103322    3.3414624 ] 4 1
train:	 Loss = 1.5750,	 Acc = 0.4021,	 Loss Con1 = 0.3049,	 Loss Con2 = 0.2845
2927 0.159
5679 0.468
2912 0.51
376 0.447
68 0.5
6 0.333
0 0.0
0 0.0
0.48080964495077977
0.4805343511450382
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5642,	 Acc = 0.4337,	 Loss Con1 = 0.2663,	 Loss Con2 = 0.2519
380 0.195
1718 0.49
815 0.448
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.4683206106870229
0.4805343511450382

 ===== Epoch 263	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   1.8866532   1.8769059
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  1.8738837   1.8237313
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 3
train:	 Loss = 1.5764,	 Acc = 0.4047,	 Loss Con1 = 0.3045,	 Loss Con2 = 0.2831
2926 0.159
5679 0.469
2912 0.518
376 0.444
69 0.536
6 0.333
0 0.0
0 0.0
0.4840743198407432
0.4805343511450382
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5774,	 Acc = 0.4070,	 Loss Con1 = 0.2647,	 Loss Con2 = 0.2465
380 0.187
1718 0.467
815 0.404
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.4389312977099237
0.4805343511450382

 ===== Epoch 264	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 0
train:	 Loss = 1.5775,	 Acc = 0.4027,	 Loss Con1 = 0.3052,	 Loss Con2 = 0.2837
2924 0.156
5681 0.468
2913 0.515
375 0.448
69 0.507
6 0.333
0 0.0
0 0.0
0.48241928350287483
0.4805343511450382
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5644,	 Acc = 0.4193,	 Loss Con1 = 0.2629,	 Loss Con2 = 0.2416
380 0.168
1718 0.489
815 0.411
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.45572519083969465
0.4805343511450382

 ===== Epoch 265	 =====
[-0.36602148 -0.3783333   2.654968    1.5664642  -0.4409929  -0.3635196
  1.8046849   3.773963   -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   2.4297707   1.5999826  -0.44088638 -0.36343393
  1.9024211   3.7711728  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 3
train:	 Loss = 1.5765,	 Acc = 0.4051,	 Loss Con1 = 0.3057,	 Loss Con2 = 0.2840
2925 0.16
5680 0.471
2912 0.515
377 0.44
68 0.5
6 0.333
0 0.0
0 0.0
0.4842419551033949
0.4805343511450382
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5605,	 Acc = 0.4237,	 Loss Con1 = 0.2637,	 Loss Con2 = 0.2366
380 0.174
1718 0.482
815 0.433
85 0.282
2 0.0
0 0.0
0 0.0
0 0.0
0.4599236641221374
0.4805343511450382

 ===== Epoch 266	 =====
[-0.36602148 -0.3783333   1.1517942   1.5415457  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0834198   1.541095   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.5771,	 Acc = 0.4015,	 Loss Con1 = 0.3055,	 Loss Con2 = 0.2829
2925 0.158
5680 0.467
2912 0.509
376 0.457
69 0.536
6 0.333
0 0.0
0 0.0
0.4801503925688378
0.4805343511450382
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5574,	 Acc = 0.4333,	 Loss Con1 = 0.2630,	 Loss Con2 = 0.2363
380 0.184
1718 0.495
815 0.438
85 0.271
2 0.0
0 0.0
0 0.0
0 0.0
0.46946564885496184
0.4805343511450382

 ===== Epoch 267	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  0.48430124  1.724036
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  0.44793263  1.8793076
 -0.38555372 -0.3798593 ] 1 6
train:	 Loss = 1.5746,	 Acc = 0.4046,	 Loss Con1 = 0.3049,	 Loss Con2 = 0.2825
2927 0.157
5676 0.469
2917 0.519
373 0.458
69 0.522
6 0.333
0 0.0
0 0.0
0.4847915053644508
0.4805343511450382
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5647,	 Acc = 0.4357,	 Loss Con1 = 0.2650,	 Loss Con2 = 0.2331
380 0.176
1718 0.491
815 0.459
85 0.259
2 0.0
0 0.0
0 0.0
0 0.0
0.4732824427480916
0.4805343511450382

 ===== Epoch 268	 =====
[-0.36602148 -0.3783333   1.1956581   2.9800196  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0643312   2.9657245  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 0
train:	 Loss = 1.5740,	 Acc = 0.4017,	 Loss Con1 = 0.3053,	 Loss Con2 = 0.2821
2924 0.16
5680 0.467
2913 0.507
376 0.444
69 0.565
6 0.333
0 0.0
0 0.0
0.47987616099071206
0.4805343511450382
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5691,	 Acc = 0.4200,	 Loss Con1 = 0.2685,	 Loss Con2 = 0.2418
380 0.189
1718 0.484
815 0.415
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.4534351145038168
0.4805343511450382

 ===== Epoch 269	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   0.6370883   2.322103   -0.42342317 -0.42019257
  2.3480172   2.3599575 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   0.62621796  2.6132576  -0.42342317 -0.42019257
  2.6231332   2.6250162 ] 6 6
train:	 Loss = 1.5743,	 Acc = 0.4024,	 Loss Con1 = 0.3053,	 Loss Con2 = 0.2833
2925 0.159
5676 0.465
2915 0.516
377 0.443
69 0.536
6 0.333
0 0.0
0 0.0
0.481145637509676
0.4805343511450382
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5933,	 Acc = 0.3973,	 Loss Con1 = 0.2696,	 Loss Con2 = 0.2509
380 0.176
1718 0.46
815 0.384
85 0.259
2 0.0
0 0.0
0 0.0
0 0.0
0.42938931297709926
0.4805343511450382

 ===== Epoch 270	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   2.118952    0.87379426 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   2.0537262   1.0255929  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 6
train:	 Loss = 1.5729,	 Acc = 0.4037,	 Loss Con1 = 0.3060,	 Loss Con2 = 0.2855
2921 0.158
5680 0.467
2915 0.518
377 0.443
69 0.522
6 0.333
0 0.0
0 0.0
0.4830330496297115
0.4805343511450382
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5627,	 Acc = 0.4233,	 Loss Con1 = 0.2657,	 Loss Con2 = 0.2493
380 0.182
1718 0.483
815 0.429
85 0.247
2 0.0
0 0.0
0 0.0
0 0.0
0.4583969465648855
0.4805343511450382

 ===== Epoch 271	 =====
[ 2.3782468   2.6973116  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.8207098   2.955684  ] [ 2.4042294   2.704906   -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.955623    2.955684  ] 0 0
train:	 Loss = 1.5758,	 Acc = 0.4041,	 Loss Con1 = 0.3058,	 Loss Con2 = 0.2867
2925 0.158
5677 0.471
2915 0.51
376 0.468
69 0.536
6 0.333
0 0.0
0 0.0
0.4837996240185779
0.4805343511450382
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5698,	 Acc = 0.4213,	 Loss Con1 = 0.2642,	 Loss Con2 = 0.2367
380 0.179
1718 0.484
815 0.426
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.45648854961832064
0.4805343511450382

 ===== Epoch 272	 =====
[-0.36602148 -0.3783333   1.6294307   2.216609   -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.6239904   2.2250984  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 1
train:	 Loss = 1.5739,	 Acc = 0.4031,	 Loss Con1 = 0.3058,	 Loss Con2 = 0.2851
2924 0.157
5681 0.464
2912 0.521
376 0.455
69 0.536
6 0.333
0 0.0
0 0.0
0.48264042459088896
0.4805343511450382
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5815,	 Acc = 0.4210,	 Loss Con1 = 0.2682,	 Loss Con2 = 0.2463
380 0.174
1718 0.483
815 0.418
85 0.306
2 0.0
0 0.0
0 0.0
0 0.0
0.4568702290076336
0.4805343511450382

 ===== Epoch 273	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   0.9826662   3.2503145  -0.42342317 -0.42019257
  2.6412096   2.7483606 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   0.9534862   3.1457973  -0.42342317 -0.42019257
  2.6747174   2.5908997 ] 5 5
train:	 Loss = 1.5775,	 Acc = 0.4047,	 Loss Con1 = 0.3065,	 Loss Con2 = 0.2858
2922 0.159
5682 0.472
2912 0.51
377 0.459
69 0.522
6 0.333
0 0.0
0 0.0
0.4839708158302012
0.4805343511450382
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5705,	 Acc = 0.4093,	 Loss Con1 = 0.2644,	 Loss Con2 = 0.2448
380 0.174
1718 0.476
815 0.398
85 0.247
2 0.0
0 0.0
0 0.0
0 0.0
0.4435114503816794
0.4805343511450382

 ===== Epoch 274	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   1.2365717   2.6592767
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  1.2125851   2.6883726
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 3 4
train:	 Loss = 1.5757,	 Acc = 0.4031,	 Loss Con1 = 0.3053,	 Loss Con2 = 0.2847
2922 0.157
5679 0.471
2916 0.51
376 0.441
69 0.551
6 0.333
0 0.0
0 0.0
0.4826442626575282
0.4805343511450382
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5724,	 Acc = 0.4127,	 Loss Con1 = 0.2640,	 Loss Con2 = 0.2456
380 0.182
1718 0.48
815 0.398
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.44618320610687023
0.4805343511450382

 ===== Epoch 275	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.5617089   2.8700893
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.5450406   2.9637454
 -0.38555372 -0.3798593 ] 4 4
train:	 Loss = 1.5799,	 Acc = 0.4037,	 Loss Con1 = 0.3046,	 Loss Con2 = 0.2832
2926 0.154
5677 0.467
2915 0.52
377 0.459
67 0.552
6 0.333
0 0.0
0 0.0
0.48440610484406105
0.4805343511450382
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5447,	 Acc = 0.4307,	 Loss Con1 = 0.2660,	 Loss Con2 = 0.2450
380 0.168
1718 0.486
815 0.46
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.4687022900763359
0.4805343511450382

 ===== Epoch 276	 =====
[ 2.762147    3.2845953  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.4163547   3.5382886 ] [ 2.8077319   3.2845953  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.407978    3.5304155 ] 0 0
train:	 Loss = 1.5754,	 Acc = 0.4022,	 Loss Con1 = 0.3054,	 Loss Con2 = 0.2841
2924 0.16
5678 0.468
2915 0.509
376 0.444
69 0.507
6 0.333
0 0.0
0 0.0
0.4803184431667404
0.4805343511450382
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5638,	 Acc = 0.4363,	 Loss Con1 = 0.2639,	 Loss Con2 = 0.2339
380 0.184
1718 0.493
815 0.456
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.4729007633587786
0.4805343511450382

 ===== Epoch 277	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   2.7525375   2.074721
 -0.3640846  -0.3725409   2.298034    1.095271   -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  2.8048968   2.1015723
 -0.3640846  -0.3725409   2.4124632   1.1027365  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 3 4
train:	 Loss = 1.5767,	 Acc = 0.4027,	 Loss Con1 = 0.3057,	 Loss Con2 = 0.2827
2922 0.161
5684 0.466
2910 0.513
377 0.446
69 0.522
6 0.333
0 0.0
0 0.0
0.48065443289851867
0.4805343511450382
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5661,	 Acc = 0.4280,	 Loss Con1 = 0.2651,	 Loss Con2 = 0.2361
380 0.171
1718 0.489
815 0.437
85 0.271
2 0.0
0 0.0
0 0.0
0 0.0
0.46526717557251906
0.4805343511450382

 ===== Epoch 278	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   3.0432024   1.6324149
 -0.3640846  -0.3725409   2.823838    0.93351835 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  3.1311536   1.7259312
 -0.3640846  -0.3725409   2.6819446   1.0131505  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 3 4
train:	 Loss = 1.5743,	 Acc = 0.4059,	 Loss Con1 = 0.3054,	 Loss Con2 = 0.2828
2928 0.158
5679 0.473
2911 0.517
375 0.451
69 0.507
6 0.333
0 0.0
0 0.0
0.48628318584070795
0.4805343511450382
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5566,	 Acc = 0.4227,	 Loss Con1 = 0.2662,	 Loss Con2 = 0.2437
380 0.163
1718 0.487
815 0.429
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.46030534351145036
0.4805343511450382

 ===== Epoch 279	 =====
[ 7.9108863   1.4214886  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  3.2358928   0.98358357 -0.40141198 -0.40778467  1.6083058   3.0376842
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.4279796   2.8060088
 -0.38555372 -0.3798593 ] 5 5
train:	 Loss = 1.5734,	 Acc = 0.4032,	 Loss Con1 = 0.3045,	 Loss Con2 = 0.2819
2921 0.158
5686 0.467
2912 0.516
374 0.455
69 0.522
6 0.333
0 0.0
0 0.0
0.4823698463579087
0.4805343511450382
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5621,	 Acc = 0.4217,	 Loss Con1 = 0.2632,	 Loss Con2 = 0.2407
380 0.187
1718 0.485
815 0.417
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.45572519083969465
0.4805343511450382

 ===== Epoch 280	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   0.6805717   2.9939985  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   0.7217663   2.7127976  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 5
train:	 Loss = 1.5758,	 Acc = 0.4041,	 Loss Con1 = 0.3056,	 Loss Con2 = 0.2819
2926 0.159
5676 0.469
2915 0.515
376 0.449
69 0.565
6 0.333
0 0.0
0 0.0
0.4835213448352135
0.4805343511450382
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5625,	 Acc = 0.4167,	 Loss Con1 = 0.2637,	 Loss Con2 = 0.2460
380 0.187
1718 0.481
815 0.41
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.45
0.4805343511450382

 ===== Epoch 281	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.5757,	 Acc = 0.4027,	 Loss Con1 = 0.3057,	 Loss Con2 = 0.2830
2923 0.16
5682 0.464
2914 0.516
374 0.46
69 0.536
6 0.333
0 0.0
0 0.0
0.48114980652294087
0.4805343511450382
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5565,	 Acc = 0.4280,	 Loss Con1 = 0.2637,	 Loss Con2 = 0.2397
380 0.184
1718 0.485
815 0.438
85 0.271
2 0.0
0 0.0
0 0.0
0 0.0
0.4633587786259542
0.4805343511450382

 ===== Epoch 282	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   0.43103713  0.79447806
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  0.49551097  0.79683053
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 0
train:	 Loss = 1.5767,	 Acc = 0.4047,	 Loss Con1 = 0.3050,	 Loss Con2 = 0.2844
2924 0.158
5678 0.467
2916 0.523
375 0.448
69 0.551
6 0.333
0 0.0
0 0.0
0.4845201238390093
0.4805343511450382
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5556,	 Acc = 0.4293,	 Loss Con1 = 0.2653,	 Loss Con2 = 0.2418
380 0.197
1718 0.491
815 0.429
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.4629770992366412
0.4805343511450382

 ===== Epoch 283	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  1.6951534   1.0728757  -0.40141198 -0.40778467  1.8769004   3.0820475
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  1.6990862   1.0756661  -0.40141198 -0.40778467  1.8295456   3.0598657
 -0.38555372 -0.3798593 ] 3 3
train:	 Loss = 1.5746,	 Acc = 0.4038,	 Loss Con1 = 0.3057,	 Loss Con2 = 0.2848
2923 0.166
5678 0.466
2916 0.51
376 0.463
69 0.565
6 0.333
0 0.0
0 0.0
0.48059701492537316
0.4805343511450382
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5549,	 Acc = 0.4290,	 Loss Con1 = 0.2662,	 Loss Con2 = 0.2363
380 0.179
1718 0.487
815 0.444
85 0.247
2 0.0
0 0.0
0 0.0
0 0.0
0.46526717557251906
0.4805343511450382

 ===== Epoch 284	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 0
train:	 Loss = 1.5752,	 Acc = 0.4029,	 Loss Con1 = 0.3049,	 Loss Con2 = 0.2814
2923 0.155
5679 0.467
2914 0.516
377 0.459
69 0.536
6 0.333
0 0.0
0 0.0
0.4829187396351575
0.4805343511450382
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5559,	 Acc = 0.4197,	 Loss Con1 = 0.2631,	 Loss Con2 = 0.2332
380 0.171
1718 0.483
815 0.426
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.45572519083969465
0.4805343511450382

 ===== Epoch 285	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  1.6945926   1.089618   -0.40141198 -0.40778467  2.0227518   3.084512
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  1.6945926   1.089618   -0.40141198 -0.40778467  2.036389    3.0771184
 -0.38555372 -0.3798593 ] 3 0
train:	 Loss = 1.5757,	 Acc = 0.4033,	 Loss Con1 = 0.3060,	 Loss Con2 = 0.2843
2927 0.159
5677 0.467
2912 0.515
377 0.464
69 0.522
6 0.333
0 0.0
0 0.0
0.48246875345647605
0.4805343511450382
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5779,	 Acc = 0.3970,	 Loss Con1 = 0.2693,	 Loss Con2 = 0.2545
380 0.184
1718 0.449
815 0.404
85 0.247
2 0.0
0 0.0
0 0.0
0 0.0
0.42786259541984734
0.4805343511450382

 ===== Epoch 286	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  3.2780209   1.067295   -0.40141198 -0.40778467  2.9724915   2.1947803
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  3.4616964   1.3351713  -0.40141198 -0.40778467  2.7739816   2.4634252
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.5758,	 Acc = 0.4050,	 Loss Con1 = 0.3059,	 Loss Con2 = 0.2829
2919 0.159
5680 0.469
2917 0.517
377 0.448
69 0.522
6 0.333
0 0.0
0 0.0
0.48425240358050614
0.4805343511450382
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5690,	 Acc = 0.4087,	 Loss Con1 = 0.2658,	 Loss Con2 = 0.2456
380 0.168
1718 0.473
815 0.4
85 0.271
2 0.0
0 0.0
0 0.0
0 0.0
0.4435114503816794
0.4805343511450382

 ===== Epoch 287	 =====
[ 0.29564986  1.2670736   1.1083359   3.0706322  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 0.3022398   1.3227642  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 2
train:	 Loss = 1.5761,	 Acc = 0.4021,	 Loss Con1 = 0.3045,	 Loss Con2 = 0.2814
2922 0.158
5681 0.465
2914 0.516
377 0.451
68 0.5
6 0.333
0 0.0
0 0.0
0.48076497899624143
0.4805343511450382
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5563,	 Acc = 0.4227,	 Loss Con1 = 0.2657,	 Loss Con2 = 0.2333
380 0.184
1718 0.484
815 0.426
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.45725190839694657
0.4805343511450382

 ===== Epoch 288	 =====
[-0.36602148 -0.3783333   2.4417365   2.18716    -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   2.4460168   2.2001843  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.5764,	 Acc = 0.4032,	 Loss Con1 = 0.3046,	 Loss Con2 = 0.2813
2924 0.158
5681 0.467
2913 0.517
375 0.437
69 0.522
6 0.333
0 0.0
0 0.0
0.48241928350287483
0.4805343511450382
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5836,	 Acc = 0.4093,	 Loss Con1 = 0.2639,	 Loss Con2 = 0.2363
380 0.195
1718 0.471
815 0.396
85 0.247
2 0.0
0 0.0
0 0.0
0 0.0
0.4404580152671756
0.4805343511450382

 ===== Epoch 289	 =====
[ 1.9848883   1.9935838  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 2.9038835   1.9986467  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 5
train:	 Loss = 1.5759,	 Acc = 0.4068,	 Loss Con1 = 0.3058,	 Loss Con2 = 0.2830
2924 0.161
5679 0.473
2913 0.516
377 0.443
69 0.536
6 0.333
0 0.0
0 0.0
0.4862892525431225
0.4805343511450382
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5730,	 Acc = 0.4037,	 Loss Con1 = 0.2657,	 Loss Con2 = 0.2430
380 0.187
1718 0.46
815 0.406
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.4351145038167939
0.4805343511450382

 ===== Epoch 290	 =====
[-0.36602148 -0.3783333   2.7183268   2.3479972  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   2.4906917   2.2749262  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 2
train:	 Loss = 1.5779,	 Acc = 0.4009,	 Loss Con1 = 0.3046,	 Loss Con2 = 0.2832
2924 0.156
5680 0.467
2913 0.511
376 0.436
69 0.522
6 0.333
0 0.0
0 0.0
0.4802078726227333
0.4805343511450382
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5615,	 Acc = 0.4203,	 Loss Con1 = 0.2653,	 Loss Con2 = 0.2419
380 0.182
1718 0.481
815 0.425
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.4549618320610687
0.4805343511450382

 ===== Epoch 291	 =====
[-0.36602148 -0.3783333   1.0843724   3.011734   -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0797651   3.0110228  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.5764,	 Acc = 0.4027,	 Loss Con1 = 0.3060,	 Loss Con2 = 0.2847
2925 0.158
5677 0.468
2915 0.51
376 0.457
69 0.551
6 0.333
0 0.0
0 0.0
0.48203029967930994
0.4805343511450382
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5557,	 Acc = 0.4307,	 Loss Con1 = 0.2707,	 Loss Con2 = 0.2396
380 0.176
1718 0.49
815 0.44
85 0.294
2 0.0
0 0.0
0 0.0
0 0.0
0.4675572519083969
0.4805343511450382

 ===== Epoch 292	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   2.362947    1.7591057
 -0.3640846  -0.3725409   1.765937    0.8265127  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  2.3625033   1.8437357
 -0.3640846  -0.3725409   1.7396171   0.85388625 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 4
train:	 Loss = 1.5757,	 Acc = 0.4061,	 Loss Con1 = 0.3054,	 Loss Con2 = 0.2837
2924 0.155
5683 0.474
2910 0.517
376 0.452
69 0.522
6 0.333
0 0.0
0 0.0
0.48717381689517913
0.4805343511450382
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5805,	 Acc = 0.4110,	 Loss Con1 = 0.2684,	 Loss Con2 = 0.2446
380 0.168
1718 0.474
815 0.412
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.44618320610687023
0.4805343511450382

 ===== Epoch 293	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 0
train:	 Loss = 1.5758,	 Acc = 0.4034,	 Loss Con1 = 0.3056,	 Loss Con2 = 0.2839
2925 0.159
5679 0.469
2912 0.513
377 0.446
69 0.536
6 0.333
0 0.0
0 0.0
0.4823620479929227
0.4805343511450382
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5664,	 Acc = 0.4233,	 Loss Con1 = 0.2692,	 Loss Con2 = 0.2443
380 0.187
1718 0.49
815 0.418
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.45763358778625957
0.4805343511450382

 ===== Epoch 294	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   1.2346625   1.394592
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  1.3474431   1.5903447
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.5746,	 Acc = 0.4024,	 Loss Con1 = 0.3055,	 Loss Con2 = 0.2840
2927 0.158
5678 0.469
2911 0.508
377 0.454
69 0.536
6 0.333
0 0.0
0 0.0
0.4814732883530583
0.4805343511450382
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5637,	 Acc = 0.4263,	 Loss Con1 = 0.2656,	 Loss Con2 = 0.2365
380 0.168
1718 0.483
815 0.448
85 0.247
2 0.0
0 0.0
0 0.0
0 0.0
0.4637404580152672
0.4805343511450382

 ===== Epoch 295	 =====
[-0.36602148 -0.3783333   1.7756457   3.061571   -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.6979084   3.0902946  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 4
train:	 Loss = 1.5734,	 Acc = 0.4047,	 Loss Con1 = 0.3052,	 Loss Con2 = 0.2828
2924 0.157
5685 0.469
2908 0.516
376 0.473
69 0.536
6 0.333
0 0.0
0 0.0
0.48474126492702346
0.4805343511450382
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5617,	 Acc = 0.4167,	 Loss Con1 = 0.2647,	 Loss Con2 = 0.2413
380 0.184
1718 0.476
815 0.422
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.45038167938931295
0.4805343511450382

 ===== Epoch 296	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.7208195   2.5373642
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.6802841   2.2933657
 -0.38555372 -0.3798593 ] 5 5
train:	 Loss = 1.5775,	 Acc = 0.4059,	 Loss Con1 = 0.3044,	 Loss Con2 = 0.2827
2925 0.162
5678 0.469
2915 0.52
375 0.456
69 0.493
6 0.333
0 0.0
0 0.0
0.4849054517306204
0.4805343511450382
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5750,	 Acc = 0.4080,	 Loss Con1 = 0.2633,	 Loss Con2 = 0.2397
380 0.166
1718 0.471
815 0.406
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.4431297709923664
0.4805343511450382

 ===== Epoch 297	 =====
[ 1.7073464   2.9782968  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.2352052   3.2994728 ] [ 1.5122478   3.051707   -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.2704765   3.4201927 ] 4 4
train:	 Loss = 1.5775,	 Acc = 0.4021,	 Loss Con1 = 0.3044,	 Loss Con2 = 0.2823
2924 0.157
5676 0.466
2916 0.516
377 0.44
69 0.478
6 0.333
0 0.0
0 0.0
0.48131357806280406
0.4805343511450382
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5799,	 Acc = 0.4037,	 Loss Con1 = 0.2653,	 Loss Con2 = 0.2452
380 0.171
1718 0.463
815 0.404
85 0.247
2 0.0
0 0.0
0 0.0
0 0.0
0.43740458015267175
0.4805343511450382

 ===== Epoch 298	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   1.3923236   2.911878   -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   1.3797358   2.9342744  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.5769,	 Acc = 0.4034,	 Loss Con1 = 0.3044,	 Loss Con2 = 0.2813
2927 0.156
5679 0.469
2914 0.515
373 0.461
69 0.507
6 0.333
0 0.0
0 0.0
0.4835748257936069
0.4805343511450382
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5721,	 Acc = 0.4017,	 Loss Con1 = 0.2599,	 Loss Con2 = 0.2338
380 0.166
1718 0.457
815 0.411
85 0.259
2 0.0
0 0.0
0 0.0
0 0.0
0.43587786259541983
0.4805343511450382

 ===== Epoch 299	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.4753343   3.2989352
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.4741983   3.2989352
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.5731,	 Acc = 0.4030,	 Loss Con1 = 0.3062,	 Loss Con2 = 0.2842
2923 0.155
5679 0.467
2915 0.519
376 0.452
69 0.493
6 0.333
0 0.0
0 0.0
0.48325041459369816
0.4805343511450382
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5654,	 Acc = 0.4023,	 Loss Con1 = 0.2647,	 Loss Con2 = 0.2450
380 0.187
1718 0.461
815 0.396
85 0.247
2 0.0
0 0.0
0 0.0
0 0.0
0.433587786259542
0.4805343511450382

 ===== Epoch 300	 =====
[ 0.895339    0.8823015  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   2.9783175   2.931786   -0.42342317 -0.42019257
  1.2237427   0.9769273 ] [ 0.7632151   0.8823015  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   2.850156    3.0636766  -0.42342317 -0.42019257
  1.17304     1.0950228 ] 2 4
train:	 Loss = 1.5754,	 Acc = 0.4020,	 Loss Con1 = 0.3051,	 Loss Con2 = 0.2838
2923 0.158
5679 0.47
2915 0.506
376 0.441
69 0.507
6 0.333
0 0.0
0 0.0
0.48070757324488667
0.4805343511450382
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5721,	 Acc = 0.4067,	 Loss Con1 = 0.2626,	 Loss Con2 = 0.2426
380 0.171
1718 0.466
815 0.411
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.44083969465648853
0.4805343511450382
