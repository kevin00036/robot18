(0.24300000000005184, array([-1.000000e+00, -1.000000e+00,  1.189783e+03,  1.790000e-01,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 4, array([-1.000000e+00, -1.000000e+00,  1.173932e+03,  2.700000e-01,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 1)
(0.24, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.543573e+03, -3.290000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.370508e+03,  2.799000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]))
(0.238, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.370508e+03,  2.799000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.356274e+03, -3.410000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]))
(0.236, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.356274e+03, -3.410000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.427118e+03, -3.330000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]))
(0.2350000000000001, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.427118e+03, -3.330000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.245642e+03, -3.340000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]))
(0.248, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.245642e+03, -3.340000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.415149e+03, -3.430000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]))
(0.256, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.415149e+03, -3.430000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.211029e+03, -3.400000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]))
(0.256, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.211029e+03, -3.400000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([-1.000000e+00, -1.000000e+00,  2.398328e+03,  4.980000e-01,
        2.350128e+03, -3.360000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]))
(0.2670000000000001, array([-1.000000e+00, -1.000000e+00,  2.398328e+03,  4.980000e-01,
        2.350128e+03, -3.360000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.383771e+03, -3.390000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]))
(0.2609999999999997, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.383771e+03, -3.390000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.435205e+03, -3.500000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]))
(0.28500000000000014, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.435205e+03, -3.500000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.406091e+03, -3.470000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]))
14 1 14

 ===== Epoch 1	 =====
[-0.36602148 -0.3783333   3.2873485   2.8169172   2.2846467   1.0345237
 -0.3640846  -0.3725409   0.9517706   0.8414438  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   3.2924101   2.8162403   2.3510432   1.0835625
 -0.3640846  -0.3725409   1.1222708   0.87379426 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 4
train:	 Loss = 1.9178,	 Acc = 0.1989
2919 0.133
5685 0.206
2916 0.235
373 0.279
69 0.435
6 0.167
0 0.0
0 0.0
0.22024533097579843
0.0
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 0
val:	 Loss = 1.9101,	 Acc = 0.2073
380 0.176
1718 0.216
815 0.215
85 0.106
2 0.0
0 0.0
0 0.0
0 0.0
0.21183206106870228
0.21183206106870228
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.9213,	 Acc1 = 0.1646,	 Acc2 = 0.1747

 ===== Epoch 2	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   2.715702    2.0359251  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   2.715702    2.0732527  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 0
train:	 Loss = 1.8509,	 Acc = 0.2548
2924 0.151
5676 0.281
2916 0.299
377 0.297
69 0.362
6 0.333
0 0.0
0 0.0
0.28836797877045556
0.21183206106870228
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.7808,	 Acc = 0.2993
380 0.224
1718 0.353
815 0.245
85 0.082
2 0.0
0 0.0
0 0.0
0 0.0
0.3103053435114504
0.3103053435114504
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.8065,	 Acc1 = 0.2284,	 Acc2 = 0.2592

 ===== Epoch 3	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   1.6273085   2.7837448
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  1.4188832   2.7150457
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 5
train:	 Loss = 1.7442,	 Acc = 0.3192
2925 0.18
5678 0.365
2914 0.367
376 0.33
69 0.333
6 0.333
0 0.0
0 0.0
0.3642596483467876
0.3103053435114504
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6887,	 Acc = 0.3537
380 0.2
1718 0.411
815 0.333
85 0.094
2 0.0
0 0.0
0 0.0
0 0.0
0.37595419847328243
0.37595419847328243
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.7286,	 Acc1 = 0.2709,	 Acc2 = 0.3027

 ===== Epoch 4	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.0861273   2.6801276 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.0702543   2.7063708 ] 3 0
train:	 Loss = 1.6988,	 Acc = 0.3432
2921 0.163
5683 0.402
2913 0.409
376 0.359
69 0.333
6 0.167
0 0.0
0 0.0
0.40145904719796616
0.37595419847328243
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 0
val:	 Loss = 1.6608,	 Acc = 0.3550
380 0.184
1718 0.426
815 0.315
85 0.071
2 0.0
0 0.0
0 0.0
0 0.0
0.3797709923664122
0.3797709923664122
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.7105,	 Acc1 = 0.2787,	 Acc2 = 0.3121

 ===== Epoch 5	 =====
[ 1.8450997   2.1885014  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.6315666   2.4728043 ] [ 1.7420392   1.9885211  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.5103225   2.2392375 ] 5 5
train:	 Loss = 1.6815,	 Acc = 0.3493
2927 0.163
5680 0.406
2909 0.426
377 0.34
69 0.391
6 0.333
0 0.0
0 0.0
0.4097998009069793
0.3797709923664122
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6654,	 Acc = 0.3553
380 0.187
1718 0.415
815 0.337
85 0.082
2 0.0
0 0.0
0 0.0
0 0.0
0.3797709923664122
0.3797709923664122
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.7186,	 Acc1 = 0.2781,	 Acc2 = 0.3114

 ===== Epoch 6	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.52366     1.9071994  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.878092    2.0578797  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.6755,	 Acc = 0.3522
2926 0.169
5678 0.407
2912 0.427
377 0.366
69 0.362
6 0.333
0 0.0
0 0.0
0.41152399911524
0.3797709923664122
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6524,	 Acc = 0.3927
380 0.189
1718 0.463
815 0.364
85 0.165
2 0.0
0 0.0
0 0.0
0 0.0
0.4221374045801527
0.4221374045801527
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6880,	 Acc1 = 0.3233,	 Acc2 = 0.3658

 ===== Epoch 7	 =====
[ 1.627725    1.3885806  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.8890489   1.5883998 ] [ 2.7676098   1.6113433  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.1654868   1.8088448 ] 6 6
train:	 Loss = 1.6682,	 Acc = 0.3561
2926 0.161
5677 0.415
2915 0.437
375 0.363
69 0.333
6 0.333
0 0.0
0 0.0
0.4191550541915505
0.4221374045801527
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 0
val:	 Loss = 1.6533,	 Acc = 0.3750
380 0.189
1718 0.434
815 0.364
85 0.129
2 0.0
0 0.0
0 0.0
0 0.0
0.4019083969465649
0.4221374045801527
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6851,	 Acc1 = 0.3122,	 Acc2 = 0.3524

 ===== Epoch 8	 =====
[-0.36602148 -0.3783333   2.9498353   2.1984866  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   2.4707904   2.1933897  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 0
train:	 Loss = 1.6631,	 Acc = 0.3538
2924 0.165
5680 0.409
2912 0.431
377 0.395
69 0.348
6 0.333
0 0.0
0 0.0
0.4148606811145511
0.4221374045801527
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 0
val:	 Loss = 1.6454,	 Acc = 0.3633
380 0.171
1718 0.432
815 0.336
85 0.094
2 0.0
0 0.0
0 0.0
0 0.0
0.39122137404580154
0.4221374045801527
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6927,	 Acc1 = 0.2866,	 Acc2 = 0.3216

 ===== Epoch 9	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.2563676   1.5120777
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.5446612   1.5983399
 -0.38555372 -0.3798593 ] 2 6
train:	 Loss = 1.6609,	 Acc = 0.3652
2925 0.157
5680 0.428
2911 0.448
377 0.403
69 0.319
6 0.167
0 0.0
0 0.0
0.43248921817980757
0.4221374045801527
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.6514,	 Acc = 0.3790
380 0.189
1718 0.452
815 0.339
85 0.141
2 0.0
0 0.0
0 0.0
0 0.0
0.4064885496183206
0.4221374045801527
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6933,	 Acc1 = 0.3004,	 Acc2 = 0.3382

 ===== Epoch 10	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   1.6830734   0.94339526
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  1.7737917   0.90574425
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 5
train:	 Loss = 1.6621,	 Acc = 0.3636
2926 0.163
5680 0.424
2910 0.445
377 0.385
69 0.304
6 0.333
0 0.0
0 0.0
0.42844503428445035
0.4221374045801527
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6474,	 Acc = 0.3673
380 0.189
1718 0.432
815 0.341
85 0.106
2 0.0
0 0.0
0 0.0
0 0.0
0.3931297709923664
0.4221374045801527
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6901,	 Acc1 = 0.2928,	 Acc2 = 0.3290

 ===== Epoch 11	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 0
train:	 Loss = 1.6571,	 Acc = 0.3646
2924 0.159
5681 0.425
2913 0.449
375 0.408
69 0.304
6 0.333
0 0.0
0 0.0
0.4310039805395843
0.4221374045801527
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6291,	 Acc = 0.3750
380 0.197
1718 0.446
815 0.335
85 0.118
2 0.0
0 0.0
0 0.0
0 0.0
0.40076335877862596
0.4221374045801527
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6770,	 Acc1 = 0.3000,	 Acc2 = 0.3377

 ===== Epoch 12	 =====
[ 0.26741847  3.2137163  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  0.96579224  0.7834941
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 6
train:	 Loss = 1.6551,	 Acc = 0.3635
2919 0.159
5679 0.42
2919 0.452
376 0.418
69 0.333
6 0.333
0 0.0
0 0.0
0.42955022654436953
0.4221374045801527
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6364,	 Acc = 0.3810
380 0.195
1718 0.445
815 0.358
85 0.141
2 0.0
0 0.0
0 0.0
0 0.0
0.4080152671755725
0.4221374045801527
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6797,	 Acc1 = 0.3161,	 Acc2 = 0.3571

 ===== Epoch 13	 =====
[-0.36602148 -0.3783333   2.9863882   1.8813426  -0.4409929  -0.3635196
  2.4495163   3.1433375  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   3.3395216   1.8445932  -0.44088638 -0.36343393
  2.4528847   3.0819492  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 5
train:	 Loss = 1.6550,	 Acc = 0.3626
2922 0.161
5681 0.423
2915 0.445
375 0.384
69 0.348
6 0.5
0 0.0
0 0.0
0.42770285208932124
0.4221374045801527
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6407,	 Acc = 0.4017
380 0.2
1718 0.475
815 0.368
85 0.153
2 0.0
0 0.0
0 0.0
0 0.0
0.4309160305343511
0.4309160305343511
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6770,	 Acc1 = 0.3270,	 Acc2 = 0.3703

 ===== Epoch 14	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.2982532   0.9926734 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 5
train:	 Loss = 1.6525,	 Acc = 0.3637
2924 0.158
5676 0.423
2917 0.45
376 0.404
69 0.362
6 0.333
0 0.0
0 0.0
0.43011941618752764
0.4309160305343511
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.6210,	 Acc = 0.3837
380 0.195
1718 0.451
815 0.357
85 0.141
2 0.0
0 0.0
0 0.0
0 0.0
0.41106870229007636
0.4309160305343511
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6799,	 Acc1 = 0.2973,	 Acc2 = 0.3345

 ===== Epoch 15	 =====
[-0.36602148 -0.3783333   1.0823408   3.004938   -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0822006   3.0178175  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 0
train:	 Loss = 1.6537,	 Acc = 0.3694
2921 0.162
5679 0.426
2917 0.461
376 0.412
69 0.319
6 0.5
0 0.0
0 0.0
0.43638775284624737
0.4309160305343511
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.6482,	 Acc = 0.3887
380 0.184
1718 0.46
815 0.361
85 0.141
2 0.0
0 0.0
0 0.0
0 0.0
0.4183206106870229
0.4309160305343511
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6878,	 Acc1 = 0.3089,	 Acc2 = 0.3484

 ===== Epoch 16	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 0
train:	 Loss = 1.6525,	 Acc = 0.3639
2922 0.155
5681 0.419
2914 0.459
376 0.412
69 0.348
6 0.333
0 0.0
0 0.0
0.4312403272164493
0.4309160305343511
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.6426,	 Acc = 0.3743
380 0.195
1718 0.439
815 0.347
85 0.141
2 0.0
0 0.0
0 0.0
0 0.0
0.40038167938931296
0.4309160305343511
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6888,	 Acc1 = 0.2965,	 Acc2 = 0.3335

 ===== Epoch 17	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   1.7392207   1.5924073
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  1.8628049   1.6236856
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 2
train:	 Loss = 1.6489,	 Acc = 0.3698
2929 0.16
5679 0.43
2909 0.458
376 0.415
69 0.319
6 0.5
0 0.0
0 0.0
0.43765903307888043
0.4309160305343511
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6225,	 Acc = 0.3947
380 0.189
1718 0.47
815 0.362
85 0.118
2 0.0
0 0.0
0 0.0
0 0.0
0.42442748091603055
0.4309160305343511
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6737,	 Acc1 = 0.3095,	 Acc2 = 0.3492

 ===== Epoch 18	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.943811    1.3574945  -0.40141198 -0.40778467  1.9291795   3.040149
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  3.143213    1.3714464  -0.40141198 -0.40778467  1.959107    2.9736037
 -0.38555372 -0.3798593 ] 5 5
train:	 Loss = 1.6498,	 Acc = 0.3703
2924 0.157
5678 0.431
2917 0.463
374 0.396
69 0.391
6 0.5
0 0.0
0 0.0
0.43940734188412206
0.4309160305343511
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6212,	 Acc = 0.3833
380 0.197
1718 0.454
815 0.35
85 0.118
2 0.0
0 0.0
0 0.0
0 0.0
0.41030534351145037
0.4309160305343511
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6878,	 Acc1 = 0.2996,	 Acc2 = 0.3372

 ===== Epoch 19	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   4.548292    2.9740906  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  5.3372087   1.6588554   4.2004256   3.0586998  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 6
train:	 Loss = 1.6479,	 Acc = 0.3692
2921 0.16
5681 0.431
2914 0.451
377 0.424
69 0.362
6 0.667
0 0.0
0 0.0
0.43660882060351497
0.4309160305343511
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6331,	 Acc = 0.3787
380 0.192
1718 0.446
815 0.353
85 0.106
2 0.0
0 0.0
0 0.0
0 0.0
0.40572519083969466
0.4309160305343511
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6840,	 Acc1 = 0.3060,	 Acc2 = 0.3449

 ===== Epoch 20	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.6662672   1.6772081
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.4238129   1.6303802
 -0.38555372 -0.3798593 ] 1 5
train:	 Loss = 1.6476,	 Acc = 0.3688
2923 0.161
5683 0.428
2911 0.456
376 0.41
69 0.406
6 0.667
0 0.0
0 0.0
0.43604201216141514
0.4309160305343511
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6283,	 Acc = 0.3800
380 0.195
1718 0.446
815 0.352
85 0.153
2 0.0
0 0.0
0 0.0
0 0.0
0.4068702290076336
0.4309160305343511
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6746,	 Acc1 = 0.3060,	 Acc2 = 0.3449

 ===== Epoch 21	 =====
[ 0.5262838   1.6012176  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 0.5193611   1.4923677  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 3 5
train:	 Loss = 1.6451,	 Acc = 0.3707
2924 0.158
5680 0.432
2913 0.459
376 0.41
69 0.333
6 0.667
0 0.0
0 0.0
0.43940734188412206
0.4309160305343511
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.6348,	 Acc = 0.3747
380 0.189
1718 0.446
815 0.334
85 0.153
2 0.0
0 0.0
0 0.0
0 0.0
0.4015267175572519
0.4309160305343511
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6816,	 Acc1 = 0.2977,	 Acc2 = 0.3350

 ===== Epoch 22	 =====
[-0.36602148 -0.3783333   1.0799052   3.011734   -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.1398736   3.0132878  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.6500,	 Acc = 0.3673
2930 0.158
5678 0.43
2908 0.451
377 0.411
69 0.348
6 0.5
0 0.0
0 0.0
0.4352732905510069
0.4309160305343511
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6397,	 Acc = 0.3740
380 0.189
1718 0.438
815 0.351
85 0.141
2 0.0
0 0.0
0 0.0
0 0.0
0.40076335877862596
0.4309160305343511
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6952,	 Acc1 = 0.2989,	 Acc2 = 0.3365

 ===== Epoch 23	 =====
[-0.36602148 -0.3783333   3.6008983   2.7331004   3.6119277   1.3368032
  2.7202542   3.7265267   3.489818    0.82153577 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   4.0985956  -4.388444    3.9876702   1.3236172
  2.7151995   3.7042036   4.060821    0.81904715 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 2
train:	 Loss = 1.6419,	 Acc = 0.3712
2925 0.163
5682 0.435
2909 0.453
377 0.395
69 0.304
6 0.833
0 0.0
0 0.0
0.43868185336724536
0.4309160305343511
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6478,	 Acc = 0.3753
380 0.189
1718 0.441
815 0.348
85 0.153
2 0.0
0 0.0
0 0.0
0 0.0
0.4022900763358779
0.4309160305343511
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6870,	 Acc1 = 0.3161,	 Acc2 = 0.3571

 ===== Epoch 24	 =====
[ 1.6067183   2.6314955  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 2.773662    2.7707222  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.6447,	 Acc = 0.3713
2925 0.16
5681 0.438
2910 0.452
377 0.393
69 0.333
6 0.5
0 0.0
0 0.0
0.43956651553687937
0.4309160305343511
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6315,	 Acc = 0.3900
380 0.189
1718 0.458
815 0.366
85 0.153
2 0.0
0 0.0
0 0.0
0 0.0
0.41908396946564885
0.4309160305343511
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6794,	 Acc1 = 0.3229,	 Acc2 = 0.3653

 ===== Epoch 25	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   1.8518958   0.83893096
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  1.9388297   0.8323941
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 3 0
train:	 Loss = 1.6445,	 Acc = 0.3711
2926 0.159
5681 0.435
2910 0.455
376 0.415
69 0.319
6 0.667
0 0.0
0 0.0
0.4398363193983632
0.4309160305343511
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6340,	 Acc = 0.3810
380 0.189
1718 0.449
815 0.353
85 0.141
2 0.0
0 0.0
0 0.0
0 0.0
0.40877862595419845
0.4309160305343511
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6859,	 Acc1 = 0.3020,	 Acc2 = 0.3402

 ===== Epoch 26	 =====
[-0.36602148 -0.3783333   0.58033544  1.4667904  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   0.5810252   1.4708827  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 0
train:	 Loss = 1.6452,	 Acc = 0.3692
2925 0.163
5677 0.433
2914 0.449
377 0.387
69 0.362
6 0.667
0 0.0
0 0.0
0.4360278668583435
0.4309160305343511
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6248,	 Acc = 0.3833
380 0.187
1718 0.46
815 0.34
85 0.129
2 0.0
0 0.0
0 0.0
0 0.0
0.4118320610687023
0.4309160305343511
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6732,	 Acc1 = 0.3047,	 Acc2 = 0.3434

 ===== Epoch 27	 =====
[-0.36602148 -0.3783333   2.7812815   2.4929774   2.5669081   0.79003274
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   3.1701624   2.6101334   2.9775758   0.8968533
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.6416,	 Acc = 0.3724
2924 0.155
5682 0.438
2910 0.461
377 0.411
69 0.275
6 0.5
0 0.0
0 0.0
0.44272445820433437
0.4309160305343511
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6265,	 Acc = 0.3937
380 0.184
1718 0.472
815 0.355
85 0.129
2 0.0
0 0.0
0 0.0
0 0.0
0.42404580152671756
0.4309160305343511
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6717,	 Acc1 = 0.3161,	 Acc2 = 0.3571

 ===== Epoch 28	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  3.5319092   2.043928   -0.40141198 -0.40778467  3.1338751   2.8676245
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  3.4600105   2.1081066  -0.40141198 -0.40778467  3.2706354   2.931705
 -0.38555372 -0.3798593 ] 4 6
train:	 Loss = 1.6385,	 Acc = 0.3707
2921 0.16
5679 0.431
2917 0.461
376 0.402
69 0.304
6 0.667
0 0.0
0 0.0
0.4387089642975572
0.4309160305343511
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6276,	 Acc = 0.3957
380 0.189
1718 0.469
815 0.362
85 0.165
2 0.0
0 0.0
0 0.0
0 0.0
0.4255725190839695
0.4309160305343511
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6718,	 Acc1 = 0.3274,	 Acc2 = 0.3708

 ===== Epoch 29	 =====
[-0.36602148 -0.3783333   2.189922    1.8813426  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   2.2242644   1.898951   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 0
train:	 Loss = 1.6397,	 Acc = 0.3731
2920 0.163
5686 0.433
2913 0.46
375 0.424
68 0.324
6 0.667
0 0.0
0 0.0
0.4407603890362511
0.4309160305343511
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6131,	 Acc = 0.3943
380 0.171
1718 0.466
815 0.375
85 0.141
2 0.0
0 0.0
0 0.0
0 0.0
0.4267175572519084
0.4309160305343511
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6626,	 Acc1 = 0.3194,	 Acc2 = 0.3611

 ===== Epoch 30	 =====
[ 2.1408272   2.4542978  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.9221152   2.79035   ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.9560632   2.937313  ] 1 6
train:	 Loss = 1.6379,	 Acc = 0.3742
2923 0.161
5681 0.437
2914 0.462
375 0.397
69 0.362
6 0.833
0 0.0
0 0.0
0.44289662797125484
0.4309160305343511
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6198,	 Acc = 0.3963
380 0.179
1718 0.469
815 0.369
85 0.165
2 0.0
0 0.0
0 0.0
0 0.0
0.42786259541984734
0.4309160305343511
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6629,	 Acc1 = 0.3369,	 Acc2 = 0.3822

 ===== Epoch 31	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.6936686   0.9649298
  3.4185035   3.126266  ] [ 4.378794    2.7023745  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.5641074   0.80226415
  3.3016667   2.9294405 ] 5 5
train:	 Loss = 1.6376,	 Acc = 0.3718
2922 0.162
5679 0.432
2915 0.459
377 0.408
69 0.348
6 0.833
0 0.0
0 0.0
0.43953128454565554
0.4309160305343511
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6137,	 Acc = 0.4023
380 0.189
1718 0.474
815 0.38
85 0.129
2 0.0
0 0.0
0 0.0
0 0.0
0.43320610687022904
0.43320610687022904
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6595,	 Acc1 = 0.3392,	 Acc2 = 0.3849

 ===== Epoch 32	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.730795    1.5071485
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.7451901   1.7831872
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.6355,	 Acc = 0.3786
2921 0.157
5680 0.443
2916 0.47
376 0.404
69 0.362
6 0.833
0 0.0
0 0.0
0.44998341991820495
0.43320610687022904
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6231,	 Acc = 0.3903
380 0.187
1718 0.466
815 0.357
85 0.106
2 0.0
0 0.0
0 0.0
0 0.0
0.4198473282442748
0.43320610687022904
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6687,	 Acc1 = 0.3282,	 Acc2 = 0.3718

 ===== Epoch 33	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.5567844   2.4067388
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.5575422   2.3229413
 -0.38555372 -0.3798593 ] 5 5
train:	 Loss = 1.6360,	 Acc = 0.3709
2926 0.157
5677 0.434
2914 0.462
376 0.394
69 0.319
6 0.667
0 0.0
0 0.0
0.440168104401681
0.43320610687022904
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6195,	 Acc = 0.3993
380 0.189
1718 0.471
815 0.373
85 0.141
2 0.0
0 0.0
0 0.0
0 0.0
0.4297709923664122
0.43320610687022904
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6667,	 Acc1 = 0.3268,	 Acc2 = 0.3700

 ===== Epoch 34	 =====
[ 0.7254892   3.4541988  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.6341,	 Acc = 0.3745
2922 0.156
5683 0.441
2912 0.459
377 0.419
68 0.279
6 0.5
0 0.0
0 0.0
0.44505858943179305
0.43320610687022904
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6186,	 Acc = 0.4023
380 0.205
1718 0.476
815 0.367
85 0.141
2 0.0
0 0.0
0 0.0
0 0.0
0.4309160305343511
0.43320610687022904
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6658,	 Acc1 = 0.3299,	 Acc2 = 0.3738

 ===== Epoch 35	 =====
[-0.36602148 -0.3783333   1.5587596   0.83023745 -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.5362645   0.8593566  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 3 4
train:	 Loss = 1.6316,	 Acc = 0.3762
2929 0.155
5677 0.443
2912 0.461
375 0.424
69 0.406
6 0.833
0 0.0
0 0.0
0.44783715012722647
0.43320610687022904
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6176,	 Acc = 0.3787
380 0.182
1718 0.455
815 0.337
85 0.129
2 0.0
0 0.0
0 0.0
0 0.0
0.4072519083969466
0.43320610687022904
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6726,	 Acc1 = 0.2979,	 Acc2 = 0.3352

 ===== Epoch 36	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.4366927   1.7536116
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.3965367   1.9828223
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.6314,	 Acc = 0.3738
2925 0.165
5683 0.431
2909 0.468
376 0.404
69 0.319
6 0.667
0 0.0
0 0.0
0.4413358398761473
0.43320610687022904
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6070,	 Acc = 0.3910
380 0.195
1718 0.46
815 0.368
85 0.094
2 0.0
0 0.0
0 0.0
0 0.0
0.41946564885496185
0.43320610687022904
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6616,	 Acc1 = 0.3142,	 Acc2 = 0.3549

 ===== Epoch 37	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.6327,	 Acc = 0.3724
2923 0.157
5681 0.434
2912 0.465
377 0.406
69 0.304
6 0.5
0 0.0
0 0.0
0.4420121614151465
0.43320610687022904
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.5990,	 Acc = 0.4017
380 0.182
1718 0.468
815 0.395
85 0.118
2 0.0
0 0.0
0 0.0
0 0.0
0.433587786259542
0.433587786259542
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6558,	 Acc1 = 0.3122,	 Acc2 = 0.3524

 ===== Epoch 38	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 0
train:	 Loss = 1.6291,	 Acc = 0.3770
2924 0.16
5680 0.443
2915 0.461
374 0.417
69 0.304
6 0.667
0 0.0
0 0.0
0.44703670942061036
0.433587786259542
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6112,	 Acc = 0.3917
380 0.182
1718 0.469
815 0.353
85 0.153
2 0.0
0 0.0
0 0.0
0 0.0
0.4221374045801527
0.433587786259542
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6671,	 Acc1 = 0.3068,	 Acc2 = 0.3459

 ===== Epoch 39	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   1.5771273   2.150396   -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   1.4552609   2.1727927  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 1
train:	 Loss = 1.6308,	 Acc = 0.3754
2925 0.163
5681 0.44
2911 0.462
376 0.402
69 0.29
6 0.667
0 0.0
0 0.0
0.4442109919274577
0.433587786259542
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6102,	 Acc = 0.3863
380 0.197
1718 0.452
815 0.361
85 0.153
2 0.0
0 0.0
0 0.0
0 0.0
0.41374045801526715
0.433587786259542
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6663,	 Acc1 = 0.3218,	 Acc2 = 0.3641

 ===== Epoch 40	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 0
train:	 Loss = 1.6306,	 Acc = 0.3768
2925 0.161
5680 0.443
2913 0.462
375 0.4
69 0.29
6 0.667
0 0.0
0 0.0
0.44642264735154263
0.433587786259542
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6060,	 Acc = 0.4030
380 0.176
1718 0.473
815 0.391
85 0.129
2 0.0
0 0.0
0 0.0
0 0.0
0.43587786259541983
0.43587786259541983
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6539,	 Acc1 = 0.3183,	 Acc2 = 0.3598

 ===== Epoch 41	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  0.8642734   1.5589057
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  0.7911579   1.8300151
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.6308,	 Acc = 0.3808
2922 0.164
5682 0.445
2914 0.469
375 0.419
69 0.304
6 0.667
0 0.0
0 0.0
0.45091753261109885
0.43587786259541983
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.5866,	 Acc = 0.4057
380 0.192
1718 0.465
815 0.407
85 0.153
2 0.0
0 0.0
0 0.0
0 0.0
0.4366412213740458
0.4366412213740458
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6553,	 Acc1 = 0.3239,	 Acc2 = 0.3666

 ===== Epoch 42	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   1.4844587   2.3814461
  6.173015    1.237508    6.026721    2.4241316  -0.42342317 -0.42019257
  5.910865    2.5042963 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  1.668351    2.3816361
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 1
train:	 Loss = 1.6279,	 Acc = 0.3757
2922 0.161
5680 0.441
2914 0.461
377 0.403
69 0.304
6 0.667
0 0.0
0 0.0
0.4451691355295158
0.4366412213740458
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6019,	 Acc = 0.4003
380 0.192
1718 0.476
815 0.366
85 0.141
2 0.0
0 0.0
0 0.0
0 0.0
0.4305343511450382
0.4366412213740458
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6497,	 Acc1 = 0.3340,	 Acc2 = 0.3787

 ===== Epoch 43	 =====
[-0.36602148 -0.3783333   2.0835092   0.9027275  -0.4409929  -0.3635196
  1.3087057   3.5479424  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.9659604   0.9726021  -0.44088638 -0.36343393
  1.4109339   3.684671   -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
train:	 Loss = 1.6275,	 Acc = 0.3742
2926 0.159
5684 0.439
2907 0.462
376 0.412
69 0.304
6 0.5
0 0.0
0 0.0
0.4440389294403893
0.4366412213740458
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.5939,	 Acc = 0.4050
380 0.2
1718 0.476
815 0.38
85 0.141
2 0.0
0 0.0
0 0.0
0 0.0
0.4347328244274809
0.4366412213740458
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6542,	 Acc1 = 0.3161,	 Acc2 = 0.3571

 ===== Epoch 44	 =====
[-0.36602148 -0.3783333   2.157023    2.6470184  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   2.2486324   2.7596176  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.6253,	 Acc = 0.3782
2925 0.161
5682 0.444
2909 0.465
377 0.408
69 0.319
6 0.5
0 0.0
0 0.0
0.44830255446201484
0.4366412213740458
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6020,	 Acc = 0.4173
380 0.208
1718 0.491
815 0.39
85 0.129
2 0.0
0 0.0
0 0.0
0 0.0
0.44770992366412216
0.44770992366412216
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6570,	 Acc1 = 0.3260,	 Acc2 = 0.3690

 ===== Epoch 45	 =====
[ 1.2447399   1.3050445  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.201257    1.5464102 ] [ 1.0324967   1.4670538  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.3247066   1.6933737 ] 6 6
train:	 Loss = 1.6258,	 Acc = 0.3775
2930 0.157
5678 0.445
2912 0.467
373 0.383
69 0.362
6 0.667
0 0.0
0 0.0
0.44910378402301393
0.44770992366412216
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6009,	 Acc = 0.4013
380 0.195
1718 0.47
815 0.378
85 0.165
2 0.0
0 0.0
0 0.0
0 0.0
0.4312977099236641
0.44770992366412216
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6517,	 Acc1 = 0.3342,	 Acc2 = 0.3790

 ===== Epoch 46	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.6253,	 Acc = 0.3773
2926 0.163
5681 0.442
2911 0.466
376 0.386
68 0.353
6 0.5
0 0.0
0 0.0
0.4466932094669321
0.44770992366412216
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6019,	 Acc = 0.4067
380 0.205
1718 0.482
815 0.372
85 0.129
2 0.0
0 0.0
0 0.0
0 0.0
0.43587786259541983
0.44770992366412216
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6538,	 Acc1 = 0.3235,	 Acc2 = 0.3661

 ===== Epoch 47	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   1.3393168   1.5857394
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  1.3657802   1.5614492
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 3
train:	 Loss = 1.6257,	 Acc = 0.3711
2924 0.159
5678 0.432
2915 0.463
376 0.391
69 0.304
6 0.667
0 0.0
0 0.0
0.43951791242812915
0.44770992366412216
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6128,	 Acc = 0.4040
380 0.184
1718 0.477
815 0.382
85 0.141
2 0.0
0 0.0
0 0.0
0 0.0
0.43587786259541983
0.44770992366412216
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6526,	 Acc1 = 0.3400,	 Acc2 = 0.3859

 ===== Epoch 48	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 0
train:	 Loss = 1.6265,	 Acc = 0.3777
2924 0.16
5678 0.441
2915 0.469
376 0.404
69 0.333
6 0.667
0 0.0
0 0.0
0.44792127377266694
0.44770992366412216
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6096,	 Acc = 0.3857
380 0.174
1718 0.456
815 0.36
85 0.165
2 0.0
0 0.0
0 0.0
0 0.0
0.416412213740458
0.44770992366412216
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6583,	 Acc1 = 0.3210,	 Acc2 = 0.3631

 ===== Epoch 49	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.4056292   1.7289653
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.3400897   1.7141775
 -0.38555372 -0.3798593 ] 1 1
train:	 Loss = 1.6219,	 Acc = 0.3804
2928 0.155
5683 0.449
2907 0.471
375 0.4
69 0.362
6 0.5
0 0.0
0 0.0
0.453429203539823
0.44770992366412216
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.5926,	 Acc = 0.3937
380 0.184
1718 0.457
815 0.388
85 0.118
2 0.0
0 0.0
0 0.0
0 0.0
0.42404580152671756
0.44770992366412216
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6494,	 Acc1 = 0.3188,	 Acc2 = 0.3603

 ===== Epoch 50	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   1.0627667   2.5211833  -0.42342317 -0.42019257
  2.60682     1.1396368 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   1.0433146   2.5709534  -0.42342317 -0.42019257
  2.3246496   1.1028959 ] 1 0
train:	 Loss = 1.6225,	 Acc = 0.3796
2919 0.159
5686 0.444
2912 0.469
376 0.42
69 0.362
6 0.667
0 0.0
0 0.0
0.4506575312189192
0.44770992366412216
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6091,	 Acc = 0.3907
380 0.197
1718 0.462
815 0.352
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.41870229007633586
0.44770992366412216
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6637,	 Acc1 = 0.3161,	 Acc2 = 0.3571

 ===== Epoch 51	 =====
[-0.36602148 -0.3783333   1.6830426   2.40463    -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.7064371   2.3134298  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 5
train:	 Loss = 1.6236,	 Acc = 0.3780
2923 0.158
5675 0.445
2918 0.468
377 0.395
69 0.319
6 0.667
0 0.0
0 0.0
0.44919845218352683
0.44770992366412216
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6006,	 Acc = 0.3890
380 0.189
1718 0.459
815 0.366
85 0.106
2 0.0
0 0.0
0 0.0
0 0.0
0.4179389312977099
0.44770992366412216
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6748,	 Acc1 = 0.3062,	 Acc2 = 0.3452

 ===== Epoch 52	 =====
[-0.36602148 -0.3783333   1.4482862   2.5858552  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.3421307   2.775472   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 6
train:	 Loss = 1.6219,	 Acc = 0.3769
2923 0.164
5678 0.44
2916 0.463
376 0.402
69 0.391
6 0.667
0 0.0
0 0.0
0.445771144278607
0.44770992366412216
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6040,	 Acc = 0.4047
380 0.184
1718 0.478
815 0.377
85 0.176
2 0.0
0 0.0
0 0.0
0 0.0
0.4366412213740458
0.44770992366412216
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6544,	 Acc1 = 0.3408,	 Acc2 = 0.3869

 ===== Epoch 53	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.9663047   0.7751533
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.6291409   0.83923364
 -0.38555372 -0.3798593 ] 2 4
train:	 Loss = 1.6228,	 Acc = 0.3772
2921 0.162
5684 0.439
2913 0.469
375 0.4
69 0.391
6 0.667
0 0.0
0 0.0
0.4467779374378247
0.44770992366412216
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5916,	 Acc = 0.4163
380 0.195
1718 0.487
815 0.399
85 0.165
2 0.0
0 0.0
0 0.0
0 0.0
0.4484732824427481
0.4484732824427481
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6441,	 Acc1 = 0.3427,	 Acc2 = 0.3892

 ===== Epoch 54	 =====
[-0.36602148 -0.3783333   2.726045    1.3897699  -0.4409929  -0.3635196
  2.6595898   3.2326293  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   2.7351875   1.5524195  -0.44088638 -0.36343393
  2.6084757   3.464231   -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.6234,	 Acc = 0.3772
2924 0.158
5685 0.441
2908 0.471
376 0.394
69 0.348
6 0.667
0 0.0
0 0.0
0.44803184431667403
0.4484732824427481
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5965,	 Acc = 0.4147
380 0.187
1718 0.482
815 0.406
85 0.165
2 0.0
0 0.0
0 0.0
0 0.0
0.44770992366412216
0.4484732824427481
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6449,	 Acc1 = 0.3351,	 Acc2 = 0.3800

 ===== Epoch 55	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.5177633   2.0395088
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.4204037   2.0321147
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.6234,	 Acc = 0.3745
2925 0.156
5680 0.443
2912 0.458
377 0.395
68 0.338
6 0.667
0 0.0
0 0.0
0.4452062368682959
0.4484732824427481
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5986,	 Acc = 0.4137
380 0.195
1718 0.483
815 0.398
85 0.165
2 0.0
0 0.0
0 0.0
0 0.0
0.4454198473282443
0.4484732824427481
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6545,	 Acc1 = 0.3330,	 Acc2 = 0.3775

 ===== Epoch 56	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 0
train:	 Loss = 1.6205,	 Acc = 0.3773
2922 0.16
5677 0.441
2917 0.468
377 0.398
69 0.391
6 0.5
0 0.0
0 0.0
0.4473800574839708
0.4484732824427481
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5934,	 Acc = 0.4053
380 0.203
1718 0.472
815 0.385
85 0.165
2 0.0
0 0.0
0 0.0
0 0.0
0.4347328244274809
0.4484732824427481
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6509,	 Acc1 = 0.3412,	 Acc2 = 0.3874

 ===== Epoch 57	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   1.2092375   2.346988   -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   1.524491    2.416666   -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 4
train:	 Loss = 1.6199,	 Acc = 0.3789
2922 0.159
5677 0.443
2917 0.471
377 0.398
69 0.391
6 0.667
0 0.0
0 0.0
0.4498120716338713
0.4484732824427481
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6056,	 Acc = 0.4063
380 0.195
1718 0.476
815 0.387
85 0.141
2 0.0
0 0.0
0 0.0
0 0.0
0.43702290076335876
0.4484732824427481
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6652,	 Acc1 = 0.3138,	 Acc2 = 0.3544

 ===== Epoch 58	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  1.7967134   1.6414675
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.6203,	 Acc = 0.3807
2924 0.159
5680 0.445
2912 0.471
377 0.432
69 0.377
6 0.667
0 0.0
0 0.0
0.45234409553295
0.4484732824427481
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.5980,	 Acc = 0.4003
380 0.203
1718 0.469
815 0.375
85 0.141
2 0.0
0 0.0
0 0.0
0 0.0
0.42900763358778626
0.4484732824427481
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6574,	 Acc1 = 0.3183,	 Acc2 = 0.3598

 ===== Epoch 59	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 0
train:	 Loss = 1.6222,	 Acc = 0.3772
2929 0.161
5677 0.442
2911 0.471
376 0.37
69 0.304
6 0.667
0 0.0
0 0.0
0.447173359884943
0.4484732824427481
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.5953,	 Acc = 0.4120
380 0.205
1718 0.485
815 0.382
85 0.165
2 0.0
0 0.0
0 0.0
0 0.0
0.44198473282442746
0.4484732824427481
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6457,	 Acc1 = 0.3351,	 Acc2 = 0.3800

 ===== Epoch 60	 =====
[-0.36602148 -0.3783333   1.7610236   2.4068954  -0.4409929  -0.3635196
  6.1516714   3.852094   -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.8863561   2.4787683  -0.44088638 -0.36343393
  5.9915867   3.882788   -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
train:	 Loss = 1.6191,	 Acc = 0.3765
2926 0.16
5678 0.438
2912 0.471
377 0.398
69 0.348
6 0.5
0 0.0
0 0.0
0.4464720194647202
0.4484732824427481
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6040,	 Acc = 0.3990
380 0.184
1718 0.466
815 0.383
85 0.165
2 0.0
0 0.0
0 0.0
0 0.0
0.4301526717557252
0.4484732824427481
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6511,	 Acc1 = 0.3336,	 Acc2 = 0.3782

 ===== Epoch 61	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   1.387175    3.267734   -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.9781088   1.050409  ] 6 2
train:	 Loss = 1.6189,	 Acc = 0.3767
2924 0.158
5676 0.439
2917 0.471
376 0.402
69 0.348
6 0.667
0 0.0
0 0.0
0.4473684210526316
0.4484732824427481
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5930,	 Acc = 0.4230
380 0.195
1718 0.502
815 0.393
85 0.153
2 0.0
0 0.0
0 0.0
0 0.0
0.45610687022900764
0.45610687022900764
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6442,	 Acc1 = 0.3355,	 Acc2 = 0.3805

 ===== Epoch 62	 =====
[ 2.1915357   1.0746875  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.0213153   1.2577323 ] [ 1.9108452   0.98862004 -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.0354245   1.1842506 ] 2 5
train:	 Loss = 1.6187,	 Acc = 0.3758
2924 0.159
5685 0.437
2910 0.47
375 0.403
69 0.377
5 0.6
0 0.0
0 0.0
0.4459310039805396
0.45610687022900764
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5908,	 Acc = 0.4160
380 0.208
1718 0.483
815 0.394
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.44618320610687023
0.45610687022900764
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6483,	 Acc1 = 0.3361,	 Acc2 = 0.3812

 ===== Epoch 63	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 0
train:	 Loss = 1.6201,	 Acc = 0.3788
2922 0.158
5682 0.446
2915 0.471
375 0.381
68 0.294
6 0.667
0 0.0
0 0.0
0.45003316382931685
0.45610687022900764
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5838,	 Acc = 0.4133
380 0.189
1718 0.472
815 0.42
85 0.176
2 0.0
0 0.0
0 0.0
0 0.0
0.44580152671755724
0.45610687022900764
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6494,	 Acc1 = 0.3373,	 Acc2 = 0.3827

 ===== Epoch 64	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 0
train:	 Loss = 1.6204,	 Acc = 0.3779
2923 0.16
5682 0.44
2914 0.471
376 0.41
67 0.358
6 0.667
0 0.0
0 0.0
0.448424543946932
0.45610687022900764
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5840,	 Acc = 0.4337
380 0.189
1718 0.502
815 0.428
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.46908396946564884
0.46908396946564884
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6379,	 Acc1 = 0.3509,	 Acc2 = 0.3991

 ===== Epoch 65	 =====
[ 1.8108475   3.3149724  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.9362243   3.5619075 ] [ 3.0426373   3.320035   -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.9794308   3.5697806 ] 3 3
train:	 Loss = 1.6223,	 Acc = 0.3788
2925 0.159
5681 0.444
2911 0.469
376 0.404
69 0.333
6 0.667
0 0.0
0 0.0
0.45007187880128274
0.46908396946564884
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.5922,	 Acc = 0.4063
380 0.189
1718 0.48
815 0.382
85 0.141
2 0.0
0 0.0
0 0.0
0 0.0
0.43778625954198475
0.46908396946564884
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6505,	 Acc1 = 0.3243,	 Acc2 = 0.3670

 ===== Epoch 66	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.6156,	 Acc = 0.3808
2926 0.163
5681 0.446
2909 0.472
377 0.39
69 0.333
6 0.667
0 0.0
0 0.0
0.45144879451448794
0.46908396946564884
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6121,	 Acc = 0.4253
380 0.189
1718 0.509
815 0.387
85 0.176
2 0.0
0 0.0
0 0.0
0 0.0
0.4595419847328244
0.46908396946564884
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6470,	 Acc1 = 0.3464,	 Acc2 = 0.3936

 ===== Epoch 67	 =====
[-0.36602148 -0.3783333   1.6042482   2.3185482  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.7421776   2.3994966  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 4
train:	 Loss = 1.6190,	 Acc = 0.3782
2920 0.162
5685 0.445
2912 0.465
376 0.378
69 0.362
6 0.667
0 0.0
0 0.0
0.4479442970822281
0.46908396946564884
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6006,	 Acc = 0.4153
380 0.192
1718 0.49
815 0.393
85 0.129
2 0.0
0 0.0
0 0.0
0 0.0
0.44770992366412216
0.46908396946564884
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6552,	 Acc1 = 0.3344,	 Acc2 = 0.3792

 ===== Epoch 68	 =====
[ 2.101745    2.5555537  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.585273    2.8612072 ] [ 2.6932247   2.5682106  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.5764562   2.837588  ] 4 3
train:	 Loss = 1.6185,	 Acc = 0.3788
2922 0.155
5682 0.442
2912 0.474
377 0.419
69 0.42
6 0.667
0 0.0
0 0.0
0.45102807870882156
0.46908396946564884
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6020,	 Acc = 0.4083
380 0.192
1718 0.471
815 0.406
85 0.141
2 0.0
0 0.0
0 0.0
0 0.0
0.4396946564885496
0.46908396946564884
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6522,	 Acc1 = 0.3315,	 Acc2 = 0.3757

 ===== Epoch 69	 =====
[ 0.21495906  1.5404643  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 0.21495906  1.5404643  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.6186,	 Acc = 0.3796
2925 0.158
5680 0.444
2912 0.472
376 0.41
69 0.362
6 0.667
0 0.0
0 0.0
0.45128828928452946
0.46908396946564884
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6013,	 Acc = 0.4060
380 0.195
1718 0.477
815 0.379
85 0.176
2 0.0
0 0.0
0 0.0
0 0.0
0.4366412213740458
0.46908396946564884
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6569,	 Acc1 = 0.3196,	 Acc2 = 0.3613

 ===== Epoch 70	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 0
train:	 Loss = 1.6200,	 Acc = 0.3799
2926 0.162
5679 0.446
2912 0.468
377 0.401
68 0.353
6 0.5
0 0.0
0 0.0
0.45056403450564037
0.46908396946564884
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6072,	 Acc = 0.4110
380 0.208
1718 0.488
815 0.374
85 0.118
2 0.0
0 0.0
0 0.0
0 0.0
0.4404580152671756
0.46908396946564884
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6565,	 Acc1 = 0.3295,	 Acc2 = 0.3733

 ===== Epoch 71	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 3 0
train:	 Loss = 1.6161,	 Acc = 0.3818
2925 0.16
5682 0.448
2911 0.472
376 0.41
68 0.353
6 0.667
0 0.0
0 0.0
0.45361052747981867
0.46908396946564884
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.5945,	 Acc = 0.4000
380 0.189
1718 0.476
815 0.364
85 0.165
2 0.0
0 0.0
0 0.0
0 0.0
0.4305343511450382
0.46908396946564884
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6479,	 Acc1 = 0.3122,	 Acc2 = 0.3524

 ===== Epoch 72	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 3 0
train:	 Loss = 1.6220,	 Acc = 0.3769
2925 0.163
5678 0.437
2914 0.474
376 0.38
69 0.377
6 0.667
0 0.0
0 0.0
0.44609089903792987
0.46908396946564884
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6094,	 Acc = 0.4003
380 0.192
1718 0.474
815 0.373
85 0.118
2 0.0
0 0.0
0 0.0
0 0.0
0.4305343511450382
0.46908396946564884
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6608,	 Acc1 = 0.3185,	 Acc2 = 0.3601

 ===== Epoch 73	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 3 0
train:	 Loss = 1.6202,	 Acc = 0.3765
2925 0.161
5679 0.441
2913 0.465
377 0.39
68 0.324
6 0.667
0 0.0
0 0.0
0.44620148180913416
0.46908396946564884
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5998,	 Acc = 0.4170
380 0.179
1718 0.492
815 0.393
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.45152671755725193
0.46908396946564884
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6446,	 Acc1 = 0.3396,	 Acc2 = 0.3854

 ===== Epoch 74	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 0
train:	 Loss = 1.6199,	 Acc = 0.3753
2922 0.162
5680 0.436
2914 0.47
377 0.393
69 0.319
6 0.667
0 0.0
0 0.0
0.4442847667477338
0.46908396946564884
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.5919,	 Acc = 0.4147
380 0.197
1718 0.492
815 0.382
85 0.153
2 0.0
0 0.0
0 0.0
0 0.0
0.44618320610687023
0.46908396946564884
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6479,	 Acc1 = 0.3235,	 Acc2 = 0.3661

 ===== Epoch 75	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 0
train:	 Loss = 1.6132,	 Acc = 0.3820
2922 0.159
5684 0.447
2912 0.477
375 0.408
69 0.362
6 0.667
0 0.0
0 0.0
0.4541233694450586
0.46908396946564884
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6245,	 Acc = 0.3933
380 0.187
1718 0.467
815 0.363
85 0.118
2 0.0
0 0.0
0 0.0
0 0.0
0.4232824427480916
0.46908396946564884
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6682,	 Acc1 = 0.3122,	 Acc2 = 0.3524

 ===== Epoch 76	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   1.7562091   2.9740906  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   1.7178763   3.1034927  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.6164,	 Acc = 0.3803
2929 0.158
5681 0.449
2908 0.47
375 0.395
69 0.275
6 0.667
0 0.0
0 0.0
0.4522624184091161
0.46908396946564884
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6079,	 Acc = 0.4030
380 0.189
1718 0.47
815 0.385
85 0.176
2 0.0
0 0.0
0 0.0
0 0.0
0.43396946564885497
0.46908396946564884
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6518,	 Acc1 = 0.3367,	 Acc2 = 0.3820

 ===== Epoch 77	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.8261354   1.1966051
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.9189512   1.2335745
 -0.38555372 -0.3798593 ] 4 1
train:	 Loss = 1.6154,	 Acc = 0.3788
2923 0.159
5678 0.444
2915 0.468
377 0.414
69 0.377
6 0.5
0 0.0
0 0.0
0.44986180210060805
0.46908396946564884
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6078,	 Acc = 0.4140
380 0.197
1718 0.49
815 0.383
85 0.165
2 0.0
0 0.0
0 0.0
0 0.0
0.4454198473282443
0.46908396946564884
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6484,	 Acc1 = 0.3371,	 Acc2 = 0.3825

 ===== Epoch 78	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.113547    1.590946
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.1264268   1.7659348
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.6139,	 Acc = 0.3774
2925 0.158
5681 0.44
2912 0.474
375 0.397
69 0.348
6 0.667
0 0.0
0 0.0
0.4485237200044233
0.46908396946564884
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.5919,	 Acc = 0.4123
380 0.182
1718 0.487
815 0.39
85 0.153
2 0.0
0 0.0
0 0.0
0 0.0
0.44580152671755724
0.46908396946564884
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6482,	 Acc1 = 0.3251,	 Acc2 = 0.3680

 ===== Epoch 79	 =====
[-0.36602148 -0.3783333   1.5518552   2.3842423  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.6361753   2.3021054  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 5
train:	 Loss = 1.6153,	 Acc = 0.3772
2922 0.167
5681 0.438
2914 0.469
376 0.383
69 0.348
6 0.667
0 0.0
0 0.0
0.4451691355295158
0.46908396946564884
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6184,	 Acc = 0.4077
380 0.168
1718 0.485
815 0.385
85 0.129
2 0.0
0 0.0
0 0.0
0 0.0
0.44236641221374046
0.46908396946564884
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6618,	 Acc1 = 0.3295,	 Acc2 = 0.3733

 ===== Epoch 80	 =====
[-0.36602148 -0.3783333   4.1617966  -4.5272284   3.690228    8.158099
  3.5504453  -4.982248    3.4532006   0.8389552  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 6
train:	 Loss = 1.6170,	 Acc = 0.3790
2928 0.158
5675 0.445
2915 0.473
375 0.376
69 0.319
6 0.667
0 0.0
0 0.0
0.4506637168141593
0.46908396946564884
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6253,	 Acc = 0.4223
380 0.195
1718 0.506
815 0.383
85 0.129
2 0.0
0 0.0
0 0.0
0 0.0
0.45534351145038165
0.46908396946564884
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6698,	 Acc1 = 0.3414,	 Acc2 = 0.3877

 ===== Epoch 81	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  0.74607635  1.6131276
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  0.8097212   1.5613705
 -0.38555372 -0.3798593 ] 1 3
train:	 Loss = 1.6170,	 Acc = 0.3751
2920 0.156
5680 0.438
2917 0.47
376 0.383
69 0.362
6 0.667
0 0.0
0 0.0
0.4457338638373121
0.46908396946564884
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6177,	 Acc = 0.4263
380 0.195
1718 0.503
815 0.404
85 0.129
2 0.0
0 0.0
0 0.0
0 0.0
0.4599236641221374
0.46908396946564884
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6656,	 Acc1 = 0.3318,	 Acc2 = 0.3760

 ===== Epoch 82	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 0
train:	 Loss = 1.6165,	 Acc = 0.3803
2926 0.16
5679 0.443
2911 0.477
377 0.401
69 0.362
6 0.667
0 0.0
0 0.0
0.4516699845166998
0.46908396946564884
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6059,	 Acc = 0.3897
380 0.192
1718 0.467
815 0.348
85 0.118
2 0.0
0 0.0
0 0.0
0 0.0
0.4183206106870229
0.46908396946564884
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6620,	 Acc1 = 0.3146,	 Acc2 = 0.3554

 ===== Epoch 83	 =====
[ 0.28585345  3.001079   -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 0.2368956   3.0744896  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 1
train:	 Loss = 1.6115,	 Acc = 0.3820
2927 0.16
5680 0.446
2910 0.478
376 0.402
69 0.348
6 0.667
0 0.0
0 0.0
0.45404269439221323
0.46908396946564884
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5870,	 Acc = 0.4220
380 0.192
1718 0.492
815 0.409
85 0.165
2 0.0
0 0.0
0 0.0
0 0.0
0.45534351145038165
0.46908396946564884
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6413,	 Acc1 = 0.3410,	 Acc2 = 0.3872

 ===== Epoch 84	 =====
[ 2.5928674   3.193465   -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 2.3438845   3.0643642  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 5
train:	 Loss = 1.6142,	 Acc = 0.3773
2924 0.158
5681 0.44
2915 0.473
374 0.404
68 0.338
6 0.667
0 0.0
0 0.0
0.44836355594869526
0.46908396946564884
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.5877,	 Acc = 0.4093
380 0.195
1718 0.478
815 0.394
85 0.141
2 0.0
0 0.0
0 0.0
0 0.0
0.4404580152671756
0.46908396946564884
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6582,	 Acc1 = 0.3122,	 Acc2 = 0.3524

 ===== Epoch 85	 =====
[ 4.5409055   1.9834584   0.6290747   2.665141   -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 3.0505717   1.7606955   0.617578    2.385907   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 5
train:	 Loss = 1.6170,	 Acc = 0.3798
2927 0.159
5680 0.446
2913 0.472
373 0.378
69 0.333
6 0.667
0 0.0
0 0.0
0.4512775135493861
0.46908396946564884
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.5988,	 Acc = 0.4037
380 0.2
1718 0.47
815 0.387
85 0.141
2 0.0
0 0.0
0 0.0
0 0.0
0.43320610687022904
0.46908396946564884
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6613,	 Acc1 = 0.3142,	 Acc2 = 0.3549

 ===== Epoch 86	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.4181306   0.88113236
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.396915    0.9328896
 -0.38555372 -0.3798593 ] 3 1
train:	 Loss = 1.6149,	 Acc = 0.3781
2927 0.157
5680 0.446
2909 0.465
377 0.395
69 0.362
6 0.5
0 0.0
0 0.0
0.4495077978099768
0.46908396946564884
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6082,	 Acc = 0.3993
380 0.187
1718 0.472
815 0.375
85 0.118
2 0.0
0 0.0
0 0.0
0 0.0
0.4301526717557252
0.46908396946564884
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6590,	 Acc1 = 0.3163,	 Acc2 = 0.3574

 ===== Epoch 87	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 0
train:	 Loss = 1.6135,	 Acc = 0.3813
2925 0.157
5679 0.451
2914 0.47
376 0.391
68 0.324
6 0.833
0 0.0
0 0.0
0.45383169302222715
0.46908396946564884
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6014,	 Acc = 0.4187
380 0.195
1718 0.49
815 0.401
85 0.165
2 0.0
0 0.0
0 0.0
0 0.0
0.45114503816793894
0.46908396946564884
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6553,	 Acc1 = 0.3301,	 Acc2 = 0.3740

 ===== Epoch 88	 =====
[-0.36602148 -0.3783333   1.4742798   1.5981785  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.4038639   1.8355335  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.6136,	 Acc = 0.3804
2921 0.16
5684 0.444
2912 0.475
376 0.388
69 0.391
6 0.667
0 0.0
0 0.0
0.45164142809771196
0.46908396946564884
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6056,	 Acc = 0.4117
380 0.203
1718 0.485
815 0.382
85 0.165
2 0.0
0 0.0
0 0.0
0 0.0
0.44198473282442746
0.46908396946564884
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6583,	 Acc1 = 0.3173,	 Acc2 = 0.3586

 ===== Epoch 89	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 0
train:	 Loss = 1.6150,	 Acc = 0.3793
2924 0.159
5688 0.446
2905 0.469
376 0.404
69 0.333
6 0.833
0 0.0
0 0.0
0.4506855373728439
0.46908396946564884
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6001,	 Acc = 0.4073
380 0.189
1718 0.479
815 0.388
85 0.129
2 0.0
0 0.0
0 0.0
0 0.0
0.4389312977099237
0.46908396946564884
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6619,	 Acc1 = 0.3138,	 Acc2 = 0.3544

 ===== Epoch 90	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 0
train:	 Loss = 1.6145,	 Acc = 0.3765
2918 0.155
5682 0.444
2918 0.466
375 0.381
69 0.333
6 0.667
0 0.0
0 0.0
0.4478453038674033
0.46908396946564884
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.5964,	 Acc = 0.4013
380 0.195
1718 0.471
815 0.379
85 0.141
2 0.0
0 0.0
0 0.0
0 0.0
0.4312977099236641
0.46908396946564884
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6532,	 Acc1 = 0.3194,	 Acc2 = 0.3611

 ===== Epoch 91	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   0.8951081   2.9837828
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  0.8236761   2.9262047
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 2
train:	 Loss = 1.6118,	 Acc = 0.3812
2923 0.159
5680 0.443
2914 0.481
376 0.383
69 0.391
6 0.667
0 0.0
0 0.0
0.45284687672747376
0.46908396946564884
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6251,	 Acc = 0.3937
380 0.184
1718 0.462
815 0.374
85 0.153
2 0.0
0 0.0
0 0.0
0 0.0
0.42404580152671756
0.46908396946564884
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6700,	 Acc1 = 0.3093,	 Acc2 = 0.3489

 ===== Epoch 92	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  3.2959945   1.6086287  -0.40141198 -0.40778467  3.09675     2.6803126
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  3.5554996   1.7676803  -0.40141198 -0.40778467  2.90468     2.852837
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.6132,	 Acc = 0.3807
2924 0.159
5683 0.449
2912 0.469
374 0.388
69 0.377
6 0.667
0 0.0
0 0.0
0.4524546660769571
0.46908396946564884
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6026,	 Acc = 0.4110
380 0.176
1718 0.486
815 0.391
85 0.141
2 0.0
0 0.0
0 0.0
0 0.0
0.4450381679389313
0.46908396946564884
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6601,	 Acc1 = 0.3309,	 Acc2 = 0.3750

 ===== Epoch 93	 =====
[-0.36602148 -0.3783333   1.6647656   2.4725895   3.0630636   0.77447426
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.6024648   2.7075245   2.7040408   0.9635352
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.6171,	 Acc = 0.3815
2924 0.164
5678 0.451
2917 0.466
376 0.37
67 0.403
6 0.667
0 0.0
0 0.0
0.4519018133569217
0.46908396946564884
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6024,	 Acc = 0.3960
380 0.184
1718 0.463
815 0.38
85 0.141
2 0.0
0 0.0
0 0.0
0 0.0
0.4267175572519084
0.46908396946564884
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6600,	 Acc1 = 0.3227,	 Acc2 = 0.3651

 ===== Epoch 94	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 0
train:	 Loss = 1.6153,	 Acc = 0.3793
2922 0.16
5679 0.446
2915 0.468
377 0.39
69 0.377
6 0.667
0 0.0
0 0.0
0.45014370992703956
0.46908396946564884
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6179,	 Acc = 0.4130
380 0.195
1718 0.491
815 0.382
85 0.129
2 0.0
0 0.0
0 0.0
0 0.0
0.4446564885496183
0.46908396946564884
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6707,	 Acc1 = 0.3268,	 Acc2 = 0.3700

 ===== Epoch 95	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  1.9299445   2.479227   -0.40141198 -0.40778467  2.6519973   2.852837
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  1.9254509   2.6857152  -0.40141198 -0.40778467  2.6535115   3.0007148
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.6117,	 Acc = 0.3809
2923 0.158
5679 0.448
2915 0.47
376 0.41
69 0.319
6 0.667
0 0.0
0 0.0
0.45284687672747376
0.46908396946564884
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.5994,	 Acc = 0.4047
380 0.192
1718 0.476
815 0.382
85 0.141
2 0.0
0 0.0
0 0.0
0 0.0
0.4354961832061069
0.46908396946564884
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6533,	 Acc1 = 0.3148,	 Acc2 = 0.3556

 ===== Epoch 96	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  1.2530978   3.7851248  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
train:	 Loss = 1.6120,	 Acc = 0.3772
2924 0.154
5682 0.445
2912 0.468
375 0.381
69 0.377
6 0.667
0 0.0
0 0.0
0.4492481203007519
0.46908396946564884
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6306,	 Acc = 0.4133
380 0.174
1718 0.491
815 0.394
85 0.118
2 0.0
0 0.0
0 0.0
0 0.0
0.4480916030534351
0.46908396946564884
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6802,	 Acc1 = 0.3346,	 Acc2 = 0.3795

 ===== Epoch 97	 =====
[ 1.609918    3.4643247  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
train:	 Loss = 1.6112,	 Acc = 0.3840
2925 0.155
5676 0.455
2915 0.475
377 0.382
69 0.406
6 0.5
0 0.0
0 0.0
0.45814442109919273
0.46908396946564884
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6065,	 Acc = 0.4087
380 0.208
1718 0.477
815 0.384
85 0.165
2 0.0
0 0.0
0 0.0
0 0.0
0.43778625954198475
0.46908396946564884
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6510,	 Acc1 = 0.3266,	 Acc2 = 0.3698

 ===== Epoch 98	 =====
[-0.36602148 -0.3783333   3.4985485  -4.1602473   2.8571918   1.2767919
 -0.3640846  -0.3725409   1.8786504   0.9310298  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  2.7739527   1.3725173
 -0.3640846  -0.3725409   1.9021068   1.0579435  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 2
train:	 Loss = 1.6190,	 Acc = 0.3770
2923 0.159
5680 0.441
2915 0.469
376 0.394
68 0.353
6 0.667
0 0.0
0 0.0
0.44731896075179656
0.46908396946564884
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.6239,	 Acc = 0.3847
380 0.187
1718 0.458
815 0.351
85 0.118
2 0.0
0 0.0
0 0.0
0 0.0
0.4133587786259542
0.46908396946564884
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6753,	 Acc1 = 0.3130,	 Acc2 = 0.3534

 ===== Epoch 99	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.6295203   1.2483623
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.5749682   1.3937756
 -0.38555372 -0.3798593 ] 0 6
train:	 Loss = 1.6114,	 Acc = 0.3790
2920 0.159
5682 0.445
2915 0.468
376 0.402
69 0.377
6 0.833
0 0.0
0 0.0
0.4501547303271441
0.46908396946564884
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6136,	 Acc = 0.4333
380 0.189
1718 0.513
815 0.41
85 0.141
2 0.0
0 0.0
0 0.0
0 0.0
0.4687022900763359
0.46908396946564884
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6549,	 Acc1 = 0.3429,	 Acc2 = 0.3894

 ===== Epoch 100	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 0
train:	 Loss = 1.6106,	 Acc = 0.3817
2924 0.16
5682 0.451
2911 0.47
377 0.371
68 0.382
6 0.833
0 0.0
0 0.0
0.4534498009730208
0.46908396946564884
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.5909,	 Acc = 0.4130
380 0.187
1718 0.487
815 0.393
85 0.129
2 0.0
0 0.0
0 0.0
0 0.0
0.44580152671755724
0.46908396946564884
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6509,	 Acc1 = 0.3214,	 Acc2 = 0.3636

 ===== Epoch 101	 =====
[-0.36602148 -0.3783333   4.244246    0.79172707 -0.4409929  -0.3635196
  4.113267    2.2029793  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  3.8442132   1.9909106  -0.40141198 -0.40778467  2.347414    3.2841473
 -0.38555372 -0.3798593 ] 5 5
train:	 Loss = 1.6133,	 Acc = 0.3790
2923 0.157
5680 0.443
2914 0.475
377 0.374
68 0.397
6 0.667
0 0.0
0 0.0
0.4506357103372029
0.46908396946564884
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6050,	 Acc = 0.4210
380 0.184
1718 0.494
815 0.407
85 0.153
2 0.0
0 0.0
0 0.0
0 0.0
0.45534351145038165
0.46908396946564884
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6487,	 Acc1 = 0.3417,	 Acc2 = 0.3879

 ===== Epoch 102	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 0.22154364  3.7883432  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 6
train:	 Loss = 1.6124,	 Acc = 0.3788
2925 0.159
5680 0.449
2913 0.463
375 0.381
69 0.333
6 0.667
0 0.0
0 0.0
0.44974013048767003
0.46908396946564884
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6257,	 Acc = 0.3963
380 0.182
1718 0.477
815 0.356
85 0.129
2 0.0
0 0.0
0 0.0
0 0.0
0.42748091603053434
0.46908396946564884
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6667,	 Acc1 = 0.3212,	 Acc2 = 0.3633

 ===== Epoch 103	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   2.5203106   2.7659636
 -0.3640846  -0.3725409   1.7041441   2.2797983  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  2.5286882   2.7817273
 -0.3640846  -0.3725409   1.6045915   2.3270798  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 4
train:	 Loss = 1.6092,	 Acc = 0.3844
2921 0.157
5682 0.452
2914 0.476
376 0.407
69 0.348
6 0.833
0 0.0
0 0.0
0.4576102575439372
0.46908396946564884
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6215,	 Acc = 0.4083
380 0.192
1718 0.481
815 0.382
85 0.165
2 0.0
0 0.0
0 0.0
0 0.0
0.4396946564885496
0.46908396946564884
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6656,	 Acc1 = 0.3206,	 Acc2 = 0.3626

 ===== Epoch 104	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   3.0984683   1.3665178  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   3.0778718   1.4237534  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 4
train:	 Loss = 1.6140,	 Acc = 0.3804
2923 0.16
5682 0.447
2911 0.471
377 0.39
69 0.319
6 0.667
0 0.0
0 0.0
0.4515201768933112
0.46908396946564884
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6093,	 Acc = 0.4043
380 0.197
1718 0.476
815 0.38
85 0.129
2 0.0
0 0.0
0 0.0
0 0.0
0.43435114503816796
0.46908396946564884
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6719,	 Acc1 = 0.3082,	 Acc2 = 0.3477

 ===== Epoch 105	 =====
[-0.36602148 -0.3783333   2.6777122   1.6163011  -0.4409929  -0.3635196
  1.5749499   3.0903203  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   2.6511161   1.5524195  -0.44088638 -0.36343393
  1.5451814   2.9759147  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 5
train:	 Loss = 1.6089,	 Acc = 0.3808
2924 0.161
5680 0.445
2913 0.476
376 0.375
69 0.362
6 0.667
0 0.0
0 0.0
0.45179124281291466
0.46908396946564884
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6160,	 Acc = 0.4120
380 0.187
1718 0.484
815 0.391
85 0.176
2 0.0
0 0.0
0 0.0
0 0.0
0.4446564885496183
0.46908396946564884
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6721,	 Acc1 = 0.3338,	 Acc2 = 0.3785

 ===== Epoch 106	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   0.40200877  2.05694
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  4.7628684   1.1892111
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  0.36103448  2.3616316
  4.9136877   1.067295   -0.40141198 -0.40778467  4.7306666   1.4208864
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.6086,	 Acc = 0.3827
2926 0.161
5684 0.45
2906 0.471
377 0.398
69 0.391
6 0.667
0 0.0
0 0.0
0.4544348595443486
0.46908396946564884
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6222,	 Acc = 0.4020
380 0.2
1718 0.473
815 0.375
85 0.129
2 0.0
0 0.0
0 0.0
0 0.0
0.4312977099236641
0.46908396946564884
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6672,	 Acc1 = 0.3241,	 Acc2 = 0.3668

 ===== Epoch 107	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 0
train:	 Loss = 1.6102,	 Acc = 0.3869
2925 0.161
5682 0.457
2911 0.476
375 0.4
69 0.333
6 0.667
0 0.0
0 0.0
0.45980316266725646
0.46908396946564884
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6278,	 Acc = 0.4183
380 0.203
1718 0.496
815 0.387
85 0.129
2 0.0
0 0.0
0 0.0
0 0.0
0.449618320610687
0.46908396946564884
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6704,	 Acc1 = 0.3313,	 Acc2 = 0.3755

 ===== Epoch 108	 =====
[ 1.7231159   3.4238222  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.0821598   3.624892  ] [ 2.3513288   3.4567304  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 3 2
train:	 Loss = 1.6089,	 Acc = 0.3853
2923 0.163
5678 0.452
2916 0.478
377 0.406
68 0.309
6 0.667
0 0.0
0 0.0
0.4571586511885019
0.46908396946564884
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6349,	 Acc = 0.3940
380 0.195
1718 0.469
815 0.358
85 0.129
2 0.0
0 0.0
0 0.0
0 0.0
0.42290076335877863
0.46908396946564884
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6825,	 Acc1 = 0.3132,	 Acc2 = 0.3536

 ===== Epoch 109	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.5253403   2.044438
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.5257198   2.0345795
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.6124,	 Acc = 0.3840
2925 0.166
5684 0.45
2907 0.476
377 0.385
69 0.304
6 0.667
0 0.0
0 0.0
0.45460577242065686
0.46908396946564884
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6255,	 Acc = 0.3933
380 0.195
1718 0.46
815 0.372
85 0.153
2 0.0
0 0.0
0 0.0
0 0.0
0.4221374045801527
0.46908396946564884
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6724,	 Acc1 = 0.3150,	 Acc2 = 0.3559

 ===== Epoch 110	 =====
[ 0.09239139  1.42402    -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 0.16952829  1.7379131  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
train:	 Loss = 1.6083,	 Acc = 0.3827
2923 0.16
5680 0.446
2913 0.48
377 0.395
69 0.362
6 0.833
0 0.0
0 0.0
0.454726368159204
0.46908396946564884
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6251,	 Acc = 0.4167
380 0.195
1718 0.498
815 0.379
85 0.141
2 0.0
0 0.0
0 0.0
0 0.0
0.4488549618320611
0.46908396946564884
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6605,	 Acc1 = 0.3417,	 Acc2 = 0.3879

 ===== Epoch 111	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 0
train:	 Loss = 1.6076,	 Acc = 0.3850
2924 0.161
5680 0.454
2912 0.474
377 0.401
69 0.362
6 0.667
0 0.0
0 0.0
0.45754091110128264
0.46908396946564884
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6013,	 Acc = 0.4037
380 0.192
1718 0.478
815 0.373
85 0.141
2 0.0
0 0.0
0 0.0
0 0.0
0.43435114503816796
0.46908396946564884
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6549,	 Acc1 = 0.3225,	 Acc2 = 0.3648

 ===== Epoch 112	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 0
train:	 Loss = 1.6093,	 Acc = 0.3814
2926 0.16
5678 0.448
2914 0.474
376 0.372
68 0.397
6 0.667
0 0.0
0 0.0
0.45299712452997126
0.46908396946564884
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6281,	 Acc = 0.3930
380 0.195
1718 0.464
815 0.361
85 0.153
2 0.0
0 0.0
0 0.0
0 0.0
0.4217557251908397
0.46908396946564884
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6640,	 Acc1 = 0.3210,	 Acc2 = 0.3631

 ===== Epoch 113	 =====
[-0.36602148 -0.3783333   2.6817741   2.0376494  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   2.263254    2.0257862  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 1
train:	 Loss = 1.6088,	 Acc = 0.3836
2922 0.16
5684 0.451
2913 0.474
375 0.392
68 0.441
6 0.667
0 0.0
0 0.0
0.4558921070086226
0.46908396946564884
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6274,	 Acc = 0.4123
380 0.197
1718 0.49
815 0.379
85 0.129
2 0.0
0 0.0
0 0.0
0 0.0
0.4435114503816794
0.46908396946564884
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6642,	 Acc1 = 0.3346,	 Acc2 = 0.3795

 ===== Epoch 114	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 3 0
train:	 Loss = 1.6066,	 Acc = 0.3869
2924 0.162
5682 0.448
2910 0.49
377 0.387
69 0.493
6 0.667
0 0.0
0 0.0
0.45953118089341
0.46908396946564884
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6347,	 Acc = 0.3963
380 0.197
1718 0.464
815 0.371
85 0.165
2 0.0
0 0.0
0 0.0
0 0.0
0.4251908396946565
0.46908396946564884
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6739,	 Acc1 = 0.3381,	 Acc2 = 0.3837

 ===== Epoch 115	 =====
[-0.36602148 -0.3783333   1.3723354   1.8224447  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.4988995   1.9397194  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.6078,	 Acc = 0.3844
2917 0.159
5683 0.451
2916 0.479
377 0.385
69 0.362
6 0.833
0 0.0
0 0.0
0.4569660810960115
0.46908396946564884
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6214,	 Acc = 0.4017
380 0.192
1718 0.469
815 0.385
85 0.153
2 0.0
0 0.0
0 0.0
0 0.0
0.43206106870229005
0.46908396946564884
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6677,	 Acc1 = 0.3301,	 Acc2 = 0.3740

 ===== Epoch 116	 =====
[-0.36602148 -0.3783333   1.683855    1.2153407  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.6024648   1.210418   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 0
train:	 Loss = 1.6092,	 Acc = 0.3836
2925 0.163
5680 0.45
2912 0.475
376 0.394
69 0.348
6 0.667
0 0.0
0 0.0
0.45504810350547387
0.46908396946564884
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6166,	 Acc = 0.3967
380 0.192
1718 0.471
815 0.36
85 0.165
2 0.0
0 0.0
0 0.0
0 0.0
0.4263358778625954
0.46908396946564884
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6630,	 Acc1 = 0.3192,	 Acc2 = 0.3608

 ===== Epoch 117	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   2.7968433   2.3080988
 -0.3640846  -0.3725409   1.6989955   1.7995174  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  2.3945944   2.5350044
 -0.3640846  -0.3725409   1.5056084   2.0931606  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.6068,	 Acc = 0.3847
2926 0.163
5678 0.448
2912 0.481
377 0.414
69 0.333
6 0.833
0 0.0
0 0.0
0.45653616456536167
0.46908396946564884
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6148,	 Acc = 0.4077
380 0.2
1718 0.481
815 0.38
85 0.129
2 0.0
0 0.0
0 0.0
0 0.0
0.43778625954198475
0.46908396946564884
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6586,	 Acc1 = 0.3295,	 Acc2 = 0.3733

 ===== Epoch 118	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.6076,	 Acc = 0.3842
2923 0.161
5677 0.453
2917 0.474
376 0.386
69 0.362
6 0.5
0 0.0
0 0.0
0.4562741846323936
0.46908396946564884
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5959,	 Acc = 0.4187
380 0.197
1718 0.487
815 0.405
85 0.176
2 0.0
0 0.0
0 0.0
0 0.0
0.45076335877862594
0.46908396946564884
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6457,	 Acc1 = 0.3365,	 Acc2 = 0.3817

 ===== Epoch 119	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 0
train:	 Loss = 1.6079,	 Acc = 0.3869
2926 0.159
5676 0.455
2916 0.481
376 0.388
68 0.412
6 0.667
0 0.0
0 0.0
0.4608493696084937
0.46908396946564884
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6199,	 Acc = 0.4010
380 0.2
1718 0.47
815 0.378
85 0.141
2 0.0
0 0.0
0 0.0
0 0.0
0.4301526717557252
0.46908396946564884
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6680,	 Acc1 = 0.3192,	 Acc2 = 0.3608

 ===== Epoch 120	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   2.9468505   2.797407   -0.42342317 -0.42019257
  1.6112862   0.921816  ] [ 1.2005554   0.846862   -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   3.4074287   2.7949183  -0.42342317 -0.42019257
  1.665516    0.95855683] 4 6
train:	 Loss = 1.6106,	 Acc = 0.3826
2923 0.16
5682 0.452
2913 0.469
375 0.392
69 0.391
6 0.667
0 0.0
0 0.0
0.4545052515201769
0.46908396946564884
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6138,	 Acc = 0.4133
380 0.197
1718 0.49
815 0.38
85 0.153
2 0.0
0 0.0
0 0.0
0 0.0
0.4446564885496183
0.46908396946564884
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6538,	 Acc1 = 0.3375,	 Acc2 = 0.3830

 ===== Epoch 121	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   3.3724198   2.0642896  -0.44088638 -0.36343393
  2.507371    3.254953   -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 3 6
train:	 Loss = 1.6067,	 Acc = 0.3835
2923 0.16
5681 0.451
2913 0.476
377 0.374
68 0.397
6 0.667
0 0.0
0 0.0
0.45561083471531233
0.46908396946564884
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6096,	 Acc = 0.4000
380 0.2
1718 0.471
815 0.372
85 0.129
2 0.0
0 0.0
0 0.0
0 0.0
0.42900763358778626
0.46908396946564884
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6601,	 Acc1 = 0.3200,	 Acc2 = 0.3618

 ===== Epoch 122	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.8132555   1.1694942
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.7958285   1.2286453
 -0.38555372 -0.3798593 ] 3 6
train:	 Loss = 1.6056,	 Acc = 0.3854
2928 0.164
5678 0.449
2912 0.481
376 0.407
68 0.353
6 0.667
0 0.0
0 0.0
0.4570796460176991
0.46908396946564884
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5939,	 Acc = 0.4363
380 0.197
1718 0.505
815 0.434
85 0.153
2 0.0
0 0.0
0 0.0
0 0.0
0.47099236641221376
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6405,	 Acc1 = 0.3511,	 Acc2 = 0.3994

 ===== Epoch 123	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   1.667032    1.1567692
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  1.7279471   1.0680034
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 5
train:	 Loss = 1.6044,	 Acc = 0.3853
2929 0.161
5677 0.452
2913 0.48
376 0.396
67 0.388
6 0.667
0 0.0
0 0.0
0.4580152671755725
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6204,	 Acc = 0.4100
380 0.2
1718 0.48
815 0.388
85 0.165
2 0.0
0 0.0
0 0.0
0 0.0
0.4404580152671756
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6606,	 Acc1 = 0.3291,	 Acc2 = 0.3728

 ===== Epoch 124	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   1.2653067   3.0014641  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   1.2653067   3.0014641  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.6071,	 Acc = 0.3859
2926 0.161
5681 0.453
2911 0.481
375 0.379
69 0.391
6 0.667
0 0.0
0 0.0
0.4586374695863747
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6433,	 Acc = 0.3930
380 0.195
1718 0.459
815 0.371
85 0.176
2 0.0
0 0.0
0 0.0
0 0.0
0.4217557251908397
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6900,	 Acc1 = 0.3280,	 Acc2 = 0.3715

 ===== Epoch 125	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.6064,	 Acc = 0.3839
2923 0.161
5679 0.448
2914 0.478
377 0.395
69 0.435
6 0.667
0 0.0
0 0.0
0.4558319513543394
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6292,	 Acc = 0.4037
380 0.2
1718 0.477
815 0.374
85 0.129
2 0.0
0 0.0
0 0.0
0 0.0
0.43320610687022904
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6711,	 Acc1 = 0.3274,	 Acc2 = 0.3708

 ===== Epoch 126	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   2.783475    1.5790715
 -0.3640846  -0.3725409   1.8271567   0.8364667  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  2.5038555   1.4125262
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 5
train:	 Loss = 1.6038,	 Acc = 0.3835
2924 0.161
5679 0.45
2914 0.479
376 0.37
69 0.391
6 0.5
0 0.0
0 0.0
0.45555064130915524
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6157,	 Acc = 0.3930
380 0.195
1718 0.464
815 0.364
85 0.129
2 0.0
0 0.0
0 0.0
0 0.0
0.4217557251908397
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6731,	 Acc1 = 0.3101,	 Acc2 = 0.3499

 ===== Epoch 127	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 0
train:	 Loss = 1.6076,	 Acc = 0.3856
2919 0.161
5683 0.45
2914 0.481
377 0.398
69 0.42
6 0.667
0 0.0
0 0.0
0.4580616642722953
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6157,	 Acc = 0.4050
380 0.197
1718 0.473
815 0.389
85 0.129
2 0.0
0 0.0
0 0.0
0 0.0
0.4351145038167939
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6725,	 Acc1 = 0.3245,	 Acc2 = 0.3673

 ===== Epoch 128	 =====
[ 2.3288596   2.1378734  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.0221972   2.3625817 ] [ 2.7500618   1.9404247  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.2051675   2.1657557 ] 5 5
train:	 Loss = 1.6057,	 Acc = 0.3851
2923 0.159
5680 0.453
2914 0.478
376 0.388
69 0.391
6 0.667
0 0.0
0 0.0
0.4581536760641238
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5997,	 Acc = 0.4160
380 0.203
1718 0.49
815 0.391
85 0.118
2 0.0
0 0.0
0 0.0
0 0.0
0.44694656488549617
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6492,	 Acc1 = 0.3421,	 Acc2 = 0.3884

 ===== Epoch 129	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   0.72972274  2.5481443
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  0.7453602   2.3971953
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 5
train:	 Loss = 1.6078,	 Acc = 0.3899
2925 0.161
5681 0.456
2911 0.487
376 0.41
69 0.391
6 0.667
0 0.0
0 0.0
0.46378414243060934
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6117,	 Acc = 0.4130
380 0.2
1718 0.488
815 0.383
85 0.153
2 0.0
0 0.0
0 0.0
0 0.0
0.4438931297709924
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6514,	 Acc1 = 0.3371,	 Acc2 = 0.3825

 ===== Epoch 130	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.6036,	 Acc = 0.3889
2921 0.167
5682 0.456
2916 0.481
375 0.381
68 0.382
6 0.833
0 0.0
0 0.0
0.46059467226704986
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6343,	 Acc = 0.3953
380 0.203
1718 0.47
815 0.353
85 0.165
2 0.0
0 0.0
0 0.0
0 0.0
0.4232824427480916
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6691,	 Acc1 = 0.3272,	 Acc2 = 0.3705

 ===== Epoch 131	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   1.3230935   1.4785005  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   1.426081    1.3540753  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 5
train:	 Loss = 1.6089,	 Acc = 0.3841
2924 0.161
5678 0.449
2914 0.477
377 0.401
69 0.406
6 0.667
0 0.0
0 0.0
0.4562140645731977
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5896,	 Acc = 0.4227
380 0.2
1718 0.493
815 0.402
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.4549618320610687
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6453,	 Acc1 = 0.3431,	 Acc2 = 0.3897

 ===== Epoch 132	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.5195239   2.8533342 ] [ 2.350103    2.8289444  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.5098238   3.0396628 ] 6 6
train:	 Loss = 1.6069,	 Acc = 0.3863
2922 0.163
5684 0.455
2911 0.476
376 0.388
69 0.362
6 0.833
0 0.0
0 0.0
0.4585452133539686
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6255,	 Acc = 0.4020
380 0.197
1718 0.47
815 0.382
85 0.153
2 0.0
0 0.0
0 0.0
0 0.0
0.4316793893129771
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6735,	 Acc1 = 0.3320,	 Acc2 = 0.3762

 ===== Epoch 133	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   2.5924995   1.0967577
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   2.5800421   3.0653806   2.4423487   1.0191034
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 2
train:	 Loss = 1.6062,	 Acc = 0.3860
2923 0.161
5684 0.45
2910 0.488
376 0.388
69 0.362
6 0.5
0 0.0
0 0.0
0.458595909342178
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6228,	 Acc = 0.3953
380 0.197
1718 0.474
815 0.348
85 0.141
2 0.0
0 0.0
0 0.0
0 0.0
0.42404580152671756
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6659,	 Acc1 = 0.3303,	 Acc2 = 0.3743

 ===== Epoch 134	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 0
train:	 Loss = 1.6027,	 Acc = 0.3927
2925 0.161
5677 0.461
2915 0.489
376 0.402
69 0.449
6 0.833
0 0.0
0 0.0
0.4675439566515537
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6205,	 Acc = 0.4053
380 0.2
1718 0.481
815 0.366
85 0.176
2 0.0
0 0.0
0 0.0
0 0.0
0.4351145038167939
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6731,	 Acc1 = 0.3243,	 Acc2 = 0.3670

 ===== Epoch 135	 =====
[-0.36602148 -0.3783333   2.9482107   2.8350396   1.7869638   0.870048
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   2.865558   -4.1008005   2.1875322   0.9768715
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 2
train:	 Loss = 1.6089,	 Acc = 0.3860
2921 0.161
5685 0.452
2911 0.482
377 0.406
68 0.324
6 0.667
0 0.0
0 0.0
0.45860506245164145
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6014,	 Acc = 0.4117
380 0.2
1718 0.483
815 0.39
85 0.141
2 0.0
0 0.0
0 0.0
0 0.0
0.44236641221374046
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6523,	 Acc1 = 0.3351,	 Acc2 = 0.3800

 ===== Epoch 136	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   0.53836465  1.3834789
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  0.48825142  1.621463
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.6049,	 Acc = 0.3876
2924 0.16
5677 0.457
2916 0.479
376 0.399
69 0.333
6 0.833
0 0.0
0 0.0
0.4611897390535161
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6050,	 Acc = 0.4230
380 0.2
1718 0.493
815 0.402
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.45534351145038165
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6442,	 Acc1 = 0.3439,	 Acc2 = 0.3907

 ===== Epoch 137	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   1.7995682   0.75002515
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 3 5
train:	 Loss = 1.6046,	 Acc = 0.3864
2921 0.159
5680 0.454
2917 0.481
375 0.384
69 0.377
6 0.833
0 0.0
0 0.0
0.45982093511661326
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6134,	 Acc = 0.4013
380 0.2
1718 0.473
815 0.369
85 0.165
2 0.0
0 0.0
0 0.0
0 0.0
0.4305343511450382
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6611,	 Acc1 = 0.3254,	 Acc2 = 0.3683

 ===== Epoch 138	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   2.3912115   1.0967577
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  2.3919206   1.0991216
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.6040,	 Acc = 0.3891
2919 0.16
5683 0.458
2915 0.483
376 0.394
69 0.391
6 0.667
0 0.0
0 0.0
0.4630345894573986
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6110,	 Acc = 0.3987
380 0.195
1718 0.472
815 0.363
85 0.176
2 0.0
0 0.0
0 0.0
0 0.0
0.42824427480916033
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6538,	 Acc1 = 0.3260,	 Acc2 = 0.3690

 ===== Epoch 139	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   0.48527417  2.3747783
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  0.45310527  2.3127317
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 2
train:	 Loss = 1.6088,	 Acc = 0.3879
2925 0.16
5674 0.452
2917 0.485
377 0.411
69 0.478
6 0.667
0 0.0
0 0.0
0.46157248700652437
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6458,	 Acc = 0.3957
380 0.203
1718 0.465
815 0.366
85 0.153
2 0.0
0 0.0
0 0.0
0 0.0
0.42366412213740456
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6859,	 Acc1 = 0.3251,	 Acc2 = 0.3680

 ===== Epoch 140	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   2.7357268   3.031326   -0.42342317 -0.42019257
  3.268159    1.7274901 ] [ 2.2718456   1.3531408  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   2.6773672   2.797407   -0.42342317 -0.42019257
  3.2320058   1.5122937 ] 5 5
train:	 Loss = 1.6033,	 Acc = 0.3913
2924 0.163
5682 0.457
2912 0.491
375 0.411
69 0.377
6 0.667
0 0.0
0 0.0
0.4651702786377709
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6201,	 Acc = 0.3970
380 0.197
1718 0.47
815 0.361
85 0.176
2 0.0
0 0.0
0 0.0
0 0.0
0.4259541984732824
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6617,	 Acc1 = 0.3264,	 Acc2 = 0.3695

 ===== Epoch 141	 =====
[ 2.1254065   1.1050643  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 2.2476404   1.3328898  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.6040,	 Acc = 0.3900
2923 0.158
5682 0.455
2913 0.492
376 0.415
68 0.382
6 0.667
0 0.0
0 0.0
0.46489773355444997
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6081,	 Acc = 0.4147
380 0.197
1718 0.489
815 0.39
85 0.129
2 0.0
0 0.0
0 0.0
0 0.0
0.44618320610687023
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6562,	 Acc1 = 0.3423,	 Acc2 = 0.3887

 ===== Epoch 142	 =====
[ 2.324227    2.1252165  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.1879728   2.357333  ] [ 2.324227    2.1252165  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.1879728   2.357333  ] 0 0
train:	 Loss = 1.6017,	 Acc = 0.3943
2923 0.161
5682 0.461
2912 0.496
376 0.412
69 0.391
6 0.667
0 0.0
0 0.0
0.4695411829740188
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6040,	 Acc = 0.4187
380 0.2
1718 0.488
815 0.398
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.45038167938931295
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6473,	 Acc1 = 0.3441,	 Acc2 = 0.3909

 ===== Epoch 143	 =====
[-0.36602148 -0.3783333   2.397466    1.158708   -0.4409929  -0.3635196
  0.93686104  2.1583333  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   2.6312153   1.024695   -0.44088638 -0.36343393
  0.95820475  1.9602164  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 5
train:	 Loss = 1.6039,	 Acc = 0.3892
2929 0.161
5677 0.455
2913 0.49
374 0.39
69 0.377
6 0.833
0 0.0
0 0.0
0.4632149574067928
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6255,	 Acc = 0.4087
380 0.197
1718 0.483
815 0.379
85 0.141
2 0.0
0 0.0
0 0.0
0 0.0
0.43931297709923667
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6609,	 Acc1 = 0.3338,	 Acc2 = 0.3785

 ===== Epoch 144	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 3 0
train:	 Loss = 1.6011,	 Acc = 0.3910
2924 0.16
5683 0.455
2911 0.494
376 0.42
68 0.382
6 0.5
0 0.0
0 0.0
0.4656125608137992
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5960,	 Acc = 0.4147
380 0.197
1718 0.489
815 0.385
85 0.176
2 0.0
0 0.0
0 0.0
0 0.0
0.44618320610687023
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6432,	 Acc1 = 0.3414,	 Acc2 = 0.3877

 ===== Epoch 145	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 0
train:	 Loss = 1.6008,	 Acc = 0.3925
2919 0.16
5685 0.457
2913 0.495
377 0.414
68 0.426
6 0.833
0 0.0
0 0.0
0.46756547684827054
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6002,	 Acc = 0.3970
380 0.197
1718 0.464
815 0.373
85 0.165
2 0.0
0 0.0
0 0.0
0 0.0
0.4259541984732824
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6589,	 Acc1 = 0.3216,	 Acc2 = 0.3638

 ===== Epoch 146	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.5105658   1.8867016
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.4181306   2.0222564
 -0.38555372 -0.3798593 ] 0 6
train:	 Loss = 1.6057,	 Acc = 0.3917
2922 0.169
5677 0.456
2918 0.49
376 0.404
69 0.304
6 0.833
0 0.0
0 0.0
0.46374087994693786
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6054,	 Acc = 0.4140
380 0.197
1718 0.489
815 0.379
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.4454198473282443
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6554,	 Acc1 = 0.3392,	 Acc2 = 0.3849

 ===== Epoch 147	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   3.6516511  -4.0486636
 -0.3640846  -0.3725409   3.2626753   2.3295684  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  3.628941    2.9684365
 -0.3640846  -0.3725409   3.2735457   2.406712   -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 2
train:	 Loss = 1.6013,	 Acc = 0.3911
2928 0.165
5676 0.455
2913 0.489
376 0.426
69 0.348
6 0.833
0 0.0
0 0.0
0.46426991150442476
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6025,	 Acc = 0.4017
380 0.195
1718 0.472
815 0.374
85 0.176
2 0.0
0 0.0
0 0.0
0 0.0
0.4316793893129771
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6482,	 Acc1 = 0.3371,	 Acc2 = 0.3825

 ===== Epoch 148	 =====
[-0.36602148 -0.3783333   1.627806    3.047979   -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.6471409   3.0631156  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 0
train:	 Loss = 1.6003,	 Acc = 0.3895
2922 0.161
5680 0.457
2916 0.484
376 0.407
69 0.362
5 1.0
0 0.0
0 0.0
0.4634092416537696
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6132,	 Acc = 0.4217
380 0.2
1718 0.497
815 0.39
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.4538167938931298
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6573,	 Acc1 = 0.3381,	 Acc2 = 0.3837

 ===== Epoch 149	 =====
[-0.36602148 -0.3783333   1.7837679   3.0139995  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.7779181   3.0178175  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.6044,	 Acc = 0.3890
2924 0.16
5677 0.454
2915 0.488
377 0.403
69 0.391
6 0.833
0 0.0
0 0.0
0.4628482972136223
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6059,	 Acc = 0.3953
380 0.197
1718 0.467
815 0.36
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.42404580152671756
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6586,	 Acc1 = 0.3241,	 Acc2 = 0.3668

 ===== Epoch 150	 =====
[-0.36602148 -0.3783333   1.8629681   0.8211762  -0.4409929  -0.3635196
  1.2289445   3.8353517  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.6613556   0.9023899  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 2
train:	 Loss = 1.6032,	 Acc = 0.3883
2924 0.167
5681 0.45
2912 0.487
376 0.423
69 0.362
6 0.667
0 0.0
0 0.0
0.4599734630694383
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.5955,	 Acc = 0.4213
380 0.195
1718 0.49
815 0.406
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.4541984732824427
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6423,	 Acc1 = 0.3423,	 Acc2 = 0.3887

 ===== Epoch 151	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  4.1733093   1.4729285 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   3.9292285   2.5435796  -0.42342317 -0.42019257
  3.8501363   1.6303895 ] 2 2
train:	 Loss = 1.6016,	 Acc = 0.3900
2926 0.16
5679 0.458
2913 0.488
375 0.395
69 0.319
6 0.833
0 0.0
0 0.0
0.4643884096438841
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6244,	 Acc = 0.3990
380 0.192
1718 0.471
815 0.364
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.42900763358778626
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6640,	 Acc1 = 0.3377,	 Acc2 = 0.3832

 ===== Epoch 152	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   2.0386703   1.4034827
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  1.8861094   1.6325766
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 6
train:	 Loss = 1.5981,	 Acc = 0.3958
2921 0.16
5685 0.464
2911 0.495
376 0.412
69 0.435
6 0.833
0 0.0
0 0.0
0.4718691278876976
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6144,	 Acc = 0.3973
380 0.197
1718 0.457
815 0.383
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.4263358778625954
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6666,	 Acc1 = 0.3303,	 Acc2 = 0.3743

 ===== Epoch 153	 =====
[ 1.2844177   2.2441921  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.09941     2.5777779 ] [ 1.2185329   2.2897573  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.1584908   2.6066458 ] 4 1
train:	 Loss = 1.6007,	 Acc = 0.3938
2925 0.161
5680 0.46
2912 0.499
376 0.402
69 0.261
6 0.833
0 0.0
0 0.0
0.4689815326772089
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5947,	 Acc = 0.4047
380 0.203
1718 0.47
815 0.385
85 0.176
2 0.0
0 0.0
0 0.0
0 0.0
0.43396946564885497
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6546,	 Acc1 = 0.3233,	 Acc2 = 0.3658

 ===== Epoch 154	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 0
train:	 Loss = 1.6009,	 Acc = 0.3897
2926 0.159
5679 0.455
2912 0.49
376 0.41
69 0.391
6 0.667
0 0.0
0 0.0
0.4642778146427781
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6066,	 Acc = 0.3990
380 0.197
1718 0.469
815 0.369
85 0.176
2 0.0
0 0.0
0 0.0
0 0.0
0.42824427480916033
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6585,	 Acc1 = 0.3324,	 Acc2 = 0.3767

 ===== Epoch 155	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   2.1505816   1.1412106
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  1.8979522   1.0591124
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 5
train:	 Loss = 1.6038,	 Acc = 0.3891
2923 0.159
5680 0.452
2914 0.495
376 0.391
69 0.42
6 0.833
0 0.0
0 0.0
0.4634604754007739
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6259,	 Acc = 0.4153
380 0.195
1718 0.497
815 0.368
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.44732824427480916
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6598,	 Acc1 = 0.3509,	 Acc2 = 0.3991

 ===== Epoch 156	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   2.3529606   1.8343564  -0.42342317 -0.42019257
  3.959918    1.764231  ] [ 4.48863     1.8670142  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   2.3432329   2.1056032  -0.42342317 -0.42019257
  3.5234344   2.0266654 ] 6 6
train:	 Loss = 1.6007,	 Acc = 0.3943
2923 0.16
5681 0.458
2913 0.499
376 0.426
69 0.449
6 0.667
0 0.0
0 0.0
0.46998341625207296
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.5884,	 Acc = 0.4107
380 0.2
1718 0.475
815 0.395
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.44122137404580153
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6450,	 Acc1 = 0.3305,	 Acc2 = 0.3745

 ===== Epoch 157	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.3042262   1.1793526
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.3826454   1.0930905
  2.3951926   3.5540347 ] 2 2
train:	 Loss = 1.5992,	 Acc = 0.3924
2924 0.162
5682 0.46
2911 0.492
376 0.396
69 0.29
6 0.833
0 0.0
0 0.0
0.4669394073418841
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6010,	 Acc = 0.4153
380 0.195
1718 0.485
815 0.39
85 0.247
2 0.0
0 0.0
0 0.0
0 0.0
0.44732824427480916
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6504,	 Acc1 = 0.3505,	 Acc2 = 0.3986

 ===== Epoch 158	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.5391045   0.9082433
  2.8775287   3.1997478 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.4334087   0.97971755
  2.8673878   3.2548592 ] 4 4
train:	 Loss = 1.6014,	 Acc = 0.3910
2924 0.16
5681 0.456
2912 0.494
376 0.399
69 0.377
6 0.833
0 0.0
0 0.0
0.4656125608137992
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5942,	 Acc = 0.4203
380 0.195
1718 0.495
815 0.393
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.4530534351145038
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6444,	 Acc1 = 0.3433,	 Acc2 = 0.3899

 ===== Epoch 159	 =====
[ 2.5928674   3.193465   -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 2.3438845   3.0643642  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 5
train:	 Loss = 1.5986,	 Acc = 0.3951
2923 0.161
5681 0.465
2914 0.493
375 0.397
69 0.42
6 0.667
0 0.0
0 0.0
0.47075732448866775
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6124,	 Acc = 0.3973
380 0.195
1718 0.468
815 0.364
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.4267175572519084
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6643,	 Acc1 = 0.3260,	 Acc2 = 0.3690

 ===== Epoch 160	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   1.3748392   2.8726504
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  1.3375098   2.8928638
 -0.3640846  -0.3725409   2.2934566   0.82402426 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 6
train:	 Loss = 1.6003,	 Acc = 0.3935
2929 0.165
5677 0.459
2911 0.497
377 0.387
68 0.353
6 0.667
0 0.0
0 0.0
0.4674189622745879
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.5896,	 Acc = 0.4173
380 0.2
1718 0.49
815 0.389
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.4488549618320611
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6428,	 Acc1 = 0.3429,	 Acc2 = 0.3894

 ===== Epoch 161	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  0.7911579   1.8300151
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  0.83851266  2.0690842
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.5987,	 Acc = 0.3935
2925 0.162
5680 0.46
2912 0.493
376 0.404
69 0.42
6 0.833
0 0.0
0 0.0
0.46853920159239193
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6045,	 Acc = 0.3980
380 0.195
1718 0.467
815 0.371
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.42748091603053434
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6571,	 Acc1 = 0.3384,	 Acc2 = 0.3839

 ===== Epoch 162	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   1.6559542   1.2434522
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  1.6565069   1.2435989
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.6023,	 Acc = 0.3946
2926 0.159
5680 0.464
2911 0.495
376 0.391
69 0.391
6 0.5
0 0.0
0 0.0
0.4708029197080292
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6062,	 Acc = 0.4033
380 0.197
1718 0.467
815 0.384
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.43320610687022904
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6590,	 Acc1 = 0.3276,	 Acc2 = 0.3710

 ===== Epoch 163	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 0
train:	 Loss = 1.6007,	 Acc = 0.3902
2925 0.158
5679 0.454
2913 0.497
376 0.41
69 0.348
6 0.833
0 0.0
0 0.0
0.46522171845626453
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6036,	 Acc = 0.4060
380 0.197
1718 0.477
815 0.378
85 0.176
2 0.0
0 0.0
0 0.0
0 0.0
0.4362595419847328
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6481,	 Acc1 = 0.3353,	 Acc2 = 0.3802

 ===== Epoch 164	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 0
train:	 Loss = 1.6007,	 Acc = 0.3945
2924 0.16
5679 0.464
2913 0.492
377 0.411
69 0.348
6 0.833
0 0.0
0 0.0
0.4702565236620964
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5958,	 Acc = 0.4207
380 0.197
1718 0.495
815 0.394
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.4530534351145038
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6475,	 Acc1 = 0.3483,	 Acc2 = 0.3959

 ===== Epoch 165	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 0
train:	 Loss = 1.6035,	 Acc = 0.3928
2925 0.161
5681 0.458
2911 0.498
376 0.404
69 0.348
6 0.667
0 0.0
0 0.0
0.46776512219396216
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6153,	 Acc = 0.4067
380 0.2
1718 0.476
815 0.374
85 0.259
2 0.0
0 0.0
0 0.0
0 0.0
0.4366412213740458
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6541,	 Acc1 = 0.3489,	 Acc2 = 0.3966

 ===== Epoch 166	 =====
[-0.36602148 -0.3783333   1.7922982   1.2515856  -0.4409929  -0.3635196
  5.124886    2.5378249  -0.40141198 -0.40778467  4.470027    2.4954653
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.7165903   1.4482336  -0.44088638 -0.36343393
  5.205771    2.7861688  -0.40141198 -0.40778467  5.2689915   2.712353
 -0.38555372 -0.3798593 ] 4 6
train:	 Loss = 1.6028,	 Acc = 0.3893
2925 0.164
5681 0.453
2910 0.489
377 0.411
69 0.333
6 0.667
0 0.0
0 0.0
0.46223598363374985
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6097,	 Acc = 0.4077
380 0.203
1718 0.485
815 0.362
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.43740458015267175
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6569,	 Acc1 = 0.3336,	 Acc2 = 0.3782

 ===== Epoch 167	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   1.5771273   2.150396   -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   1.4552609   2.1727927  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 0
train:	 Loss = 1.6000,	 Acc = 0.3917
2924 0.161
5684 0.459
2909 0.489
376 0.412
69 0.319
6 0.833
0 0.0
0 0.0
0.46616541353383456
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6190,	 Acc = 0.3990
380 0.2
1718 0.471
815 0.36
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.42786259541984734
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6618,	 Acc1 = 0.3303,	 Acc2 = 0.3743

 ===== Epoch 168	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   1.6909852   2.8446887  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   1.7052884   2.832246   -0.42342317 -0.42019257
  2.5420096   0.9375621 ] 1 6
train:	 Loss = 1.6012,	 Acc = 0.3932
2922 0.162
5681 0.457
2915 0.498
375 0.408
69 0.377
6 0.833
0 0.0
0 0.0
0.46783108556267966
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6115,	 Acc = 0.4143
380 0.197
1718 0.491
815 0.38
85 0.176
2 0.0
0 0.0
0 0.0
0 0.0
0.44580152671755724
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6592,	 Acc1 = 0.3406,	 Acc2 = 0.3867

 ===== Epoch 169	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   1.1028891   0.93672734
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  1.0655029   1.2547126
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 6
train:	 Loss = 1.6017,	 Acc = 0.3917
2926 0.16
5680 0.457
2911 0.493
376 0.412
69 0.42
6 0.667
0 0.0
0 0.0
0.4666003096660031
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6171,	 Acc = 0.3990
380 0.197
1718 0.467
815 0.368
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.42824427480916033
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6629,	 Acc1 = 0.3305,	 Acc2 = 0.3745

 ===== Epoch 170	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 0
train:	 Loss = 1.5998,	 Acc = 0.3967
2923 0.162
5680 0.463
2913 0.499
377 0.414
69 0.464
6 0.667
0 0.0
0 0.0
0.472636815920398
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6003,	 Acc = 0.4190
380 0.2
1718 0.494
815 0.384
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.45076335877862594
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6463,	 Acc1 = 0.3466,	 Acc2 = 0.3939

 ===== Epoch 171	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 0
train:	 Loss = 1.6005,	 Acc = 0.3959
2924 0.16
5679 0.463
2918 0.499
373 0.416
68 0.338
6 0.833
0 0.0
0 0.0
0.47213622291021673
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6066,	 Acc = 0.4277
380 0.197
1718 0.505
815 0.395
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.46106870229007635
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6431,	 Acc1 = 0.3542,	 Acc2 = 0.4031

 ===== Epoch 172	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   1.7266152   2.8304203
 -0.3640846  -0.3725409   1.3270996   0.8837483  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  1.7611847   2.6905954
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 5
train:	 Loss = 1.6015,	 Acc = 0.3942
2925 0.16
5679 0.461
2912 0.497
377 0.419
69 0.348
6 0.5
0 0.0
0 0.0
0.4699767776180471
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6341,	 Acc = 0.3953
380 0.197
1718 0.47
815 0.351
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.42404580152671756
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6684,	 Acc1 = 0.3357,	 Acc2 = 0.3807

 ===== Epoch 173	 =====
[ 0.6132332   1.2012572  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 0.60837656  1.2113827  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 1
train:	 Loss = 1.5976,	 Acc = 0.3968
2925 0.162
5679 0.466
2913 0.495
376 0.402
69 0.435
6 0.667
0 0.0
0 0.0
0.4728519296693575
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6056,	 Acc = 0.4023
380 0.197
1718 0.469
815 0.378
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.43206106870229005
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6592,	 Acc1 = 0.3307,	 Acc2 = 0.3748

 ===== Epoch 174	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.1192292   1.9138126
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.1862829   2.0961952
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.5974,	 Acc = 0.3944
2919 0.161
5684 0.461
2914 0.496
376 0.407
69 0.362
6 0.833
0 0.0
0 0.0
0.4696651563708697
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5878,	 Acc = 0.4293
380 0.2
1718 0.501
815 0.404
85 0.271
2 0.0
0 0.0
0 0.0
0 0.0
0.46259541984732827
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6388,	 Acc1 = 0.3544,	 Acc2 = 0.4033

 ===== Epoch 175	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 0
train:	 Loss = 1.5999,	 Acc = 0.3923
2927 0.161
5677 0.454
2913 0.502
377 0.411
68 0.382
6 0.667
0 0.0
0 0.0
0.46731556243778344
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6088,	 Acc = 0.4093
380 0.203
1718 0.475
815 0.385
85 0.247
2 0.0
0 0.0
0 0.0
0 0.0
0.43931297709923667
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6532,	 Acc1 = 0.3437,	 Acc2 = 0.3904

 ===== Epoch 176	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  3.00545     0.9600006
  3.3510458   2.934689  ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   2.2013412   3.362297    2.8273978   0.80472875
  3.2549314   2.7483606 ] 5 2
train:	 Loss = 1.6031,	 Acc = 0.3943
2922 0.161
5679 0.459
2915 0.499
377 0.408
69 0.42
6 0.833
0 0.0
0 0.0
0.4697103692239664
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6210,	 Acc = 0.3980
380 0.192
1718 0.473
815 0.36
85 0.176
2 0.0
0 0.0
0 0.0
0 0.0
0.42786259541984734
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6617,	 Acc1 = 0.3388,	 Acc2 = 0.3844

 ===== Epoch 177	 =====
[-0.36602148 -0.3783333   1.6464885   3.025326    2.555832    0.85893476
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.541544    2.9702542   2.5202832   0.8101669
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 5
train:	 Loss = 1.5977,	 Acc = 0.3951
2925 0.16
5682 0.463
2910 0.497
376 0.412
69 0.362
6 0.5
0 0.0
0 0.0
0.4711931881012938
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6052,	 Acc = 0.4253
380 0.2
1718 0.492
815 0.407
85 0.259
2 0.0
0 0.0
0 0.0
0 0.0
0.4580152671755725
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6513,	 Acc1 = 0.3559,	 Acc2 = 0.4051

 ===== Epoch 178	 =====
[-0.36602148 -0.3783333   3.2910037   2.8327742   2.6646886   1.1056483
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   3.4593332  -4.1913967   2.6528487   1.1724718
 -0.3640846  -0.3725409   1.6211835   0.8314898  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 2
train:	 Loss = 1.5953,	 Acc = 0.3950
2921 0.16
5685 0.459
2912 0.502
376 0.404
68 0.426
6 0.833
0 0.0
0 0.0
0.4707637891013596
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5913,	 Acc = 0.4200
380 0.2
1718 0.487
815 0.401
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.45190839694656487
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6556,	 Acc1 = 0.3357,	 Acc2 = 0.3807

 ===== Epoch 179	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 1.743684    2.763128   -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
train:	 Loss = 1.5978,	 Acc = 0.3959
2923 0.164
5685 0.46
2909 0.501
376 0.404
69 0.435
6 0.833
0 0.0
0 0.0
0.4709784411276949
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5974,	 Acc = 0.4200
380 0.197
1718 0.498
815 0.379
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.45229007633587787
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6432,	 Acc1 = 0.3524,	 Acc2 = 0.4008

 ===== Epoch 180	 =====
[ 1.4384111   3.3858511  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 6
train:	 Loss = 1.5967,	 Acc = 0.3996
2921 0.162
5681 0.465
2915 0.509
376 0.404
69 0.406
6 0.833
0 0.0
0 0.0
0.4764010169116834
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6045,	 Acc = 0.4170
380 0.197
1718 0.498
815 0.373
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.4488549618320611
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6551,	 Acc1 = 0.3443,	 Acc2 = 0.3912

 ===== Epoch 181	 =====
[-0.36602148 -0.3783333   1.8337251   0.94123775 -0.4409929  -0.3635196
  3.3252041   3.1405468  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.9545878   0.88653547 -0.44088638 -0.36343393
  3.6150405   3.0986912  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 3 2
train:	 Loss = 1.5986,	 Acc = 0.3944
2925 0.16
5679 0.461
2912 0.498
377 0.408
69 0.406
6 0.667
0 0.0
0 0.0
0.47008736038925136
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5815,	 Acc = 0.4203
380 0.197
1718 0.488
815 0.402
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.45267175572519086
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6450,	 Acc1 = 0.3404,	 Acc2 = 0.3864

 ===== Epoch 182	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.403102    1.9261358
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.1905751   1.9261358
 -0.38555372 -0.3798593 ] 4 0
train:	 Loss = 1.5974,	 Acc = 0.3962
2919 0.162
5683 0.46
2914 0.504
377 0.419
69 0.348
6 0.667
0 0.0
0 0.0
0.47165432644491107
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5972,	 Acc = 0.4140
380 0.2
1718 0.485
815 0.388
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.4450381679389313
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6523,	 Acc1 = 0.3386,	 Acc2 = 0.3842

 ===== Epoch 183	 =====
[-0.36602148 -0.3783333   1.6554244   2.8123865  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.6552639   2.8117106  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.5978,	 Acc = 0.3925
2923 0.161
5683 0.457
2911 0.498
376 0.412
69 0.377
6 0.5
0 0.0
0 0.0
0.46744057490326146
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6047,	 Acc = 0.4023
380 0.203
1718 0.473
815 0.369
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.4312977099236641
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6544,	 Acc1 = 0.3414,	 Acc2 = 0.3877

 ===== Epoch 184	 =====
[ 0.22182536  0.51018643 -0.4208693  -0.3341335   3.2108784  -4.077558
 -0.3640846  -0.3725409   2.5440574   2.8894818  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  3.1300068   2.7817273
 -0.3640846  -0.3725409   2.4931366   2.757591   -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 2
train:	 Loss = 1.5980,	 Acc = 0.3919
2925 0.162
5681 0.456
2911 0.496
376 0.404
69 0.348
6 0.667
0 0.0
0 0.0
0.4661063806258985
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.5925,	 Acc = 0.4113
380 0.197
1718 0.48
815 0.385
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.44236641221374046
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6461,	 Acc1 = 0.3388,	 Acc2 = 0.3844

 ===== Epoch 185	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 0
train:	 Loss = 1.5973,	 Acc = 0.3952
2924 0.161
5683 0.458
2910 0.503
376 0.418
69 0.42
6 0.5
0 0.0
0 0.0
0.47080937638213183
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5904,	 Acc = 0.4230
380 0.2
1718 0.497
815 0.391
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.45534351145038165
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6398,	 Acc1 = 0.3522,	 Acc2 = 0.4006

 ===== Epoch 186	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.0473853   0.9611812 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.0024152   0.96380556] 1 0
train:	 Loss = 1.5947,	 Acc = 0.3972
2925 0.161
5678 0.464
2914 0.501
376 0.426
69 0.348
6 0.833
0 0.0
0 0.0
0.473515426296583
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5799,	 Acc = 0.4097
380 0.195
1718 0.48
815 0.38
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.44083969465648853
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6428,	 Acc1 = 0.3427,	 Acc2 = 0.3892

 ===== Epoch 187	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   3.0802515  -4.284264
 -0.3640846  -0.3725409   2.7277167   2.1578617  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  3.4375436   2.7417183
 -0.3640846  -0.3725409   2.7666225   2.2424707  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 2
train:	 Loss = 1.5961,	 Acc = 0.3965
2927 0.16
5676 0.463
2913 0.502
377 0.424
69 0.362
6 0.833
0 0.0
0 0.0
0.47306713859086386
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6128,	 Acc = 0.4103
380 0.2
1718 0.476
815 0.39
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.44083969465648853
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6600,	 Acc1 = 0.3396,	 Acc2 = 0.3854

 ===== Epoch 188	 =====
[-0.36602148 -0.3783333   0.11732113  1.8609549  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   0.10259322  1.8423283  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 0
train:	 Loss = 1.5979,	 Acc = 0.3945
2928 0.159
5680 0.46
2909 0.496
376 0.436
69 0.391
6 0.833
0 0.0
0 0.0
0.4705752212389381
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6180,	 Acc = 0.4117
380 0.197
1718 0.484
815 0.38
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.44274809160305345
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6575,	 Acc1 = 0.3530,	 Acc2 = 0.4016

 ===== Epoch 189	 =====
[ 2.1835077   2.3808873  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 4
train:	 Loss = 1.5967,	 Acc = 0.3970
2921 0.161
5683 0.462
2914 0.503
376 0.418
69 0.377
5 1.0
0 0.0
0 0.0
0.4730850005526694
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5939,	 Acc = 0.4043
380 0.197
1718 0.467
815 0.388
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.43435114503816796
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6555,	 Acc1 = 0.3390,	 Acc2 = 0.3847

 ===== Epoch 190	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 3 0
train:	 Loss = 1.5981,	 Acc = 0.3948
2921 0.163
5676 0.459
2919 0.497
377 0.432
69 0.362
6 0.667
0 0.0
0 0.0
0.46965845031502157
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6052,	 Acc = 0.4017
380 0.195
1718 0.47
815 0.374
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.4316793893129771
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6499,	 Acc1 = 0.3443,	 Acc2 = 0.3912

 ===== Epoch 191	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   1.9993293   0.96117634
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  1.7749373   1.0524443
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 4
train:	 Loss = 1.6007,	 Acc = 0.3977
2924 0.162
5680 0.464
2913 0.501
376 0.42
69 0.406
6 0.833
0 0.0
0 0.0
0.47401592215833704
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.5964,	 Acc = 0.4070
380 0.203
1718 0.476
815 0.379
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.4366412213740458
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6594,	 Acc1 = 0.3332,	 Acc2 = 0.3777

 ===== Epoch 192	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.5958,	 Acc = 0.3950
2919 0.169
5686 0.458
2913 0.495
375 0.421
69 0.362
6 0.667
0 0.0
0 0.0
0.46800751464250195
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6008,	 Acc = 0.4180
380 0.197
1718 0.494
815 0.379
85 0.247
2 0.0
0 0.0
0 0.0
0 0.0
0.45
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6544,	 Acc1 = 0.3489,	 Acc2 = 0.3966

 ===== Epoch 193	 =====
[ 0.22182536  0.51018643 -0.4208693  -0.3341335   3.2108784  -4.077558
 -0.3640846  -0.3725409   2.5440574   2.8894818  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  3.1300068   2.7817273
 -0.3640846  -0.3725409   2.4931366   2.757591   -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 2
train:	 Loss = 1.5969,	 Acc = 0.3971
2923 0.161
5674 0.464
2919 0.501
377 0.414
69 0.377
6 0.833
0 0.0
0 0.0
0.47330016583747925
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.5913,	 Acc = 0.4123
380 0.2
1718 0.477
815 0.387
85 0.306
2 0.0
0 0.0
0 0.0
0 0.0
0.4431297709923664
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6517,	 Acc1 = 0.3423,	 Acc2 = 0.3887

 ===== Epoch 194	 =====
[ 3.1467044   2.7580652  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  0.9829205   2.7247806  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 3.0727792   2.6314955  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  0.9739329   2.6354883  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 5
train:	 Loss = 1.5969,	 Acc = 0.3970
2926 0.162
5680 0.461
2911 0.505
376 0.423
69 0.377
6 0.833
0 0.0
0 0.0
0.4730148197301482
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6217,	 Acc = 0.4050
380 0.197
1718 0.473
815 0.38
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.4351145038167939
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6674,	 Acc1 = 0.3417,	 Acc2 = 0.3879

 ===== Epoch 195	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   0.49405882  1.5412866
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  0.47984755  1.4080809
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 5
train:	 Loss = 1.5959,	 Acc = 0.3964
2923 0.16
5682 0.462
2912 0.501
377 0.427
68 0.397
6 0.667
0 0.0
0 0.0
0.4728579325594251
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.5888,	 Acc = 0.4267
380 0.195
1718 0.496
815 0.406
85 0.271
2 0.0
0 0.0
0 0.0
0 0.0
0.46030534351145036
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6463,	 Acc1 = 0.3483,	 Acc2 = 0.3959

 ===== Epoch 196	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 0
train:	 Loss = 1.5960,	 Acc = 0.4014
2922 0.163
5682 0.467
2915 0.507
376 0.428
67 0.433
6 0.833
0 0.0
0 0.0
0.4783329648463409
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6040,	 Acc = 0.3960
380 0.197
1718 0.463
815 0.366
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.4248091603053435
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6673,	 Acc1 = 0.3247,	 Acc2 = 0.3675

 ===== Epoch 197	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.4700303   1.0856967
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.4151      1.1226661
 -0.38555372 -0.3798593 ] 4 1
train:	 Loss = 1.5965,	 Acc = 0.3980
2919 0.162
5682 0.465
2916 0.502
376 0.418
69 0.362
6 0.667
0 0.0
0 0.0
0.4740855343131838
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6232,	 Acc = 0.3943
380 0.2
1718 0.463
815 0.361
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.42251908396946564
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6706,	 Acc1 = 0.3332,	 Acc2 = 0.3777

 ===== Epoch 198	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 3 0
train:	 Loss = 1.5993,	 Acc = 0.3932
2921 0.16
5685 0.458
2911 0.494
376 0.441
69 0.435
6 0.667
0 0.0
0 0.0
0.46855311152868356
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.5918,	 Acc = 0.4330
380 0.197
1718 0.504
815 0.412
85 0.259
2 0.0
0 0.0
0 0.0
0 0.0
0.467175572519084
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6404,	 Acc1 = 0.3598,	 Acc2 = 0.4098

 ===== Epoch 199	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   1.1212234   1.3679203
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  1.2996887   1.3191718
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 3
train:	 Loss = 1.5985,	 Acc = 0.3927
2922 0.164
5681 0.456
2913 0.496
377 0.414
69 0.377
6 0.833
0 0.0
0 0.0
0.4665045323900066
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6093,	 Acc = 0.4073
380 0.2
1718 0.474
815 0.384
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.43740458015267175
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6561,	 Acc1 = 0.3429,	 Acc2 = 0.3894

 ===== Epoch 200	 =====
[-0.36602148 -0.3783333   2.6598408   1.9946084  -0.4409929  -0.3635196
  2.0434072   3.8018668  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   3.0170474   2.0665545  -0.44088638 -0.36343393
  2.050148    3.8325615  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 3 2
train:	 Loss = 1.5978,	 Acc = 0.3961
2924 0.162
5679 0.464
2913 0.497
377 0.416
69 0.348
6 0.667
0 0.0
0 0.0
0.47191508182220254
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.5775,	 Acc = 0.4227
380 0.2
1718 0.492
815 0.399
85 0.247
2 0.0
0 0.0
0 0.0
0 0.0
0.4549618320610687
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6411,	 Acc1 = 0.3462,	 Acc2 = 0.3934

 ===== Epoch 201	 =====
[ 2.9780552   2.6821234  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  0.7503761   2.2922716  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  0.76273406  2.5908422  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.5964,	 Acc = 0.3966
2922 0.162
5684 0.462
2912 0.5
375 0.421
69 0.435
6 0.833
0 0.0
0 0.0
0.47247402166703517
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6077,	 Acc = 0.4033
380 0.195
1718 0.474
815 0.369
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.433587786259542
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6528,	 Acc1 = 0.3481,	 Acc2 = 0.3956

 ===== Epoch 202	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   0.72972274  2.5481443
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  0.7453602   2.3971953
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 5
train:	 Loss = 1.5943,	 Acc = 0.3969
2925 0.16
5676 0.463
2915 0.504
377 0.411
69 0.333
6 0.667
0 0.0
0 0.0
0.473515426296583
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5795,	 Acc = 0.4167
380 0.2
1718 0.485
815 0.385
85 0.318
2 0.0
0 0.0
0 0.0
0 0.0
0.4480916030534351
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6444,	 Acc1 = 0.3410,	 Acc2 = 0.3872

 ===== Epoch 203	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.1108738   1.3548331 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.1135193   1.278727  ] 3 3
train:	 Loss = 1.5954,	 Acc = 0.3981
2923 0.161
5680 0.463
2915 0.502
376 0.449
68 0.426
6 0.5
0 0.0
0 0.0
0.4746268656716418
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.5956,	 Acc = 0.4063
380 0.2
1718 0.481
815 0.362
85 0.247
2 0.0
0 0.0
0 0.0
0 0.0
0.4362595419847328
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6573,	 Acc1 = 0.3435,	 Acc2 = 0.3902

 ===== Epoch 204	 =====
[-0.36602148 -0.3783333   2.0144641   2.1237311  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.821375    2.3496685  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.5951,	 Acc = 0.4003
2922 0.161
5680 0.467
2916 0.505
375 0.427
69 0.435
6 0.667
0 0.0
0 0.0
0.47778023435772715
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6129,	 Acc = 0.4047
380 0.197
1718 0.473
815 0.373
85 0.259
2 0.0
0 0.0
0 0.0
0 0.0
0.4347328244274809
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6579,	 Acc1 = 0.3470,	 Acc2 = 0.3944

 ===== Epoch 205	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   2.8946233   2.9660015
 -0.3640846  -0.3725409   1.9284266   2.3146374  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  2.8954394   2.9662137
 -0.3640846  -0.3725409   1.9284266   2.3146374  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.5954,	 Acc = 0.3962
2923 0.16
5680 0.462
2914 0.503
376 0.404
69 0.42
6 0.5
0 0.0
0 0.0
0.472636815920398
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6038,	 Acc = 0.4010
380 0.203
1718 0.469
815 0.369
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.4297709923664122
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6617,	 Acc1 = 0.3359,	 Acc2 = 0.3810

 ===== Epoch 206	 =====
[-0.36602148 -0.3783333   2.1273744   2.4227526  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   2.2096426   2.1730053  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 5
train:	 Loss = 1.5990,	 Acc = 0.3926
2926 0.162
5676 0.455
2915 0.5
376 0.412
69 0.377
6 0.667
0 0.0
0 0.0
0.46726387967263877
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.5868,	 Acc = 0.4153
380 0.2
1718 0.488
815 0.38
85 0.259
2 0.0
0 0.0
0 0.0
0 0.0
0.44656488549618323
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6501,	 Acc1 = 0.3384,	 Acc2 = 0.3839

 ===== Epoch 207	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.911093    1.2734783 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.9331373   1.2419862 ] 1 3
train:	 Loss = 1.6004,	 Acc = 0.3974
2924 0.16
5678 0.463
2915 0.504
376 0.431
69 0.348
6 0.5
0 0.0
0 0.0
0.4742370632463512
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6160,	 Acc = 0.4010
380 0.192
1718 0.469
815 0.375
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.4312977099236641
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6757,	 Acc1 = 0.3276,	 Acc2 = 0.3710

 ===== Epoch 208	 =====
[ 3.182328    1.8037293  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   1.9324311   3.3498545  -0.42342317 -0.42019257
  2.6204877   1.9977977 ] [ 3.137344    1.8695456  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   1.749345    3.3797169  -0.42342317 -0.42019257
  2.840493    2.068655  ] 6 6
train:	 Loss = 1.5979,	 Acc = 0.3947
2926 0.163
5677 0.457
2913 0.502
377 0.432
69 0.377
6 0.5
0 0.0
0 0.0
0.46980756469807566
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6116,	 Acc = 0.3937
380 0.203
1718 0.464
815 0.352
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.4213740458015267
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6574,	 Acc1 = 0.3388,	 Acc2 = 0.3844

 ===== Epoch 209	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   1.6377759   2.9442286  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   1.6846923   3.0363033  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.5956,	 Acc = 0.3957
2925 0.162
5678 0.46
2914 0.503
376 0.415
69 0.377
6 0.667
0 0.0
0 0.0
0.4714143536437023
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6079,	 Acc = 0.4040
380 0.197
1718 0.469
815 0.379
85 0.259
2 0.0
0 0.0
0 0.0
0 0.0
0.43396946564885497
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6603,	 Acc1 = 0.3443,	 Acc2 = 0.3912

 ===== Epoch 210	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   4.0916586   2.3992274
 -0.3640846  -0.3725409   4.2713714   1.7572128  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  4.034661    2.4038634
 -0.3640846  -0.3725409   4.246769    1.7547243  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 0
train:	 Loss = 1.5973,	 Acc = 0.3955
2926 0.16
5680 0.463
2912 0.495
375 0.432
69 0.377
6 0.667
0 0.0
0 0.0
0.47179827471798275
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6149,	 Acc = 0.4023
380 0.203
1718 0.474
815 0.363
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.4312977099236641
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6646,	 Acc1 = 0.3328,	 Acc2 = 0.3772

 ===== Epoch 211	 =====
[ 2.2391586   2.1783757  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.1703377   2.417693  ] [ 2.6374593   2.2517862  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.2355897   2.501672  ] 3 6
train:	 Loss = 1.5988,	 Acc = 0.3997
2921 0.162
5683 0.465
2912 0.51
377 0.419
69 0.304
6 0.667
0 0.0
0 0.0
0.4765115507903172
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6037,	 Acc = 0.4087
380 0.203
1718 0.474
815 0.383
85 0.271
2 0.0
0 0.0
0 0.0
0 0.0
0.4385496183206107
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6489,	 Acc1 = 0.3445,	 Acc2 = 0.3914

 ===== Epoch 212	 =====
[ 1.241288    3.3807886  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.5976,	 Acc = 0.4011
2921 0.162
5683 0.468
2912 0.508
377 0.422
69 0.391
6 0.667
0 0.0
0 0.0
0.47828009284845807
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6142,	 Acc = 0.4057
380 0.197
1718 0.476
815 0.373
85 0.247
2 0.0
0 0.0
0 0.0
0 0.0
0.43587786259541983
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6592,	 Acc1 = 0.3410,	 Acc2 = 0.3872

 ===== Epoch 213	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 0
train:	 Loss = 1.5964,	 Acc = 0.3960
2923 0.16
5682 0.461
2912 0.501
377 0.44
68 0.368
6 0.667
0 0.0
0 0.0
0.4720840243228303
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6041,	 Acc = 0.4057
380 0.197
1718 0.478
815 0.368
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.43587786259541983
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6615,	 Acc1 = 0.3386,	 Acc2 = 0.3842

 ===== Epoch 214	 =====
[ 1.8855289   1.2822617  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   3.1791418   3.1607285  -0.42342317 -0.42019257
  1.5006224   1.3679547 ] [ 1.3469057   1.3784549  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   3.4068575   3.203033   -0.42342317 -0.42019257
  1.4706419   1.4493096 ] 4 6
train:	 Loss = 1.5958,	 Acc = 0.3980
2925 0.16
5679 0.465
2912 0.501
377 0.44
69 0.406
6 0.667
0 0.0
0 0.0
0.47506358509344243
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5893,	 Acc = 0.4083
380 0.203
1718 0.478
815 0.377
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.4381679389312977
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6516,	 Acc1 = 0.3412,	 Acc2 = 0.3874

 ===== Epoch 215	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  0.73584795  2.7813625
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  0.7600933   2.5348995
 -0.38555372 -0.3798593 ] 5 5
train:	 Loss = 1.5941,	 Acc = 0.3981
2922 0.161
5688 0.462
2908 0.508
376 0.426
68 0.382
6 0.833
0 0.0
0 0.0
0.4749060358169357
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.5914,	 Acc = 0.4257
380 0.197
1718 0.496
815 0.404
85 0.247
2 0.0
0 0.0
0 0.0
0 0.0
0.4587786259541985
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6444,	 Acc1 = 0.3534,	 Acc2 = 0.4021

 ===== Epoch 216	 =====
[ 2.6817782   2.1834385  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 1.9406786   2.1581244  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 3 1
train:	 Loss = 1.5953,	 Acc = 0.3976
2924 0.162
5678 0.46
2914 0.506
377 0.44
69 0.406
6 0.5
0 0.0
0 0.0
0.47368421052631576
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6113,	 Acc = 0.3967
380 0.195
1718 0.461
815 0.373
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.4259541984732824
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6700,	 Acc1 = 0.3355,	 Acc2 = 0.3805

 ===== Epoch 217	 =====
[ 3.1487222   2.4416409  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.1209216   1.5556114  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 3.6584933   2.2872257  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.0636275   1.4049308  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 5
train:	 Loss = 1.5939,	 Acc = 0.3980
2922 0.161
5682 0.466
2912 0.501
377 0.419
69 0.362
6 0.5
0 0.0
0 0.0
0.47446385142604464
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6002,	 Acc = 0.4047
380 0.203
1718 0.473
815 0.375
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.43396946564885497
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6535,	 Acc1 = 0.3445,	 Acc2 = 0.3914

 ===== Epoch 218	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.5968,	 Acc = 0.4000
2925 0.16
5677 0.47
2914 0.499
377 0.44
69 0.362
6 0.667
0 0.0
0 0.0
0.4776069888311401
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5810,	 Acc = 0.4257
380 0.2
1718 0.492
815 0.411
85 0.247
2 0.0
0 0.0
0 0.0
0 0.0
0.4583969465648855
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6442,	 Acc1 = 0.3483,	 Acc2 = 0.3959

 ===== Epoch 219	 =====
[ 2.3487678   2.1252165  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.1879728   2.357333  ] [ 2.3047001   2.1252165  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.1879728   2.357333  ] 0 0
train:	 Loss = 1.5954,	 Acc = 0.3959
2922 0.161
5682 0.457
2912 0.507
377 0.438
69 0.362
6 0.667
0 0.0
0 0.0
0.4719212911784214
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6063,	 Acc = 0.4090
380 0.197
1718 0.48
815 0.375
85 0.259
2 0.0
0 0.0
0 0.0
0 0.0
0.4396946564885496
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6589,	 Acc1 = 0.3445,	 Acc2 = 0.3914

 ===== Epoch 220	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 0
train:	 Loss = 1.5989,	 Acc = 0.3981
2926 0.16
5681 0.465
2911 0.504
376 0.426
68 0.397
6 0.5
0 0.0
0 0.0
0.4752267197522672
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.5918,	 Acc = 0.4107
380 0.197
1718 0.477
815 0.385
85 0.271
2 0.0
0 0.0
0 0.0
0 0.0
0.4416030534351145
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6528,	 Acc1 = 0.3460,	 Acc2 = 0.3931

 ===== Epoch 221	 =====
[ 4.9262505   3.0036106  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  1.4873247   3.8018668  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 3.1936631   3.0668955  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
train:	 Loss = 1.5940,	 Acc = 0.3988
2924 0.162
5679 0.466
2914 0.502
376 0.42
69 0.406
6 0.667
0 0.0
0 0.0
0.47545333923042904
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6157,	 Acc = 0.4090
380 0.2
1718 0.48
815 0.374
85 0.259
2 0.0
0 0.0
0 0.0
0 0.0
0.43931297709923667
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6594,	 Acc1 = 0.3514,	 Acc2 = 0.3996

 ===== Epoch 222	 =====
[ 3.399315    3.0162675  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 3 4
train:	 Loss = 1.5942,	 Acc = 0.3999
2922 0.162
5680 0.466
2915 0.504
377 0.435
68 0.397
6 0.667
0 0.0
0 0.0
0.4767853194782224
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6077,	 Acc = 0.4197
380 0.197
1718 0.485
815 0.402
85 0.271
2 0.0
0 0.0
0 0.0
0 0.0
0.45190839694656487
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6574,	 Acc1 = 0.3412,	 Acc2 = 0.3874

 ===== Epoch 223	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   1.6223437   1.1056483
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  1.8032076   1.1280171
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 2
train:	 Loss = 1.5920,	 Acc = 0.4000
2923 0.161
5680 0.464
2913 0.51
377 0.432
69 0.42
6 0.667
0 0.0
0 0.0
0.4771697070204533
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6087,	 Acc = 0.3967
380 0.2
1718 0.463
815 0.367
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.4251908396946565
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6709,	 Acc1 = 0.3320,	 Acc2 = 0.3762

 ===== Epoch 224	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 0
train:	 Loss = 1.5954,	 Acc = 0.3980
2922 0.159
5683 0.466
2912 0.501
376 0.426
69 0.391
6 0.833
0 0.0
0 0.0
0.4750165819146584
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5903,	 Acc = 0.4150
380 0.195
1718 0.48
815 0.398
85 0.271
2 0.0
0 0.0
0 0.0
0 0.0
0.44694656488549617
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6546,	 Acc1 = 0.3458,	 Acc2 = 0.3929

 ===== Epoch 225	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.773478    2.0296502
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.8185596   1.5441179
 -0.38555372 -0.3798593 ] 1 6
train:	 Loss = 1.5949,	 Acc = 0.3978
2925 0.16
5682 0.463
2911 0.504
375 0.437
69 0.42
6 0.333
0 0.0
0 0.0
0.4747318367798297
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5943,	 Acc = 0.4057
380 0.203
1718 0.465
815 0.393
85 0.247
2 0.0
0 0.0
0 0.0
0 0.0
0.4351145038167939
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6575,	 Acc1 = 0.3348,	 Acc2 = 0.3797

 ===== Epoch 226	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.6412642   1.9458528
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.7977234   1.9359941
 -0.38555372 -0.3798593 ] 2 0
train:	 Loss = 1.5965,	 Acc = 0.4003
2925 0.162
5682 0.467
2911 0.505
375 0.435
69 0.377
6 0.667
0 0.0
0 0.0
0.47738582328873164
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6034,	 Acc = 0.4143
380 0.2
1718 0.481
815 0.388
85 0.282
2 0.0
0 0.0
0 0.0
0 0.0
0.4454198473282443
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6568,	 Acc1 = 0.3487,	 Acc2 = 0.3964

 ===== Epoch 227	 =====
[ 2.2330327   1.9961153  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.5901239   2.4518094 ] [ 1.9998713   2.2163467  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.4530067   2.7089953 ] 6 6
train:	 Loss = 1.5932,	 Acc = 0.3993
2924 0.165
5683 0.462
2911 0.505
375 0.451
69 0.406
6 0.667
0 0.0
0 0.0
0.47523219814241485
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.5994,	 Acc = 0.4010
380 0.2
1718 0.461
815 0.385
85 0.247
2 0.0
0 0.0
0 0.0
0 0.0
0.4301526717557252
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6611,	 Acc1 = 0.3260,	 Acc2 = 0.3690

 ===== Epoch 228	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 0
train:	 Loss = 1.5931,	 Acc = 0.3988
2924 0.163
5677 0.466
2916 0.501
376 0.431
69 0.362
6 0.833
0 0.0
0 0.0
0.47512162759840776
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5997,	 Acc = 0.4217
380 0.197
1718 0.488
815 0.405
85 0.259
2 0.0
0 0.0
0 0.0
0 0.0
0.4541984732824427
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6568,	 Acc1 = 0.3476,	 Acc2 = 0.3951

 ===== Epoch 229	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   3.3204622   1.466058   -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   3.3250394   1.4585924  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.5943,	 Acc = 0.3975
2923 0.161
5679 0.464
2914 0.504
377 0.408
69 0.377
6 0.5
0 0.0
0 0.0
0.47385295743504696
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6220,	 Acc = 0.3950
380 0.197
1718 0.466
815 0.352
85 0.259
2 0.0
0 0.0
0 0.0
0 0.0
0.42366412213740456
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6783,	 Acc1 = 0.3303,	 Acc2 = 0.3743

 ===== Epoch 230	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.7348667   2.9634595  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 2
train:	 Loss = 1.5922,	 Acc = 0.3971
2923 0.161
5682 0.457
2912 0.512
376 0.431
69 0.406
6 0.667
0 0.0
0 0.0
0.4734107241569928
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5975,	 Acc = 0.4210
380 0.195
1718 0.493
815 0.394
85 0.247
2 0.0
0 0.0
0 0.0
0 0.0
0.4538167938931298
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6541,	 Acc1 = 0.3522,	 Acc2 = 0.4006

 ===== Epoch 231	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.7539558   2.744313   -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.7281184   2.8866224  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 4
train:	 Loss = 1.5956,	 Acc = 0.3941
2922 0.161
5680 0.457
2915 0.499
376 0.434
69 0.362
6 0.833
0 0.0
0 0.0
0.4694892770285209
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6009,	 Acc = 0.4050
380 0.203
1718 0.47
815 0.378
85 0.259
2 0.0
0 0.0
0 0.0
0 0.0
0.43435114503816796
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6573,	 Acc1 = 0.3414,	 Acc2 = 0.3877

 ===== Epoch 232	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  1.6002265   1.0979892  -0.40141198 -0.40778467  1.8659132   3.084512
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  1.6002265   1.0951988  -0.40141198 -0.40778467  1.876521    3.0771184
 -0.38555372 -0.3798593 ] 0 1
train:	 Loss = 1.5995,	 Acc = 0.4012
2926 0.16
5683 0.466
2907 0.514
377 0.438
69 0.377
6 0.333
0 0.0
0 0.0
0.4794293297942933
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6061,	 Acc = 0.3977
380 0.195
1718 0.464
815 0.369
85 0.247
2 0.0
0 0.0
0 0.0
0 0.0
0.42709923664122135
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6634,	 Acc1 = 0.3313,	 Acc2 = 0.3755

 ===== Epoch 233	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 0
train:	 Loss = 1.5932,	 Acc = 0.3991
2929 0.162
5680 0.466
2908 0.505
377 0.432
68 0.324
6 0.333
0 0.0
0 0.0
0.4759376037172254
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5895,	 Acc = 0.4160
380 0.197
1718 0.485
815 0.395
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.44770992366412216
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6565,	 Acc1 = 0.3454,	 Acc2 = 0.3924

 ===== Epoch 234	 =====
[-0.36602148 -0.3783333   1.1546367   2.635692   -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.1650541   2.6418421  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 0
train:	 Loss = 1.5958,	 Acc = 0.3988
2924 0.161
5680 0.464
2912 0.507
377 0.432
69 0.391
6 0.5
0 0.0
0 0.0
0.47578505086245027
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5919,	 Acc = 0.4007
380 0.197
1718 0.467
815 0.374
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.4301526717557252
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6501,	 Acc1 = 0.3414,	 Acc2 = 0.3877

 ===== Epoch 235	 =====
[-0.36602148 -0.3783333   2.0039036   0.97068685 -0.4409929  -0.3635196
  5.363609    2.3508697  -0.40141198 -0.40778467  4.663613    2.419062
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   2.094706    0.94315827 -0.44088638 -0.36343393
  5.1883583   2.3118043  -0.40141198 -0.40778467  4.621184    2.3944156
 -0.38555372 -0.3798593 ] 3 5
train:	 Loss = 1.5977,	 Acc = 0.3950
2925 0.165
5677 0.458
2914 0.498
377 0.427
69 0.406
6 0.5
0 0.0
0 0.0
0.4694238637620259
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5941,	 Acc = 0.4127
380 0.2
1718 0.483
815 0.382
85 0.259
2 0.0
0 0.0
0 0.0
0 0.0
0.4435114503816794
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6533,	 Acc1 = 0.3437,	 Acc2 = 0.3904

 ===== Epoch 236	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.5904,	 Acc = 0.4018
2927 0.162
5677 0.469
2913 0.509
376 0.428
69 0.391
6 0.667
0 0.0
0 0.0
0.47959296537993584
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.5976,	 Acc = 0.4143
380 0.195
1718 0.483
815 0.393
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.44618320610687023
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6508,	 Acc1 = 0.3472,	 Acc2 = 0.3946

 ===== Epoch 237	 =====
[-0.36602148 -0.3783333   2.9364324  -4.4434114  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   2.9480047   2.8343596   1.7875444   0.87018055
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 2
train:	 Loss = 1.5960,	 Acc = 0.3971
2926 0.161
5681 0.461
2911 0.506
375 0.421
69 0.391
6 0.667
0 0.0
0 0.0
0.4736783897367839
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.5851,	 Acc = 0.4200
380 0.2
1718 0.488
815 0.401
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.45190839694656487
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6517,	 Acc1 = 0.3466,	 Acc2 = 0.3939

 ===== Epoch 238	 =====
[ 0.9664347   3.0668955  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 0.982314    3.0846152  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 0
train:	 Loss = 1.5924,	 Acc = 0.4007
2925 0.162
5679 0.465
2914 0.512
375 0.424
69 0.391
6 0.667
0 0.0
0 0.0
0.4778281543735486
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.5771,	 Acc = 0.4187
380 0.2
1718 0.485
815 0.396
85 0.282
2 0.0
0 0.0
0 0.0
0 0.0
0.45038167938931295
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6512,	 Acc1 = 0.3412,	 Acc2 = 0.3874

 ===== Epoch 239	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 0
train:	 Loss = 1.5943,	 Acc = 0.3981
2923 0.161
5681 0.468
2914 0.501
375 0.405
69 0.348
6 0.333
0 0.0
0 0.0
0.4747374239911553
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5852,	 Acc = 0.4203
380 0.197
1718 0.491
815 0.394
85 0.259
2 0.0
0 0.0
0 0.0
0 0.0
0.45267175572519086
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6505,	 Acc1 = 0.3487,	 Acc2 = 0.3964

 ===== Epoch 240	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.5943,	 Acc = 0.3982
2924 0.161
5681 0.462
2912 0.508
376 0.431
69 0.362
6 0.5
0 0.0
0 0.0
0.4749004865103936
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6004,	 Acc = 0.4193
380 0.197
1718 0.49
815 0.394
85 0.247
2 0.0
0 0.0
0 0.0
0 0.0
0.45152671755725193
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6606,	 Acc1 = 0.3431,	 Acc2 = 0.3897

 ===== Epoch 241	 =====
[ 1.7291275   1.1835375  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.1593156   1.3705791 ] [ 1.9905337   1.1860689  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   1.9478788   3.3448777  -0.42342317 -0.42019257
  2.3202405   1.3653303 ] 2 2
train:	 Loss = 1.5968,	 Acc = 0.3969
2926 0.16
5681 0.461
2909 0.506
377 0.438
69 0.333
6 0.167
0 0.0
0 0.0
0.473457199734572
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5964,	 Acc = 0.4223
380 0.2
1718 0.492
815 0.4
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.4545801526717557
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6535,	 Acc1 = 0.3553,	 Acc2 = 0.4043

 ===== Epoch 242	 =====
[-0.36602148 -0.3783333   1.1379843   2.2030172  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.1073823   2.3791122  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
train:	 Loss = 1.5933,	 Acc = 0.3972
2925 0.161
5679 0.462
2913 0.505
376 0.431
69 0.348
6 0.667
0 0.0
0 0.0
0.473515426296583
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6067,	 Acc = 0.4143
380 0.2
1718 0.477
815 0.401
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.4454198473282443
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6593,	 Acc1 = 0.3464,	 Acc2 = 0.3936

 ===== Epoch 243	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   1.8106445   2.4547935
 -0.3640846  -0.3725409   3.646015    0.95840335 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  2.0072134   2.6239138
 -0.3640846  -0.3725409   3.744996    1.1948111  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.5930,	 Acc = 0.4011
2923 0.161
5681 0.465
2912 0.512
377 0.446
69 0.391
6 0.5
0 0.0
0 0.0
0.4787175234936429
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6083,	 Acc = 0.4063
380 0.195
1718 0.473
815 0.388
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.43702290076335876
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6712,	 Acc1 = 0.3336,	 Acc2 = 0.3782

 ===== Epoch 244	 =====
[-0.36602148 -0.3783333   1.7049747   2.9437747  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.6698843   2.9702542  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 3 0
train:	 Loss = 1.5925,	 Acc = 0.3988
2928 0.163
5675 0.463
2913 0.51
377 0.414
69 0.362
6 0.333
0 0.0
0 0.0
0.47522123893805307
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5998,	 Acc = 0.4140
380 0.197
1718 0.484
815 0.388
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.4454198473282443
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6561,	 Acc1 = 0.3476,	 Acc2 = 0.3951

 ===== Epoch 245	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.8196955   2.0296502
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.8231046   1.810298
 -0.38555372 -0.3798593 ] 5 5
train:	 Loss = 1.5950,	 Acc = 0.3998
2923 0.16
5683 0.465
2911 0.511
376 0.428
69 0.362
6 0.333
0 0.0
0 0.0
0.4772802653399668
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6034,	 Acc = 0.4180
380 0.2
1718 0.481
815 0.405
85 0.259
2 0.0
0 0.0
0 0.0
0 0.0
0.449618320610687
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6559,	 Acc1 = 0.3503,	 Acc2 = 0.3984

 ===== Epoch 246	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.5918,	 Acc = 0.4002
2924 0.161
5681 0.466
2912 0.509
376 0.428
69 0.377
6 0.5
0 0.0
0 0.0
0.4774436090225564
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5926,	 Acc = 0.4133
380 0.197
1718 0.487
815 0.382
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.4446564885496183
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6585,	 Acc1 = 0.3427,	 Acc2 = 0.3892

 ===== Epoch 247	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   0.4806905   2.2725365
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  0.5447923   2.3594089
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.5930,	 Acc = 0.3966
2926 0.161
5675 0.461
2916 0.502
376 0.431
69 0.42
6 0.5
0 0.0
0 0.0
0.4727936297279363
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.5991,	 Acc = 0.4077
380 0.192
1718 0.474
815 0.385
85 0.247
2 0.0
0 0.0
0 0.0
0 0.0
0.4389312977099237
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6566,	 Acc1 = 0.3307,	 Acc2 = 0.3748

 ===== Epoch 248	 =====
[-0.36602148 -0.3783333   2.2374423   2.0965476  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   2.2271066   2.0959983  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.5924,	 Acc = 0.3981
2926 0.161
5676 0.466
2915 0.502
376 0.415
69 0.333
6 0.5
0 0.0
0 0.0
0.4747843397478434
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5917,	 Acc = 0.4120
380 0.195
1718 0.478
815 0.396
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.4435114503816794
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6521,	 Acc1 = 0.3390,	 Acc2 = 0.3847

 ===== Epoch 249	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  3.1600647   1.3853983  -0.40141198 -0.40778467  3.3732998   1.8817724
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  3.2729664   1.5109653  -0.40141198 -0.40778467  3.2903345   1.9852868
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.5896,	 Acc = 0.3994
2925 0.162
5681 0.463
2911 0.508
376 0.444
69 0.348
6 0.5
0 0.0
0 0.0
0.47627999557668915
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6071,	 Acc = 0.4087
380 0.197
1718 0.481
815 0.374
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.43931297709923667
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6591,	 Acc1 = 0.3452,	 Acc2 = 0.3921

 ===== Epoch 250	 =====
[-0.36602148 -0.3783333   0.8219976   0.88687027 -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 5
train:	 Loss = 1.5907,	 Acc = 0.4012
2921 0.164
5683 0.466
2913 0.508
376 0.439
69 0.391
6 0.667
0 0.0
0 0.0
0.47783795733392287
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5890,	 Acc = 0.4153
380 0.195
1718 0.487
815 0.388
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.44732824427480916
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6506,	 Acc1 = 0.3487,	 Acc2 = 0.3964

 ===== Epoch 251	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 0
train:	 Loss = 1.5935,	 Acc = 0.4000
2928 0.162
5678 0.464
2911 0.512
376 0.418
69 0.377
6 0.5
0 0.0
0 0.0
0.47699115044247786
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6061,	 Acc = 0.3990
380 0.197
1718 0.463
815 0.379
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.42824427480916033
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6669,	 Acc1 = 0.3361,	 Acc2 = 0.3812

 ===== Epoch 252	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 0
train:	 Loss = 1.5895,	 Acc = 0.3969
2923 0.165
5685 0.461
2909 0.502
376 0.436
69 0.319
6 0.333
0 0.0
0 0.0
0.47197346600331674
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5887,	 Acc = 0.4193
380 0.195
1718 0.482
815 0.411
85 0.247
2 0.0
0 0.0
0 0.0
0 0.0
0.45190839694656487
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6394,	 Acc1 = 0.3547,	 Acc2 = 0.4036

 ===== Epoch 253	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   1.4140662   2.0458791  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   1.3597107   2.1429307  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 4
train:	 Loss = 1.5904,	 Acc = 0.3993
2921 0.163
5682 0.462
2914 0.51
376 0.426
69 0.391
6 0.5
0 0.0
0 0.0
0.4756272797612468
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5831,	 Acc = 0.4293
380 0.2
1718 0.499
815 0.412
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.46259541984732827
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6424,	 Acc1 = 0.3547,	 Acc2 = 0.4036

 ===== Epoch 254	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   1.7785617   0.89227444
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  1.6091353   0.83017147
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 3
train:	 Loss = 1.5904,	 Acc = 0.4009
2927 0.159
5680 0.468
2911 0.51
376 0.439
68 0.324
6 0.5
0 0.0
0 0.0
0.4791505364450835
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6202,	 Acc = 0.4123
380 0.195
1718 0.481
815 0.395
85 0.176
2 0.0
0 0.0
0 0.0
0 0.0
0.4438931297709924
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6676,	 Acc1 = 0.3404,	 Acc2 = 0.3864

 ===== Epoch 255	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.0866573   1.0840373  -0.40141198 -0.40778467  2.7330682   1.5958753
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.070931    1.2319274  -0.40141198 -0.40778467  2.9569592   1.7363591
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.5917,	 Acc = 0.4009
2921 0.161
5681 0.468
2914 0.508
377 0.427
69 0.348
6 0.333
0 0.0
0 0.0
0.47828009284845807
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5919,	 Acc = 0.4163
380 0.2
1718 0.488
815 0.389
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.44770992366412216
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6502,	 Acc1 = 0.3456,	 Acc2 = 0.3926

 ===== Epoch 256	 =====
[ 1.1847621   2.0087724  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 1.6051054   2.0720572  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 1
train:	 Loss = 1.5885,	 Acc = 0.3969
2923 0.162
5681 0.462
2913 0.503
376 0.426
69 0.377
6 0.333
0 0.0
0 0.0
0.4728579325594251
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6177,	 Acc = 0.4130
380 0.2
1718 0.473
815 0.406
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.4438931297709924
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6727,	 Acc1 = 0.3526,	 Acc2 = 0.4011

 ===== Epoch 257	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   2.921675    2.1329768  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 2
train:	 Loss = 1.5928,	 Acc = 0.3988
2921 0.162
5682 0.463
2914 0.509
377 0.438
68 0.324
6 0.333
0 0.0
0 0.0
0.4754062120039792
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6007,	 Acc = 0.4170
380 0.197
1718 0.486
815 0.398
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.4488549618320611
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6524,	 Acc1 = 0.3530,	 Acc2 = 0.4016

 ===== Epoch 258	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 0
train:	 Loss = 1.5908,	 Acc = 0.3991
2923 0.163
5676 0.463
2917 0.508
377 0.432
69 0.348
6 0.333
0 0.0
0 0.0
0.47529021558872303
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5863,	 Acc = 0.4227
380 0.205
1718 0.487
815 0.41
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.4541984732824427
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6523,	 Acc1 = 0.3514,	 Acc2 = 0.3996

 ===== Epoch 259	 =====
[-0.36602148 -0.3783333   2.3507583   0.7871965  -0.4409929  -0.3635196
  1.42947     2.8949935  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   2.2742198   0.9567477  -0.44088638 -0.36343393
  1.4216074   3.1377566  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.5849,	 Acc = 0.4020
2924 0.162
5677 0.468
2915 0.512
377 0.422
69 0.391
6 0.5
0 0.0
0 0.0
0.47965501990269793
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6021,	 Acc = 0.4067
380 0.197
1718 0.475
815 0.38
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.43702290076335876
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6641,	 Acc1 = 0.3447,	 Acc2 = 0.3917

 ===== Epoch 260	 =====
[ 1.6177543   1.3328898  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.7973423   1.5359129 ] [ 2.4623237   1.5429956  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.9833996   1.7511092 ] 6 6
train:	 Loss = 1.5891,	 Acc = 0.3981
2924 0.163
5678 0.465
2914 0.502
377 0.424
69 0.348
6 0.333
0 0.0
0 0.0
0.4742370632463512
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5797,	 Acc = 0.4247
380 0.197
1718 0.49
815 0.412
85 0.247
2 0.0
0 0.0
0 0.0
0 0.0
0.45763358778625957
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6359,	 Acc1 = 0.3563,	 Acc2 = 0.4056

 ===== Epoch 261	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.4060074   2.1923158
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.4135845   2.3426583
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.5909,	 Acc = 0.3956
2924 0.161
5682 0.459
2911 0.505
376 0.431
69 0.304
6 0.333
0 0.0
0 0.0
0.4713622291021672
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5598,	 Acc = 0.4253
380 0.195
1718 0.493
815 0.413
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.4587786259541985
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6311,	 Acc1 = 0.3536,	 Acc2 = 0.4023

 ===== Epoch 262	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   3.0844526   1.9146907
 -0.3640846  -0.3725409   2.2368143   1.6601611  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  3.1441422  -5.1089597
 -0.3640846  -0.3725409   2.40903     1.59546    -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 2
train:	 Loss = 1.5888,	 Acc = 0.3996
2924 0.162
5681 0.465
2913 0.507
376 0.431
68 0.338
6 0.333
0 0.0
0 0.0
0.4764484741264927
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6008,	 Acc = 0.4170
380 0.197
1718 0.484
815 0.4
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.4488549618320611
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6544,	 Acc1 = 0.3483,	 Acc2 = 0.3959

 ===== Epoch 263	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   1.850368    1.9013549
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  1.7218355   2.154918
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.5904,	 Acc = 0.3972
2925 0.163
5679 0.459
2913 0.509
376 0.431
69 0.348
6 0.333
0 0.0
0 0.0
0.473073095211766
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5870,	 Acc = 0.4197
380 0.195
1718 0.483
815 0.411
85 0.247
2 0.0
0 0.0
0 0.0
0 0.0
0.45229007633587787
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6454,	 Acc1 = 0.3532,	 Acc2 = 0.4018

 ===== Epoch 264	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 3 0
train:	 Loss = 1.5914,	 Acc = 0.3957
2926 0.161
5680 0.457
2912 0.508
375 0.44
69 0.348
6 0.333
0 0.0
0 0.0
0.47157708471577087
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.5747,	 Acc = 0.4143
380 0.195
1718 0.485
815 0.39
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.44618320610687023
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6460,	 Acc1 = 0.3423,	 Acc2 = 0.3887

 ===== Epoch 265	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.4163547   2.509545  ] [ 2.2699068   2.0366175  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.3396406   2.2681053 ] 2 5
train:	 Loss = 1.5892,	 Acc = 0.3988
2925 0.161
5680 0.466
2911 0.505
377 0.432
69 0.319
6 0.333
0 0.0
0 0.0
0.47583766449187215
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5876,	 Acc = 0.4010
380 0.189
1718 0.469
815 0.375
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.4316793893129771
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6466,	 Acc1 = 0.3375,	 Acc2 = 0.3830

 ===== Epoch 266	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 3 0
train:	 Loss = 1.5870,	 Acc = 0.3977
2924 0.161
5680 0.462
2913 0.507
376 0.439
69 0.304
6 0.167
0 0.0
0 0.0
0.4742370632463512
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5651,	 Acc = 0.4250
380 0.195
1718 0.487
815 0.422
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.4583969465648855
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6416,	 Acc1 = 0.3464,	 Acc2 = 0.3936

 ===== Epoch 267	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.3919905   0.92549574
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.35979     0.83923364
 -0.38555372 -0.3798593 ] 5 3
train:	 Loss = 1.5883,	 Acc = 0.3964
2926 0.161
5678 0.457
2913 0.512
376 0.439
69 0.29
6 0.0
0 0.0
0 0.0
0.4726830347268303
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5802,	 Acc = 0.4230
380 0.195
1718 0.487
815 0.417
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.45610687022900764
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6436,	 Acc1 = 0.3563,	 Acc2 = 0.4056

 ===== Epoch 268	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 0
train:	 Loss = 1.5876,	 Acc = 0.3996
2919 0.162
5681 0.465
2916 0.509
377 0.422
69 0.348
6 0.0
0 0.0
0 0.0
0.47618521383578294
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5970,	 Acc = 0.4013
380 0.197
1718 0.47
815 0.372
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.4309160305343511
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6630,	 Acc1 = 0.3311,	 Acc2 = 0.3752

 ===== Epoch 269	 =====
[ 0.8501243   3.071958   -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 0.8496158   3.071958   -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.5853,	 Acc = 0.4020
2926 0.163
5675 0.468
2915 0.513
377 0.424
69 0.304
6 0.333
0 0.0
0 0.0
0.4792081397920814
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5831,	 Acc = 0.4187
380 0.197
1718 0.495
815 0.383
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.45076335877862594
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6437,	 Acc1 = 0.3474,	 Acc2 = 0.3949

 ===== Epoch 270	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.0870659   1.034663  ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 5
train:	 Loss = 1.5894,	 Acc = 0.3956
2926 0.161
5679 0.462
2912 0.5
376 0.426
69 0.377
6 0.333
0 0.0
0 0.0
0.4716876797168768
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5854,	 Acc = 0.4113
380 0.195
1718 0.479
815 0.391
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.44274809160305345
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6552,	 Acc1 = 0.3491,	 Acc2 = 0.3969

 ===== Epoch 271	 =====
[-0.36602148 -0.3783333   2.2362232   2.071629   -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   2.084146    2.105058   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 1
train:	 Loss = 1.5884,	 Acc = 0.3962
2920 0.163
5682 0.461
2914 0.499
377 0.432
69 0.362
6 0.333
0 0.0
0 0.0
0.47159593280282935
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5810,	 Acc = 0.4107
380 0.195
1718 0.475
815 0.396
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.44198473282442746
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6460,	 Acc1 = 0.3489,	 Acc2 = 0.3966

 ===== Epoch 272	 =====
[ 2.2382505   2.3606362  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.2272131   2.6013973 ] [ 2.2212496   2.3606362  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.2272131   2.6013973 ] 0 0
train:	 Loss = 1.5852,	 Acc = 0.3953
2925 0.161
5680 0.457
2912 0.505
376 0.434
69 0.391
6 0.5
0 0.0
0 0.0
0.4709720225588853
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5808,	 Acc = 0.4140
380 0.197
1718 0.48
815 0.395
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.4454198473282443
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6421,	 Acc1 = 0.3493,	 Acc2 = 0.3971

 ===== Epoch 273	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  1.617639    3.2661142  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  1.6378613   2.9759147  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 5
train:	 Loss = 1.5843,	 Acc = 0.3997
2925 0.164
5678 0.463
2915 0.513
375 0.411
69 0.319
6 0.5
0 0.0
0 0.0
0.4760588300342807
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5874,	 Acc = 0.4033
380 0.197
1718 0.47
815 0.38
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.43320610687022904
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6451,	 Acc1 = 0.3483,	 Acc2 = 0.3959

 ===== Epoch 274	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.6689198   1.7486823
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.6393704   1.8571261
 -0.38555372 -0.3798593 ] 2 6
train:	 Loss = 1.5867,	 Acc = 0.3960
2921 0.17
5681 0.457
2915 0.499
376 0.436
69 0.362
6 0.333
0 0.0
0 0.0
0.46877417928595116
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5652,	 Acc = 0.4310
380 0.195
1718 0.499
815 0.415
85 0.282
2 0.0
0 0.0
0 0.0
0 0.0
0.46526717557251906
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6308,	 Acc1 = 0.3588,	 Acc2 = 0.4085

 ===== Epoch 275	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 3 0
train:	 Loss = 1.5839,	 Acc = 0.3973
2924 0.161
5685 0.461
2908 0.507
376 0.428
69 0.391
6 0.333
0 0.0
0 0.0
0.4735736399823087
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5766,	 Acc = 0.4183
380 0.2
1718 0.487
815 0.399
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.45
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6362,	 Acc1 = 0.3549,	 Acc2 = 0.4038

 ===== Epoch 276	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   2.5583606   1.9314079  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 3 2
train:	 Loss = 1.5841,	 Acc = 0.3981
2920 0.163
5682 0.463
2914 0.506
377 0.427
69 0.319
6 0.5
0 0.0
0 0.0
0.474027409372237
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6032,	 Acc = 0.4067
380 0.195
1718 0.477
815 0.377
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.43740458015267175
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6508,	 Acc1 = 0.3491,	 Acc2 = 0.3969

 ===== Epoch 277	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.493327    1.1314737  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 2.763031    1.7606955  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6124086   1.0979892  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 5
train:	 Loss = 1.5823,	 Acc = 0.4012
2926 0.162
5682 0.468
2909 0.508
376 0.439
69 0.362
6 0.0
0 0.0
0 0.0
0.4785445697854457
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.5939,	 Acc = 0.4157
380 0.2
1718 0.485
815 0.395
85 0.176
2 0.0
0 0.0
0 0.0
0 0.0
0.44694656488549617
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6649,	 Acc1 = 0.3406,	 Acc2 = 0.3867

 ===== Epoch 278	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  3.446531    2.2559967  -0.40141198 -0.40778467  3.1872914   3.2841473
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  3.4206915   2.4959693  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.5845,	 Acc = 0.3995
2923 0.161
5680 0.463
2914 0.513
376 0.441
69 0.275
6 0.167
0 0.0
0 0.0
0.476506357103372
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5766,	 Acc = 0.4247
380 0.195
1718 0.497
815 0.399
85 0.247
2 0.0
0 0.0
0 0.0
0 0.0
0.4580152671755725
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6473,	 Acc1 = 0.3470,	 Acc2 = 0.3944

 ===== Epoch 279	 =====
[ 0.47273228  2.365699   -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 0.5585252   2.2315352  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 3 5
train:	 Loss = 1.5830,	 Acc = 0.3991
2923 0.164
5684 0.462
2910 0.511
377 0.424
68 0.338
6 0.333
0 0.0
0 0.0
0.47529021558872303
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5634,	 Acc = 0.4247
380 0.2
1718 0.492
815 0.407
85 0.247
2 0.0
0 0.0
0 0.0
0 0.0
0.45725190839694657
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6294,	 Acc1 = 0.3549,	 Acc2 = 0.4038

 ===== Epoch 280	 =====
[ 4.02162     9.909256   -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 3 6
train:	 Loss = 1.5875,	 Acc = 0.3973
2925 0.162
5683 0.46
2909 0.508
377 0.432
68 0.353
6 0.333
0 0.0
0 0.0
0.4732942607541745
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5784,	 Acc = 0.4250
380 0.197
1718 0.494
815 0.407
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.4580152671755725
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6407,	 Acc1 = 0.3511,	 Acc2 = 0.3994

 ===== Epoch 281	 =====
[ 0.5782857   2.826413   -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 0.6247025   2.5606163  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 5
train:	 Loss = 1.5823,	 Acc = 0.3990
2923 0.163
5683 0.462
2912 0.511
375 0.421
69 0.377
6 0.333
0 0.0
0 0.0
0.4754007739082366
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5840,	 Acc = 0.4063
380 0.187
1718 0.478
815 0.379
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.4381679389312977
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6460,	 Acc1 = 0.3538,	 Acc2 = 0.4026

 ===== Epoch 282	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   2.2013412   3.2602684   3.0819755   0.87373847
  3.6534984   2.7693553 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   2.3049      3.3249695   3.320262    0.99943465
  3.2002616   2.8822021 ] 2 6
train:	 Loss = 1.5824,	 Acc = 0.3964
2926 0.163
5678 0.46
2912 0.504
377 0.427
69 0.348
6 0.167
0 0.0
0 0.0
0.4719088697190887
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5774,	 Acc = 0.4200
380 0.197
1718 0.487
815 0.401
85 0.247
2 0.0
0 0.0
0 0.0
0 0.0
0.45229007633587787
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6442,	 Acc1 = 0.3454,	 Acc2 = 0.3924

 ===== Epoch 283	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 0
train:	 Loss = 1.5839,	 Acc = 0.4017
2921 0.162
5680 0.469
2916 0.509
376 0.434
69 0.362
6 0.167
0 0.0
0 0.0
0.4791643638775285
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5669,	 Acc = 0.4193
380 0.195
1718 0.486
815 0.402
85 0.247
2 0.0
0 0.0
0 0.0
0 0.0
0.45190839694656487
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6379,	 Acc1 = 0.3559,	 Acc2 = 0.4051

 ===== Epoch 284	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.8842702   2.183447   -0.40141198 -0.40778467  3.3672383   3.0228965
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  3.2201664   2.0104432  -0.40141198 -0.40778467  3.2217653   2.8651597
 -0.38555372 -0.3798593 ] 2 5
train:	 Loss = 1.5799,	 Acc = 0.3967
2925 0.163
5681 0.459
2914 0.505
374 0.449
68 0.338
6 0.167
0 0.0
0 0.0
0.47218843304213204
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5947,	 Acc = 0.4107
380 0.197
1718 0.481
815 0.383
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.4416030534351145
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6494,	 Acc1 = 0.3443,	 Acc2 = 0.3912

 ===== Epoch 285	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 0
train:	 Loss = 1.5818,	 Acc = 0.4028
2926 0.163
5680 0.467
2914 0.514
373 0.437
69 0.333
6 0.5
0 0.0
0 0.0
0.4803140898031409
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5689,	 Acc = 0.4130
380 0.205
1718 0.48
815 0.39
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.4431297709923664
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6375,	 Acc1 = 0.3495,	 Acc2 = 0.3974

 ===== Epoch 286	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.4302528   1.6303802
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.4279796   1.657491
 -0.38555372 -0.3798593 ] 3 4
train:	 Loss = 1.5813,	 Acc = 0.4012
2922 0.164
5680 0.468
2914 0.505
377 0.44
69 0.362
6 0.333
0 0.0
0 0.0
0.4778907804554499
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5668,	 Acc = 0.4280
380 0.195
1718 0.491
815 0.426
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.4618320610687023
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6366,	 Acc1 = 0.3608,	 Acc2 = 0.4110

 ===== Epoch 287	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   0.9706514   2.2748213  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   0.8636596   2.3818269  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 4
train:	 Loss = 1.5800,	 Acc = 0.3997
2922 0.16
5683 0.465
2912 0.51
376 0.428
69 0.333
6 0.333
0 0.0
0 0.0
0.4771169577713907
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5755,	 Acc = 0.4160
380 0.195
1718 0.485
815 0.398
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.4480916030534351
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6390,	 Acc1 = 0.3495,	 Acc2 = 0.3974

 ===== Epoch 288	 =====
[ 3.4376943   2.545428   -0.4208693  -0.3341335  -0.4409929  -0.3635196
  1.0638051   3.4921348  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 2.7637274   2.6112442  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  1.1536772   3.6037502  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
train:	 Loss = 1.5801,	 Acc = 0.4011
2922 0.162
5681 0.468
2914 0.509
376 0.423
69 0.362
6 0.333
0 0.0
0 0.0
0.4783329648463409
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5705,	 Acc = 0.4190
380 0.184
1718 0.488
815 0.399
85 0.271
2 0.0
0 0.0
0 0.0
0 0.0
0.4530534351145038
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6351,	 Acc1 = 0.3509,	 Acc2 = 0.3991

 ===== Epoch 289	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 0
train:	 Loss = 1.5798,	 Acc = 0.4017
2927 0.163
5673 0.469
2916 0.507
377 0.438
69 0.377
6 0.333
0 0.0
0 0.0
0.4790399292113704
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5662,	 Acc = 0.4227
380 0.197
1718 0.495
815 0.396
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.45534351145038165
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6334,	 Acc1 = 0.3557,	 Acc2 = 0.4048

 ===== Epoch 290	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 0
train:	 Loss = 1.5772,	 Acc = 0.4035
2922 0.163
5683 0.47
2913 0.515
376 0.426
68 0.324
6 0.333
0 0.0
0 0.0
0.48120716338713243
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5829,	 Acc = 0.4250
380 0.197
1718 0.494
815 0.409
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.4580152671755725
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6306,	 Acc1 = 0.3650,	 Acc2 = 0.4160

 ===== Epoch 291	 =====
[-0.36602148 -0.3783333   3.3239014   1.9923431  -0.4409929  -0.3635196
  2.4899583   3.221468   -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   3.7489107   1.9329246  -0.44088638 -0.36343393
  2.6320682   3.154499   -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 2
train:	 Loss = 1.5791,	 Acc = 0.4026
2924 0.166
5684 0.462
2908 0.52
377 0.435
69 0.391
6 0.333
0 0.0
0 0.0
0.4789915966386555
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5721,	 Acc = 0.4113
380 0.189
1718 0.476
815 0.4
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.4435114503816794
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6326,	 Acc1 = 0.3553,	 Acc2 = 0.4043

 ===== Epoch 292	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 0
train:	 Loss = 1.5814,	 Acc = 0.4000
2924 0.162
5678 0.466
2915 0.509
376 0.42
69 0.348
6 0.333
0 0.0
0 0.0
0.47678018575851394
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5816,	 Acc = 0.4207
380 0.197
1718 0.487
815 0.406
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.4530534351145038
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6324,	 Acc1 = 0.3530,	 Acc2 = 0.4016

 ===== Epoch 293	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  3.1920803   2.155543   -0.40141198 -0.40778467  3.4880865   2.5546165
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  3.440351    2.3118043  -0.40141198 -0.40778467  4.029063    2.7000299
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.5809,	 Acc = 0.4000
2921 0.162
5682 0.464
2913 0.511
377 0.443
69 0.348
6 0.333
0 0.0
0 0.0
0.47695368630485246
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5707,	 Acc = 0.4270
380 0.197
1718 0.5
815 0.401
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.46030534351145036
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6324,	 Acc1 = 0.3571,	 Acc2 = 0.4066

 ===== Epoch 294	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 0
train:	 Loss = 1.5802,	 Acc = 0.3988
2921 0.164
5682 0.462
2914 0.511
376 0.418
69 0.333
6 0.167
0 0.0
0 0.0
0.4746324748535426
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5776,	 Acc = 0.4113
380 0.195
1718 0.474
815 0.398
85 0.247
2 0.0
0 0.0
0 0.0
0 0.0
0.44274809160305345
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6343,	 Acc1 = 0.3540,	 Acc2 = 0.4028

 ===== Epoch 295	 =====
[ 0.3022921   1.4797108  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  3.7077205   1.3574945  -0.40141198 -0.40778467  3.922611    1.6451678
  4.880942    3.2889757 ] [ 0.2825558   1.5556525  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  3.1729832   1.4356251  -0.40141198 -0.40778467  3.9093516   1.6895312
  4.281328    3.3152192 ] 2 3
train:	 Loss = 1.5801,	 Acc = 0.4012
2919 0.162
5686 0.466
2912 0.509
376 0.455
69 0.304
6 0.333
0 0.0
0 0.0
0.4781743839098243
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5671,	 Acc = 0.4273
380 0.2
1718 0.495
815 0.411
85 0.247
2 0.0
0 0.0
0 0.0
0 0.0
0.46030534351145036
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6387,	 Acc1 = 0.3547,	 Acc2 = 0.4036

 ===== Epoch 296	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.3242564   2.5266635  -0.40141198 -0.40778467  3.1410728   3.244713
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.3321207   2.3536599  -0.40141198 -0.40778467  2.714505    3.0820475
 -0.38555372 -0.3798593 ] 5 5
train:	 Loss = 1.5798,	 Acc = 0.4007
2927 0.163
5679 0.464
2911 0.509
376 0.455
69 0.391
6 0.333
0 0.0
0 0.0
0.47760203517310035
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5687,	 Acc = 0.4190
380 0.192
1718 0.492
815 0.394
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.45190839694656487
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6388,	 Acc1 = 0.3524,	 Acc2 = 0.4008

 ===== Epoch 297	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 0
train:	 Loss = 1.5763,	 Acc = 0.4021
2925 0.163
5680 0.465
2911 0.513
377 0.462
69 0.377
6 0.167
0 0.0
0 0.0
0.4792657303992038
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5919,	 Acc = 0.4227
380 0.195
1718 0.491
815 0.405
85 0.247
2 0.0
0 0.0
0 0.0
0 0.0
0.45572519083969465
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6516,	 Acc1 = 0.3507,	 Acc2 = 0.3989

 ===== Epoch 298	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.498822    1.2557563
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.4757137   1.3001196
 -0.38555372 -0.3798593 ] 1 4
train:	 Loss = 1.5788,	 Acc = 0.4043
2920 0.165
5680 0.471
2916 0.512
377 0.422
69 0.362
6 0.333
0 0.0
0 0.0
0.48165340406719714
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5613,	 Acc = 0.4223
380 0.197
1718 0.484
815 0.416
85 0.247
2 0.0
0 0.0
0 0.0
0 0.0
0.4549618320610687
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6360,	 Acc1 = 0.3427,	 Acc2 = 0.3892

 ===== Epoch 299	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  1.9170259   1.0198586  -0.40141198 -0.40778467  1.8715965   3.0894415
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.7382467   3.0771184
 -0.38555372 -0.3798593 ] 3 5
train:	 Loss = 1.5801,	 Acc = 0.4030
2919 0.163
5681 0.468
2916 0.515
377 0.432
69 0.348
6 0.333
0 0.0
0 0.0
0.4804950823295392
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5880,	 Acc = 0.4023
380 0.195
1718 0.468
815 0.377
85 0.259
2 0.0
0 0.0
0 0.0
0 0.0
0.43244274809160305
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6402,	 Acc1 = 0.3526,	 Acc2 = 0.4011

 ===== Epoch 300	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.2553558   1.5194716
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.2208824   1.5022192
 -0.38555372 -0.3798593 ] 3 3
train:	 Loss = 1.5777,	 Acc = 0.4009
2922 0.163
5682 0.466
2913 0.51
376 0.434
69 0.333
6 0.333
0 0.0
0 0.0
0.4778907804554499
0.47099236641221376
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.5727,	 Acc = 0.4160
380 0.192
1718 0.482
815 0.401
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.4484732824427481
0.47099236641221376
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0541778   2.542186   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
Testing:	 Loss = 1.6409,	 Acc1 = 0.3499,	 Acc2 = 0.3979
