(0.24300000000005184, array([-1.000000e+00, -1.000000e+00,  1.189783e+03,  1.790000e-01,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 4, array([  0.   ,   0.   , -15.851,   0.091,   0.   ,   0.   ,   0.   ,
         0.   ,   0.   ,   0.   ,   0.   ,   0.   ,   0.   ,   0.   ]), 1)
((0.236, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.364685e+03, -3.300000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), array([0., 0., 0.])), (0.24, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.543573e+03, -3.290000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), array([0., 0., 0.])))
(0.236, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.364685e+03, -3.300000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([0.   , 0.   , 0.   , 0.   , 5.823, 3.129, 0.   , 0.   , 0.   ,
       0.   , 0.   , 0.   , 0.   , 0.   ]))
(0.236, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.364685e+03, -3.300000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([ 0.   ,  0.   ,  0.   ,  0.   , -8.411, -0.011,  0.   ,  0.   ,
        0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ]))
(0.236, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.364685e+03, -3.300000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  6.2433e+01,
       -3.0000e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
        0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]))
(0.236, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.364685e+03, -3.300000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([ 0.00000e+00,  0.00000e+00,  0.00000e+00,  0.00000e+00,
       -1.19043e+02, -4.00000e-03,  0.00000e+00,  0.00000e+00,
        0.00000e+00,  0.00000e+00,  0.00000e+00,  0.00000e+00,
        0.00000e+00,  0.00000e+00]))
(0.236, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.364685e+03, -3.300000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  5.0464e+01,
       -1.3000e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
        0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]))
(0.236, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.364685e+03, -3.300000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([ 0.00000e+00,  0.00000e+00,  0.00000e+00,  0.00000e+00,
       -1.53656e+02, -1.00000e-02,  0.00000e+00,  0.00000e+00,
        0.00000e+00,  0.00000e+00,  0.00000e+00,  0.00000e+00,
        0.00000e+00,  0.00000e+00]))
(0.236, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.364685e+03, -3.300000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([ 0.000000e+00,  0.000000e+00,  2.399328e+03,  1.498000e+00,
       -1.455700e+01, -6.000000e-03,  0.000000e+00,  0.000000e+00,
        0.000000e+00,  0.000000e+00,  0.000000e+00,  0.000000e+00,
        0.000000e+00,  0.000000e+00]))
(0.236, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.364685e+03, -3.300000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  1.9086e+01,
       -9.0000e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
        0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]))
(0.236, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.364685e+03, -3.300000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([ 0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  7.052e+01,
       -2.000e-02,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,
        0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00]))
(0.236, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.364685e+03, -3.300000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  4.1406e+01,
       -1.7000e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
        0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]))
14 1 14

 ===== Epoch 1	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.2313027   2.1039715
 -0.39521116 -0.38799495] [ 2.1411612   3.567229    0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.01156525 -0.6711872
 -0.01438999 -0.01411033] 5 2
train:	 Loss = 1.4169,	 Acc = 0.4638
45196 0.276
88651 0.507
46208 0.553
5474 0.555
879 0.52
88 0.307
0 0.0
0 0.0
0.5238145789101203
0.0
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2729,	 Acc = 0.4984
5196 0.371
27109 0.509
13039 0.526
1257 0.519
29 0.69
0 0.0
0 0.0
0 0.0
0.5143601872858039
0.5143601872858039
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 1
Testing:	 Loss = 1.3521,	 Acc1 = 0.2253,	 Acc2 = 0.2155

 ===== Epoch 2	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006  1.6888614   1.2285949
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.2668224   0.1627554
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 2 2
train:	 Loss = 1.3170,	 Acc = 0.4935
45197 0.276
88650 0.546
46206 0.592
5475 0.61
880 0.528
88 0.341
0 0.0
0 0.0
0.5632382394779863
0.5143601872858039
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2360,	 Acc = 0.5180
5196 0.39
27109 0.521
13039 0.561
1257 0.523
29 0.69
0 0.0
0 0.0
0 0.0
0.5340300236520732
0.5340300236520732
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3260,	 Acc1 = 0.2315,	 Acc2 = 0.2229

 ===== Epoch 3	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006  2.8248563   1.7571421
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-1.3513822e-02 -1.6987352e-02  3.2026558e+00  3.0621347e+00
 -3.7602153e-02 -7.0954627e-01 -1.3048708e-02 -8.5879778e-03
  1.9501297e-02  3.3417623e-02 -7.8236684e-03  2.0873190e-03
 -1.4389989e-02 -1.4110334e-02] 5 5
train:	 Loss = 1.2990,	 Acc = 0.5005
45193 0.277
88650 0.553
46211 0.603
5474 0.623
880 0.52
88 0.33
0 0.0
0 0.0
0.571976532699235
0.5340300236520732
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2869,	 Acc = 0.5092
5196 0.373
27109 0.525
13039 0.532
1257 0.498
29 0.966
0 0.0
0 0.0
0 0.0
0.526355167253946
0.5340300236520732
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3571,	 Acc1 = 0.2686,	 Acc2 = 0.2676

 ===== Epoch 4	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.0490887   1.596811
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.7748139   0.14397575
 -0.01438999 -0.01411033] 1 1
train:	 Loss = 1.2939,	 Acc = 0.5042
45192 0.277
88652 0.558
46210 0.61
5474 0.621
880 0.526
88 0.409
0 0.0
0 0.0
0.5767635735718734
0.5340300236520732
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2969,	 Acc = 0.5005
5196 0.398
27109 0.511
13039 0.527
1257 0.437
29 0.0
0 0.0
0 0.0
0 0.0
0.5133947965439012
0.5340300236520732
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 1
Testing:	 Loss = 1.3684,	 Acc1 = 0.2767,	 Acc2 = 0.2773

 ===== Epoch 5	 =====
[ 2.6291697   2.6452618  -0.42115992 -0.34173006 -0.4351751  -0.3592861
  1.5979753   1.8895504  -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [ 3.5334486e-01 -1.2053754e+00  2.0342596e-02  2.5701299e-02
  7.4981754e-03  5.3155841e-03 -2.2428308e+00 -2.4059832e+00
  1.9501297e-02  3.3417623e-02 -7.8236684e-03  2.0873190e-03
 -1.4389989e-02 -1.4110334e-02] 5 5
train:	 Loss = 1.2880,	 Acc = 0.5059
45197 0.283
88645 0.561
46211 0.605
5475 0.619
880 0.502
88 0.364
0 0.0
0 0.0
0.5772935406478461
0.5340300236520732
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2898,	 Acc = 0.4854
5196 0.375
27109 0.489
13039 0.519
1257 0.502
29 0.966
0 0.0
0 0.0
0 0.0
0.49922768740647777
0.5340300236520732
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 0
Testing:	 Loss = 1.3733,	 Acc1 = 0.2554,	 Acc2 = 0.2517

 ===== Epoch 6	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.3147535   1.4367847
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.42546067  0.31368548
 -0.01438999 -0.01411033] 1 1
train:	 Loss = 1.2832,	 Acc = 0.5086
45191 0.288
88651 0.564
46212 0.606
5474 0.613
880 0.493
88 0.42
0 0.0
0 0.0
0.5791373270584905
0.5340300236520732
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2784,	 Acc = 0.4972
5196 0.383
27109 0.496
13039 0.542
1257 0.527
29 0.966
0 0.0
0 0.0
0 0.0
0.5115364193657382
0.5340300236520732
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3527,	 Acc1 = 0.2814,	 Acc2 = 0.2831

 ===== Epoch 7	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006  1.6973623   1.1726048
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-1.3513822e-02 -1.6987352e-02  1.5717810e+00  2.6199763e+00
 -2.6980531e+00 -1.4499390e+00 -1.3048708e-02 -8.5879778e-03
  1.9501297e-02  3.3417623e-02 -7.8236684e-03  2.0873190e-03
 -1.4389989e-02 -1.4110334e-02] 5 6
train:	 Loss = 1.2808,	 Acc = 0.5100
45191 0.288
88652 0.566
46210 0.607
5475 0.612
880 0.494
88 0.443
0 0.0
0 0.0
0.5809419341141503
0.5340300236520732
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2747,	 Acc = 0.4970
5196 0.356
27109 0.499
13039 0.547
1257 0.518
29 0.483
0 0.0
0 0.0
0 0.0
0.5146256697398272
0.5340300236520732
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3417,	 Acc1 = 0.2899,	 Acc2 = 0.2932

 ===== Epoch 8	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167    1.5171138   2.143687   -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  1.0158701  -0.15028821 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 2 2
train:	 Loss = 1.2790,	 Acc = 0.5094
45193 0.289
88649 0.566
46211 0.606
5475 0.609
880 0.497
88 0.432
0 0.0
0 0.0
0.5799735320552288
0.5340300236520732
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2591,	 Acc = 0.5236
5196 0.366
27109 0.538
13039 0.551
1257 0.558
29 0.966
0 0.0
0 0.0
0 0.0
0.5433701790799826
0.5433701790799826
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3384,	 Acc1 = 0.2703,	 Acc2 = 0.2696

 ===== Epoch 9	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167    1.9687479   2.2318313  -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798 -2.792915   -2.8923676   2.4232059   1.6018099
 -0.01438999 -0.01411033] 6 6
train:	 Loss = 1.2791,	 Acc = 0.5102
45191 0.289
88652 0.566
46210 0.608
5475 0.608
880 0.492
88 0.386
0 0.0
0 0.0
0.5808428576483493
0.5433701790799826
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2486,	 Acc = 0.5135
5196 0.381
27109 0.521
13039 0.551
1257 0.527
29 0.0
0 0.0
0 0.0
0 0.0
0.5300719216102717
0.5433701790799826
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3161,	 Acc1 = 0.3031,	 Acc2 = 0.3091

 ===== Epoch 10	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006  3.6130974   2.3864713
 -0.35938287 -0.366167    4.2708044   1.690723   -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-1.3513822e-02 -1.6987352e-02  2.0342596e-02  2.5701299e-02
  9.5312601e-01 -6.4433341e+00 -1.3048708e-02 -8.5879778e-03
  4.5435283e-02  3.4949967e-01 -7.8236684e-03  2.0873190e-03
 -1.4389989e-02 -1.4110334e-02] 2 4
train:	 Loss = 1.2779,	 Acc = 0.5111
45196 0.291
88645 0.568
46213 0.607
5474 0.606
880 0.485
88 0.5
0 0.0
0 0.0
0.5816206652512385
0.5433701790799826
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2405,	 Acc = 0.5252
5196 0.371
27109 0.526
13039 0.582
1257 0.544
29 0.966
0 0.0
0 0.0
0 0.0
0.5445527827388136
0.5445527827388136
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3198,	 Acc1 = 0.2709,	 Acc2 = 0.2704

 ===== Epoch 11	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
  1.1320511   2.88528   ] [ 1.5110112e+00  2.7908473e+00  2.0342596e-02  2.5701299e-02
  7.4981754e-03  5.3155841e-03 -1.3048708e-02 -8.5879778e-03
  1.9501297e-02  3.3417623e-02 -7.8236684e-03  2.0873190e-03
  5.3312089e-02  6.1955922e-03] 4 2
train:	 Loss = 1.2750,	 Acc = 0.5130
45193 0.291
88649 0.572
46211 0.608
5475 0.598
880 0.484
88 0.5
0 0.0
0 0.0
0.5839508007614842
0.5445527827388136
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2292,	 Acc = 0.5393
5196 0.41
27109 0.549
13039 0.574
1257 0.514
29 0.0
0 0.0
0 0.0
0 0.0
0.5555582371965053
0.5555582371965053
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3112,	 Acc1 = 0.2552,	 Acc2 = 0.2515

 ===== Epoch 12	 =====
[ 1.61435     1.3418301  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
  1.7794828   1.5190208 ] [ 0.7490981   0.18068038  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
  0.19882305  0.22375908] 6 2
train:	 Loss = 1.2761,	 Acc = 0.5133
45195 0.292
88648 0.572
46210 0.607
5475 0.602
880 0.505
88 0.466
0 0.0
0 0.0
0.5842633810093347
0.5555582371965053
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2207,	 Acc = 0.5474
5196 0.397
27109 0.564
13039 0.572
1257 0.552
29 0.414
0 0.0
0 0.0
0 0.0
0.5663223439687214
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3024,	 Acc1 = 0.2730,	 Acc2 = 0.2729

 ===== Epoch 13	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167    1.7931768   2.6994863  -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798 -0.83628434  0.28466234 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 1 1
train:	 Loss = 1.2753,	 Acc = 0.5137
45197 0.291
88649 0.572
46208 0.608
5474 0.605
880 0.502
88 0.455
0 0.0
0 0.0
0.5848024402154297
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2189,	 Acc = 0.5412
5196 0.387
27109 0.553
13039 0.58
1257 0.547
29 0.0
0 0.0
0 0.0
0 0.0
0.5605541342858522
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.2969,	 Acc1 = 0.2785,	 Acc2 = 0.2796

 ===== Epoch 14	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 5 1
train:	 Loss = 1.2739,	 Acc = 0.5144
45192 0.292
88650 0.573
46211 0.61
5475 0.604
880 0.5
88 0.42
0 0.0
0 0.0
0.58546113344279
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2475,	 Acc = 0.5270
5196 0.398
27109 0.534
13039 0.558
1257 0.57
29 0.966
0 0.0
0 0.0
0 0.0
0.5431771009316021
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 1
Testing:	 Loss = 1.3207,	 Acc1 = 0.2742,	 Acc2 = 0.2744

 ===== Epoch 15	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  1.79912     1.2472152
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  3.7539      2.8916638  -2.7274115  -1.8869765
  3.96241     1.4218087 ] 5 6
train:	 Loss = 1.2720,	 Acc = 0.5145
45196 0.293
88647 0.575
46210 0.607
5475 0.598
880 0.503
88 0.443
0 0.0
0 0.0
0.585449398443029
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2441,	 Acc = 0.5253
5196 0.371
27109 0.543
13039 0.551
1257 0.531
29 0.069
0 0.0
0 0.0
0 0.0
0.5446493218130039
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3174,	 Acc1 = 0.2730,	 Acc2 = 0.2729

 ===== Epoch 16	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
  4.5109982   2.1540923  -0.41133043 -0.41984478  4.6754894   2.3526278
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   1.0178797   2.8541253
  0.2058887  -0.70090014  0.0195013   0.03341762  0.8957284  -0.643366
 -0.01438999 -0.01411033] 5 5
train:	 Loss = 1.2698,	 Acc = 0.5160
45192 0.292
88649 0.576
46212 0.61
5475 0.597
880 0.495
88 0.5
0 0.0
0 0.0
0.5876974466398687
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2831,	 Acc = 0.5159
5196 0.386
27109 0.541
13039 0.515
1257 0.519
29 0.69
0 0.0
0 0.0
0 0.0
0.5321957812424579
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3541,	 Acc1 = 0.2655,	 Acc2 = 0.2639

 ===== Epoch 17	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006  2.677254    2.9956446
 -0.35938287 -0.366167    1.9404484   2.2587643  -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.3398655   0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 0 0
train:	 Loss = 1.2685,	 Acc = 0.5168
45192 0.295
88650 0.576
46213 0.61
5473 0.606
880 0.482
88 0.489
0 0.0
0 0.0
0.5879451395572666
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2965,	 Acc = 0.5066
5196 0.397
27109 0.539
13039 0.488
1257 0.467
29 0.0
0 0.0
0 0.0
0 0.0
0.5203697446541488
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 0
Testing:	 Loss = 1.3669,	 Acc1 = 0.2839,	 Acc2 = 0.2860

 ===== Epoch 18	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  1.5196749   1.9661026
 -0.39521116 -0.38799495] [ 2.890766    2.20737     0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -2.386007   -2.6993573
 -0.01438999 -0.01411033] 6 6
train:	 Loss = 1.2660,	 Acc = 0.5167
45198 0.294
88644 0.577
46211 0.61
5475 0.601
880 0.474
88 0.534
0 0.0
0 0.0
0.5879842602159974
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2463,	 Acc = 0.5261
5196 0.398
27109 0.541
13039 0.549
1257 0.5
29 0.966
0 0.0
0 0.0
0 0.0
0.5421634406526041
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3211,	 Acc1 = 0.2857,	 Acc2 = 0.2883

 ===== Epoch 19	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167    1.0897388   3.2087646  -0.42690593 -0.4244443
  2.8354046   2.4281187 ] [-1.3513822e-02 -1.6987352e-02  2.0342596e-02  2.5701299e-02
  7.4981754e-03  5.3155841e-03 -1.3048708e-02 -8.5879778e-03
 -1.7542351e+00 -3.9702885e+00 -7.8236684e-03  2.0873190e-03
 -1.6495048e-01  5.6315815e-01] 4 4
train:	 Loss = 1.2676,	 Acc = 0.5164
45194 0.294
88652 0.576
46208 0.611
5474 0.598
880 0.478
88 0.455
0 0.0
0 0.0
0.5874014522087444
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2190,	 Acc = 0.5425
5196 0.392
27109 0.556
13039 0.573
1257 0.557
29 0.966
0 0.0
0 0.0
0 0.0
0.5613747164164695
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.2878,	 Acc1 = 0.2886,	 Acc2 = 0.2917

 ===== Epoch 20	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  1.7416996   1.5697297
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762  0.13481748  0.51399857
 -0.01438999 -0.01411033] 6 4
train:	 Loss = 1.2658,	 Acc = 0.5165
45195 0.294
88649 0.575
46209 0.612
5475 0.603
880 0.473
88 0.466
0 0.0
0 0.0
0.5876037678431151
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2327,	 Acc = 0.5254
5196 0.405
27109 0.537
13039 0.55
1257 0.528
29 0.034
0 0.0
0 0.0
0 0.0
0.5404981416228218
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3013,	 Acc1 = 0.2888,	 Acc2 = 0.2920

 ===== Epoch 21	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 3 1
train:	 Loss = 1.2660,	 Acc = 0.5175
45196 0.296
88651 0.577
46207 0.61
5474 0.605
880 0.48
88 0.511
0 0.0
0 0.0
0.5884288747346073
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2465,	 Acc = 0.5147
5196 0.385
27109 0.523
13039 0.549
1257 0.516
29 0.966
0 0.0
0 0.0
0 0.0
0.5308925037408891
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3168,	 Acc1 = 0.2855,	 Acc2 = 0.2880

 ===== Epoch 22	 =====
[-0.3681874  -0.38585573  1.4933751   0.8366779  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.3407815   0.04665667  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 3 3
train:	 Loss = 1.2651,	 Acc = 0.5179
45194 0.296
88649 0.577
46210 0.612
5475 0.604
880 0.485
88 0.489
0 0.0
0 0.0
0.5887885521790208
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.3684,	 Acc = 0.4859
5196 0.391
27109 0.516
13039 0.463
1257 0.462
29 0.966
0 0.0
0 0.0
0 0.0
0.49780373606217115
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.4334,	 Acc1 = 0.2940,	 Acc2 = 0.2982

 ===== Epoch 23	 =====
[ 3.1136622   1.8223108  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167    1.9514211   3.2748728  -0.42690593 -0.4244443
  2.8068554   1.9866314 ] [-0.14931458 -0.02413197  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.15052985  0.03341762 -0.00782367  0.00208732
  0.03866025 -0.02571372] 0 0
train:	 Loss = 1.2642,	 Acc = 0.5179
45194 0.295
88649 0.577
46211 0.611
5474 0.611
880 0.469
88 0.5
0 0.0
0 0.0
0.5890574797242785
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2684,	 Acc = 0.5052
5196 0.38
27109 0.533
13039 0.503
1257 0.467
29 0.0
0 0.0
0 0.0
0 0.0
0.5209248443307428
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3343,	 Acc1 = 0.2909,	 Acc2 = 0.2945

 ===== Epoch 24	 =====
[-0.3681874  -0.38585573  0.5948588   1.4761242  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.01788894  0.02779684  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 0 0
train:	 Loss = 1.2648,	 Acc = 0.5182
45198 0.296
88645 0.578
46212 0.61
5473 0.61
880 0.487
88 0.489
0 0.0
0 0.0
0.5893289360075868
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2720,	 Acc = 0.5208
5196 0.396
27109 0.54
13039 0.534
1257 0.503
29 0.0
0 0.0
0 0.0
0 0.0
0.5364676352753777
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3399,	 Acc1 = 0.2870,	 Acc2 = 0.2898

 ===== Epoch 25	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  1.7290673   2.561893
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.16402876  0.127283
 -0.01438999 -0.01411033] 1 3
train:	 Loss = 1.2649,	 Acc = 0.5175
45190 0.296
88654 0.576
46210 0.611
5474 0.61
880 0.476
88 0.568
0 0.0
0 0.0
0.5882694294651324
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.3102,	 Acc = 0.5007
5196 0.398
27109 0.534
13039 0.475
1257 0.472
29 0.172
0 0.0
0 0.0
0 0.0
0.5136120094608293
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3709,	 Acc1 = 0.2913,	 Acc2 = 0.2950

 ===== Epoch 26	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 1 1
train:	 Loss = 1.2645,	 Acc = 0.5182
45197 0.296
88649 0.577
46208 0.613
5474 0.605
880 0.491
88 0.5
0 0.0
0 0.0
0.5891478354411567
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2934,	 Acc = 0.4911
5196 0.379
27109 0.515
13039 0.488
1257 0.461
29 0.69
0 0.0
0 0.0
0 0.0
0.5051648404691799
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3639,	 Acc1 = 0.2752,	 Acc2 = 0.2756

 ===== Epoch 27	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 0 1
train:	 Loss = 1.2652,	 Acc = 0.5181
45193 0.296
88653 0.576
46207 0.613
5475 0.605
880 0.489
88 0.58
0 0.0
0 0.0
0.5889825410642379
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2171,	 Acc = 0.5412
5196 0.391
27109 0.549
13039 0.583
1257 0.549
29 0.69
0 0.0
0 0.0
0 0.0
0.5599507650721629
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.2876,	 Acc1 = 0.2946,	 Acc2 = 0.2990

 ===== Epoch 28	 =====
[-0.3681874  -0.38585573  2.4256876   1.8346708  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-1.3513822e-02 -1.6987352e-02 -3.4542413e+00 -1.9713457e+00
  1.1551402e+00  1.0988841e+00 -1.3048708e-02 -8.5879778e-03
  1.9501297e-02  3.3417623e-02 -7.8236684e-03  2.0873190e-03
 -1.4389989e-02 -1.4110334e-02] 6 6
train:	 Loss = 1.2649,	 Acc = 0.5179
45196 0.297
88649 0.576
46208 0.613
5475 0.607
880 0.5
88 0.489
0 0.0
0 0.0
0.5886765746638358
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2838,	 Acc = 0.5095
5196 0.38
27109 0.526
13039 0.528
1257 0.516
29 0.0
0 0.0
0 0.0
0 0.0
0.5257759328088044
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 0
Testing:	 Loss = 1.3535,	 Acc1 = 0.2779,	 Acc2 = 0.2788

 ===== Epoch 29	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762  1.1020184   2.1304138
 -0.01438999 -0.01411033] 4 6
train:	 Loss = 1.2648,	 Acc = 0.5171
45193 0.297
88650 0.574
46211 0.612
5474 0.609
880 0.493
88 0.511
0 0.0
0 0.0
0.5875671429481327
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.3051,	 Acc = 0.5082
5196 0.406
27109 0.53
13039 0.504
1257 0.485
29 0.966
0 0.0
0 0.0
0 0.0
0.5210696529420283
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3737,	 Acc1 = 0.2919,	 Acc2 = 0.2957

 ===== Epoch 30	 =====
[-0.3681874  -0.38585573  0.58908963  1.4875427  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.02637785  0.02151022  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 0 0
train:	 Loss = 1.2649,	 Acc = 0.5174
45194 0.297
88650 0.575
46211 0.613
5473 0.604
880 0.49
88 0.5
0 0.0
0 0.0
0.5879746925025832
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2920,	 Acc = 0.5128
5196 0.367
27109 0.535
13039 0.525
1257 0.522
29 0.0
0 0.0
0 0.0
0 0.0
0.5311338514263648
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3620,	 Acc1 = 0.2876,	 Acc2 = 0.2905

 ===== Epoch 31	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
  1.3041764   3.3687096  -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
  0.01803937 -0.9548487   0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 5 5
train:	 Loss = 1.2644,	 Acc = 0.5172
45194 0.296
88648 0.574
46211 0.614
5475 0.606
880 0.484
88 0.477
0 0.0
0 0.0
0.5878543828112837
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2703,	 Acc = 0.5061
5196 0.397
27109 0.524
13039 0.513
1257 0.488
29 0.966
0 0.0
0 0.0
0 0.0
0.5197663754404596
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3447,	 Acc1 = 0.2699,	 Acc2 = 0.2691

 ===== Epoch 32	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
  1.4673343   3.4967136  -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-1.3513822e-02 -1.6987352e-02  2.0342596e-02  2.5701299e-02
  7.4981754e-03  5.3155841e-03 -2.0940073e+00 -4.1140895e+00
  1.9501297e-02  3.3417623e-02 -7.8236684e-03  2.0873190e-03
 -1.4389989e-02 -1.4110334e-02] 6 1
train:	 Loss = 1.2642,	 Acc = 0.5178
45192 0.298
88653 0.574
46209 0.615
5474 0.607
880 0.485
88 0.545
0 0.0
0 0.0
0.5881008322482024
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.3151,	 Acc = 0.4843
5196 0.401
27109 0.514
13039 0.457
1257 0.488
29 0.0
0 0.0
0 0.0
0 0.0
0.49471448568808224
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 1
Testing:	 Loss = 1.3791,	 Acc1 = 0.2664,	 Acc2 = 0.2649

 ===== Epoch 33	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  1.8416109   2.4215622
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -2.7793238  -3.2140505
 -0.01438999 -0.01411033] 1 6
train:	 Loss = 1.2645,	 Acc = 0.5183
45195 0.297
88646 0.576
46213 0.613
5475 0.609
879 0.491
88 0.466
0 0.0
0 0.0
0.5891182652635155
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2886,	 Acc = 0.5164
5196 0.376
27109 0.539
13039 0.527
1257 0.511
29 0.172
0 0.0
0 0.0
0 0.0
0.5339334845778829
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3496,	 Acc1 = 0.2969,	 Acc2 = 0.3017

 ===== Epoch 34	 =====
[-0.3681874  -0.38585573  2.8831873   1.1975083  -0.4351751  -0.3592861
  2.8863938   3.015987   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -4.0126204  -1.3866909   0.00749818  0.00531558
 -3.7105706  -3.6031692   0.0195013   0.03341762  2.4377053   3.0985942
 -0.01438999 -0.01411033] 5 5
train:	 Loss = 1.2635,	 Acc = 0.5177
45194 0.297
88650 0.576
46212 0.612
5473 0.605
879 0.479
88 0.455
0 0.0
0 0.0
0.5884276231051223
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2606,	 Acc = 0.5210
5196 0.411
27109 0.532
13039 0.545
1257 0.493
29 0.069
0 0.0
0 0.0
0 0.0
0.5348023362455954
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3392,	 Acc1 = 0.2583,	 Acc2 = 0.2552

 ===== Epoch 35	 =====
[-0.3681874  -0.38585573  2.5468636   2.9285805   2.3318186   0.89265394
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-1.3513822e-02 -1.6987352e-02  9.2079365e-01 -6.3677831e+00
  7.2272503e-01  7.0198975e+00 -1.3048708e-02 -8.5879778e-03
  1.9501297e-02  3.3417623e-02 -7.8236684e-03  2.0873190e-03
 -1.4389989e-02 -1.4110334e-02] 2 2
train:	 Loss = 1.2648,	 Acc = 0.5177
45198 0.296
88648 0.575
46207 0.614
5475 0.605
880 0.486
88 0.568
0 0.0
0 0.0
0.5885716712196917
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2574,	 Acc = 0.5118
5196 0.383
27109 0.517
13039 0.554
1257 0.478
29 0.966
0 0.0
0 0.0
0 0.0
0.5279963315151808
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3401,	 Acc1 = 0.2484,	 Acc2 = 0.2433

 ===== Epoch 36	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006  2.9392285   2.9956446
 -0.35938287 -0.366167    1.9404484   2.2587643  -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013  -0.3248692   0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 0 0
train:	 Loss = 1.2635,	 Acc = 0.5185
45190 0.296
88654 0.576
46209 0.614
5475 0.613
880 0.505
88 0.58
0 0.0
0 0.0
0.5895220302039546
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.3343,	 Acc = 0.5029
5196 0.386
27109 0.537
13039 0.484
1257 0.449
29 0.138
0 0.0
0 0.0
0 0.0
0.5175701115026307
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3965,	 Acc1 = 0.2994,	 Acc2 = 0.3047

 ===== Epoch 37	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 0 1
train:	 Loss = 1.2638,	 Acc = 0.5181
45196 0.299
88654 0.573
46204 0.617
5474 0.605
880 0.489
88 0.568
0 0.0
0 0.0
0.5882236376503892
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2512,	 Acc = 0.5130
5196 0.394
27109 0.519
13039 0.549
1257 0.512
29 0.0
0 0.0
0 0.0
0 0.0
0.5278997924409905
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3195,	 Acc1 = 0.2897,	 Acc2 = 0.2930

 ===== Epoch 38	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 0 1
train:	 Loss = 1.2651,	 Acc = 0.5173
45196 0.295
88652 0.575
46207 0.614
5474 0.607
879 0.488
88 0.568
0 0.0
0 0.0
0.5883793347487615
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2514,	 Acc = 0.5197
5196 0.386
27109 0.531
13039 0.551
1257 0.503
29 0.0
0 0.0
0 0.0
0 0.0
0.5364676352753777
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3203,	 Acc1 = 0.2808,	 Acc2 = 0.2823

 ===== Epoch 39	 =====
[-0.3681874  -0.38585573  2.1837473  -4.3496876  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-1.3513822e-02 -1.6987352e-02 -3.1589522e+00  3.7033691e+00
  1.4085816e+00  1.1350528e+00 -1.3048708e-02 -8.5879778e-03
  1.9501297e-02  3.3417623e-02 -7.8236684e-03  2.0873190e-03
 -1.4389989e-02 -1.4110334e-02] 1 6
train:	 Loss = 1.2644,	 Acc = 0.5172
45198 0.295
88647 0.575
46209 0.614
5474 0.607
880 0.49
88 0.58
0 0.0
0 0.0
0.5881753457232233
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2819,	 Acc = 0.5209
5196 0.372
27109 0.538
13039 0.547
1257 0.523
29 0.0
0 0.0
0 0.0
0 0.0
0.5395810204180141
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3540,	 Acc1 = 0.2866,	 Acc2 = 0.2893

 ===== Epoch 40	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006  1.8948084   1.6429222
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.03446048  0.39040485
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
train:	 Loss = 1.2644,	 Acc = 0.5187
45197 0.296
88645 0.576
46212 0.616
5475 0.604
879 0.491
88 0.58
0 0.0
0 0.0
0.5898980176788229
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2423,	 Acc = 0.5176
5196 0.389
27109 0.529
13039 0.549
1257 0.48
29 0.138
0 0.0
0 0.0
0 0.0
0.5337162716609548
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3166,	 Acc1 = 0.2690,	 Acc2 = 0.2681

 ===== Epoch 41	 =====
[ 2.3354788   2.1417794  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
  2.171281    2.3366864 ] [ 0.01297739 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01742156 -0.01411033] 0 0
train:	 Loss = 1.2642,	 Acc = 0.5177
45196 0.296
88648 0.576
46209 0.614
5475 0.606
880 0.485
88 0.523
0 0.0
0 0.0
0.5887119603680113
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2500,	 Acc = 0.5179
5196 0.409
27109 0.528
13039 0.547
1257 0.469
29 0.0
0 0.0
0 0.0
0 0.0
0.5315441424916735
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 0
Testing:	 Loss = 1.3236,	 Acc1 = 0.2686,	 Acc2 = 0.2676

 ===== Epoch 42	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
  4.423903    1.1528156  -0.41133043 -0.41984478  2.7289443   2.340318
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
  0.2296996   0.22722141  0.0195013   0.03341762 -0.26130545  0.16623276
 -0.01438999 -0.01411033] 2 3
train:	 Loss = 1.2637,	 Acc = 0.5177
45194 0.296
88650 0.575
46210 0.614
5474 0.607
880 0.495
88 0.557
0 0.0
0 0.0
0.5885974720810746
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2583,	 Acc = 0.5173
5196 0.409
27109 0.535
13039 0.533
1257 0.446
29 0.0
0 0.0
0 0.0
0 0.0
0.5309166385094367
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3231,	 Acc1 = 0.2987,	 Acc2 = 0.3039

 ===== Epoch 43	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006  3.3727605   3.0068426
 -0.35938287 -0.366167    3.037188    2.359151   -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-1.3513822e-02 -1.6987352e-02  2.0342596e-02  2.5701299e-02
 -4.8236318e+00 -3.1924150e+00 -1.3048708e-02 -8.5879778e-03
 -6.1707750e-02  4.4675571e-01 -7.8236684e-03  2.0873190e-03
  2.2738941e+00  1.4247096e+00] 4 6
train:	 Loss = 1.2639,	 Acc = 0.5191
45192 0.298
88652 0.577
46209 0.614
5475 0.61
880 0.491
88 0.523
0 0.0
0 0.0
0.5899266828964502
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2500,	 Acc = 0.5199
5196 0.385
27109 0.53
13039 0.551
1257 0.516
29 0.966
0 0.0
0 0.0
0 0.0
0.536733117729401
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3225,	 Acc1 = 0.2734,	 Acc2 = 0.2734

 ===== Epoch 44	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 5 1
train:	 Loss = 1.2638,	 Acc = 0.5181
45194 0.297
88655 0.576
46205 0.613
5474 0.606
880 0.489
88 0.5
0 0.0
0 0.0
0.5887673210570268
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2441,	 Acc = 0.5268
5196 0.393
27109 0.541
13039 0.555
1257 0.496
29 0.0
0 0.0
0 0.0
0 0.0
0.543514987691268
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3128,	 Acc1 = 0.2818,	 Acc2 = 0.2835

 ===== Epoch 45	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006  0.7696329   2.0393324
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.10309069 -1.0371913
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 5 5
train:	 Loss = 1.2640,	 Acc = 0.5175
45192 0.296
88652 0.575
46209 0.614
5475 0.605
880 0.484
88 0.534
0 0.0
0 0.0
0.5884405253920625
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2633,	 Acc = 0.5227
5196 0.372
27109 0.53
13039 0.562
1257 0.562
29 0.966
0 0.0
0 0.0
0 0.0
0.5415842062074625
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 0
Testing:	 Loss = 1.3435,	 Acc1 = 0.2624,	 Acc2 = 0.2602

 ===== Epoch 46	 =====
[-0.3681874  -0.38585573  2.2888486   1.5058128  -0.4351751  -0.3592861
  3.6301816   3.9233944  -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-1.3513822e-02 -1.6987352e-02 -3.2872291e+00 -1.6695883e+00
  1.0561168e+00  2.5945623e+00 -4.5578785e+00 -4.5675693e+00
  1.9501297e-02  3.3417623e-02 -7.8236684e-03  2.0873190e-03
 -1.4389989e-02 -1.4110334e-02] 6 6
train:	 Loss = 1.2647,	 Acc = 0.5170
45193 0.296
88652 0.575
46209 0.612
5474 0.609
880 0.486
88 0.489
0 0.0
0 0.0
0.5875742199387133
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2875,	 Acc = 0.5166
5196 0.406
27109 0.528
13039 0.538
1257 0.507
29 0.966
0 0.0
0 0.0
0 0.0
0.5304098083699378
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3566,	 Acc1 = 0.2802,	 Acc2 = 0.2816

 ===== Epoch 47	 =====
[ 2.766802    2.1596696  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
  2.1532724   2.3654222 ] [ 0.05138285  0.10685268  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.3084404   0.15994045] 1 3
train:	 Loss = 1.2659,	 Acc = 0.5178
45198 0.297
88648 0.574
46209 0.615
5473 0.613
880 0.469
88 0.523
0 0.0
0 0.0
0.5883098133023822
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2462,	 Acc = 0.5308
5196 0.392
27109 0.541
13039 0.565
1257 0.53
29 0.966
0 0.0
0 0.0
0 0.0
0.5482454023265917
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 0
Testing:	 Loss = 1.3214,	 Acc1 = 0.2748,	 Acc2 = 0.2751

 ===== Epoch 48	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 5 1
train:	 Loss = 1.2646,	 Acc = 0.5186
45199 0.298
88645 0.576
46209 0.614
5475 0.61
880 0.481
88 0.455
0 0.0
0 0.0
0.5890995562538482
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2494,	 Acc = 0.5288
5196 0.386
27109 0.533
13039 0.573
1257 0.554
29 0.966
0 0.0
0 0.0
0 0.0
0.5467490466766424
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3248,	 Acc1 = 0.2736,	 Acc2 = 0.2736

 ===== Epoch 49	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  1.1617553   1.4786378
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762  0.48744676  0.2747357
 -0.01438999 -0.01411033] 2 2
train:	 Loss = 1.2643,	 Acc = 0.5181
45192 0.298
88651 0.576
46210 0.612
5475 0.609
880 0.492
88 0.466
0 0.0
0 0.0
0.5885325256185246
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2882,	 Acc = 0.5179
5196 0.361
27109 0.535
13039 0.546
1257 0.508
29 0.207
0 0.0
0 0.0
0 0.0
0.5376502389342086
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3581,	 Acc1 = 0.2866,	 Acc2 = 0.2893

 ===== Epoch 50	 =====
[-0.3681874  -0.38585573  3.3200805   2.7869887   2.3032255   1.0180719
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-1.3513822e-02 -1.6987352e-02  1.4509855e-01 -6.4537001e+00
  1.8201512e-01  1.0318358e-01 -1.3048708e-02 -8.5879778e-03
  1.8218995e+00  1.4868548e+00 -7.8236684e-03  2.0873190e-03
 -1.4389989e-02 -1.4110334e-02] 4 2
train:	 Loss = 1.2663,	 Acc = 0.5166
45196 0.296
88647 0.573
46211 0.614
5474 0.604
880 0.485
88 0.545
0 0.0
0 0.0
0.5869992922859165
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2636,	 Acc = 0.5389
5196 0.394
27109 0.548
13039 0.58
1257 0.529
29 0.0
0 0.0
0 0.0
0 0.0
0.5571511319206449
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 1
Testing:	 Loss = 1.3401,	 Acc1 = 0.2653,	 Acc2 = 0.2637

 ===== Epoch 51	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  1.6724126   1.8750107
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.19770153  0.02712645
 -0.01438999 -0.01411033] 1 1
train:	 Loss = 1.2654,	 Acc = 0.5161
45195 0.296
88651 0.572
46209 0.613
5473 0.608
880 0.48
88 0.545
0 0.0
0 0.0
0.5864431249601914
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2417,	 Acc = 0.5220
5196 0.382
27109 0.537
13039 0.547
1257 0.529
29 0.0
0 0.0
0 0.0
0 0.0
0.5395086161123714
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3093,	 Acc1 = 0.2814,	 Acc2 = 0.2831

 ===== Epoch 52	 =====
[ 7.903553    1.4312813  -0.42115992 -0.34173006 -0.4351751  -0.3592861
  3.3619294   1.0162778  -0.41133043 -0.41984478  1.6260934   3.0296621
 -0.39521116 -0.38799495] [-7.4872246e+00 -1.7102617e+00  2.0342596e-02  2.5701299e-02
  7.4981754e-03  5.3155841e-03 -4.2522912e+00 -1.4778618e+00
  1.9501297e-02  3.3417623e-02  6.7471974e-02 -1.5058252e+00
 -1.4389989e-02 -1.4110334e-02] 5 5
train:	 Loss = 1.2647,	 Acc = 0.5168
45195 0.297
88649 0.573
46211 0.613
5473 0.609
880 0.469
88 0.557
0 0.0
0 0.0
0.5872074507611411
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2565,	 Acc = 0.5100
5196 0.402
27109 0.527
13039 0.522
1257 0.477
29 0.0
0 0.0
0 0.0
0 0.0
0.5235313993338804
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 0
Testing:	 Loss = 1.3262,	 Acc1 = 0.2789,	 Acc2 = 0.2801

 ===== Epoch 53	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
  3.3027065   1.3263322  -0.41133043 -0.41984478  2.7457871   2.382171
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -4.1848254  -1.8073903   1.3080193   3.7777746   0.19514827 -1.4251435
  3.4020982   3.9397438 ] 5 5
train:	 Loss = 1.2641,	 Acc = 0.5183
45193 0.297
88652 0.576
46209 0.614
5474 0.612
880 0.474
88 0.523
0 0.0
0 0.0
0.5891028499041068
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2684,	 Acc = 0.5188
5196 0.385
27109 0.542
13039 0.527
1257 0.49
29 0.0
0 0.0
0 0.0
0 0.0
0.5355746488391177
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3293,	 Acc1 = 0.2971,	 Acc2 = 0.3019

 ===== Epoch 54	 =====
[-0.3681874  -0.38585573  3.0525866   2.4056046  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.12755288  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 3
train:	 Loss = 1.2649,	 Acc = 0.5173
45194 0.296
88652 0.575
46207 0.614
5475 0.61
880 0.473
88 0.511
0 0.0
0 0.0
0.5882153118851821
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2388,	 Acc = 0.5292
5196 0.414
27109 0.546
13039 0.543
1257 0.498
29 0.0
0 0.0
0 0.0
0 0.0
0.5436356615340059
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 0
Testing:	 Loss = 1.3153,	 Acc1 = 0.2769,	 Acc2 = 0.2776

 ===== Epoch 55	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006  2.268837    1.893758
 -0.35938287 -0.366167    0.9603706   2.5256457  -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-1.3513822e-02 -1.6987352e-02  2.0342596e-02  2.5701299e-02
  1.3887575e-01  5.6273764e-01 -1.3048708e-02 -8.5879778e-03
 -1.6013675e+00 -3.2165544e+00 -7.8236684e-03  2.0873190e-03
 -1.4389989e-02 -1.4110334e-02] 4 4
train:	 Loss = 1.2647,	 Acc = 0.5174
45194 0.297
88650 0.573
46210 0.616
5474 0.605
880 0.481
88 0.511
0 0.0
0 0.0
0.5878189976079603
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2313,	 Acc = 0.5250
5196 0.389
27109 0.536
13039 0.56
1257 0.506
29 0.0
0 0.0
0 0.0
0 0.0
0.5420669015784139
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3006,	 Acc1 = 0.2833,	 Acc2 = 0.2853

 ===== Epoch 56	 =====
[-0.3681874  -0.38585573  2.269477    2.108719   -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 0 0
train:	 Loss = 1.2642,	 Acc = 0.5172
45195 0.296
88651 0.573
46208 0.616
5475 0.615
879 0.482
88 0.455
0 0.0
0 0.0
0.587908082745345
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2546,	 Acc = 0.5302
5196 0.419
27109 0.548
13039 0.543
1257 0.496
29 0.0
0 0.0
0 0.0
0 0.0
0.5441183569049572
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3231,	 Acc1 = 0.2746,	 Acc2 = 0.2749

 ===== Epoch 57	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.7316234   1.6263545
 -0.39521116 -0.38799495] [ 3.9628108   2.1121085   0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  1.3209853   2.6620317  -3.866673   -2.315424
  3.4809158   2.7271898 ] 5 5
train:	 Loss = 1.2653,	 Acc = 0.5175
45198 0.296
88651 0.574
46205 0.616
5474 0.615
880 0.469
88 0.5
0 0.0
0 0.0
0.5882885815793572
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2475,	 Acc = 0.5157
5196 0.392
27109 0.524
13039 0.547
1257 0.511
29 0.69
0 0.0
0 0.0
0 0.0
0.5312545252691027
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3130,	 Acc1 = 0.2824,	 Acc2 = 0.2843

 ===== Epoch 58	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
  2.1217046   3.6275623  -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-1.3513822e-02 -1.6987352e-02  1.1849389e+00  1.9053982e+00
  7.4981754e-03  5.3155841e-03 -2.8394523e+00 -4.2531567e+00
  1.9501297e-02  3.3417623e-02 -7.8236684e-03  2.0873190e-03
 -1.4389989e-02 -1.4110334e-02] 6 6
train:	 Loss = 1.2645,	 Acc = 0.5179
45195 0.297
88648 0.575
46211 0.616
5474 0.603
880 0.475
88 0.511
0 0.0
0 0.0
0.5885874834573004
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2535,	 Acc = 0.5342
5196 0.415
27109 0.553
13039 0.548
1257 0.499
29 0.0
0 0.0
0 0.0
0 0.0
0.5492107930684945
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3223,	 Acc1 = 0.2635,	 Acc2 = 0.2614

 ===== Epoch 59	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006  1.2506914   1.2644287
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.17760375  0.4265735
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 6 2
train:	 Loss = 1.2643,	 Acc = 0.5174
45198 0.297
88648 0.574
46209 0.614
5473 0.614
880 0.484
88 0.5
0 0.0
0 0.0
0.5878851788418803
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2631,	 Acc = 0.5261
5196 0.412
27109 0.537
13039 0.547
1257 0.533
29 0.966
0 0.0
0 0.0
0 0.0
0.5403291982429889
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3352,	 Acc1 = 0.2771,	 Acc2 = 0.2778

 ===== Epoch 60	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167    2.5116313   2.7043831  -0.42690593 -0.4244443
  3.4512131   1.9160979 ] [-1.3513822e-02 -1.6987352e-02  2.0342596e-02  2.5701299e-02
  7.4981754e-03  5.3155841e-03 -1.3048708e-02 -8.5879778e-03
 -3.4344127e+00 -3.4137681e+00 -7.8236684e-03  2.0873190e-03
 -4.4388342e+00 -2.5726571e+00] 6 1
train:	 Loss = 1.2644,	 Acc = 0.5173
45191 0.297
88651 0.574
46212 0.614
5474 0.605
880 0.483
88 0.477
0 0.0
0 0.0
0.587813594706486
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2293,	 Acc = 0.5326
5196 0.385
27109 0.537
13039 0.585
1257 0.527
29 0.034
0 0.0
0 0.0
0 0.0
0.5510450354781098
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 0
Testing:	 Loss = 1.3084,	 Acc1 = 0.2742,	 Acc2 = 0.2744

 ===== Epoch 61	 =====
[-0.3681874  -0.38585573  2.1157405   1.103875   -0.4351751  -0.3592861
  4.9720182   2.6689534  -0.41133043 -0.41984478  4.819805    2.7046857
 -0.39521116 -0.38799495] [-1.3513822e-02 -1.6987352e-02  1.8433468e-01  2.6503894e-03
  7.4981754e-03  5.3155841e-03  6.6360766e-01  1.2574403e-02
  1.9501297e-02  3.3417623e-02  1.5633155e-01 -8.6892748e+00
 -1.4389989e-02 -1.4110334e-02] 3 2
train:	 Loss = 1.2647,	 Acc = 0.5172
45195 0.296
88651 0.574
46208 0.614
5475 0.603
879 0.469
88 0.489
0 0.0
0 0.0
0.5878585431100983
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2582,	 Acc = 0.5159
5196 0.381
27109 0.532
13039 0.541
1257 0.466
29 0.0
0 0.0
0 0.0
0 0.0
0.5327991504561471
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 0
Testing:	 Loss = 1.3354,	 Acc1 = 0.2694,	 Acc2 = 0.2686

 ===== Epoch 62	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
  2.5565963   1.4059793  -0.41133043 -0.41984478  1.7581602   3.3004758
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
  0.21382634  0.21210542  0.0195013   0.03341762  0.10441789 -0.01738757
 -0.01438999 -0.01411033] 2 2
train:	 Loss = 1.2642,	 Acc = 0.5185
45195 0.297
88646 0.575
46212 0.617
5475 0.613
880 0.489
88 0.477
0 0.0
0 0.0
0.5892951925322538
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2898,	 Acc = 0.5115
5196 0.382
27109 0.52
13039 0.549
1257 0.498
29 0.0
0 0.0
0 0.0
0 0.0
0.5277549838297051
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3471,	 Acc1 = 0.3134,	 Acc2 = 0.3216

 ===== Epoch 63	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
  2.0983677   2.947976  ] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762  3.2603493   1.5545138
  0.98750454  0.4848353 ] 2 2
train:	 Loss = 1.2637,	 Acc = 0.5169
45199 0.296
88646 0.574
46210 0.615
5473 0.602
880 0.477
88 0.523
0 0.0
0 0.0
0.5876911753257323
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.3000,	 Acc = 0.5016
5196 0.378
27109 0.522
13039 0.509
1257 0.494
29 0.69
0 0.0
0 0.0
0 0.0
0.517159820437322
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3632,	 Acc1 = 0.2890,	 Acc2 = 0.2922

 ===== Epoch 64	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
  2.1314068   1.693472    0.0195013   0.03341762  2.4826014   4.1947517
 -0.01438999 -0.01411033] 1 6
train:	 Loss = 1.2638,	 Acc = 0.5172
45194 0.296
88650 0.575
46209 0.614
5475 0.604
880 0.472
88 0.534
0 0.0
0 0.0
0.5878473057706189
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2460,	 Acc = 0.5216
5196 0.384
27109 0.529
13039 0.563
1257 0.51
29 0.483
0 0.0
0 0.0
0 0.0
0.5389293816672298
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3169,	 Acc1 = 0.2748,	 Acc2 = 0.2751

 ===== Epoch 65	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
  1.3465617   3.2890625  -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-1.3513822e-02 -1.6987352e-02  2.9601884e+00  1.6371695e+00
  7.4981754e-03  5.3155841e-03 -1.9564255e+00 -3.8933961e+00
  1.9501297e-02  3.3417623e-02 -7.8236684e-03  2.0873190e-03
 -1.4389989e-02 -1.4110334e-02] 4 6
train:	 Loss = 1.2639,	 Acc = 0.5181
45192 0.298
88652 0.574
46209 0.617
5475 0.6
880 0.483
88 0.466
0 0.0
0 0.0
0.5885396025590217
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2755,	 Acc = 0.5272
5196 0.389
27109 0.542
13039 0.553
1257 0.512
29 0.172
0 0.0
0 0.0
0 0.0
0.5445045132017184
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3536,	 Acc1 = 0.2657,	 Acc2 = 0.2642

 ===== Epoch 66	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167    1.7036576   2.045749   -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.6357504  -0.12327264 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 2 2
train:	 Loss = 1.2653,	 Acc = 0.5165
45196 0.297
88649 0.573
46210 0.614
5473 0.605
880 0.472
88 0.534
0 0.0
0 0.0
0.5868648266100496
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.3391,	 Acc = 0.4927
5196 0.4
27109 0.511
13039 0.492
1257 0.489
29 0.0
0 0.0
0 0.0
0 0.0
0.5042718540329199
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 0
Testing:	 Loss = 1.4024,	 Acc1 = 0.2721,	 Acc2 = 0.2719

 ===== Epoch 67	 =====
[ 0.21243043  1.5514015  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01317826 -0.02175043  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 0 0
train:	 Loss = 1.2649,	 Acc = 0.5172
45196 0.295
88649 0.573
46208 0.618
5475 0.604
880 0.477
88 0.489
0 0.0
0 0.0
0.5880537862703468
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2426,	 Acc = 0.5361
5196 0.39
27109 0.54
13039 0.586
1257 0.549
29 0.138
0 0.0
0 0.0
0 0.0
0.554399768306222
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3217,	 Acc1 = 0.2653,	 Acc2 = 0.2637

 ===== Epoch 68	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.0023873   1.7543755
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.0022113   0.19961828
 -0.01438999 -0.01411033] 2 4
train:	 Loss = 1.2643,	 Acc = 0.5167
45194 0.296
88651 0.574
46209 0.611
5474 0.607
880 0.473
88 0.534
0 0.0
0 0.0
0.5871466787448161
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2905,	 Acc = 0.5141
5196 0.363
27109 0.525
13039 0.549
1257 0.56
29 0.0
0 0.0
0 0.0
0 0.0
0.5330646329101704
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3597,	 Acc1 = 0.2876,	 Acc2 = 0.2905

 ===== Epoch 69	 =====
[ 2.8158808   1.2293772  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167    3.2941926   2.386084   -0.42690593 -0.4244443
  3.5394998   1.3361561 ] [-2.8903942  -1.5221201   0.0203426   0.0257013   0.00749818  0.00531558
  1.4534295   2.3102043  -4.359125   -3.0625658   3.2089055   2.3251626
 -4.540388   -1.9286692 ] 6 5
train:	 Loss = 1.2650,	 Acc = 0.5171
45196 0.296
88648 0.573
46209 0.616
5475 0.61
880 0.474
88 0.477
0 0.0
0 0.0
0.5879830148619958
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2699,	 Acc = 0.5096
5196 0.407
27109 0.526
13039 0.515
1257 0.506
29 0.966
0 0.0
0 0.0
0 0.0
0.5224936042863348
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3300,	 Acc1 = 0.2934,	 Acc2 = 0.2975

 ===== Epoch 70	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 0 1
train:	 Loss = 1.2628,	 Acc = 0.5182
45195 0.298
88651 0.574
46208 0.616
5474 0.615
880 0.478
88 0.534
0 0.0
0 0.0
0.5887431794537902
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2777,	 Acc = 0.5194
5196 0.389
27109 0.54
13039 0.532
1257 0.497
29 0.0
0 0.0
0 0.0
0 0.0
0.5357194574504031
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3528,	 Acc1 = 0.2717,	 Acc2 = 0.2714

 ===== Epoch 71	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [ 2.8770502e+00  1.1761637e+00  2.0342596e-02  2.5701299e-02
  7.4981754e-03  5.3155841e-03 -1.3048708e-02 -8.5879778e-03
  1.9501297e-02  3.3417623e-02 -7.8236684e-03  2.0873190e-03
  3.2682087e+00  1.6016612e+00] 6 2
train:	 Loss = 1.2648,	 Acc = 0.5170
45194 0.297
88648 0.574
46211 0.614
5475 0.602
880 0.477
88 0.545
0 0.0
0 0.0
0.5873589899647563
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2531,	 Acc = 0.5242
5196 0.4
27109 0.539
13039 0.542
1257 0.534
29 0.207
0 0.0
0 0.0
0 0.0
0.5397740985663948
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 1
Testing:	 Loss = 1.3214,	 Acc1 = 0.2874,	 Acc2 = 0.2903

 ===== Epoch 72	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167    2.4504118   2.5893056  -0.42690593 -0.4244443
  3.4134383   1.9317721 ] [-1.3513822e-02 -1.6987352e-02  2.0342596e-02  2.5701299e-02
  7.4981754e-03  5.3155841e-03 -1.3048708e-02 -8.5879778e-03
  1.6281436e-01 -1.5659037e+00 -7.8236684e-03  2.0873190e-03
 -4.3953834e+00 -2.5900621e+00] 5 5
train:	 Loss = 1.2643,	 Acc = 0.5171
45190 0.296
88653 0.574
46211 0.615
5475 0.605
880 0.483
87 0.552
0 0.0
0 0.0
0.5878589727258574
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2498,	 Acc = 0.5196
5196 0.398
27109 0.538
13039 0.534
1257 0.48
29 0.0
0 0.0
0 0.0
0 0.0
0.5348264710141429
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 1
Testing:	 Loss = 1.3192,	 Acc1 = 0.2942,	 Acc2 = 0.2985

 ===== Epoch 73	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 6 1
train:	 Loss = 1.2641,	 Acc = 0.5179
45191 0.298
88651 0.574
46211 0.616
5475 0.606
880 0.472
88 0.523
0 0.0
0 0.0
0.5881674392272036
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2832,	 Acc = 0.5088
5196 0.389
27109 0.527
13039 0.523
1257 0.465
29 0.207
0 0.0
0 0.0
0 0.0
0.5237486122508085
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3553,	 Acc1 = 0.2694,	 Acc2 = 0.2686

 ===== Epoch 74	 =====
[-0.3681874  -0.38585573  2.150774    1.485259   -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [ 3.5881677e+00  2.9527919e+00 -3.1187084e+00 -1.6507285e+00
  7.4981754e-03  5.3155841e-03  2.3291793e+00  3.4076250e+00
  1.9501297e-02  3.3417623e-02 -7.8236684e-03  2.0873190e-03
 -1.4389989e-02 -1.4110334e-02] 5 5
train:	 Loss = 1.2625,	 Acc = 0.5192
45193 0.297
88650 0.577
46211 0.617
5474 0.611
880 0.47
88 0.489
0 0.0
0 0.0
0.590263476359313
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2971,	 Acc = 0.5025
5196 0.39
27109 0.517
13039 0.515
1257 0.517
29 0.966
0 0.0
0 0.0
0 0.0
0.5166047207607279
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3690,	 Acc1 = 0.2785,	 Acc2 = 0.2796

 ===== Epoch 75	 =====
[ 0.55035603  2.2593439  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
  0.5570914   2.4986522 ] [ 0.12670134  0.05684036  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
  0.05179708  0.00619559] 2 0
train:	 Loss = 1.2640,	 Acc = 0.5177
45195 0.296
88650 0.575
46208 0.614
5475 0.613
880 0.481
88 0.534
0 0.0
0 0.0
0.5885874834573004
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.3226,	 Acc = 0.5101
5196 0.381
27109 0.53
13039 0.519
1257 0.514
29 0.966
0 0.0
0 0.0
0 0.0
0.5262827629483033
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3890,	 Acc1 = 0.2775,	 Acc2 = 0.2783

 ===== Epoch 76	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.2378101   0.90500504
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762  0.27418548  0.15788639
  3.1838338   4.2588367 ] 2 2
train:	 Loss = 1.2655,	 Acc = 0.5169
45196 0.297
88650 0.574
46208 0.611
5474 0.611
880 0.49
88 0.455
0 0.0
0 0.0
0.5871479122434536
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2636,	 Acc = 0.5091
5196 0.39
27109 0.525
13039 0.525
1257 0.506
29 0.0
0 0.0
0 0.0
0 0.0
0.524110633779022
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 1
Testing:	 Loss = 1.3381,	 Acc1 = 0.2725,	 Acc2 = 0.2724

 ===== Epoch 77	 =====
[-0.3681874  -0.38585573  1.261327    1.5560548  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.04354458  1.0567056   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 6 6
train:	 Loss = 1.2642,	 Acc = 0.5167
45196 0.295
88649 0.574
46209 0.614
5474 0.604
880 0.49
88 0.455
0 0.0
0 0.0
0.5877140835102619
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2453,	 Acc = 0.5256
5196 0.405
27109 0.534
13039 0.56
1257 0.486
29 0.966
0 0.0
0 0.0
0 0.0
0.5407636240768451
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3255,	 Acc1 = 0.2725,	 Acc2 = 0.2724

 ===== Epoch 78	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  1.5384312   1.8110001
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762  0.2882164   1.0314741
 -0.01438999 -0.01411033] 2 4
train:	 Loss = 1.2637,	 Acc = 0.5175
45194 0.297
88650 0.573
46209 0.616
5475 0.607
880 0.473
88 0.466
0 0.0
0 0.0
0.5879463843399244
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2771,	 Acc = 0.5087
5196 0.396
27109 0.533
13039 0.506
1257 0.484
29 0.103
0 0.0
0 0.0
0 0.0
0.5228556258145485
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3473,	 Acc1 = 0.2765,	 Acc2 = 0.2771

 ===== Epoch 79	 =====
[ 0.99644667  1.9347637  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
  1.1992536   2.1485972 ] [ 0.414216    0.7903544   0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
  0.01238815  0.963475  ] 6 4
train:	 Loss = 1.2660,	 Acc = 0.5164
45197 0.297
88646 0.572
46211 0.615
5475 0.613
879 0.454
88 0.5
0 0.0
0 0.0
0.5866637414277525
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2935,	 Acc = 0.5065
5196 0.384
27109 0.518
13039 0.529
1257 0.523
29 0.966
0 0.0
0 0.0
0 0.0
0.5219385046097408
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 1
Testing:	 Loss = 1.3594,	 Acc1 = 0.3004,	 Acc2 = 0.3059

 ===== Epoch 80	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006  2.677254    2.9956446
 -0.35938287 -0.366167    1.9404484   2.2587643  -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.3398655   0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 0 0
train:	 Loss = 1.2646,	 Acc = 0.5164
45197 0.296
88650 0.573
46208 0.613
5473 0.608
880 0.482
88 0.534
0 0.0
0 0.0
0.5868548255826297
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2990,	 Acc = 0.5044
5196 0.413
27109 0.528
13039 0.496
1257 0.46
29 0.138
0 0.0
0 0.0
0 0.0
0.5158806777043008
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3587,	 Acc1 = 0.2981,	 Acc2 = 0.3032

 ===== Epoch 81	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
  3.8078542   1.2068617  -0.41133043 -0.41984478  3.0455208   2.2295306
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.04678266  0.4388452   0.0195013   0.03341762  0.02117212  0.32203183
 -0.01438999 -0.01411033] 4 4
train:	 Loss = 1.2641,	 Acc = 0.5178
45197 0.297
88648 0.574
46210 0.617
5473 0.606
880 0.473
88 0.511
0 0.0
0 0.0
0.588539197021918
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.3529,	 Acc = 0.4730
5196 0.38
27109 0.499
13039 0.46
1257 0.433
29 0.483
0 0.0
0 0.0
0 0.0
0.48467442197229327
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 1
Testing:	 Loss = 1.4138,	 Acc1 = 0.2868,	 Acc2 = 0.2895

 ===== Epoch 82	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167    1.1526902   1.7054137  -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798 -0.06921812  0.21712345 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 1
train:	 Loss = 1.2648,	 Acc = 0.5174
45191 0.296
88651 0.573
46211 0.616
5475 0.608
880 0.485
88 0.511
0 0.0
0 0.0
0.58809667032306
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2747,	 Acc = 0.5187
5196 0.384
27109 0.533
13039 0.54
1257 0.542
29 0.31
0 0.0
0 0.0
0 0.0
0.5355746488391177
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3440,	 Acc1 = 0.2888,	 Acc2 = 0.2920

 ===== Epoch 83	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
  1.664748    3.820991   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
  0.03920437  0.07908474  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 0
train:	 Loss = 1.2637,	 Acc = 0.5178
45192 0.296
88650 0.575
46211 0.615
5475 0.6
880 0.478
88 0.534
0 0.0
0 0.0
0.5885891411425013
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2590,	 Acc = 0.5160
5196 0.37
27109 0.523
13039 0.562
1257 0.516
29 0.0
0 0.0
0 0.0
0 0.0
0.5344161799488343
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3331,	 Acc1 = 0.2820,	 Acc2 = 0.2838

 ===== Epoch 84	 =====
[-0.3681874  -0.38585573  1.5869356   2.777854   -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.03147098 -0.49399194  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 5 5
train:	 Loss = 1.2632,	 Acc = 0.5175
45198 0.295
88650 0.573
46206 0.618
5474 0.612
880 0.485
88 0.489
0 0.0
0 0.0
0.588479667086583
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2557,	 Acc = 0.5227
5196 0.39
27109 0.536
13039 0.551
1257 0.515
29 0.0
0 0.0
0 0.0
0 0.0
0.539363807501086
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 0
Testing:	 Loss = 1.3276,	 Acc1 = 0.2717,	 Acc2 = 0.2714

 ===== Epoch 85	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-1.3513822e-02 -1.6987352e-02  2.0342596e-02  2.5701299e-02
  2.3782303e+00  1.3392988e+00 -1.3048708e-02 -8.5879778e-03
  1.9501297e-02  3.3417623e-02 -7.8236684e-03  2.0873190e-03
 -1.4389989e-02 -1.4110334e-02] 2 6
train:	 Loss = 1.2653,	 Acc = 0.5172
45194 0.297
88652 0.574
46207 0.614
5475 0.609
880 0.47
88 0.523
0 0.0
0 0.0
0.5876845338353314
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2753,	 Acc = 0.5153
5196 0.401
27109 0.527
13039 0.535
1257 0.523
29 0.483
0 0.0
0 0.0
0 0.0
0.5295650914707728
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3477,	 Acc1 = 0.2800,	 Acc2 = 0.2813

 ===== Epoch 86	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 2 1
train:	 Loss = 1.2640,	 Acc = 0.5171
45194 0.297
88651 0.573
46211 0.615
5473 0.607
879 0.474
88 0.534
0 0.0
0 0.0
0.5876562256726727
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2846,	 Acc = 0.5063
5196 0.377
27109 0.519
13039 0.532
1257 0.512
29 0.0
0 0.0
0 0.0
0 0.0
0.5225660085919775
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3509,	 Acc1 = 0.2917,	 Acc2 = 0.2955

 ===== Epoch 87	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 6 1
train:	 Loss = 1.2644,	 Acc = 0.5161
45196 0.296
88648 0.572
46209 0.614
5475 0.607
880 0.477
88 0.443
0 0.0
0 0.0
0.586411889596603
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2860,	 Acc = 0.5156
5196 0.41
27109 0.535
13039 0.523
1257 0.467
29 0.138
0 0.0
0 0.0
0 0.0
0.5289134527199885
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3568,	 Acc1 = 0.2709,	 Acc2 = 0.2704

 ===== Epoch 88	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  1.8657267   1.8085382
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762  0.7287684  -0.09528711
 -0.01438999 -0.01411033] 2 2
train:	 Loss = 1.2653,	 Acc = 0.5166
45195 0.296
88648 0.573
46210 0.615
5475 0.609
880 0.475
88 0.409
0 0.0
0 0.0
0.5872286820333897
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2731,	 Acc = 0.5185
5196 0.391
27109 0.535
13039 0.537
1257 0.498
29 0.138
0 0.0
0 0.0
0 0.0
0.534488584254477
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3403,	 Acc1 = 0.2855,	 Acc2 = 0.2880

 ===== Epoch 89	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167    1.4287515   1.9184291  -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  1.1980832  -0.10166018 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 2 2
train:	 Loss = 1.2640,	 Acc = 0.5177
45195 0.296
88649 0.575
46209 0.615
5475 0.605
880 0.47
88 0.489
0 0.0
0 0.0
0.5885520980035527
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2922,	 Acc = 0.5094
5196 0.358
27109 0.525
13039 0.535
1257 0.511
29 0.966
0 0.0
0 0.0
0 0.0
0.5282859487377516
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3539,	 Acc1 = 0.2831,	 Acc2 = 0.2850

 ===== Epoch 90	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [ 6.9605643e-01  2.9385028e+00  2.0342596e-02  2.5701299e-02
  7.4981754e-03  5.3155841e-03 -1.3048708e-02 -8.5879778e-03
  1.9501297e-02  3.3417623e-02 -7.8236684e-03  2.0873190e-03
 -1.4389989e-02 -1.4110334e-02] 1 1
train:	 Loss = 1.2652,	 Acc = 0.5165
45194 0.294
88649 0.574
46212 0.614
5473 0.609
880 0.48
88 0.432
0 0.0
0 0.0
0.5874792996560558
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2719,	 Acc = 0.5196
5196 0.406
27109 0.538
13039 0.532
1257 0.47
29 0.0
0 0.0
0 0.0
0 0.0
0.5339334845778829
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 0
Testing:	 Loss = 1.3426,	 Acc1 = 0.2798,	 Acc2 = 0.2811

 ===== Epoch 91	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762  1.9938787   2.878806
 -0.01438999 -0.01411033] 6 5
train:	 Loss = 1.2639,	 Acc = 0.5165
45192 0.296
88649 0.573
46212 0.613
5475 0.609
880 0.48
88 0.386
0 0.0
0 0.0
0.5868269829587273
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.3132,	 Acc = 0.5105
5196 0.387
27109 0.534
13039 0.511
1257 0.496
29 0.966
0 0.0
0 0.0
0 0.0
0.5260655500313752
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 0
Testing:	 Loss = 1.3802,	 Acc1 = 0.2709,	 Acc2 = 0.2704

 ===== Epoch 92	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006  3.5891411   2.3819923
 -0.35938287 -0.366167    4.2892866   1.66379    -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.02416473  0.04999445
 -0.01304871 -0.00858798 -0.42886573  0.07123941 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 0 3
train:	 Loss = 1.2642,	 Acc = 0.5171
45193 0.297
88648 0.573
46212 0.616
5475 0.609
880 0.474
88 0.455
0 0.0
0 0.0
0.5875246810046496
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2659,	 Acc = 0.5101
5196 0.384
27109 0.518
13039 0.543
1257 0.502
29 0.793
0 0.0
0 0.0
0 0.0
0.5258966066515423
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 0
Testing:	 Loss = 1.3455,	 Acc1 = 0.2600,	 Acc2 = 0.2572

 ===== Epoch 93	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [ 3.6273568e+00  2.9242134e+00  2.0342596e-02  2.5701299e-02
  7.4981754e-03  5.3155841e-03 -1.3048708e-02 -8.5879778e-03
  1.9501297e-02  3.3417623e-02 -7.8236684e-03  2.0873190e-03
 -1.4389989e-02 -1.4110334e-02] 1 5
train:	 Loss = 1.2639,	 Acc = 0.5171
45194 0.297
88651 0.574
46209 0.614
5475 0.606
879 0.506
88 0.443
0 0.0
0 0.0
0.587627917510014
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2857,	 Acc = 0.5087
5196 0.394
27109 0.53
13039 0.513
1257 0.491
29 0.0
0 0.0
0 0.0
0 0.0
0.5230969735000242
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3526,	 Acc1 = 0.2771,	 Acc2 = 0.2778

 ===== Epoch 94	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
  1.7367455   3.5109363  -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-1.3513822e-02 -1.6987352e-02  2.0342596e-02  2.5701299e-02
  7.4981754e-03  5.3155841e-03 -2.4009149e+00 -4.1292057e+00
  3.0639482e+00  3.7777746e+00 -7.8236684e-03  2.0873190e-03
  3.9836314e+00  2.7358923e+00] 5 5
train:	 Loss = 1.2645,	 Acc = 0.5179
45195 0.297
88649 0.574
46211 0.617
5473 0.614
880 0.459
88 0.489
0 0.0
0 0.0
0.5886370230925471
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2596,	 Acc = 0.5138
5196 0.41
27109 0.525
13039 0.539
1257 0.451
29 0.0
0 0.0
0 0.0
0 0.0
0.526861997393445
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3326,	 Acc1 = 0.2705,	 Acc2 = 0.2699

 ===== Epoch 95	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006  0.98755807  0.7963509
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.24966484  0.0287188
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 2 3
train:	 Loss = 1.2653,	 Acc = 0.5165
45192 0.296
88649 0.573
46212 0.613
5475 0.606
880 0.46
88 0.477
0 0.0
0 0.0
0.5870180603521485
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2717,	 Acc = 0.5078
5196 0.39
27109 0.524
13039 0.524
1257 0.485
29 0.0
0 0.0
0 0.0
0 0.0
0.5225660085919775
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3389,	 Acc1 = 0.2750,	 Acc2 = 0.2753

 ===== Epoch 96	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167    1.3294146   1.4360837  -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-1.3513822e-02 -1.6987352e-02  2.7956924e+00  2.6094987e+00
  7.4981754e-03  5.3155841e-03 -1.3048708e-02 -8.5879778e-03
 -2.0374479e+00 -2.0143621e+00 -7.8236684e-03  2.0873190e-03
 -1.4389989e-02 -1.4110334e-02] 5 5
train:	 Loss = 1.2653,	 Acc = 0.5167
45195 0.297
88655 0.573
46204 0.615
5474 0.607
880 0.472
88 0.511
0 0.0
0 0.0
0.5871083714906477
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2835,	 Acc = 0.5109
5196 0.403
27109 0.526
13039 0.526
1257 0.481
29 0.138
0 0.0
0 0.0
0 0.0
0.5244002510015929
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 0
Testing:	 Loss = 1.3685,	 Acc1 = 0.2389,	 Acc2 = 0.2319

 ===== Epoch 97	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167    2.2765744   1.120233   -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
  2.6863568   2.0411396  -3.156658   -1.6658612   3.8477523   2.7174425
 -0.01438999 -0.01411033] 6 6
train:	 Loss = 1.2642,	 Acc = 0.5176
45195 0.297
88653 0.574
46206 0.614
5474 0.609
880 0.48
88 0.5
0 0.0
0 0.0
0.5882477831013228
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2683,	 Acc = 0.5335
5196 0.381
27109 0.556
13039 0.548
1257 0.547
29 0.0
0 0.0
0 0.0
0 0.0
0.5526137954337018
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 1
Testing:	 Loss = 1.3391,	 Acc1 = 0.2777,	 Acc2 = 0.2786

 ===== Epoch 98	 =====
[-0.3681874  -0.38585573  1.6269151   1.11301    -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.12749198  0.12419154  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 2 3
train:	 Loss = 1.2645,	 Acc = 0.5169
45196 0.295
88649 0.573
46208 0.615
5475 0.608
880 0.476
88 0.466
0 0.0
0 0.0
0.587791932059448
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2912,	 Acc = 0.4989
5196 0.398
27109 0.518
13039 0.503
1257 0.465
29 0.069
0 0.0
0 0.0
0 0.0
0.5116088236713809
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 0
Testing:	 Loss = 1.3657,	 Acc1 = 0.2806,	 Acc2 = 0.2821

 ===== Epoch 99	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  1.4159354   1.3136876
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.20939325  0.16623276
 -0.01438999 -0.01411033] 1 1
train:	 Loss = 1.2641,	 Acc = 0.5176
45194 0.295
88649 0.575
46210 0.616
5475 0.608
880 0.485
88 0.455
0 0.0
0 0.0
0.5887885521790208
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2560,	 Acc = 0.5365
5196 0.412
27109 0.551
13039 0.558
1257 0.523
29 0.276
0 0.0
0 0.0
0 0.0
0.5520345609885601
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 1
Testing:	 Loss = 1.3329,	 Acc1 = 0.2692,	 Acc2 = 0.2684

 ===== Epoch 100	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 3 1
train:	 Loss = 1.2650,	 Acc = 0.5171
45195 0.295
88655 0.574
46204 0.614
5474 0.606
880 0.481
88 0.523
0 0.0
0 0.0
0.5880071620158386
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.3303,	 Acc = 0.4884
5196 0.37
27109 0.506
13039 0.5
1257 0.493
29 0.103
0 0.0
0 0.0
0 0.0
0.5032823285224695
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3989,	 Acc1 = 0.2703,	 Acc2 = 0.2696

 ===== Epoch 101	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-1.3513822e-02 -1.6987352e-02  2.0342596e-02  2.5701299e-02
  7.4981754e-03  5.3155841e-03 -1.3048708e-02 -8.5879778e-03
  3.0257318e+00  1.4247190e+00 -7.8236684e-03  2.0873190e-03
 -1.4389989e-02 -1.4110334e-02] 2 2
train:	 Loss = 1.2650,	 Acc = 0.5176
45192 0.297
88653 0.576
46209 0.612
5474 0.606
880 0.462
88 0.5
0 0.0
0 0.0
0.5881362169506879
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2815,	 Acc = 0.5116
5196 0.397
27109 0.526
13039 0.531
1257 0.48
29 0.0
0 0.0
0 0.0
0 0.0
0.525969010957185
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3517,	 Acc1 = 0.2715,	 Acc2 = 0.2711

 ===== Epoch 102	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006  1.6262654   1.5779736
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013  -0.2895736   0.00744315
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 1 1
train:	 Loss = 1.2647,	 Acc = 0.5168
45194 0.296
88645 0.574
46214 0.612
5475 0.612
880 0.482
88 0.443
0 0.0
0 0.0
0.5873094506801037
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2290,	 Acc = 0.5216
5196 0.395
27109 0.532
13039 0.551
1257 0.504
29 0.966
0 0.0
0 0.0
0 0.0
0.5373606217116378
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 0
Testing:	 Loss = 1.3004,	 Acc1 = 0.2810,	 Acc2 = 0.2826

 ===== Epoch 103	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167    1.704236    2.1338933  -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798 -0.07877064 -0.43665314 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 5 5
train:	 Loss = 1.2652,	 Acc = 0.5164
45194 0.296
88650 0.574
46210 0.612
5474 0.601
880 0.487
88 0.5
0 0.0
0 0.0
0.5869272904842111
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2461,	 Acc = 0.5293
5196 0.411
27109 0.546
13039 0.544
1257 0.512
29 0.0
0 0.0
0 0.0
0 0.0
0.5441666264420524
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3079,	 Acc1 = 0.3027,	 Acc2 = 0.3086

 ===== Epoch 104	 =====
[-0.3681874  -0.38585573  1.9599429   1.0924563  -0.4351751  -0.3592861
  1.532364    3.843747   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-1.3513822e-02 -1.6987352e-02 -6.7788488e-01  2.2268181e-01
  7.4981754e-03  5.3155841e-03 -2.1680877e+00 -4.4829197e+00
  1.9501297e-02  3.3417623e-02 -7.8236684e-03  2.0873190e-03
 -1.4389989e-02 -1.4110334e-02] 1 1
train:	 Loss = 1.2643,	 Acc = 0.5173
45196 0.296
88645 0.575
46212 0.614
5475 0.601
880 0.489
88 0.443
0 0.0
0 0.0
0.5881174805378627
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2624,	 Acc = 0.5077
5196 0.387
27109 0.525
13039 0.521
1257 0.498
29 0.0
0 0.0
0 0.0
0 0.0
0.5228314910460009
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3287,	 Acc1 = 0.2886,	 Acc2 = 0.2917

 ===== Epoch 105	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 6 1
train:	 Loss = 1.2651,	 Acc = 0.5172
45194 0.295
88649 0.575
46210 0.613
5475 0.607
880 0.489
88 0.432
0 0.0
0 0.0
0.5881374644378706
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.3071,	 Acc = 0.4946
5196 0.392
27109 0.51
13039 0.507
1257 0.465
29 0.0
0 0.0
0 0.0
0 0.0
0.5075059130182942
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 0
Testing:	 Loss = 1.3779,	 Acc1 = 0.2744,	 Acc2 = 0.2746

 ===== Epoch 106	 =====
[ 0.2124063   1.5488458  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01315646 -0.01936889  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 0 0
train:	 Loss = 1.2648,	 Acc = 0.5166
45196 0.296
88647 0.573
46210 0.614
5475 0.609
880 0.476
88 0.534
0 0.0
0 0.0
0.5872682236376504
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2381,	 Acc = 0.5415
5196 0.383
27109 0.557
13039 0.574
1257 0.547
29 0.0
0 0.0
0 0.0
0 0.0
0.5613747164164695
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3146,	 Acc1 = 0.2824,	 Acc2 = 0.2843

 ===== Epoch 107	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  1.8437365   2.0784957  -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 6
train:	 Loss = 1.2636,	 Acc = 0.5175
45194 0.297
88648 0.574
46212 0.616
5475 0.608
879 0.472
88 0.455
0 0.0
0 0.0
0.5880030006652418
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2942,	 Acc = 0.5234
5196 0.373
27109 0.539
13039 0.549
1257 0.55
29 0.31
0 0.0
0 0.0
0 0.0
0.542284114495342
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3682,	 Acc1 = 0.2641,	 Acc2 = 0.2622

 ===== Epoch 108	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 0 1
train:	 Loss = 1.2642,	 Acc = 0.5167
45194 0.296
88651 0.573
46209 0.616
5474 0.605
880 0.49
88 0.489
0 0.0
0 0.0
0.5873306818020977
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.3189,	 Acc = 0.5028
5196 0.352
27109 0.524
13039 0.523
1257 0.481
29 0.138
0 0.0
0 0.0
0 0.0
0.5216730221557175
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 1
Testing:	 Loss = 1.3828,	 Acc1 = 0.2944,	 Acc2 = 0.2987

 ===== Epoch 109	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  1.1586927   2.549583
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
  3.4450312   1.4848714   0.0195013   0.03341762  0.66376066  0.413842
 -0.01438999 -0.01411033] 2 2
train:	 Loss = 1.2649,	 Acc = 0.5177
45195 0.296
88650 0.574
46209 0.617
5474 0.609
880 0.469
88 0.477
0 0.0
0 0.0
0.5884034790978124
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2693,	 Acc = 0.5179
5196 0.372
27109 0.537
13039 0.536
1257 0.511
29 0.966
0 0.0
0 0.0
0 0.0
0.536226287589902
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3358,	 Acc1 = 0.2814,	 Acc2 = 0.2831

 ===== Epoch 110	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 1
train:	 Loss = 1.2645,	 Acc = 0.5167
45197 0.295
88649 0.574
46207 0.613
5475 0.612
880 0.469
88 0.466
0 0.0
0 0.0
0.5876403937749028
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2676,	 Acc = 0.5259
5196 0.386
27109 0.546
13039 0.543
1257 0.51
29 0.207
0 0.0
0 0.0
0 0.0
0.5433701790799826
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3330,	 Acc1 = 0.2903,	 Acc2 = 0.2937

 ===== Epoch 111	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167    1.4558959   2.8904655  -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798 -0.0569336   0.01450673 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 0 0
train:	 Loss = 1.2656,	 Acc = 0.5170
45193 0.295
88652 0.574
46209 0.615
5474 0.604
880 0.497
88 0.432
0 0.0
0 0.0
0.5879139154865785
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2624,	 Acc = 0.5238
5196 0.381
27109 0.53
13039 0.566
1257 0.551
29 0.207
0 0.0
0 0.0
0 0.0
0.5418014191243906
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3367,	 Acc1 = 0.2760,	 Acc2 = 0.2766

 ===== Epoch 112	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.0146363   1.6115828
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.43621555  0.34428886
 -0.01438999 -0.01411033] 1 1
train:	 Loss = 1.2640,	 Acc = 0.5170
45196 0.296
88651 0.574
46209 0.614
5472 0.604
880 0.484
88 0.489
0 0.0
0 0.0
0.5877777777777777
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2745,	 Acc = 0.5169
5196 0.377
27109 0.534
13039 0.536
1257 0.517
29 0.966
0 0.0
0 0.0
0 0.0
0.5344644494859294
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3458,	 Acc1 = 0.2758,	 Acc2 = 0.2763

 ===== Epoch 113	 =====
[-0.3681874  -0.38585573  2.5794237   1.1609684  -0.4351751  -0.3592861
  1.9626105   3.573516   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-1.3513822e-02 -1.6987352e-02 -1.3248011e+00 -9.9228360e-03
  7.4981754e-03  5.3155841e-03 -2.6582155e+00 -4.1957159e+00
  1.9501297e-02  3.3417623e-02 -7.8236684e-03  2.0873190e-03
 -1.4389989e-02 -1.4110334e-02] 1 1
train:	 Loss = 1.2657,	 Acc = 0.5158
45196 0.296
88650 0.572
46208 0.613
5474 0.609
880 0.45
88 0.523
0 0.0
0 0.0
0.5860155697098373
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.3200,	 Acc = 0.5022
5196 0.392
27109 0.528
13039 0.492
1257 0.513
29 0.0
0 0.0
0 0.0
0 0.0
0.5160254863155862
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3873,	 Acc1 = 0.2717,	 Acc2 = 0.2714

 ===== Epoch 114	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167    3.0522046   2.4472954  -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [ 1.5628173e+00  2.1906993e+00  2.0342596e-02  2.5701299e-02
  7.4981754e-03  5.3155841e-03 -1.3048708e-02 -8.5879778e-03
 -4.0731807e+00 -3.1301045e+00 -7.8236684e-03  2.0873190e-03
  2.1354582e+00  2.6517677e+00] 4 6
train:	 Loss = 1.2661,	 Acc = 0.5165
45192 0.296
88651 0.573
46210 0.613
5475 0.611
880 0.485
88 0.42
0 0.0
0 0.0
0.5868906754232011
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2459,	 Acc = 0.5308
5196 0.386
27109 0.548
13039 0.558
1257 0.488
29 0.0
0 0.0
0 0.0
0 0.0
0.5489453106144712
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3302,	 Acc1 = 0.2659,	 Acc2 = 0.2644

 ===== Epoch 115	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  1.534603    2.0325751
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762  0.2727831   0.92575324
 -0.01438999 -0.01411033] 4 4
train:	 Loss = 1.2650,	 Acc = 0.5161
45197 0.296
88646 0.573
46211 0.612
5475 0.607
879 0.498
88 0.443
0 0.0
0 0.0
0.5865292748002463
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2406,	 Acc = 0.5246
5196 0.393
27109 0.538
13039 0.551
1257 0.52
29 0.0
0 0.0
0 0.0
0 0.0
0.5411497803736062
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3098,	 Acc1 = 0.2897,	 Acc2 = 0.2930

 ===== Epoch 116	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006  2.0918686   1.6406826
 -0.35938287 -0.366167    0.9753854   2.1877592  -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-1.3513822e-02 -1.6987352e-02  5.7983727e+00  1.0671833e+00
 -3.1985633e+00 -1.8946000e+00  5.2593675e+00  2.7818229e+00
 -1.6191096e+00 -2.8437397e+00 -7.8236684e-03  2.0873190e-03
 -1.4389989e-02 -1.4110334e-02] 5 6
train:	 Loss = 1.2635,	 Acc = 0.5177
45196 0.296
88646 0.575
46211 0.615
5475 0.607
880 0.477
88 0.455
0 0.0
0 0.0
0.588761500353857
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2874,	 Acc = 0.5123
5196 0.384
27109 0.529
13039 0.525
1257 0.557
29 0.034
0 0.0
0 0.0
0 0.0
0.5283342182748467
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3567,	 Acc1 = 0.2804,	 Acc2 = 0.2818

 ===== Epoch 117	 =====
[-0.3681874  -0.38585573  1.6846178   1.1061589  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [ 3.8891616e+00  3.1480782e+00 -2.5497637e+00 -1.3028693e+00
  7.4981754e-03  5.3155841e-03  2.6870193e+00  4.2934217e+00
  1.9501297e-02  3.3417623e-02 -7.8236684e-03  2.0873190e-03
 -1.4389989e-02 -1.4110334e-02] 5 5
train:	 Loss = 1.2633,	 Acc = 0.5181
45189 0.296
88650 0.574
46214 0.618
5475 0.611
880 0.468
88 0.386
0 0.0
0 0.0
0.5890507901236315
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2662,	 Acc = 0.5088
5196 0.374
27109 0.52
13039 0.543
1257 0.495
29 0.0
0 0.0
0 0.0
0 0.0
0.5257276632717093
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 1
Testing:	 Loss = 1.3372,	 Acc1 = 0.2692,	 Acc2 = 0.2684

 ===== Epoch 118	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   1.8071285   1.730771
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 6 6
train:	 Loss = 1.2637,	 Acc = 0.5181
45195 0.295
88649 0.576
46211 0.617
5473 0.603
880 0.466
88 0.477
0 0.0
0 0.0
0.5893871947119977
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2679,	 Acc = 0.5205
5196 0.399
27109 0.536
13039 0.541
1257 0.48
29 0.0
0 0.0
0 0.0
0 0.0
0.5357194574504031
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3337,	 Acc1 = 0.2969,	 Acc2 = 0.3017

 ===== Epoch 119	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  1.2130504   1.8774726
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762  0.2723147   0.5418198
 -0.01438999 -0.01411033] 2 4
train:	 Loss = 1.2644,	 Acc = 0.5168
45195 0.295
88650 0.573
46209 0.615
5474 0.605
880 0.474
88 0.489
0 0.0
0 0.0
0.5876179220246142
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2394,	 Acc = 0.5359
5196 0.399
27109 0.546
13039 0.571
1257 0.53
29 0.034
0 0.0
0 0.0
0 0.0
0.5530964908046532
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 0
Testing:	 Loss = 1.3146,	 Acc1 = 0.2903,	 Acc2 = 0.2937

 ===== Epoch 120	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [ 2.6996231e+00  2.2907238e+00  2.0342596e-02  2.5701299e-02
  7.4981754e-03  5.3155841e-03  1.4051431e+00  2.6881039e+00
  1.9501297e-02  3.3417623e-02 -7.8236684e-03  2.0873190e-03
 -1.4389989e-02 -1.4110334e-02] 1 5
train:	 Loss = 1.2658,	 Acc = 0.5175
45198 0.296
88649 0.574
46207 0.615
5474 0.615
880 0.459
88 0.443
0 0.0
0 0.0
0.5883664312304491
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2586,	 Acc = 0.5228
5196 0.385
27109 0.537
13039 0.547
1257 0.539
29 0.103
0 0.0
0 0.0
0 0.0
0.5401119853260608
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 0
Testing:	 Loss = 1.3356,	 Acc1 = 0.2589,	 Acc2 = 0.2560

 ===== Epoch 121	 =====
[-0.3681874  -0.38585573  1.6594768   2.229757   -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.01480808  0.02989237  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 0
train:	 Loss = 1.2635,	 Acc = 0.5172
45195 0.295
88650 0.574
46209 0.614
5474 0.606
880 0.491
88 0.489
0 0.0
0 0.0
0.5881203954678311
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.3028,	 Acc = 0.5073
5196 0.38
27109 0.527
13039 0.518
1257 0.492
29 0.621
0 0.0
0 0.0
0 0.0
0.5232900516484047
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 0
Testing:	 Loss = 1.3729,	 Acc1 = 0.2686,	 Acc2 = 0.2676

 ===== Epoch 122	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 0 1
train:	 Loss = 1.2653,	 Acc = 0.5169
45195 0.297
88651 0.574
46209 0.614
5473 0.601
880 0.469
88 0.466
0 0.0
0 0.0
0.5873702238483804
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2388,	 Acc = 0.5276
5196 0.401
27109 0.539
13039 0.561
1257 0.464
29 0.0
0 0.0
0 0.0
0 0.0
0.5433943138485302
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 0
Testing:	 Loss = 1.3233,	 Acc1 = 0.2688,	 Acc2 = 0.2679

 ===== Epoch 123	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167    1.3877456   2.865981   -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798 -2.1063745  -3.5920708   2.4372368   2.3446376
 -0.01438999 -0.01411033] 6 6
train:	 Loss = 1.2642,	 Acc = 0.5176
45194 0.295
88647 0.575
46212 0.614
5475 0.612
880 0.477
88 0.5
0 0.0
0 0.0
0.5887107047317094
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2306,	 Acc = 0.5303
5196 0.407
27109 0.545
13039 0.554
1257 0.488
29 0.034
0 0.0
0 0.0
0 0.0
0.545759521166192
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3126,	 Acc1 = 0.2548,	 Acc2 = 0.2510

 ===== Epoch 124	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167    1.3299931   1.0247433  -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798 -0.07808922  1.7732197  -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 6
train:	 Loss = 1.2654,	 Acc = 0.5166
45194 0.296
88650 0.572
46211 0.615
5473 0.611
880 0.453
88 0.466
0 0.0
0 0.0
0.587125447622822
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.3149,	 Acc = 0.5018
5196 0.372
27109 0.523
13039 0.513
1257 0.471
29 0.483
0 0.0
0 0.0
0 0.0
0.5180769416421297
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3879,	 Acc1 = 0.2752,	 Acc2 = 0.2756

 ===== Epoch 125	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
  1.3157896   3.6588523  -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.00775899  0.19396625  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
train:	 Loss = 1.2638,	 Acc = 0.5171
45193 0.295
88649 0.573
46212 0.616
5474 0.613
880 0.486
88 0.466
0 0.0
0 0.0
0.5881120712228332
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2698,	 Acc = 0.5213
5196 0.365
27109 0.53
13039 0.562
1257 0.566
29 0.034
0 0.0
0 0.0
0 0.0
0.5408842979195829
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3432,	 Acc1 = 0.2847,	 Acc2 = 0.2870

 ===== Epoch 126	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
  2.7772362   1.6250086  -0.41133043 -0.41984478  2.049472    3.2906282
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
  0.85939264  0.70186335  0.0195013   0.03341762 -3.033273   -4.196141
 -0.01438999 -0.01411033] 2 2
train:	 Loss = 1.2647,	 Acc = 0.5164
45194 0.296
88649 0.572
46211 0.614
5474 0.607
880 0.495
88 0.443
0 0.0
0 0.0
0.5868352889555704
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2728,	 Acc = 0.5211
5196 0.386
27109 0.53
13039 0.554
1257 0.538
29 0.966
0 0.0
0 0.0
0 0.0
0.5380363952309697
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3521,	 Acc1 = 0.2668,	 Acc2 = 0.2654

 ===== Epoch 127	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 6 1
train:	 Loss = 1.2656,	 Acc = 0.5162
45196 0.295
88650 0.573
46209 0.613
5473 0.602
880 0.482
88 0.534
0 0.0
0 0.0
0.5868152866242038
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2940,	 Acc = 0.5185
5196 0.405
27109 0.54
13039 0.522
1257 0.508
29 0.0
0 0.0
0 0.0
0 0.0
0.5327267461505044
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3571,	 Acc1 = 0.2868,	 Acc2 = 0.2895

 ===== Epoch 128	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  1.5713524   1.5820395
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762  0.14604223 -0.00904119
 -0.01438999 -0.01411033] 3 3
train:	 Loss = 1.2650,	 Acc = 0.5170
45192 0.296
88652 0.573
46209 0.615
5475 0.613
880 0.474
88 0.477
0 0.0
0 0.0
0.5875983694729094
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2604,	 Acc = 0.5164
5196 0.414
27109 0.539
13039 0.515
1257 0.477
29 0.0
0 0.0
0 0.0
0 0.0
0.529275474248202
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3330,	 Acc1 = 0.2666,	 Acc2 = 0.2652

 ===== Epoch 129	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 5 1
train:	 Loss = 1.2642,	 Acc = 0.5175
45195 0.295
88651 0.575
46207 0.614
5475 0.606
880 0.481
88 0.545
0 0.0
0 0.0
0.5886441001832966
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2990,	 Acc = 0.5044
5196 0.381
27109 0.527
13039 0.508
1257 0.501
29 0.034
0 0.0
0 0.0
0 0.0
0.5198146449775547
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3630,	 Acc1 = 0.2926,	 Acc2 = 0.2965

 ===== Epoch 130	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  0.42141673  2.0054936
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -1.0442393  -2.7438712
 -0.01438999 -0.01411033] 1 6
train:	 Loss = 1.2661,	 Acc = 0.5157
45195 0.296
88651 0.57
46209 0.615
5473 0.604
880 0.472
88 0.5
0 0.0
0 0.0
0.5858061867927332
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.3094,	 Acc = 0.5025
5196 0.385
27109 0.513
13039 0.525
1257 0.522
29 0.966
0 0.0
0 0.0
0 0.0
0.517159820437322
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3689,	 Acc1 = 0.2843,	 Acc2 = 0.2865

 ===== Epoch 131	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
  1.444108    3.8181462  -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-1.3513822e-02 -1.6987352e-02  2.5441682e+00  1.8257679e+00
  7.4981754e-03  5.3155841e-03 -2.0675485e+00 -4.4557109e+00
  1.9501297e-02  3.3417623e-02 -7.8236684e-03  2.0873190e-03
 -1.4389989e-02 -1.4110334e-02] 4 6
train:	 Loss = 1.2639,	 Acc = 0.5163
45197 0.296
88647 0.572
46209 0.615
5475 0.605
880 0.469
88 0.455
0 0.0
0 0.0
0.586840671200787
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.3146,	 Acc = 0.5056
5196 0.363
27109 0.528
13039 0.516
1257 0.517
29 0.69
0 0.0
0 0.0
0 0.0
0.5235313993338804
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3822,	 Acc1 = 0.2826,	 Acc2 = 0.2845

 ===== Epoch 132	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
  1.2112752   2.2963192  -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
  0.18935297  1.4667323   0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
train:	 Loss = 1.2644,	 Acc = 0.5174
45192 0.296
88651 0.575
46211 0.614
5474 0.609
880 0.474
88 0.455
0 0.0
0 0.0
0.5883556021060975
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2615,	 Acc = 0.5181
5196 0.408
27109 0.54
13039 0.523
1257 0.453
29 0.0
0 0.0
0 0.0
0 0.0
0.5318820292513394
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3270,	 Acc1 = 0.2930,	 Acc2 = 0.2970

 ===== Epoch 133	 =====
[ 0.70979065  0.9022414  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
  0.96953386  1.5791048 ] [-0.98749214 -1.2172831   0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.00933892 -0.71321434] 5 5
train:	 Loss = 1.2652,	 Acc = 0.5166
45194 0.295
88648 0.573
46212 0.614
5474 0.616
880 0.473
88 0.5
0 0.0
0 0.0
0.5874509914933971
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2899,	 Acc = 0.5096
5196 0.387
27109 0.521
13039 0.53
1257 0.554
29 0.966
0 0.0
0 0.0
0 0.0
0.5249312159096394
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 1
Testing:	 Loss = 1.3630,	 Acc1 = 0.2711,	 Acc2 = 0.2706

 ===== Epoch 134	 =====
[ 3.472854    1.76864    -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167    1.740043    3.2503884  -0.42690593 -0.4244443
  2.827938    1.9291598 ] [-0.49247676  0.02111727  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.26791102  0.05773163 -0.00782367  0.00208732
 -0.27307355  0.03520406] 0 0
train:	 Loss = 1.2645,	 Acc = 0.5168
45193 0.297
88649 0.574
46212 0.613
5474 0.612
880 0.468
88 0.489
0 0.0
0 0.0
0.5872699093437507
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2771,	 Acc = 0.5213
5196 0.38
27109 0.537
13039 0.545
1257 0.51
29 0.621
0 0.0
0 0.0
0 0.0
0.5389052468986822
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 1
Testing:	 Loss = 1.3521,	 Acc1 = 0.2507,	 Acc2 = 0.2460

 ===== Epoch 135	 =====
[-0.3681874  -0.38585573  3.2380598   1.1518337  -0.4351751  -0.3592861
  2.2767313   2.603529   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-1.3513822e-02 -1.6987352e-02 -4.4457436e+00 -1.3447800e+00
  4.0944796e+00  2.9732687e+00 -3.0160556e+00 -3.1648057e+00
  2.8005238e+00  2.8781562e+00 -7.8236684e-03  2.0873190e-03
 -1.4389989e-02 -1.4110334e-02] 6 5
train:	 Loss = 1.2676,	 Acc = 0.5160
45197 0.298
88647 0.571
46210 0.614
5474 0.603
880 0.457
88 0.5
0 0.0
0 0.0
0.5858498644717939
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2650,	 Acc = 0.5271
5196 0.411
27109 0.547
13039 0.535
1257 0.514
29 0.0
0 0.0
0 0.0
0 0.0
0.5416807452816528
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3307,	 Acc1 = 0.2810,	 Acc2 = 0.2826

 ===== Epoch 136	 =====
[-0.3681874  -0.38585573  1.5634419   1.0033907  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-1.3513822e-02 -1.6987352e-02 -2.4018683e+00 -1.2085701e+00
  7.4981754e-03  5.3155841e-03 -1.3048708e-02 -8.5879778e-03
  1.9501297e-02  3.3417623e-02 -7.8236684e-03  2.0873190e-03
 -1.4389989e-02 -1.4110334e-02] 1 6
train:	 Loss = 1.2650,	 Acc = 0.5178
45195 0.297
88648 0.574
46211 0.614
5474 0.609
880 0.489
88 0.466
0 0.0
0 0.0
0.5882690143735713
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2536,	 Acc = 0.5288
5196 0.394
27109 0.537
13039 0.565
1257 0.546
29 0.138
0 0.0
0 0.0
0 0.0
0.5457353863976444
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3248,	 Acc1 = 0.2798,	 Acc2 = 0.2811

 ===== Epoch 137	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  1.5901098   2.500344
 -0.39521116 -0.38799495] [ 2.5577166   1.86681     0.0203426   0.0257013   0.00749818  0.00531558
  3.6421413   1.6360314   0.0195013   0.03341762 -0.01016143  0.853418
 -0.01438999 -0.01411033] 6 6
train:	 Loss = 1.2651,	 Acc = 0.5176
45197 0.296
88650 0.574
46208 0.615
5475 0.609
878 0.474
88 0.477
0 0.0
0 0.0
0.5883198041033553
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2641,	 Acc = 0.5158
5196 0.376
27109 0.536
13039 0.53
1257 0.53
29 0.0
0 0.0
0 0.0
0 0.0
0.5333301153641936
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3267,	 Acc1 = 0.3008,	 Acc2 = 0.3064

 ===== Epoch 138	 =====
[ 1.490829    2.6452618  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
  1.1935439   2.9140155 ] [-0.17355497 -0.00269812  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
  0.006325    0.02650152] 3 0
train:	 Loss = 1.2646,	 Acc = 0.5172
45197 0.296
88648 0.575
46210 0.614
5473 0.606
880 0.462
88 0.5
0 0.0
0 0.0
0.5879305586026794
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2484,	 Acc = 0.5232
5196 0.377
27109 0.53
13039 0.566
1257 0.546
29 0.241
0 0.0
0 0.0
0 0.0
0.5416083409760101
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3229,	 Acc1 = 0.2781,	 Acc2 = 0.2791

 ===== Epoch 139	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-1.3513822e-02 -1.6987352e-02  2.0342596e-02  2.5701299e-02
  7.4981754e-03  5.3155841e-03  1.8899814e+00  3.7552927e+00
  1.9501297e-02  3.3417623e-02 -7.8236684e-03  2.0873190e-03
 -1.4389989e-02 -1.4110334e-02] 4 5
train:	 Loss = 1.2646,	 Acc = 0.5170
45192 0.297
88651 0.575
46211 0.611
5474 0.604
880 0.47
88 0.466
0 0.0
0 0.0
0.5872374455075582
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2564,	 Acc = 0.5225
5196 0.378
27109 0.545
13039 0.536
1257 0.502
29 0.0
0 0.0
0 0.0
0 0.0
0.5406188154655597
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3289,	 Acc1 = 0.2806,	 Acc2 = 0.2821

 ===== Epoch 140	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  1.6766231   2.1926014
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
  3.6057615   2.657872    0.0195013   0.03341762 -2.5777543  -2.9553127
 -0.01438999 -0.01411033] 6 5
train:	 Loss = 1.2665,	 Acc = 0.5156
45194 0.296
88648 0.572
46211 0.614
5475 0.596
880 0.476
88 0.477
0 0.0
0 0.0
0.5858798884658392
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2892,	 Acc = 0.5062
5196 0.373
27109 0.527
13039 0.517
1257 0.504
29 0.0
0 0.0
0 0.0
0 0.0
0.5229521648887387
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 1
Testing:	 Loss = 1.3590,	 Acc1 = 0.2690,	 Acc2 = 0.2681

 ===== Epoch 141	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
  4.0958467   1.9947983  -0.41133043 -0.41984478  4.037359    2.4437196
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.23264658  0.03373678  0.0195013   0.03341762 -0.13128996  0.03547284
 -0.01438999 -0.01411033] 3 3
train:	 Loss = 1.2650,	 Acc = 0.5150
45196 0.295
88647 0.572
46210 0.612
5475 0.601
880 0.487
88 0.443
0 0.0
0 0.0
0.5854352441613588
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2540,	 Acc = 0.5312
5196 0.383
27109 0.54
13039 0.569
1257 0.572
29 0.0
0 0.0
0 0.0
0 0.0
0.549741757976541
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3314,	 Acc1 = 0.2651,	 Acc2 = 0.2634

 ===== Epoch 142	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
  0.68638474  1.9208404  -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-1.3513822e-02 -1.6987352e-02  3.2157354e+00  1.1090941e+00
  7.4981754e-03  5.3155841e-03 -1.2043656e+00 -2.4392385e+00
  1.9501297e-02  3.3417623e-02 -7.8236684e-03  2.0873190e-03
 -1.4389989e-02 -1.4110334e-02] 1 6
train:	 Loss = 1.2642,	 Acc = 0.5165
45195 0.296
88647 0.574
46211 0.613
5475 0.6
880 0.482
88 0.477
0 0.0
0 0.0
0.5870022151294046
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2725,	 Acc = 0.5213
5196 0.412
27109 0.527
13039 0.553
1257 0.524
29 0.966
0 0.0
0 0.0
0 0.0
0.5349954143939759
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 0
Testing:	 Loss = 1.3569,	 Acc1 = 0.2511,	 Acc2 = 0.2465

 ===== Epoch 143	 =====
[-0.3681874  -0.38585573  1.8441247   2.5060892  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.03342234  0.5873053   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
train:	 Loss = 1.2657,	 Acc = 0.5167
45195 0.297
88648 0.573
46212 0.614
5473 0.607
880 0.493
88 0.511
0 0.0
0 0.0
0.5869172900404102
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2449,	 Acc = 0.5217
5196 0.394
27109 0.537
13039 0.539
1257 0.543
29 0.0
0 0.0
0 0.0
0 0.0
0.5377709127769464
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3116,	 Acc1 = 0.2758,	 Acc2 = 0.2763

 ===== Epoch 144	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 3 1
train:	 Loss = 1.2638,	 Acc = 0.5173
45195 0.296
88655 0.575
46204 0.614
5474 0.599
880 0.491
88 0.534
0 0.0
0 0.0
0.5880496245603357
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2547,	 Acc = 0.5121
5196 0.405
27109 0.521
13039 0.533
1257 0.537
29 0.966
0 0.0
0 0.0
0 0.0
0.5255828546604238
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 0
Testing:	 Loss = 1.3244,	 Acc1 = 0.2727,	 Acc2 = 0.2726

 ===== Epoch 145	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167    1.6210711   1.7078621  -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-1.3513822e-02 -1.6987352e-02  2.0342596e-02  2.5701299e-02
  2.1698892e+00  1.4946111e+00 -1.3048708e-02 -8.5879778e-03
 -2.3820834e+00 -2.3142347e+00 -7.8236684e-03  2.0873190e-03
 -1.4389989e-02 -1.4110334e-02] 5 6
train:	 Loss = 1.2633,	 Acc = 0.5179
45195 0.295
88651 0.575
46207 0.616
5475 0.605
880 0.507
88 0.557
0 0.0
0 0.0
0.5891324194450145
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.3145,	 Acc = 0.5046
5196 0.38
27109 0.529
13039 0.506
1257 0.489
29 0.0
0 0.0
0 0.0
0 0.0
0.5202008012743158
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 0
Testing:	 Loss = 1.3903,	 Acc1 = 0.2692,	 Acc2 = 0.2684

 ===== Epoch 146	 =====
[-0.3681874  -0.38585573  1.6673075   2.8532174  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-1.3513822e-02 -1.6987352e-02 -2.5286365e+00 -2.9059556e+00
  7.4981754e-03  5.3155841e-03 -1.3048708e-02 -8.5879778e-03
  1.9501297e-02  3.3417623e-02 -7.8236684e-03  2.0873190e-03
 -1.4389989e-02 -1.4110334e-02] 1 6
train:	 Loss = 1.2631,	 Acc = 0.5163
45192 0.296
88648 0.572
46213 0.614
5475 0.602
880 0.493
88 0.511
0 0.0
0 0.0
0.5866146747438148
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.3191,	 Acc = 0.4935
5196 0.393
27109 0.506
13039 0.506
1257 0.5
29 0.483
0 0.0
0 0.0
0 0.0
0.50605782690544
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 1
Testing:	 Loss = 1.3927,	 Acc1 = 0.2763,	 Acc2 = 0.2768

 ===== Epoch 147	 =====
[-0.3681874  -0.38585573  2.2385654   2.161245   -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.023361    0.04875221  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 1 0
train:	 Loss = 1.2635,	 Acc = 0.5172
45192 0.296
88648 0.575
46214 0.613
5474 0.605
880 0.511
88 0.455
0 0.0
0 0.0
0.5880300628432316
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2490,	 Acc = 0.5278
5196 0.414
27109 0.541
13039 0.55
1257 0.493
29 0.0
0 0.0
0 0.0
0 0.0
0.5420427668098663
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3225,	 Acc1 = 0.2773,	 Acc2 = 0.2781

 ===== Epoch 148	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-1.3513822e-02 -1.6987352e-02  2.0342596e-02  2.5701299e-02
  1.2075933e+00  3.1009228e+00 -1.3048708e-02 -8.5879778e-03
  1.9501297e-02  3.3417623e-02 -7.8236684e-03  2.0873190e-03
 -1.4389989e-02 -1.4110334e-02] 5 6
train:	 Loss = 1.2654,	 Acc = 0.5159
45194 0.296
88650 0.573
46209 0.612
5475 0.605
880 0.481
88 0.455
0 0.0
0 0.0
0.586254971621067
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2914,	 Acc = 0.5009
5196 0.372
27109 0.515
13039 0.517
1257 0.548
29 0.966
0 0.0
0 0.0
0 0.0
0.5171115509002269
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3595,	 Acc1 = 0.2895,	 Acc2 = 0.2927

 ===== Epoch 149	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167    1.6043226   1.960053   -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-1.3513822e-02 -1.6987352e-02  2.0342596e-02  2.5701299e-02
  2.2694020e+00  1.0690982e+00 -1.3048708e-02 -8.5879778e-03
 -2.3622925e+00 -2.5924950e+00 -7.8236684e-03  2.0873190e-03
 -1.4389989e-02 -1.4110334e-02] 5 6
train:	 Loss = 1.2638,	 Acc = 0.5170
45192 0.296
88651 0.574
46211 0.615
5474 0.601
880 0.499
88 0.477
0 0.0
0 0.0
0.5877823699258337
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2897,	 Acc = 0.5068
5196 0.418
27109 0.532
13039 0.498
1257 0.449
29 0.0
0 0.0
0 0.0
0 0.0
0.5179804025679394
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 0
Testing:	 Loss = 1.3598,	 Acc1 = 0.2855,	 Acc2 = 0.2880

 ===== Epoch 150	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
  3.2661262   1.4116683  -0.41133043 -0.41984478  1.9805672   2.9656515
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -4.1431537  -1.8980862   0.0195013   0.03341762  0.0660696  -0.643366
 -0.01438999 -0.01411033] 5 5
train:	 Loss = 1.2634,	 Acc = 0.5163
45196 0.295
88654 0.574
46205 0.613
5473 0.601
880 0.502
88 0.42
0 0.0
0 0.0
0.5869497523000707
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2597,	 Acc = 0.5202
5196 0.382
27109 0.528
13039 0.556
1257 0.547
29 0.897
0 0.0
0 0.0
0 0.0
0.5375536998600183
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3312,	 Acc1 = 0.2746,	 Acc2 = 0.2749

 ===== Epoch 151	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 0 1
train:	 Loss = 1.2637,	 Acc = 0.5167
45193 0.296
88648 0.574
46213 0.614
5474 0.597
880 0.501
88 0.466
0 0.0
0 0.0
0.5873477562401365
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.3140,	 Acc = 0.5074
5196 0.392
27109 0.524
13039 0.517
1257 0.528
29 0.0
0 0.0
0 0.0
0 0.0
0.5218661003040981
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3845,	 Acc1 = 0.2835,	 Acc2 = 0.2855

 ===== Epoch 152	 =====
[-0.3681874  -0.38585573  1.6499959   2.238892   -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.02839167  0.02360576  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 0 0
train:	 Loss = 1.2638,	 Acc = 0.5168
45195 0.294
88651 0.573
46207 0.615
5475 0.609
880 0.483
88 0.489
0 0.0
0 0.0
0.5879576223805918
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2616,	 Acc = 0.5117
5196 0.388
27109 0.527
13039 0.53
1257 0.507
29 0.0
0 0.0
0 0.0
0 0.0
0.5271998841531109
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3245,	 Acc1 = 0.3043,	 Acc2 = 0.3106

 ===== Epoch 153	 =====
[ 1.4303269   2.4638035  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
  1.5137457   2.6893535 ] [ 1.1238613   0.09970806  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
  0.03815577  0.12513031] 4 2
train:	 Loss = 1.2654,	 Acc = 0.5167
45195 0.297
88648 0.573
46211 0.615
5474 0.604
880 0.493
88 0.477
0 0.0
0 0.0
0.5871366798536458
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2414,	 Acc = 0.5321
5196 0.387
27109 0.539
13039 0.572
1257 0.565
29 0.0
0 0.0
0 0.0
0 0.0
0.5502244533474924
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3181,	 Acc1 = 0.2785,	 Acc2 = 0.2796

 ===== Epoch 154	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
  1.9244311   2.9845493 ] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
  0.06644892  0.12222945] 4 4
train:	 Loss = 1.2655,	 Acc = 0.5165
45196 0.295
88649 0.573
46209 0.615
5474 0.604
880 0.461
88 0.557
0 0.0
0 0.0
0.587452229299363
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2933,	 Acc = 0.5053
5196 0.384
27109 0.525
13039 0.515
1257 0.494
29 0.0
0 0.0
0 0.0
0 0.0
0.5204904184968866
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3635,	 Acc1 = 0.2839,	 Acc2 = 0.2860

 ===== Epoch 155	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167    1.355404    1.0884031  -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798 -0.11494067  1.6813668  -0.00782367  0.00208732
 -0.01438999 -0.01411033] 6 6
train:	 Loss = 1.2653,	 Acc = 0.5171
45193 0.295
88647 0.574
46213 0.616
5475 0.597
880 0.475
88 0.455
0 0.0
0 0.0
0.5881474561757358
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.3609,	 Acc = 0.4919
5196 0.388
27109 0.522
13039 0.472
1257 0.491
29 0.172
0 0.0
0 0.0
0 0.0
0.5049476275522518
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.4226,	 Acc1 = 0.2791,	 Acc2 = 0.2803

 ===== Epoch 156	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  1.7906979   1.2866062
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
  1.8562475   2.3041577   0.0195013   0.03341762 -2.717122   -1.9314904
 -0.01438999 -0.01411033] 6 6
train:	 Loss = 1.2654,	 Acc = 0.5167
45194 0.295
88649 0.574
46210 0.615
5475 0.601
880 0.482
88 0.42
0 0.0
0 0.0
0.5875076078187145
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2875,	 Acc = 0.5040
5196 0.37
27109 0.512
13039 0.535
1257 0.561
29 0.966
0 0.0
0 0.0
0 0.0
0.5207800357194574
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 0
Testing:	 Loss = 1.3614,	 Acc1 = 0.2595,	 Acc2 = 0.2567

 ===== Epoch 157	 =====
[ 0.21228158  1.5514015  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01337935 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 0 0
train:	 Loss = 1.2638,	 Acc = 0.5170
45195 0.298
88649 0.573
46211 0.614
5474 0.604
879 0.468
88 0.511
0 0.0
0 0.0
0.5871862194888925
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2935,	 Acc = 0.4890
5196 0.377
27109 0.505
13039 0.499
1257 0.516
29 0.207
0 0.0
0 0.0
0 0.0
0.5030168460684462
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3564,	 Acc1 = 0.2934,	 Acc2 = 0.2975

 ===== Epoch 158	 =====
[ 0.2128018   1.5462899  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 0 0
train:	 Loss = 1.2646,	 Acc = 0.5165
45194 0.295
88651 0.575
46208 0.614
5475 0.594
880 0.468
88 0.523
0 0.0
0 0.0
0.5874226833307384
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2564,	 Acc = 0.5331
5196 0.385
27109 0.541
13039 0.575
1257 0.555
29 0.0
0 0.0
0 0.0
0 0.0
0.551648404691799
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3295,	 Acc1 = 0.2835,	 Acc2 = 0.2855

 ===== Epoch 159	 =====
[-0.3681874  -0.38585573  2.1948755   2.067612   -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-1.3513822e-02 -1.6987352e-02 -3.1725345e+00 -2.1850905e+00
  7.4981754e-03  5.3155841e-03 -1.3048708e-02 -8.5879778e-03
  1.9501297e-02  3.3417623e-02 -7.8236684e-03  2.0873190e-03
 -1.4389989e-02 -1.4110334e-02] 2 1
train:	 Loss = 1.2641,	 Acc = 0.5170
45193 0.297
88649 0.573
46211 0.615
5475 0.601
880 0.485
88 0.591
0 0.0
0 0.0
0.5873760642024586
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2620,	 Acc = 0.5172
5196 0.408
27109 0.538
13039 0.522
1257 0.492
29 0.0
0 0.0
0 0.0
0 0.0
0.5309407732779843
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3257,	 Acc1 = 0.2899,	 Acc2 = 0.2932

 ===== Epoch 160	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 1
train:	 Loss = 1.2647,	 Acc = 0.5168
45195 0.295
88649 0.574
46210 0.615
5474 0.602
880 0.497
88 0.545
0 0.0
0 0.0
0.5877665409303543
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2551,	 Acc = 0.5219
5196 0.41
27109 0.535
13039 0.54
1257 0.521
29 0.0
0 0.0
0 0.0
0 0.0
0.5358401312931409
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3282,	 Acc1 = 0.2690,	 Acc2 = 0.2681

 ===== Epoch 161	 =====
[-0.3681874  -0.38585573  0.7576631   1.8506569  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -1.4184135  -1.9860144   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 5 6
train:	 Loss = 1.2649,	 Acc = 0.5167
45192 0.295
88651 0.574
46211 0.613
5474 0.597
880 0.494
88 0.489
0 0.0
0 0.0
0.5874922153654532
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2962,	 Acc = 0.5034
5196 0.376
27109 0.524
13039 0.512
1257 0.508
29 0.0
0 0.0
0 0.0
0 0.0
0.5193078148380557
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3651,	 Acc1 = 0.2841,	 Acc2 = 0.2863

 ===== Epoch 162	 =====
[-0.3681874  -0.38585573  1.9096594   1.5492038  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-1.3513822e-02 -1.6987352e-02  3.5436511e-01 -2.3833640e-01
  7.4981754e-03  5.3155841e-03  1.8721228e+00  4.5503936e+00
  1.9501297e-02  3.3417623e-02 -7.8236684e-03  2.0873190e-03
 -1.4389989e-02 -1.4110334e-02] 2 5
train:	 Loss = 1.2637,	 Acc = 0.5176
45196 0.295
88652 0.575
46206 0.616
5474 0.605
880 0.494
88 0.523
0 0.0
0 0.0
0.5886836518046709
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.3409,	 Acc = 0.4896
5196 0.383
27109 0.513
13039 0.487
1257 0.461
29 0.0
0 0.0
0 0.0
0 0.0
0.5029203069942559
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.4035,	 Acc1 = 0.2791,	 Acc2 = 0.2803

 ===== Epoch 163	 =====
[ 1.5914108   2.024215   -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.0729555   0.24498193  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 1 4
train:	 Loss = 1.2638,	 Acc = 0.5170
45199 0.294
88645 0.575
46209 0.614
5475 0.598
880 0.478
88 0.545
0 0.0
0 0.0
0.5883422861065698
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2282,	 Acc = 0.5282
5196 0.374
27109 0.542
13039 0.564
1257 0.512
29 0.0
0 0.0
0 0.0
0 0.0
0.5474730897330694
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 1
Testing:	 Loss = 1.2983,	 Acc1 = 0.2855,	 Acc2 = 0.2880

 ===== Epoch 164	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167    2.1991847   2.7533522  -0.42690593 -0.4244443
  2.5797696   1.09582   ] [-1.3513822e-02 -1.6987352e-02  2.0342596e-02  2.5701299e-02
  7.4981754e-03  5.3155841e-03 -1.3048708e-02 -8.5879778e-03
 -6.7795688e-01  3.8820736e-02 -7.8236684e-03  2.0873190e-03
 -3.4364352e+00 -1.6617913e+00] 2 1
train:	 Loss = 1.2653,	 Acc = 0.5171
45197 0.297
88646 0.573
46211 0.615
5474 0.604
880 0.492
88 0.477
0 0.0
0 0.0
0.5874988499564753
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2796,	 Acc = 0.5066
5196 0.383
27109 0.532
13039 0.504
1257 0.499
29 0.0
0 0.0
0 0.0
0 0.0
0.5221798522952165
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3539,	 Acc1 = 0.2699,	 Acc2 = 0.2691

 ===== Epoch 165	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 0 1
train:	 Loss = 1.2680,	 Acc = 0.5146
45191 0.295
88652 0.57
46210 0.613
5475 0.603
880 0.498
88 0.455
0 0.0
0 0.0
0.5847634549379003
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2591,	 Acc = 0.5121
5196 0.41
27109 0.526
13039 0.523
1257 0.522
29 0.966
0 0.0
0 0.0
0 0.0
0.5248346768354492
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3307,	 Acc1 = 0.2855,	 Acc2 = 0.2880

 ===== Epoch 166	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  1.5414938   2.0325751
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.01203222  0.00765157
 -0.01438999 -0.01411033] 0 0
train:	 Loss = 1.2663,	 Acc = 0.5159
45196 0.297
88650 0.571
46208 0.616
5474 0.599
880 0.493
88 0.443
0 0.0
0 0.0
0.5859094125973107
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.3226,	 Acc = 0.4973
5196 0.352
27109 0.523
13039 0.501
1257 0.507
29 0.138
0 0.0
0 0.0
0 0.0
0.5154462518704446
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3877,	 Acc1 = 0.2707,	 Acc2 = 0.2701

 ===== Epoch 167	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 3 1
train:	 Loss = 1.2673,	 Acc = 0.5163
45193 0.297
88650 0.571
46211 0.616
5475 0.604
879 0.486
88 0.409
0 0.0
0 0.0
0.5865551332951175
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.3011,	 Acc = 0.5022
5196 0.414
27109 0.518
13039 0.507
1257 0.496
29 0.0
0 0.0
0 0.0
0 0.0
0.5132258531640681
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3771,	 Acc1 = 0.2635,	 Acc2 = 0.2614

 ===== Epoch 168	 =====
[-0.3681874  -0.38585573  2.686174    1.7730099  -0.4351751  -0.3592861
  2.8805873   3.912016   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-1.3513822e-02 -1.6987352e-02  5.5658871e-01  7.8219026e-01
  2.9596276e+00  1.0988841e+00 -3.7039559e+00 -4.5554762e+00
  1.9501297e-02  3.3417623e-02 -7.8236684e-03  2.0873190e-03
 -1.4389989e-02 -1.4110334e-02] 2 2
train:	 Loss = 1.2673,	 Acc = 0.5160
45194 0.296
88648 0.572
46212 0.613
5474 0.607
880 0.478
88 0.477
0 0.0
0 0.0
0.586339896109043
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2787,	 Acc = 0.5166
5196 0.361
27109 0.525
13039 0.556
1257 0.554
29 0.966
0 0.0
0 0.0
0 0.0
0.536057344210069
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 0
Testing:	 Loss = 1.3482,	 Acc1 = 0.2639,	 Acc2 = 0.2619

 ===== Epoch 169	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006  3.300892    2.5566816
 -0.35938287 -0.366167    2.7588165   1.9037385  -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-1.3513822e-02 -1.6987352e-02  2.0342596e-02  2.5701299e-02
  4.8185609e-02 -6.2497263e+00 -1.3048708e-02 -8.5879778e-03
  9.5252663e-02  7.0070195e-01 -7.8236684e-03  2.0873190e-03
 -1.4389989e-02 -1.4110334e-02] 4 4
train:	 Loss = 1.2674,	 Acc = 0.5148
45194 0.296
88654 0.57
46208 0.614
5472 0.601
880 0.502
88 0.455
0 0.0
0 0.0
0.5847829471628144
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2944,	 Acc = 0.5070
5196 0.404
27109 0.517
13039 0.528
1257 0.515
29 0.0
0 0.0
0 0.0
0 0.0
0.5199835883573877
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3614,	 Acc1 = 0.2812,	 Acc2 = 0.2828

 ===== Epoch 170	 =====
[-0.3681874  -0.38585573  1.7913681   2.4307258  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-1.3513822e-02 -1.6987352e-02 -2.6800528e+00 -2.5182810e+00
  7.4981754e-03  5.3155841e-03  3.5290349e+00  3.8278496e+00
  1.9501297e-02  3.3417623e-02 -7.8236684e-03  2.0873190e-03
 -1.4389989e-02 -1.4110334e-02] 5 5
train:	 Loss = 1.2658,	 Acc = 0.5163
45192 0.296
88654 0.572
46208 0.615
5474 0.603
880 0.481
88 0.5
0 0.0
0 0.0
0.5865934439223235
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2995,	 Acc = 0.4991
5196 0.391
27109 0.514
13039 0.516
1257 0.468
29 0.034
0 0.0
0 0.0
0 0.0
0.5126466187189265
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 1
Testing:	 Loss = 1.3652,	 Acc1 = 0.2967,	 Acc2 = 0.3014

 ===== Epoch 171	 =====
[ 3.6957982   2.4561362  -0.42115992 -0.34173006 -0.4351751  -0.3592861
  1.4870746   1.6961219  -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-3.6854200e+00 -2.6652586e+00  2.0342596e-02  2.5701299e-02
  7.4981754e-03  5.3155841e-03  2.5086662e-01 -5.8601862e-01
  1.9501297e-02  3.3417623e-02 -7.8236684e-03  2.0873190e-03
 -1.4389989e-02 -1.4110334e-02] 5 5
train:	 Loss = 1.2665,	 Acc = 0.5163
45191 0.298
88652 0.57
46211 0.616
5474 0.607
880 0.49
88 0.5
0 0.0
0 0.0
0.5860372952124836
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2773,	 Acc = 0.5073
5196 0.379
27109 0.529
13039 0.519
1257 0.465
29 0.0
0 0.0
0 0.0
0 0.0
0.5234831297967852
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3396,	 Acc1 = 0.2936,	 Acc2 = 0.2977

 ===== Epoch 172	 =====
[-0.3681874  -0.38585573  2.2534025   0.9828371  -0.4351751  -0.3592861
  3.3845742   3.2207935  -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-1.3513822e-02 -1.6987352e-02  2.3061684e-01  8.0733669e-01
  7.4981754e-03  5.3155841e-03 -4.2780871e+00 -3.8208396e+00
  1.9501297e-02  3.3417623e-02 -7.8236684e-03  2.0873190e-03
 -1.4389989e-02 -1.4110334e-02] 6 6
train:	 Loss = 1.2674,	 Acc = 0.5153
45192 0.295
88652 0.571
46211 0.614
5473 0.605
880 0.478
88 0.42
0 0.0
0 0.0
0.585623903074223
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2469,	 Acc = 0.5289
5196 0.369
27109 0.544
13039 0.562
1257 0.531
29 0.0
0 0.0
0 0.0
0 0.0
0.5489211758459236
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3232,	 Acc1 = 0.2721,	 Acc2 = 0.2719

 ===== Epoch 173	 =====
[-0.3681874  -0.38585573  2.5468636   2.35993    -0.4351751  -0.3592861
  0.98947394  3.7384994  -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [ 2.2662408e+00  3.0647242e+00 -3.6021369e+00 -2.4533193e+00
  7.4981754e-03  5.3155841e-03  4.9127441e-02 -2.7959757e+00
  1.9501297e-02  3.3417623e-02 -7.8236684e-03  2.0873190e-03
 -1.4389989e-02 -1.4110334e-02] 5 5
train:	 Loss = 1.2671,	 Acc = 0.5157
45196 0.297
88651 0.571
46206 0.614
5475 0.608
880 0.481
88 0.443
0 0.0
0 0.0
0.5857607926397735
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2962,	 Acc = 0.4961
5196 0.369
27109 0.518
13039 0.501
1257 0.484
29 0.69
0 0.0
0 0.0
0 0.0
0.5120432495052373
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 1
Testing:	 Loss = 1.3618,	 Acc1 = 0.2845,	 Acc2 = 0.2868

 ===== Epoch 174	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  3.0378644   0.9542439
  3.3273487   2.9114034 ] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  3.4590843   1.9596272  -4.240815   -1.5559034
 -4.296356   -3.6778796 ] 5 6
train:	 Loss = 1.2671,	 Acc = 0.5153
45193 0.298
88649 0.57
46211 0.612
5475 0.605
880 0.498
88 0.466
0 0.0
0 0.0
0.5848708095369525
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.3008,	 Acc = 0.4879
5196 0.368
27109 0.507
13039 0.496
1257 0.494
29 0.0
0 0.0
0 0.0
0 0.0
0.5028961722257084
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3644,	 Acc1 = 0.2818,	 Acc2 = 0.2835

 ===== Epoch 175	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 1 1
train:	 Loss = 1.2675,	 Acc = 0.5144
45197 0.297
88646 0.57
46212 0.611
5473 0.602
880 0.499
88 0.477
0 0.0
0 0.0
0.5840734895505276
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.3049,	 Acc = 0.4905
5196 0.399
27109 0.494
13039 0.518
1257 0.498
29 0.966
0 0.0
0 0.0
0 0.0
0.5019549162523531
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3776,	 Acc1 = 0.2527,	 Acc2 = 0.2485

 ===== Epoch 176	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-1.3513822e-02 -1.6987352e-02  2.0342596e-02  2.5701299e-02
  6.0891714e+00 -3.8349395e+00 -1.3048708e-02 -8.5879778e-03
  5.5780745e+00  2.6782410e+00 -7.8236684e-03  2.0873190e-03
 -1.4389989e-02 -1.4110334e-02] 2 1
train:	 Loss = 1.2665,	 Acc = 0.5156
45197 0.297
88650 0.571
46206 0.613
5475 0.601
880 0.497
88 0.398
0 0.0
0 0.0
0.5854464645892752
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2902,	 Acc = 0.4952
5196 0.392
27109 0.506
13039 0.514
1257 0.509
29 0.0
0 0.0
0 0.0
0 0.0
0.5081092822319834
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 0
Testing:	 Loss = 1.3551,	 Acc1 = 0.2893,	 Acc2 = 0.2925

 ===== Epoch 177	 =====
[ 0.57422566  2.2695668  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-8.6500603e-01 -2.4914064e+00  1.9510779e+00  1.5638257e+00
  7.4981754e-03  5.3155841e-03 -1.3048708e-02 -8.5879778e-03
  1.9501297e-02  3.3417623e-02 -7.8236684e-03  2.0873190e-03
 -1.4389989e-02 -1.4110334e-02] 5 6
train:	 Loss = 1.2667,	 Acc = 0.5157
45194 0.297
88649 0.571
46210 0.614
5475 0.605
880 0.493
88 0.5
0 0.0
0 0.0
0.5855472675545994
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2643,	 Acc = 0.5124
5196 0.392
27109 0.521
13039 0.544
1257 0.506
29 0.0
0 0.0
0 0.0
0 0.0
0.5274412318385867
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3311,	 Acc1 = 0.2948,	 Acc2 = 0.2992

 ===== Epoch 178	 =====
[-0.3681874  -0.38585573  2.144592    1.286574   -0.4351751  -0.3592861
  1.2861769   3.9148607  -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.7502625   0.38194263  0.00749818  0.00531558
  0.9248751  -0.05998233  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 2 2
train:	 Loss = 1.2673,	 Acc = 0.5163
45195 0.298
88652 0.571
46206 0.615
5475 0.606
880 0.487
88 0.443
0 0.0
0 0.0
0.5860751162412156
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2964,	 Acc = 0.5060
5196 0.371
27109 0.525
13039 0.524
1257 0.484
29 0.0
0 0.0
0 0.0
0 0.0
0.5230004344258339
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3638,	 Acc1 = 0.2754,	 Acc2 = 0.2758

 ===== Epoch 179	 =====
[-0.3681874  -0.38585573  2.12316     1.398477   -0.4351751  -0.3592861
  1.5370092   3.8465917  -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.30758065  0.05503882  0.00749818  0.00531558
  0.25218958 -0.03881995  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 2 2
train:	 Loss = 1.2659,	 Acc = 0.5160
45197 0.296
88651 0.572
46207 0.614
5473 0.603
880 0.498
88 0.477
0 0.0
0 0.0
0.5863098818816835
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.3061,	 Acc = 0.5008
5196 0.366
27109 0.514
13039 0.526
1257 0.535
29 0.034
0 0.0
0 0.0
0 0.0
0.5176907853453685
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3700,	 Acc1 = 0.3000,	 Acc2 = 0.3054

 ===== Epoch 180	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167    3.3490582   1.4262899  -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.02154766  0.0226114  -0.00782367  0.00208732
 -0.01438999 -0.01411033] 0 0
train:	 Loss = 1.2671,	 Acc = 0.5156
45195 0.296
88650 0.57
46209 0.616
5475 0.607
880 0.49
87 0.494
0 0.0
0 0.0
0.585749570066737
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.3511,	 Acc = 0.4694
5196 0.363
27109 0.485
13039 0.477
1257 0.488
29 0.966
0 0.0
0 0.0
0 0.0
0.4827436404884877
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 0
Testing:	 Loss = 1.4187,	 Acc1 = 0.2814,	 Acc2 = 0.2831

 ===== Epoch 181	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 3 1
train:	 Loss = 1.2676,	 Acc = 0.5154
45192 0.295
88652 0.57
46210 0.615
5474 0.604
880 0.494
88 0.443
0 0.0
0 0.0
0.585708826360188
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2750,	 Acc = 0.5140
5196 0.357
27109 0.526
13039 0.551
1257 0.532
29 0.0
0 0.0
0 0.0
0 0.0
0.5336438673553121
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3377,	 Acc1 = 0.3031,	 Acc2 = 0.3091

 ===== Epoch 182	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [ 3.1720550e+00  1.4238437e+00  2.0342596e-02  2.5701299e-02
  7.4981754e-03  5.3155841e-03 -1.3048708e-02 -8.5879778e-03
  1.9501297e-02  3.3417623e-02 -7.8236684e-03  2.0873190e-03
 -1.4389989e-02 -1.4110334e-02] 1 2
train:	 Loss = 1.2684,	 Acc = 0.5155
45199 0.297
88647 0.57
46207 0.614
5475 0.601
880 0.502
88 0.455
0 0.0
0 0.0
0.5854901377948576
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.3222,	 Acc = 0.4877
5196 0.384
27109 0.508
13039 0.49
1257 0.481
29 0.0
0 0.0
0 0.0
0 0.0
0.5007723125935222
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3797,	 Acc1 = 0.3105,	 Acc2 = 0.3181

 ===== Epoch 183	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167    1.6095197   2.1632748  -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.01404151  0.04962696 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 3 0
train:	 Loss = 1.2670,	 Acc = 0.5155
45197 0.298
88647 0.569
46209 0.616
5475 0.603
880 0.493
88 0.443
0 0.0
0 0.0
0.58516337695242
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2987,	 Acc = 0.4971
5196 0.363
27109 0.51
13039 0.525
1257 0.5
29 0.0
0 0.0
0 0.0
0 0.0
0.5139498962204953
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 0
Testing:	 Loss = 1.3670,	 Acc1 = 0.2829,	 Acc2 = 0.2848

 ===== Epoch 184	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 0 1
train:	 Loss = 1.2663,	 Acc = 0.5154
45196 0.297
88648 0.569
46209 0.615
5475 0.607
880 0.495
88 0.511
0 0.0
0 0.0
0.5853290870488322
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2653,	 Acc = 0.5236
5196 0.401
27109 0.53
13039 0.559
1257 0.537
29 0.0
0 0.0
0 0.0
0 0.0
0.5390259207414201
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 0
Testing:	 Loss = 1.3452,	 Acc1 = 0.2697,	 Acc2 = 0.2689

 ===== Epoch 185	 =====
[-0.3681874  -0.38585573  1.104705    3.0290647  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.09378727  0.02151022  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 0 0
train:	 Loss = 1.2669,	 Acc = 0.5149
45191 0.297
88651 0.57
46211 0.614
5475 0.597
880 0.5
88 0.534
0 0.0
0 0.0
0.5846502246912707
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2712,	 Acc = 0.5163
5196 0.403
27109 0.525
13039 0.543
1257 0.532
29 0.0
0 0.0
0 0.0
0 0.0
0.5305063474441281
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3457,	 Acc1 = 0.2655,	 Acc2 = 0.2639

 ===== Epoch 186	 =====
[-0.3681874  -0.38585573  3.4886556   3.0678885   2.1714654   1.3674506
 -0.35938287 -0.366167    0.862189    1.4997435  -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0107851   0.03198791  0.1795647   0.03935663
 -0.01304871 -0.00858798  0.134834    0.08744875 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 3
train:	 Loss = 1.2670,	 Acc = 0.5162
45197 0.297
88651 0.572
46211 0.615
5472 0.605
877 0.483
88 0.455
0 0.0
0 0.0
0.5864089625545829
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2696,	 Acc = 0.5131
5196 0.382
27109 0.526
13039 0.538
1257 0.517
29 0.0
0 0.0
0 0.0
0 0.0
0.5294926871651301
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 1
Testing:	 Loss = 1.3318,	 Acc1 = 0.2969,	 Acc2 = 0.3017

 ===== Epoch 187	 =====
[-0.3681874  -0.38585573  2.168497    2.492387   -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-1.35138221e-02 -1.69873517e-02  1.05357036e-01 -5.69431305e-01
  7.49817537e-03  5.31558413e-03  1.11080229e+00  3.92156839e+00
  1.95012968e-02  3.34176235e-02 -7.82366842e-03  2.08731904e-03
 -1.43899890e-02 -1.41103342e-02] 5 5
train:	 Loss = 1.2676,	 Acc = 0.5158
45190 0.296
88652 0.571
46211 0.614
5475 0.608
880 0.501
88 0.489
0 0.0
0 0.0
0.5861746847267632
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2918,	 Acc = 0.5009
5196 0.36
27109 0.514
13039 0.53
1257 0.501
29 0.0
0 0.0
0 0.0
0 0.0
0.5186079065501762
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 0
Testing:	 Loss = 1.3588,	 Acc1 = 0.2849,	 Acc2 = 0.2873

 ===== Epoch 188	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762  2.449864    4.2030983
 -0.01438999 -0.01411033] 1 5
train:	 Loss = 1.2678,	 Acc = 0.5158
45195 0.295
88648 0.572
46211 0.614
5474 0.607
880 0.5
88 0.466
0 0.0
0 0.0
0.5863157373266997
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2411,	 Acc = 0.5205
5196 0.382
27109 0.533
13039 0.548
1257 0.527
29 0.966
0 0.0
0 0.0
0 0.0
0.5378191823140416
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3081,	 Acc1 = 0.2928,	 Acc2 = 0.2967

 ===== Epoch 189	 =====
[-0.3681874  -0.38585573  2.4207413   1.7410375  -0.4351751  -0.3592861
  0.9343145   3.7015202  -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.3991357  -0.0078273   0.00749818  0.00531558
  1.0293853  -0.5829954   0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 2 2
train:	 Loss = 1.2666,	 Acc = 0.5162
45192 0.296
88649 0.572
46214 0.615
5474 0.602
879 0.49
88 0.5
0 0.0
0 0.0
0.5865580592198381
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.3105,	 Acc = 0.5015
5196 0.39
27109 0.506
13039 0.533
1257 0.545
29 0.0
0 0.0
0 0.0
0 0.0
0.5155186561760873
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3853,	 Acc1 = 0.2637,	 Acc2 = 0.2617

 ===== Epoch 190	 =====
[-0.3681874  -0.38585573  1.0886306   1.7615912  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.05102891 -0.08536217  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 5 3
train:	 Loss = 1.2666,	 Acc = 0.5150
45190 0.297
88653 0.569
46211 0.615
5474 0.601
880 0.487
88 0.455
0 0.0
0 0.0
0.5847947008619592
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2318,	 Acc = 0.5276
5196 0.402
27109 0.531
13039 0.572
1257 0.529
29 0.0
0 0.0
0 0.0
0 0.0
0.5433219095428875
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3058,	 Acc1 = 0.2771,	 Acc2 = 0.2778

 ===== Epoch 191	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  1.4921132   1.2939922
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.16917272  0.0632941
 -0.01438999 -0.01411033] 1 1
train:	 Loss = 1.2672,	 Acc = 0.5163
45197 0.297
88647 0.571
46211 0.616
5473 0.606
880 0.478
88 0.455
0 0.0
0 0.0
0.5865646607548531
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2658,	 Acc = 0.5223
5196 0.409
27109 0.533
13039 0.548
1257 0.514
29 0.034
0 0.0
0 0.0
0 0.0
0.5364676352753777
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3430,	 Acc1 = 0.2606,	 Acc2 = 0.2580

 ===== Epoch 192	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-1.3513822e-02 -1.6987352e-02  2.8505244e+00  1.6162142e+00
  7.4981754e-03  5.3155841e-03 -1.3048708e-02 -8.5879778e-03
  1.9501297e-02  3.3417623e-02 -7.8236684e-03  2.0873190e-03
 -1.4389989e-02 -1.4110334e-02] 4 6
train:	 Loss = 1.2673,	 Acc = 0.5161
45193 0.298
88649 0.57
46211 0.616
5475 0.601
880 0.499
88 0.534
0 0.0
0 0.0
0.5859111271522898
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2810,	 Acc = 0.5121
5196 0.358
27109 0.513
13039 0.569
1257 0.552
29 0.0
0 0.0
0 0.0
0 0.0
0.5313269295747454
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 0
Testing:	 Loss = 1.3583,	 Acc1 = 0.2503,	 Acc2 = 0.2455

 ===== Epoch 193	 =====
[ 1.740201    2.7858279  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-1.918492   -2.9724772   0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762  2.9306366   1.362547
 -0.01438999 -0.01411033] 4 6
train:	 Loss = 1.2680,	 Acc = 0.5159
45194 0.298
88650 0.57
46210 0.614
5474 0.607
880 0.49
88 0.432
0 0.0
0 0.0
0.5856746542865635
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2635,	 Acc = 0.5214
5196 0.389
27109 0.532
13039 0.554
1257 0.506
29 0.0
0 0.0
0 0.0
0 0.0
0.5380363952309697
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 1
Testing:	 Loss = 1.3301,	 Acc1 = 0.2833,	 Acc2 = 0.2853

 ===== Epoch 194	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [ 4.0135641e+00  2.3240654e+00  2.0342596e-02  2.5701299e-02
  7.4981754e-03  5.3155841e-03 -1.3048708e-02 -8.5879778e-03
  1.9501297e-02  3.3417623e-02 -7.8236684e-03  2.0873190e-03
 -1.4389989e-02 -1.4110334e-02] 6 5
train:	 Loss = 1.2664,	 Acc = 0.5160
45192 0.297
88651 0.57
46210 0.615
5475 0.606
880 0.494
88 0.455
0 0.0
0 0.0
0.5859352884560947
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2652,	 Acc = 0.5231
5196 0.37
27109 0.524
13039 0.577
1257 0.576
29 0.966
0 0.0
0 0.0
0 0.0
0.5423565188009847
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3394,	 Acc1 = 0.2715,	 Acc2 = 0.2711

 ===== Epoch 195	 =====
[ 3.7940521   1.6715217  -0.42115992 -0.34173006 -0.4351751  -0.3592861
  2.7435582   1.4571809  -0.41133043 -0.41984478  1.8301264   3.2709322
 -0.39521116 -0.38799495] [-3.774195   -1.9341264   0.0203426   0.0257013   0.00749818  0.00531558
  0.96588624  0.9769743   0.0195013   0.03341762 -2.765293   -4.1738844
 -0.01438999 -0.01411033] 2 2
train:	 Loss = 1.2680,	 Acc = 0.5149
45198 0.296
88652 0.569
46204 0.614
5474 0.604
880 0.491
88 0.489
0 0.0
0 0.0
0.5849905872694589
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2632,	 Acc = 0.5202
5196 0.372
27109 0.531
13039 0.556
1257 0.536
29 0.0
0 0.0
0 0.0
0 0.0
0.5387845730559444
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 0
Testing:	 Loss = 1.3373,	 Acc1 = 0.2622,	 Acc2 = 0.2599

 ===== Epoch 196	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 1 1
train:	 Loss = 1.2669,	 Acc = 0.5165
45196 0.298
88647 0.571
46210 0.615
5475 0.607
880 0.505
88 0.5
0 0.0
0 0.0
0.586411889596603
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2953,	 Acc = 0.5011
5196 0.371
27109 0.513
13039 0.53
1257 0.495
29 0.0
0 0.0
0 0.0
0 0.0
0.5174735724284404
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 0
Testing:	 Loss = 1.3596,	 Acc1 = 0.2923,	 Acc2 = 0.2962

 ===== Epoch 197	 =====
[ 3.0293334   2.8701675  -0.42115992 -0.34173006 -0.4351751  -0.3592861
  1.4818498   3.7128983  -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [ 0.14875877  0.07112959  0.0203426   0.0257013   0.00749818  0.00531558
  0.13577491  0.13954867  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
train:	 Loss = 1.2663,	 Acc = 0.5160
45195 0.297
88650 0.572
46209 0.613
5474 0.609
880 0.495
88 0.455
0 0.0
0 0.0
0.5861034246042137
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.3239,	 Acc = 0.5050
5196 0.405
27109 0.525
13039 0.505
1257 0.497
29 0.069
0 0.0
0 0.0
0 0.0
0.5175942462711782
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 0
Testing:	 Loss = 1.3986,	 Acc1 = 0.2564,	 Acc2 = 0.2530

 ===== Epoch 198	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
  1.3175305   3.1269238  -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -1.923354   -3.721074    0.0195013   0.03341762  2.7940748   2.8148172
 -0.01438999 -0.01411033] 5 6
train:	 Loss = 1.2670,	 Acc = 0.5154
45193 0.297
88652 0.569
46209 0.616
5475 0.606
879 0.499
88 0.511
0 0.0
0 0.0
0.5851185042072709
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2621,	 Acc = 0.5122
5196 0.398
27109 0.532
13039 0.521
1257 0.48
29 0.0
0 0.0
0 0.0
0 0.0
0.5265482454023266
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3336,	 Acc1 = 0.2680,	 Acc2 = 0.2669

 ===== Epoch 199	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006  1.529281    1.0874997
 -0.35938287 -0.366167    5.480166    0.8998722  -0.42690593 -0.4244443
  5.408007    0.9234049 ] [-1.3513822e-02 -1.6987352e-02  2.0342596e-02  2.5701299e-02
  4.6781218e-01 -2.6597891e-02 -1.3048708e-02 -8.5879778e-03
 -6.9421778e+00 -1.4227211e+00 -7.8236684e-03  2.0873190e-03
 -6.6896853e+00 -1.4703352e+00] 3 2
train:	 Loss = 1.2673,	 Acc = 0.5151
45196 0.298
88654 0.569
46204 0.614
5474 0.601
880 0.478
88 0.443
0 0.0
0 0.0
0.5846779900920028
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.3015,	 Acc = 0.4963
5196 0.387
27109 0.514
13039 0.504
1257 0.502
29 0.0
0 0.0
0 0.0
0 0.0
0.5099676594101462
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3662,	 Acc1 = 0.2866,	 Acc2 = 0.2893

 ===== Epoch 200	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
  1.5213325   3.8352134  -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-1.3513822e-02 -1.6987352e-02  2.5366232e+00  2.2155375e+00
  7.4981754e-03  5.3155841e-03 -2.1555207e+00 -4.4738498e+00
  1.9501297e-02  3.3417623e-02 -7.8236684e-03  2.0873190e-03
 -1.4389989e-02 -1.4110334e-02] 6 6
train:	 Loss = 1.2679,	 Acc = 0.5153
45194 0.297
88651 0.57
46209 0.613
5474 0.606
880 0.487
88 0.466
0 0.0
0 0.0
0.5850660287894014
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2954,	 Acc = 0.5084
5196 0.378
27109 0.525
13039 0.523
1257 0.551
29 0.0
0 0.0
0 0.0
0 0.0
0.5247381377612589
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3613,	 Acc1 = 0.2783,	 Acc2 = 0.2793

 ===== Epoch 201	 =====
[ 0.3403557   1.5616245  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.65369904 -1.8317202   1.3428956   1.5156283   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 5 5
train:	 Loss = 1.2667,	 Acc = 0.5161
45194 0.296
88650 0.571
46211 0.617
5474 0.603
879 0.487
88 0.511
0 0.0
0 0.0
0.5864602058003425
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2749,	 Acc = 0.5215
5196 0.371
27109 0.532
13039 0.56
1257 0.537
29 0.034
0 0.0
0 0.0
0 0.0
0.5404016025486316
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 0
Testing:	 Loss = 1.3404,	 Acc1 = 0.2866,	 Acc2 = 0.2893

 ===== Epoch 202	 =====
[-0.3681874  -0.38585573  3.4247692   2.933148    2.3731625   1.19948
 -0.35938287 -0.366167    1.0054185   0.9880165  -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-1.3513822e-02 -1.6987352e-02  1.4711082e-01 -6.4557953e+00
 -8.3191961e-02  9.8928452e-02 -1.3048708e-02 -8.5879778e-03
  3.3832181e-02  2.4413900e-01 -7.8236684e-03  2.0873190e-03
 -1.4389989e-02 -1.4110334e-02] 4 4
train:	 Loss = 1.2672,	 Acc = 0.5160
45194 0.297
88654 0.57
46206 0.616
5474 0.606
880 0.498
88 0.5
0 0.0
0 0.0
0.586099276726444
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2798,	 Acc = 0.5102
5196 0.39
27109 0.528
13039 0.521
1257 0.527
29 0.0
0 0.0
0 0.0
0 0.0
0.5252208331322102
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3462,	 Acc1 = 0.2754,	 Acc2 = 0.2758

 ===== Epoch 203	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-1.3513822e-02 -1.6987352e-02  2.7408605e+00  3.1669114e+00
  7.4981754e-03  5.3155841e-03 -1.3048708e-02 -8.5879778e-03
  1.9501297e-02  3.3417623e-02 -7.8236684e-03  2.0873190e-03
 -1.4389989e-02 -1.4110334e-02] 2 5
train:	 Loss = 1.2670,	 Acc = 0.5165
45196 0.298
88649 0.571
46208 0.615
5475 0.604
880 0.501
88 0.42
0 0.0
0 0.0
0.5865251238499646
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2491,	 Acc = 0.5179
5196 0.369
27109 0.52
13039 0.569
1257 0.566
29 0.0
0 0.0
0 0.0
0 0.0
0.5366124438866632
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3182,	 Acc1 = 0.2796,	 Acc2 = 0.2808

 ===== Epoch 204	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-1.3513822e-02 -1.6987352e-02  2.0342596e-02  2.5701299e-02
  2.7806993e+00  1.7371536e+00 -1.3048708e-02 -8.5879778e-03
  1.9501297e-02  3.3417623e-02 -7.8236684e-03  2.0873190e-03
 -1.4389989e-02 -1.4110334e-02] 4 6
train:	 Loss = 1.2666,	 Acc = 0.5161
45196 0.297
88646 0.57
46211 0.617
5475 0.604
880 0.491
88 0.477
0 0.0
0 0.0
0.5862349610757254
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2708,	 Acc = 0.5254
5196 0.376
27109 0.531
13039 0.571
1257 0.558
29 0.069
0 0.0
0 0.0
0 0.0
0.5441183569049572
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3442,	 Acc1 = 0.2672,	 Acc2 = 0.2659

 ===== Epoch 205	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.671906    1.9316354
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762  0.37660757  0.01321582
 -0.01438999 -0.01411033] 2 0
train:	 Loss = 1.2664,	 Acc = 0.5168
45194 0.297
88648 0.573
46213 0.614
5473 0.603
880 0.501
88 0.455
0 0.0
0 0.0
0.587082985378834
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2868,	 Acc = 0.5062
5196 0.386
27109 0.523
13039 0.522
1257 0.497
29 0.0
0 0.0
0 0.0
0 0.0
0.521311000627504
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3545,	 Acc1 = 0.2674,	 Acc2 = 0.2662

 ===== Epoch 206	 =====
[ 0.6027304   1.2293772  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.00332341 -0.01460581  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 0 0
train:	 Loss = 1.2675,	 Acc = 0.5165
45198 0.297
88650 0.571
46206 0.616
5474 0.601
880 0.483
88 0.455
0 0.0
0 0.0
0.5865971209783578
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2958,	 Acc = 0.5065
5196 0.389
27109 0.511
13039 0.54
1257 0.551
29 0.0
0 0.0
0 0.0
0 0.0
0.5211661920162186
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 0
Testing:	 Loss = 1.3743,	 Acc1 = 0.2608,	 Acc2 = 0.2582

 ===== Epoch 207	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
  3.6615353   1.9947983  -0.41133043 -0.41984478  3.524405    2.387095
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
  0.29915294  0.80465204  0.0195013   0.03341762  0.89432746  0.7143117
 -0.01438999 -0.01411033] 6 4
train:	 Loss = 1.2681,	 Acc = 0.5158
45192 0.298
88649 0.57
46212 0.614
5475 0.607
880 0.492
88 0.432
0 0.0
0 0.0
0.5853620562758308
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2694,	 Acc = 0.5302
5196 0.375
27109 0.534
13039 0.582
1257 0.562
29 0.069
0 0.0
0 0.0
0 0.0
0.5497176232079934
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3484,	 Acc1 = 0.2496,	 Acc2 = 0.2448

 ===== Epoch 208	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006  3.077557    1.8624035
 -0.35938287 -0.366167    2.219975    1.582991   -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013  -0.2660437   0.36700165
 -0.01304871 -0.00858798 -0.18113741  0.4899806  -0.00782367  0.00208732
 -0.01438999 -0.01411033] 6 4
train:	 Loss = 1.2676,	 Acc = 0.5151
45193 0.298
88652 0.57
46208 0.612
5475 0.605
880 0.482
88 0.443
0 0.0
0 0.0
0.5846443458383757
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.3227,	 Acc = 0.4987
5196 0.413
27109 0.522
13039 0.489
1257 0.468
29 0.0
0 0.0
0 0.0
0 0.0
0.5094366945020997
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3870,	 Acc1 = 0.2771,	 Acc2 = 0.2778

 ===== Epoch 209	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167    2.977702    2.0604396  -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.03861055 -0.05303217 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 3 3
train:	 Loss = 1.2666,	 Acc = 0.5156
45196 0.297
88650 0.57
46207 0.615
5475 0.605
880 0.494
88 0.477
0 0.0
0 0.0
0.5856192498230715
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.3043,	 Acc = 0.5039
5196 0.396
27109 0.516
13039 0.522
1257 0.5
29 0.966
0 0.0
0 0.0
0 0.0
0.5174011681227977
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 0
Testing:	 Loss = 1.3715,	 Acc1 = 0.2963,	 Acc2 = 0.3009

 ===== Epoch 210	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
  1.1247622   2.3020084  -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
  0.14768139  0.9648815   0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
train:	 Loss = 1.2671,	 Acc = 0.5156
45195 0.296
88651 0.57
46208 0.617
5474 0.601
880 0.493
88 0.432
0 0.0
0 0.0
0.5857141846129893
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2661,	 Acc = 0.5181
5196 0.377
27109 0.525
13039 0.563
1257 0.497
29 0.0
0 0.0
0 0.0
0 0.0
0.5356953226818555
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3354,	 Acc1 = 0.2829,	 Acc2 = 0.2848

 ===== Epoch 211	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-1.3513822e-02 -1.6987352e-02  2.0342596e-02  2.5701299e-02
  2.3860741e+00  1.4605702e+00 -1.3048708e-02 -8.5879778e-03
  1.9501297e-02  3.3417623e-02 -7.8236684e-03  2.0873190e-03
 -1.4389989e-02 -1.4110334e-02] 6 2
train:	 Loss = 1.2673,	 Acc = 0.5149
45196 0.297
88646 0.568
46211 0.614
5475 0.61
880 0.478
88 0.455
0 0.0
0 0.0
0.5845010615711252
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2775,	 Acc = 0.5127
5196 0.377
27109 0.517
13039 0.554
1257 0.557
29 0.0
0 0.0
0 0.0
0 0.0
0.5296616305449631
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3390,	 Acc1 = 0.2890,	 Acc2 = 0.2922

 ===== Epoch 212	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [ 2.8906474e+00  2.3478808e+00  2.0342596e-02  2.5701299e-02
  7.4981754e-03  5.3155841e-03 -1.3048708e-02 -8.5879778e-03
  1.9501297e-02  3.3417623e-02 -7.8236684e-03  2.0873190e-03
  2.9736540e+00  3.0346794e+00] 3 5
train:	 Loss = 1.2687,	 Acc = 0.5139
45192 0.296
88648 0.568
46214 0.614
5475 0.598
879 0.497
88 0.443
0 0.0
0 0.0
0.5835998980920568
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.3134,	 Acc = 0.5057
5196 0.391
27109 0.521
13039 0.522
1257 0.5
29 0.0
0 0.0
0 0.0
0 0.0
0.5199835883573877
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3770,	 Acc1 = 0.2930,	 Acc2 = 0.2970

 ===== Epoch 213	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 5 1
train:	 Loss = 1.2681,	 Acc = 0.5157
45196 0.296
88651 0.57
46206 0.616
5475 0.599
880 0.487
88 0.42
0 0.0
0 0.0
0.5858174097664544
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.3350,	 Acc = 0.4919
5196 0.368
27109 0.515
13039 0.491
1257 0.524
29 0.379
0 0.0
0 0.0
0 0.0
0.5075059130182942
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.4004,	 Acc1 = 0.2804,	 Acc2 = 0.2818

 ===== Epoch 214	 =====
[-0.3681874  -0.38585573  2.4207413   1.7410375  -0.4351751  -0.3592861
  0.9343145   3.7015202  -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0480105  -0.11889077  0.00749818  0.00531558
  0.5961397  -0.5890418   0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 2 2
train:	 Loss = 1.2667,	 Acc = 0.5153
45194 0.297
88649 0.569
46210 0.615
5475 0.602
880 0.5
88 0.511
0 0.0
0 0.0
0.5851792614400362
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.3265,	 Acc = 0.4898
5196 0.405
27109 0.511
13039 0.479
1257 0.486
29 0.966
0 0.0
0 0.0
0 0.0
0.5004344258338562
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3921,	 Acc1 = 0.2849,	 Acc2 = 0.2873

 ===== Epoch 215	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.1918743   1.8159239
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762  0.28120023 -0.01738757
 -0.01438999 -0.01411033] 2 2
train:	 Loss = 1.2668,	 Acc = 0.5155
45193 0.296
88653 0.569
46209 0.618
5473 0.596
880 0.497
88 0.477
0 0.0
0 0.0
0.5856492785008103
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2853,	 Acc = 0.5065
5196 0.359
27109 0.519
13039 0.54
1257 0.505
29 0.0
0 0.0
0 0.0
0 0.0
0.5249794854467346
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 0
Testing:	 Loss = 1.3513,	 Acc1 = 0.2890,	 Acc2 = 0.2922

 ===== Epoch 216	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167    1.13594     2.9761615  -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-1.3513822e-02 -1.6987352e-02  2.0342596e-02  2.5701299e-02
  7.4981754e-03  5.3155841e-03 -1.3048708e-02 -8.5879778e-03
 -1.8088287e+00 -3.7136407e+00 -7.8236684e-03  2.0873190e-03
 -1.4389989e-02 -1.4110334e-02] 1 1
train:	 Loss = 1.2672,	 Acc = 0.5168
45190 0.297
88652 0.572
46212 0.616
5475 0.599
879 0.511
88 0.42
0 0.0
0 0.0
0.5871583655329569
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.3029,	 Acc = 0.5041
5196 0.348
27109 0.514
13039 0.546
1257 0.523
29 0.034
0 0.0
0 0.0
0 0.0
0.5236279384080706
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 1
Testing:	 Loss = 1.3795,	 Acc1 = 0.2686,	 Acc2 = 0.2676

 ===== Epoch 217	 =====
[-0.3681874  -0.38585573  1.7769424   1.5172313  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.00374216  0.1598157   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 2 2
train:	 Loss = 1.2666,	 Acc = 0.5160
45195 0.298
88648 0.57
46210 0.615
5475 0.603
880 0.5
88 0.511
0 0.0
0 0.0
0.5857566471574865
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.3370,	 Acc = 0.4991
5196 0.363
27109 0.523
13039 0.501
1257 0.531
29 0.0
0 0.0
0 0.0
0 0.0
0.5161944296954192
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3974,	 Acc1 = 0.2954,	 Acc2 = 0.3000

 ===== Epoch 218	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 2 1
train:	 Loss = 1.2674,	 Acc = 0.5145
45194 0.297
88649 0.569
46210 0.613
5475 0.602
880 0.494
88 0.477
0 0.0
0 0.0
0.584025703811694
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2913,	 Acc = 0.5133
5196 0.361
27109 0.514
13039 0.571
1257 0.527
29 0.966
0 0.0
0 0.0
0 0.0
0.5324371289279336
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 0
Testing:	 Loss = 1.3766,	 Acc1 = 0.2401,	 Acc2 = 0.2333

 ===== Epoch 219	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 5 1
train:	 Loss = 1.2674,	 Acc = 0.5150
45195 0.297
88652 0.57
46206 0.613
5475 0.599
880 0.503
88 0.466
0 0.0
0 0.0
0.5848437024507965
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2549,	 Acc = 0.5259
5196 0.386
27109 0.536
13039 0.557
1257 0.558
29 0.966
0 0.0
0 0.0
0 0.0
0.543514987691268
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3247,	 Acc1 = 0.2771,	 Acc2 = 0.2778

 ===== Epoch 220	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-1.3513822e-02 -1.6987352e-02  1.0466025e+00  2.6367407e+00
  7.4981754e-03  5.3155841e-03 -1.3048708e-02 -8.5879778e-03
  1.9501297e-02  3.3417623e-02 -7.8236684e-03  2.0873190e-03
 -1.4389989e-02 -1.4110334e-02] 2 2
train:	 Loss = 1.2669,	 Acc = 0.5153
45194 0.295
88650 0.57
46209 0.615
5475 0.609
880 0.476
88 0.409
0 0.0
0 0.0
0.5856534231645695
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.3130,	 Acc = 0.4960
5196 0.399
27109 0.512
13039 0.504
1257 0.477
29 0.138
0 0.0
0 0.0
0 0.0
0.5081816865376261
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3729,	 Acc1 = 0.2785,	 Acc2 = 0.2796

 ===== Epoch 221	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006  2.8476524   2.1244376
 -0.35938287 -0.366167    2.429043    1.066367   -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013  -0.36016324  0.03297393
 -0.01304871 -0.00858798 -0.01393884  0.07664252 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 3 2
train:	 Loss = 1.2678,	 Acc = 0.5161
45197 0.297
88649 0.571
46209 0.615
5473 0.601
880 0.489
88 0.477
0 0.0
0 0.0
0.5863735765999759
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2527,	 Acc = 0.5287
5196 0.396
27109 0.542
13039 0.556
1257 0.517
29 0.0
0 0.0
0 0.0
0 0.0
0.5453492301008833
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3305,	 Acc1 = 0.2624,	 Acc2 = 0.2602

 ===== Epoch 222	 =====
[-0.3681874  -0.38585573  1.6776115   3.0838745  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-1.3513822e-02 -1.6987352e-02 -2.5412126e+00 -3.1176047e+00
  7.4981754e-03  5.3155841e-03  2.1545572e+00  3.6918054e+00
  1.9501297e-02  3.3417623e-02 -7.8236684e-03  2.0873190e-03
 -1.4389989e-02 -1.4110334e-02] 5 6
train:	 Loss = 1.2673,	 Acc = 0.5150
45193 0.298
88652 0.569
46209 0.615
5474 0.601
880 0.485
88 0.545
0 0.0
0 0.0
0.5844957290361846
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.3744,	 Acc = 0.4840
5196 0.365
27109 0.506
13039 0.483
1257 0.509
29 0.966
0 0.0
0 0.0
0 0.0
0.4989863397210021
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.4346,	 Acc1 = 0.2841,	 Acc2 = 0.2863

 ===== Epoch 223	 =====
[-0.3681874  -0.38585573  2.2278485   3.0153625   2.372004    0.7896321
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.29450247  0.10323618  0.03151905  0.3393433
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 2 2
train:	 Loss = 1.2671,	 Acc = 0.5156
45194 0.297
88650 0.57
46209 0.616
5475 0.601
880 0.509
88 0.455
0 0.0
0 0.0
0.5856109609205814
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.3201,	 Acc = 0.4908
5196 0.386
27109 0.509
13039 0.494
1257 0.5
29 0.103
0 0.0
0 0.0
0 0.0
0.5039581020418015
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 0
Testing:	 Loss = 1.3849,	 Acc1 = 0.2839,	 Acc2 = 0.2860

 ===== Epoch 224	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  1.6061869   2.5372732
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.153271   -0.00904119
 -0.01438999 -0.01411033] 3 0
train:	 Loss = 1.2657,	 Acc = 0.5163
45191 0.299
88653 0.57
46210 0.617
5474 0.603
880 0.498
88 0.534
0 0.0
0 0.0
0.5859099111850253
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2948,	 Acc = 0.5031
5196 0.39
27109 0.514
13039 0.526
1257 0.519
29 0.0
0 0.0
0 0.0
0 0.0
0.5173528985857025
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3657,	 Acc1 = 0.2715,	 Acc2 = 0.2711

 ===== Epoch 225	 =====
[ 1.9978535   2.6759307  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [ 8.0090559e-01 -6.9381207e-02  2.0342596e-02  2.5701299e-02
  7.4981754e-03  5.3155841e-03  1.1657013e+00  3.2383258e+00
  1.9501297e-02  3.3417623e-02 -7.8236684e-03  2.0873190e-03
 -1.4389989e-02 -1.4110334e-02] 2 2
train:	 Loss = 1.2662,	 Acc = 0.5168
45195 0.297
88648 0.573
46210 0.615
5475 0.607
880 0.501
88 0.398
0 0.0
0 0.0
0.5871508340351448
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2747,	 Acc = 0.5097
5196 0.389
27109 0.524
13039 0.529
1257 0.516
29 0.0
0 0.0
0 0.0
0 0.0
0.5248588116039967
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3357,	 Acc1 = 0.2971,	 Acc2 = 0.3019

 ===== Epoch 226	 =====
[-0.3681874  -0.38585573  2.471025    1.8597918  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.22564706  0.25411487  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 0 1
train:	 Loss = 1.2676,	 Acc = 0.5158
45193 0.297
88653 0.57
46208 0.616
5474 0.612
880 0.497
88 0.455
0 0.0
0 0.0
0.5857695873406793
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2673,	 Acc = 0.5148
5196 0.39
27109 0.519
13039 0.557
1257 0.512
29 0.0
0 0.0
0 0.0
0 0.0
0.5304580779070329
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 0
Testing:	 Loss = 1.3385,	 Acc1 = 0.2783,	 Acc2 = 0.2793

 ===== Epoch 227	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  1.550298    2.9041028
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762  0.12078802 -0.0312982
 -0.01438999 -0.01411033] 4 0
train:	 Loss = 1.2673,	 Acc = 0.5158
45193 0.297
88649 0.57
46211 0.616
5475 0.607
880 0.491
88 0.443
0 0.0
0 0.0
0.585819126274743
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.3096,	 Acc = 0.5041
5196 0.389
27109 0.509
13039 0.538
1257 0.515
29 0.966
0 0.0
0 0.0
0 0.0
0.518559637013081
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 1
Testing:	 Loss = 1.3776,	 Acc1 = 0.2758,	 Acc2 = 0.2763

 ===== Epoch 228	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-1.3513822e-02 -1.6987352e-02  2.0342596e-02  2.5701299e-02
  2.7929544e+00  1.0925014e+00 -1.3048708e-02 -8.5879778e-03
  1.9501297e-02  3.3417623e-02 -7.8236684e-03  2.0873190e-03
 -1.4389989e-02 -1.4110334e-02] 2 2
train:	 Loss = 1.2689,	 Acc = 0.5155
45193 0.298
88650 0.569
46211 0.615
5474 0.6
880 0.505
88 0.455
0 0.0
0 0.0
0.585132658188432
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2852,	 Acc = 0.5252
5196 0.396
27109 0.54
13039 0.544
1257 0.532
29 0.966
0 0.0
0 0.0
0 0.0
0.5413669932905344
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3644,	 Acc1 = 0.2567,	 Acc2 = 0.2532

 ===== Epoch 229	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
  1.3297236   2.603529   -0.41133043 -0.41984478  2.68607     2.0153415
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
  0.07161742 -0.00858798  0.0195013   0.03341762 -0.01203222  0.00208732
 -0.01438999 -0.01411033] 0 0
train:	 Loss = 1.2672,	 Acc = 0.5168
45197 0.296
88650 0.573
46209 0.615
5472 0.604
880 0.509
88 0.5
0 0.0
0 0.0
0.5873573061380477
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2635,	 Acc = 0.5218
5196 0.407
27109 0.534
13039 0.544
1257 0.51
29 0.0
0 0.0
0 0.0
0 0.0
0.5361780180528068
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 0
Testing:	 Loss = 1.3368,	 Acc1 = 0.2709,	 Acc2 = 0.2704

 ===== Epoch 230	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167    1.3478967   1.9135323  -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.13415259  0.9654545  -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
train:	 Loss = 1.2674,	 Acc = 0.5159
45193 0.298
88650 0.571
46210 0.614
5475 0.601
880 0.498
88 0.5
0 0.0
0 0.0
0.5856705094725518
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2731,	 Acc = 0.5149
5196 0.396
27109 0.522
13039 0.55
1257 0.504
29 0.0
0 0.0
0 0.0
0 0.0
0.5298064391562485
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3398,	 Acc1 = 0.2878,	 Acc2 = 0.2908

 ===== Epoch 231	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006  2.345729    1.4615139
 -0.35938287 -0.366167    0.8806692   1.7005167  -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013  -0.01897465  0.63081974
 -0.01304871 -0.00858798  0.14438863  1.3950019  -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
train:	 Loss = 1.2669,	 Acc = 0.5157
45195 0.297
88650 0.571
46208 0.615
5475 0.597
880 0.503
88 0.489
0 0.0
0 0.0
0.5857141846129893
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2906,	 Acc = 0.4998
5196 0.378
27109 0.513
13039 0.522
1257 0.498
29 0.0
0 0.0
0 0.0
0 0.0
0.5151083651107786
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3552,	 Acc1 = 0.2855,	 Acc2 = 0.2880

 ===== Epoch 232	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167    2.968463    2.7337644  -0.42690593 -0.4244443
  1.5941257   0.90773076] [ 2.0195446   1.5453023   0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.2965728   0.42784482 -0.00782367  0.00208732
 -0.14120546  0.4790336 ] 4 4
train:	 Loss = 1.2676,	 Acc = 0.5171
45193 0.296
88650 0.574
46211 0.615
5475 0.603
879 0.488
88 0.489
0 0.0
0 0.0
0.5877794526655485
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2537,	 Acc = 0.5272
5196 0.383
27109 0.54
13039 0.556
1257 0.551
29 0.0
0 0.0
0 0.0
0 0.0
0.5453250953323358
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3249,	 Acc1 = 0.2777,	 Acc2 = 0.2786

 ===== Epoch 233	 =====
[-0.3681874  -0.38585573  2.354795    2.967404    2.1618056   0.903852
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-1.3513822e-02 -1.6987352e-02 -3.3677166e+00 -3.0107322e+00
  7.7321726e-01  4.6486965e-01 -1.3048708e-02 -8.5879778e-03
  1.9501297e-02  3.3417623e-02 -7.8236684e-03  2.0873190e-03
 -1.4389989e-02 -1.4110334e-02] 2 2
train:	 Loss = 1.2662,	 Acc = 0.5161
45192 0.298
88654 0.57
46210 0.616
5472 0.607
880 0.482
88 0.455
0 0.0
0 0.0
0.5858291343486384
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.3280,	 Acc = 0.5013
5196 0.381
27109 0.511
13039 0.529
1257 0.519
29 0.0
0 0.0
0 0.0
0 0.0
0.5164599121494425
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3924,	 Acc1 = 0.2847,	 Acc2 = 0.2870

 ===== Epoch 234	 =====
[ 2.5781603   2.3743525  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
  2.20686     2.579635  ] [-0.3802695   0.05445882  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
  0.01339712  0.0874193 ] 3 0
train:	 Loss = 1.2682,	 Acc = 0.5153
45194 0.297
88648 0.57
46212 0.614
5474 0.604
880 0.505
88 0.477
0 0.0
0 0.0
0.5851721843993716
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2430,	 Acc = 0.5337
5196 0.417
27109 0.539
13039 0.571
1257 0.52
29 0.31
0 0.0
0 0.0
0 0.0
0.5483660761693295
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 0
Testing:	 Loss = 1.3196,	 Acc1 = 0.2674,	 Acc2 = 0.2662

 ===== Epoch 235	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006  3.310166   -4.115106
 -0.35938287 -0.366167    3.0383432   2.2048984  -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-1.3513822e-02 -1.6987352e-02  2.0342596e-02  2.5701299e-02
 -4.7442179e+00  3.5732424e+00 -1.3048708e-02 -8.5879778e-03
 -6.3072696e-02  6.1695373e-01 -7.8236684e-03  2.0873190e-03
  2.2738941e+00  1.4247096e+00] 4 4
train:	 Loss = 1.2670,	 Acc = 0.5160
45196 0.297
88651 0.571
46208 0.615
5473 0.605
880 0.502
88 0.466
0 0.0
0 0.0
0.5858740268931352
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2576,	 Acc = 0.5217
5196 0.359
27109 0.536
13039 0.557
1257 0.53
29 0.0
0 0.0
0 0.0
0 0.0
0.5420427668098663
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3272,	 Acc1 = 0.2851,	 Acc2 = 0.2875

 ===== Epoch 236	 =====
[ 0.38634267  1.7175251  -0.42115992 -0.34173006 -0.4351751  -0.3592861
  3.742243    1.6022522  -0.41133043 -0.41984478  4.5973983   1.7986904
  4.6419806   3.4338737 ] [ 0.1251798   0.17829885  0.0203426   0.0257013   0.00749818  0.00531558
  0.82896703  0.20605904  0.0195013   0.03341762 -0.15888338  0.15510426
 -0.16293098  0.04100575] 2 3
train:	 Loss = 1.2675,	 Acc = 0.5165
45194 0.298
88649 0.571
46210 0.616
5475 0.599
880 0.492
88 0.5
0 0.0
0 0.0
0.5862620486617316
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.3067,	 Acc = 0.4992
5196 0.363
27109 0.5
13039 0.544
1257 0.591
29 0.0
0 0.0
0 0.0
0 0.0
0.516315103538157
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3710,	 Acc1 = 0.2874,	 Acc2 = 0.2903

 ===== Epoch 237	 =====
[-0.3681874  -0.38585573  3.314723   -4.3542547   2.434213    1.0359888
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-1.3513822e-02 -1.6987352e-02  1.5465450e-01  6.7125607e+00
 -6.9956303e-02  1.6062783e-01 -1.3048708e-02 -8.5879778e-03
  1.6936009e+00  1.5868124e+00 -7.8236684e-03  2.0873190e-03
 -1.4389989e-02 -1.4110334e-02] 4 2
train:	 Loss = 1.2656,	 Acc = 0.5165
45191 0.296
88653 0.571
46209 0.615
5475 0.611
880 0.506
88 0.477
0 0.0
0 0.0
0.5868511376101341
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2840,	 Acc = 0.5083
5196 0.348
27109 0.52
13039 0.543
1257 0.568
29 0.0
0 0.0
0 0.0
0 0.0
0.5283583530433943
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3512,	 Acc1 = 0.2971,	 Acc2 = 0.3019

 ===== Epoch 238	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 1
train:	 Loss = 1.2677,	 Acc = 0.5168
45196 0.297
88646 0.573
46211 0.614
5475 0.6
880 0.498
88 0.42
0 0.0
0 0.0
0.5871479122434536
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.3348,	 Acc = 0.4887
5196 0.4
27109 0.499
13039 0.501
1257 0.491
29 0.966
0 0.0
0 0.0
0 0.0
0.499831056620167
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.4070,	 Acc1 = 0.2562,	 Acc2 = 0.2527

 ===== Epoch 239	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167    1.5096062   2.1510324  -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.12118665 -0.06113684 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 3 3
train:	 Loss = 1.2675,	 Acc = 0.5161
45197 0.297
88649 0.572
46210 0.614
5474 0.6
878 0.499
88 0.432
0 0.0
0 0.0
0.5861400292995704
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2875,	 Acc = 0.5136
5196 0.412
27109 0.528
13039 0.526
1257 0.513
29 0.0
0 0.0
0 0.0
0 0.0
0.5264275715595887
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3601,	 Acc1 = 0.2727,	 Acc2 = 0.2726

 ===== Epoch 240	 =====
[ 2.2518482   1.6689659  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
  2.3649845   1.8560138 ] [ 0.8857154   0.7617759   0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.12705967  0.9199622 ] 4 4
train:	 Loss = 1.2669,	 Acc = 0.5154
45196 0.295
88646 0.572
46211 0.613
5475 0.595
880 0.491
88 0.443
0 0.0
0 0.0
0.5857891012031139
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2711,	 Acc = 0.5109
5196 0.384
27109 0.523
13039 0.537
1257 0.515
29 0.0
0 0.0
0 0.0
0 0.0
0.5268378626248974
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 1
Testing:	 Loss = 1.3436,	 Acc1 = 0.2787,	 Acc2 = 0.2798

 ===== Epoch 241	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  1.6276248   1.3481549
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
  1.4077891   1.8809103   0.0195013   0.03341762  0.6983688   0.39714926
 -0.01438999 -0.01411033] 2 2
train:	 Loss = 1.2674,	 Acc = 0.5156
45199 0.295
88649 0.571
46208 0.614
5474 0.612
879 0.503
87 0.494
0 0.0
0 0.0
0.5861058621202149
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2924,	 Acc = 0.4957
5196 0.371
27109 0.508
13039 0.519
1257 0.521
29 0.0
0 0.0
0 0.0
0 0.0
0.5113192064488101
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 1
Testing:	 Loss = 1.3585,	 Acc1 = 0.2798,	 Acc2 = 0.2811

 ===== Epoch 242	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
  2.2732491   1.1585046  -0.41133043 -0.41984478  3.0206382   1.7272941
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -3.012089   -1.6290218   3.4590843   1.9596272  -4.219769   -2.429491
 -0.01438999 -0.01411033] 5 6
train:	 Loss = 1.2668,	 Acc = 0.5156
45194 0.296
88651 0.571
46209 0.614
5474 0.603
880 0.49
88 0.5
0 0.0
0 0.0
0.5857454246932103
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2558,	 Acc = 0.5278
5196 0.406
27109 0.535
13039 0.562
1257 0.533
29 0.0
0 0.0
0 0.0
0 0.0
0.5430564270888643
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 0
Testing:	 Loss = 1.3270,	 Acc1 = 0.2699,	 Acc2 = 0.2691

 ===== Epoch 243	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
  1.937169    2.9323022 ] [ 2.3521409e+00  2.8765826e+00  2.0342596e-02  2.5701299e-02
  7.4981754e-03  5.3155841e-03 -1.3048708e-02 -8.5879778e-03
  1.9501297e-02  3.3417623e-02 -7.8236684e-03  2.0873190e-03
 -1.4389989e-02 -1.4110334e-02] 0 0
train:	 Loss = 1.2666,	 Acc = 0.5154
45196 0.297
88646 0.57
46211 0.615
5475 0.604
880 0.489
88 0.5
0 0.0
0 0.0
0.5851167728237792
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2775,	 Acc = 0.5122
5196 0.404
27109 0.528
13039 0.527
1257 0.484
29 0.138
0 0.0
0 0.0
0 0.0
0.5258242023458995
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3438,	 Acc1 = 0.2723,	 Acc2 = 0.2721

 ===== Epoch 244	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  1.4500045   1.685441
 -0.39521116 -0.38799495] [ 2.4198542   2.3502624   0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -2.300889   -2.382195
  2.7447786   3.0172741 ] 5 5
train:	 Loss = 1.2669,	 Acc = 0.5166
45194 0.298
88647 0.571
46212 0.615
5475 0.603
880 0.502
88 0.5
0 0.0
0 0.0
0.5864955910036659
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2660,	 Acc = 0.5116
5196 0.397
27109 0.516
13039 0.547
1257 0.522
29 0.103
0 0.0
0 0.0
0 0.0
0.5258966066515423
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3332,	 Acc1 = 0.2798,	 Acc2 = 0.2811

 ===== Epoch 245	 =====
[-0.3681874  -0.38585573  1.03711     2.967404   -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.08322364 -1.0807424   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 5 5
train:	 Loss = 1.2684,	 Acc = 0.5153
45194 0.298
88647 0.57
46213 0.614
5475 0.601
879 0.485
88 0.432
0 0.0
0 0.0
0.5848183323661378
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.3070,	 Acc = 0.4929
5196 0.389
27109 0.517
13039 0.489
1257 0.462
29 0.0
0 0.0
0 0.0
0 0.0
0.5059130182941546
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 0
Testing:	 Loss = 1.3748,	 Acc1 = 0.2849,	 Acc2 = 0.2873

 ===== Epoch 246	 =====
[-0.3681874  -0.38585573  1.4566916   2.4969544  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.10887928  0.1828666   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 1 4
train:	 Loss = 1.2670,	 Acc = 0.5153
45196 0.297
88652 0.57
46207 0.614
5473 0.602
880 0.497
88 0.432
0 0.0
0 0.0
0.5850176928520877
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2886,	 Acc = 0.5070
5196 0.371
27109 0.512
13039 0.543
1257 0.587
29 0.31
0 0.0
0 0.0
0 0.0
0.5239899599362842
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3567,	 Acc1 = 0.2808,	 Acc2 = 0.2823

 ===== Epoch 247	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  1.6299216   3.0542815
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
  3.2955453   1.6511474   0.0195013   0.03341762 -0.10977733  0.11615449
 -0.01438999 -0.01411033] 2 6
train:	 Loss = 1.2668,	 Acc = 0.5147
45195 0.298
88650 0.569
46211 0.612
5472 0.604
880 0.499
88 0.455
0 0.0
0 0.0
0.5840298370146
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.3002,	 Acc = 0.4989
5196 0.336
27109 0.519
13039 0.523
1257 0.51
29 0.0
0 0.0
0 0.0
0 0.0
0.5193078148380557
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 0
Testing:	 Loss = 1.3658,	 Acc1 = 0.2763,	 Acc2 = 0.2768

 ===== Epoch 248	 =====
[-0.3681874  -0.38585573  2.3358362   1.0079582  -0.4351751  -0.3592861
  4.949955    2.6689534  -0.41133043 -0.41984478  4.748605    2.7834675
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -3.3445773  -1.2127613   0.9938589   1.7477915
 -6.0613375  -3.2343392   0.0195013   0.03341762 -6.3308663  -3.623023
 -0.01438999 -0.01411033] 5 6
train:	 Loss = 1.2665,	 Acc = 0.5151
45193 0.297
88648 0.569
46212 0.616
5475 0.603
880 0.508
88 0.466
0 0.0
0 0.0
0.5848212706028888
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2750,	 Acc = 0.5080
5196 0.383
27109 0.507
13039 0.556
1257 0.525
29 0.966
0 0.0
0 0.0
0 0.0
0.5237244774822609
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3412,	 Acc1 = 0.2936,	 Acc2 = 0.2977

 ===== Epoch 249	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  0.7295725   3.251237
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458  0.04381921
 -0.01438999 -0.01411033] 1 0
train:	 Loss = 1.2662,	 Acc = 0.5151
45193 0.298
88651 0.568
46210 0.615
5474 0.606
880 0.501
88 0.523
0 0.0
0 0.0
0.5846514228289562
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2584,	 Acc = 0.5173
5196 0.389
27109 0.522
13039 0.559
1257 0.522
29 0.207
0 0.0
0 0.0
0 0.0
0.5333301153641936
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 0
Testing:	 Loss = 1.3425,	 Acc1 = 0.2664,	 Acc2 = 0.2649

 ===== Epoch 250	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
  4.413451    1.7217228  -0.41133043 -0.41984478  2.8912518   2.7342288
 -0.39521116 -0.38799495] [-1.3513822e-02 -1.6987352e-02  2.0342596e-02  2.5701299e-02
  7.4981754e-03  5.3155841e-03 -5.4501634e+00 -2.2276149e+00
  1.9501297e-02  3.3417623e-02 -4.0616951e+00 -3.5673804e+00
 -1.4389989e-02 -1.4110334e-02] 1 1
train:	 Loss = 1.2676,	 Acc = 0.5142
45197 0.295
88649 0.57
46209 0.612
5473 0.604
880 0.499
88 0.443
0 0.0
0 0.0
0.584285805278169
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.3139,	 Acc = 0.4983
5196 0.37
27109 0.516
13039 0.512
1257 0.515
29 0.0
0 0.0
0 0.0
0 0.0
0.5144808611285417
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3751,	 Acc1 = 0.2926,	 Acc2 = 0.2965

 ===== Epoch 251	 =====
[-0.3681874  -0.38585573  0.71108866  3.072456   -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-1.3513822e-02 -1.6987352e-02 -1.3615693e+00 -3.1071270e+00
  7.4981754e-03  5.3155841e-03 -1.3048708e-02 -8.5879778e-03
  1.9501297e-02  3.3417623e-02 -7.8236684e-03  2.0873190e-03
 -1.4389989e-02 -1.4110334e-02] 5 6
train:	 Loss = 1.2666,	 Acc = 0.5152
45191 0.297
88651 0.569
46213 0.615
5474 0.607
880 0.501
87 0.517
0 0.0
0 0.0
0.5850252998832313
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.3079,	 Acc = 0.4978
5196 0.337
27109 0.513
13039 0.527
1257 0.538
29 0.0
0 0.0
0 0.0
0 0.0
0.518004537336487
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3685,	 Acc1 = 0.2810,	 Acc2 = 0.2826

 ===== Epoch 252	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 3 1
train:	 Loss = 1.2654,	 Acc = 0.5170
45195 0.297
88651 0.572
46207 0.616
5475 0.603
880 0.514
88 0.477
0 0.0
0 0.0
0.5872499133056384
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2945,	 Acc = 0.5114
5196 0.378
27109 0.519
13039 0.548
1257 0.526
29 0.276
0 0.0
0 0.0
0 0.0
0.528092870589371
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 0
Testing:	 Loss = 1.3718,	 Acc1 = 0.2651,	 Acc2 = 0.2634

 ===== Epoch 253	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  0.87388813  2.465877
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762  0.12640038 -0.15371177
 -0.01438999 -0.01411033] 2 3
train:	 Loss = 1.2672,	 Acc = 0.5149
45193 0.297
88650 0.569
46210 0.615
5475 0.604
880 0.492
88 0.466
0 0.0
0 0.0
0.5845877299137315
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.3485,	 Acc = 0.4867
5196 0.364
27109 0.516
13039 0.475
1257 0.501
29 0.0
0 0.0
0 0.0
0 0.0
0.5021238596321861
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.4153,	 Acc1 = 0.2806,	 Acc2 = 0.2821

 ===== Epoch 254	 =====
[ 1.5262883   0.9891368  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
  1.5941257   1.1689657 ] [-0.06924615  1.2380837   0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.11846942  1.5204375 ] 6 6
train:	 Loss = 1.2674,	 Acc = 0.5159
45195 0.297
88652 0.571
46207 0.616
5474 0.594
880 0.506
88 0.489
0 0.0
0 0.0
0.5858769577002286
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.3229,	 Acc = 0.4989
5196 0.384
27109 0.512
13039 0.516
1257 0.52
29 0.379
0 0.0
0 0.0
0 0.0
0.5133947965439012
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3896,	 Acc1 = 0.2839,	 Acc2 = 0.2860

 ===== Epoch 255	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  1.4209116   1.8282338
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -2.2653456  -2.5435584
 -0.01438999 -0.01411033] 6 6
train:	 Loss = 1.2677,	 Acc = 0.5152
45195 0.298
88650 0.57
46209 0.613
5474 0.606
880 0.505
88 0.455
0 0.0
0 0.0
0.5847587773618022
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2998,	 Acc = 0.5064
5196 0.393
27109 0.52
13039 0.523
1257 0.488
29 0.966
0 0.0
0 0.0
0 0.0
0.5205628228025293
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 0
Testing:	 Loss = 1.3648,	 Acc1 = 0.2886,	 Acc2 = 0.2917

 ===== Epoch 256	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
  3.338705    1.9947983  -0.41133043 -0.41984478  3.5366552   2.7268429
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
  0.0279604  -0.01463437  0.0195013   0.03341762 -0.5844705  -0.0424267
 -0.01438999 -0.01411033] 2 0
train:	 Loss = 1.2678,	 Acc = 0.5159
45197 0.298
88647 0.57
46210 0.617
5474 0.602
880 0.494
88 0.466
0 0.0
0 0.0
0.585722475035209
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2964,	 Acc = 0.4971
5196 0.37
27109 0.512
13039 0.516
1257 0.521
29 0.0
0 0.0
0 0.0
0 0.0
0.5129603707100449
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3586,	 Acc1 = 0.2893,	 Acc2 = 0.2925

 ===== Epoch 257	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 6 1
train:	 Loss = 1.2677,	 Acc = 0.5155
45194 0.297
88651 0.57
46208 0.614
5475 0.604
880 0.489
88 0.42
0 0.0
0 0.0
0.5852641859280123
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.3048,	 Acc = 0.4956
5196 0.387
27109 0.513
13039 0.503
1257 0.497
29 0.0
0 0.0
0 0.0
0 0.0
0.509195346816624
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 0
Testing:	 Loss = 1.3841,	 Acc1 = 0.2618,	 Acc2 = 0.2594

 ===== Epoch 258	 =====
[-0.3681874  -0.38585573  3.1679919   3.0290647   2.8414714   1.2532307
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-1.3513822e-02 -1.6987352e-02 -4.3602252e+00 -3.0673118e+00
 -9.2996709e-02  5.1805878e-01 -1.3048708e-02 -8.5879778e-03
  3.0154955e+00  2.0622861e+00 -7.8236684e-03  2.0873190e-03
 -1.4389989e-02 -1.4110334e-02] 2 6
train:	 Loss = 1.2677,	 Acc = 0.5155
45196 0.297
88648 0.57
46212 0.614
5473 0.604
879 0.519
88 0.477
0 0.0
0 0.0
0.5855343241330503
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2803,	 Acc = 0.5242
5196 0.376
27109 0.53
13039 0.568
1257 0.58
29 0.0
0 0.0
0 0.0
0 0.0
0.5427668098662933
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3473,	 Acc1 = 0.2967,	 Acc2 = 0.3014

 ===== Epoch 259	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 1 1
train:	 Loss = 1.2672,	 Acc = 0.5157
45192 0.297
88653 0.571
46208 0.615
5475 0.601
880 0.497
88 0.466
0 0.0
0 0.0
0.5857017494196909
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.3650,	 Acc = 0.4733
5196 0.382
27109 0.501
13039 0.455
1257 0.464
29 0.0
0 0.0
0 0.0
0 0.0
0.48477096104648354
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 1
Testing:	 Loss = 1.4268,	 Acc1 = 0.2779,	 Acc2 = 0.2788

 ===== Epoch 260	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 1 1
train:	 Loss = 1.2676,	 Acc = 0.5154
45193 0.298
88652 0.569
46208 0.615
5475 0.601
880 0.497
88 0.443
0 0.0
0 0.0
0.5847221927347614
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2514,	 Acc = 0.5259
5196 0.391
27109 0.538
13039 0.552
1257 0.551
29 0.069
0 0.0
0 0.0
0 0.0
0.5427909446348409
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 0
Testing:	 Loss = 1.3262,	 Acc1 = 0.2562,	 Acc2 = 0.2527

 ===== Epoch 261	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  1.6846616   1.6706694
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.42452386 -0.03408032
 -0.01438999 -0.01411033] 1 1
train:	 Loss = 1.2664,	 Acc = 0.5160
45196 0.298
88648 0.57
46209 0.616
5475 0.602
880 0.489
88 0.409
0 0.0
0 0.0
0.5856121726822364
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2582,	 Acc = 0.5125
5196 0.399
27109 0.525
13039 0.532
1257 0.522
29 0.0
0 0.0
0 0.0
0 0.0
0.5267171887821596
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3249,	 Acc1 = 0.2932,	 Acc2 = 0.2972

 ===== Epoch 262	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006  2.9392285   2.9956446
 -0.35938287 -0.366167    1.9404484   2.2587643  -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798 -0.11971691  0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 0 0
train:	 Loss = 1.2680,	 Acc = 0.5148
45195 0.297
88650 0.57
46210 0.613
5473 0.601
880 0.501
88 0.432
0 0.0
0 0.0
0.584532310457817
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2874,	 Acc = 0.5146
5196 0.377
27109 0.527
13039 0.546
1257 0.51
29 0.0
0 0.0
0 0.0
0 0.0
0.5317854901771492
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3540,	 Acc1 = 0.3027,	 Acc2 = 0.3086

 ===== Epoch 263	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  1.216113    1.21521
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08545852  1.426536
 -0.01438999 -0.01411033] 6 6
train:	 Loss = 1.2678,	 Acc = 0.5157
45194 0.296
88653 0.57
46206 0.615
5475 0.609
880 0.491
88 0.443
0 0.0
0 0.0
0.5859577359131506
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.3326,	 Acc = 0.4876
5196 0.403
27109 0.517
13039 0.466
1257 0.449
29 0.0
0 0.0
0 0.0
0 0.0
0.49816575759038473
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3937,	 Acc1 = 0.2765,	 Acc2 = 0.2771

 ===== Epoch 264	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006  2.8955662   2.45142
 -0.35938287 -0.366167    1.9623939   1.6760322  -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-1.3513822e-02 -1.6987352e-02  4.4829078e+00  2.7813327e+00
 -1.5427411e-01 -1.4520665e+00 -1.3048708e-02 -8.5879778e-03
 -2.7854071e+00 -2.2791145e+00 -7.8236684e-03  2.0873190e-03
 -1.4389989e-02 -1.4110334e-02] 5 5
train:	 Loss = 1.2684,	 Acc = 0.5170
45196 0.297
88650 0.573
46209 0.615
5473 0.608
880 0.502
88 0.443
0 0.0
0 0.0
0.5873743807501769
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.3047,	 Acc = 0.5061
5196 0.395
27109 0.522
13039 0.516
1257 0.512
29 0.31
0 0.0
0 0.0
0 0.0
0.5200077231259352
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3705,	 Acc1 = 0.2870,	 Acc2 = 0.2898

 ===== Epoch 265	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 5 1
train:	 Loss = 1.2688,	 Acc = 0.5143
45192 0.296
88651 0.57
46210 0.611
5475 0.606
880 0.505
88 0.386
0 0.0
0 0.0
0.5840740531053615
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2873,	 Acc = 0.5060
5196 0.409
27109 0.519
13039 0.519
1257 0.51
29 0.0
0 0.0
0 0.0
0 0.0
0.51817348071632
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3628,	 Acc1 = 0.2697,	 Acc2 = 0.2689

 ===== Epoch 266	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  1.5713524   1.9636407
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762  0.16521709  0.6364121
 -0.01438999 -0.01411033] 6 4
train:	 Loss = 1.2670,	 Acc = 0.5147
45196 0.296
88646 0.569
46211 0.614
5475 0.597
880 0.492
88 0.489
0 0.0
0 0.0
0.5845859872611465
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2972,	 Acc = 0.5027
5196 0.366
27109 0.517
13039 0.529
1257 0.504
29 0.0
0 0.0
0 0.0
0 0.0
0.5198629145146498
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3632,	 Acc1 = 0.2800,	 Acc2 = 0.2813

 ===== Epoch 267	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  1.3217659   2.0079556
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762  0.10629012 -0.07859434
 -0.01438999 -0.01411033] 3 3
train:	 Loss = 1.2676,	 Acc = 0.5167
45195 0.299
88650 0.571
46210 0.615
5474 0.607
879 0.503
88 0.443
0 0.0
0 0.0
0.5864785104139391
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2922,	 Acc = 0.5053
5196 0.42
27109 0.526
13039 0.5
1257 0.479
29 0.0
0 0.0
0 0.0
0 0.0
0.5159530820099435
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 1
Testing:	 Loss = 1.3557,	 Acc1 = 0.2831,	 Acc2 = 0.2850

 ===== Epoch 268	 =====
[-0.3681874  -0.38585573  1.1685896   1.6611067  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.10435245  1.0881388   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
train:	 Loss = 1.2667,	 Acc = 0.5168
45195 0.296
88653 0.573
46206 0.616
5475 0.596
879 0.498
88 0.375
0 0.0
0 0.0
0.5872428362148888
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2760,	 Acc = 0.5109
5196 0.379
27109 0.515
13039 0.551
1257 0.57
29 0.0
0 0.0
0 0.0
0 0.0
0.5275136361442294
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3481,	 Acc1 = 0.2694,	 Acc2 = 0.2686

 ===== Epoch 269	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 0 1
train:	 Loss = 1.2671,	 Acc = 0.5158
45195 0.297
88646 0.571
46213 0.613
5474 0.609
880 0.493
88 0.443
0 0.0
0 0.0
0.5858628035187295
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.3221,	 Acc = 0.5003
5196 0.415
27109 0.526
13039 0.484
1257 0.479
29 0.0
0 0.0
0 0.0
0 0.0
0.5110054544576917
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3845,	 Acc1 = 0.2810,	 Acc2 = 0.2826

 ===== Epoch 270	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 1 1
train:	 Loss = 1.2675,	 Acc = 0.5167
45194 0.298
88651 0.572
46208 0.616
5475 0.602
880 0.509
88 0.455
0 0.0
0 0.0
0.5867432874269296
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2447,	 Acc = 0.5334
5196 0.411
27109 0.543
13039 0.563
1257 0.525
29 0.0
0 0.0
0 0.0
0 0.0
0.5486798281604479
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3200,	 Acc1 = 0.2682,	 Acc2 = 0.2671

 ===== Epoch 271	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006  1.5945811   2.2274594
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013  -0.24055137 -0.03298059
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 1 1
train:	 Loss = 1.2670,	 Acc = 0.5166
45196 0.298
88651 0.57
46206 0.617
5475 0.611
880 0.492
88 0.489
0 0.0
0 0.0
0.5864685067232838
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2984,	 Acc = 0.5022
5196 0.378
27109 0.51
13039 0.537
1257 0.495
29 0.483
0 0.0
0 0.0
0 0.0
0.5178597287252015
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3620,	 Acc1 = 0.2890,	 Acc2 = 0.2922

 ===== Epoch 272	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 6 1
train:	 Loss = 1.2674,	 Acc = 0.5155
45194 0.296
88648 0.571
46211 0.615
5475 0.599
880 0.495
88 0.489
0 0.0
0 0.0
0.5856958854085575
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2798,	 Acc = 0.5118
5196 0.412
27109 0.516
13039 0.543
1257 0.525
29 0.0
0 0.0
0 0.0
0 0.0
0.5242795771588551
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3614,	 Acc1 = 0.2463,	 Acc2 = 0.2408

 ===== Epoch 273	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006  0.4118336   2.655224
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-1.3513822e-02 -1.6987352e-02  2.0342596e-02  2.5701299e-02
 -1.0671022e+00 -2.8583870e+00 -1.3048708e-02 -8.5879778e-03
  1.9501297e-02  3.3417623e-02 -7.8236684e-03  2.0873190e-03
 -1.4389989e-02 -1.4110334e-02] 6 6
train:	 Loss = 1.2672,	 Acc = 0.5160
45195 0.297
88651 0.571
46209 0.615
5473 0.594
880 0.507
88 0.466
0 0.0
0 0.0
0.5860184995152193
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2606,	 Acc = 0.5130
5196 0.381
27109 0.518
13039 0.553
1257 0.541
29 0.0
0 0.0
0 0.0
0 0.0
0.5295409567022252
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 0
Testing:	 Loss = 1.3243,	 Acc1 = 0.2870,	 Acc2 = 0.2898

 ===== Epoch 274	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 0 1
train:	 Loss = 1.2669,	 Acc = 0.5157
45197 0.297
88650 0.57
46209 0.615
5473 0.604
879 0.51
88 0.455
0 0.0
0 0.0
0.5857932469444228
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.3124,	 Acc = 0.4895
5196 0.362
27109 0.505
13039 0.508
1257 0.479
29 0.966
0 0.0
0 0.0
0 0.0
0.5055027272288459
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3773,	 Acc1 = 0.2725,	 Acc2 = 0.2724

 ===== Epoch 275	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
  0.82897806  0.98087656] [-1.3513822e-02 -1.6987352e-02  2.0342596e-02  2.5701299e-02
  7.4981754e-03  5.3155841e-03 -1.3048708e-02 -8.5879778e-03
  3.1192250e+00  3.5211265e+00 -7.8236684e-03  2.0873190e-03
 -1.4225439e+00 -1.5341539e+00] 5 5
train:	 Loss = 1.2667,	 Acc = 0.5166
45194 0.298
88649 0.572
46211 0.614
5474 0.61
880 0.5
88 0.409
0 0.0
0 0.0
0.5865734384509773
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2638,	 Acc = 0.5064
5196 0.376
27109 0.518
13039 0.535
1257 0.5
29 0.0
0 0.0
0 0.0
0 0.0
0.5226625476661679
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 0
Testing:	 Loss = 1.3457,	 Acc1 = 0.2595,	 Acc2 = 0.2567

 ===== Epoch 276	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [ 9.3669420e-01  3.5981889e+00  2.0342596e-02  2.5701299e-02
  7.4981754e-03  5.3155841e-03 -1.3048708e-02 -8.5879778e-03
  1.9501297e-02  3.3417623e-02 -7.8236684e-03  2.0873190e-03
 -1.4389989e-02 -1.4110334e-02] 3 5
train:	 Loss = 1.2668,	 Acc = 0.5156
45195 0.296
88646 0.571
46212 0.615
5475 0.601
880 0.498
88 0.489
0 0.0
0 0.0
0.5857212617037388
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2793,	 Acc = 0.5147
5196 0.388
27109 0.519
13039 0.557
1257 0.534
29 0.0
0 0.0
0 0.0
0 0.0
0.5306511560554135
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 1
Testing:	 Loss = 1.3474,	 Acc1 = 0.2893,	 Acc2 = 0.2925

 ===== Epoch 277	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006  2.861949    2.2431366
 -0.35938287 -0.366167    2.0501795   1.9967797  -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-1.3513822e-02 -1.6987352e-02  2.0342596e-02  2.5701299e-02
 -4.1755648e+00 -2.4669151e+00 -1.3048708e-02 -8.5879778e-03
  1.0549081e-01  1.0329934e+00 -7.8236684e-03  2.0873190e-03
 -1.4389989e-02 -1.4110334e-02] 6 4
train:	 Loss = 1.2684,	 Acc = 0.5153
45191 0.297
88651 0.571
46211 0.613
5475 0.597
880 0.487
88 0.443
0 0.0
0 0.0
0.5850465305544743
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2613,	 Acc = 0.5197
5196 0.406
27109 0.535
13039 0.533
1257 0.528
29 0.0
0 0.0
0 0.0
0 0.0
0.5340058888835256
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3340,	 Acc1 = 0.2719,	 Acc2 = 0.2716

 ===== Epoch 278	 =====
[-0.3681874  -0.38585573  1.1220152   1.5788924  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.08272135 -0.03087821  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 3 3
train:	 Loss = 1.2671,	 Acc = 0.5155
45194 0.296
88651 0.57
46208 0.615
5475 0.607
880 0.501
88 0.466
0 0.0
0 0.0
0.5856251150019108
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2845,	 Acc = 0.5134
5196 0.404
27109 0.518
13039 0.547
1257 0.512
29 0.966
0 0.0
0 0.0
0 0.0
0.5271516146160158
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 1
Testing:	 Loss = 1.3532,	 Acc1 = 0.2793,	 Acc2 = 0.2806

 ===== Epoch 279	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  1.6957628   2.3083127
 -0.39521116 -0.38799495] [ 2.4427729   2.5360227   0.0203426   0.0257013   0.00749818  0.00531558
  2.5236433   2.8846118   0.0195013   0.03341762 -2.6011376  -3.0860727
 -0.01438999 -0.01411033] 6 5
train:	 Loss = 1.2665,	 Acc = 0.5165
45195 0.297
88651 0.571
46208 0.616
5475 0.602
879 0.505
88 0.466
0 0.0
0 0.0
0.5866695918641764
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2903,	 Acc = 0.5261
5196 0.402
27109 0.531
13039 0.565
1257 0.532
29 0.172
0 0.0
0 0.0
0 0.0
0.5417531495872955
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3584,	 Acc1 = 0.2824,	 Acc2 = 0.2843

 ===== Epoch 280	 =====
[ 4.772678    3.0874062  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
  2.6873837   3.2849696 ] [-1.9866244e+00 -6.8381822e-01  2.0342596e-02  2.5701299e-02
  7.4981754e-03  5.3155841e-03 -1.3048708e-02 -8.5879778e-03
  1.9501297e-02  3.3417623e-02 -7.8236684e-03  2.0873190e-03
 -3.5602207e+00 -4.0927005e+00] 6 1
train:	 Loss = 1.2675,	 Acc = 0.5156
45195 0.298
88652 0.57
46207 0.614
5474 0.604
880 0.497
88 0.489
0 0.0
0 0.0
0.5853603300755126
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.3267,	 Acc = 0.5060
5196 0.366
27109 0.522
13039 0.527
1257 0.527
29 0.034
0 0.0
0 0.0
0 0.0
0.5235313993338804
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3918,	 Acc1 = 0.2802,	 Acc2 = 0.2816

 ===== Epoch 281	 =====
[-0.3681874  -0.38585573  2.49493     3.0975769   2.095733    0.8590599
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-1.35138221e-02 -1.69873517e-02 -3.53875184e+00 -3.13017797e+00
  1.07012585e-01  4.01042700e-01 -1.30487084e-02 -8.58797785e-03
  1.95012968e-02  3.34176235e-02 -7.82366842e-03  2.08731904e-03
 -1.43899890e-02 -1.41103342e-02] 2 4
train:	 Loss = 1.2666,	 Acc = 0.5154
45191 0.298
88654 0.57
46208 0.614
5475 0.606
880 0.491
88 0.477
0 0.0
0 0.0
0.5849899154311595
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2778,	 Acc = 0.5192
5196 0.381
27109 0.523
13039 0.561
1257 0.571
29 0.931
0 0.0
0 0.0
0 0.0
0.5364435005068301
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3523,	 Acc1 = 0.2622,	 Acc2 = 0.2599

 ===== Epoch 282	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  1.1452947   0.8902334
  3.0826945   3.057695  ] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762  0.11236946  0.38323858
  0.14880283  0.37750396] 6 4
train:	 Loss = 1.2668,	 Acc = 0.5152
45198 0.298
88649 0.569
46207 0.614
5474 0.606
880 0.492
88 0.511
0 0.0
0 0.0
0.5847004203881159
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.3321,	 Acc = 0.4924
5196 0.385
27109 0.511
13039 0.494
1257 0.512
29 0.966
0 0.0
0 0.0
0 0.0
0.5058164792199643
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3961,	 Acc1 = 0.2923,	 Acc2 = 0.2962

 ===== Epoch 283	 =====
[-0.3681874  -0.38585573  1.6063074   2.508373   -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-1.3513822e-02 -1.6987352e-02  9.3787268e-02 -1.2819140e+00
  7.4981754e-03  5.3155841e-03  4.2844009e+00  4.2934217e+00
  1.9501297e-02  3.3417623e-02 -7.8236684e-03  2.0873190e-03
 -1.4389989e-02 -1.4110334e-02] 5 5
train:	 Loss = 1.2672,	 Acc = 0.5151
45198 0.297
88647 0.57
46208 0.614
5475 0.603
880 0.489
88 0.455
0 0.0
0 0.0
0.584933969341392
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.3280,	 Acc = 0.4760
5196 0.401
27109 0.499
13039 0.461
1257 0.452
29 0.0
0 0.0
0 0.0
0 0.0
0.48542259979726793
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 1
Testing:	 Loss = 1.3912,	 Acc1 = 0.2928,	 Acc2 = 0.2967

 ===== Epoch 284	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 1 1
train:	 Loss = 1.2674,	 Acc = 0.5160
45194 0.298
88650 0.572
46209 0.614
5475 0.596
880 0.476
88 0.489
0 0.0
0 0.0
0.5858232721405218
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.3205,	 Acc = 0.4853
5196 0.378
27109 0.508
13039 0.483
1257 0.484
29 0.0
0 0.0
0 0.0
0 0.0
0.4987691268040739
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3797,	 Acc1 = 0.2878,	 Acc2 = 0.2908

 ===== Epoch 285	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  1.5740325   0.96162975
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762  0.19467984  0.37767437
 -0.01438999 -0.01411033] 2 2
train:	 Loss = 1.2672,	 Acc = 0.5153
45196 0.297
88647 0.57
46211 0.613
5474 0.601
880 0.494
88 0.443
0 0.0
0 0.0
0.5851804670912951
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2330,	 Acc = 0.5319
5196 0.403
27109 0.534
13039 0.58
1257 0.531
29 0.0
0 0.0
0 0.0
0 0.0
0.5480523241782111
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3108,	 Acc1 = 0.2653,	 Acc2 = 0.2637

 ===== Epoch 286	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  1.70074     1.5918872
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.14298311  0.04103709
 -0.01438999 -0.01411033] 3 0
train:	 Loss = 1.2664,	 Acc = 0.5166
45195 0.297
88651 0.571
46207 0.616
5475 0.61
880 0.491
88 0.489
0 0.0
0 0.0
0.5869172900404102
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2838,	 Acc = 0.5229
5196 0.393
27109 0.524
13039 0.568
1257 0.589
29 0.138
0 0.0
0 0.0
0 0.0
0.5391707293527055
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3549,	 Acc1 = 0.2752,	 Acc2 = 0.2756

 ===== Epoch 287	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 1 1
train:	 Loss = 1.2667,	 Acc = 0.5148
45197 0.295
88648 0.569
46209 0.614
5474 0.608
880 0.502
88 0.511
0 0.0
0 0.0
0.5849156752701717
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2978,	 Acc = 0.4987
5196 0.367
27109 0.516
13039 0.517
1257 0.501
29 0.0
0 0.0
0 0.0
0 0.0
0.5152290389535165
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 0
Testing:	 Loss = 1.3755,	 Acc1 = 0.2573,	 Acc2 = 0.2540

 ===== Epoch 288	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006  1.5679207   1.262189
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.15750325  0.00318802
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 0 0
train:	 Loss = 1.2669,	 Acc = 0.5148
45194 0.296
88652 0.569
46207 0.614
5475 0.603
880 0.492
88 0.477
0 0.0
0 0.0
0.5846980226748383
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2838,	 Acc = 0.5037
5196 0.4
27109 0.517
13039 0.52
1257 0.496
29 0.0
0 0.0
0 0.0
0 0.0
0.5167012598349182
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 0
Testing:	 Loss = 1.3554,	 Acc1 = 0.2818,	 Acc2 = 0.2835

 ===== Epoch 289	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 1 1
train:	 Loss = 1.2668,	 Acc = 0.5167
45193 0.297
88650 0.571
46211 0.616
5474 0.601
880 0.507
88 0.489
0 0.0
0 0.0
0.5868594438900802
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2867,	 Acc = 0.5022
5196 0.375
27109 0.516
13039 0.521
1257 0.531
29 0.966
0 0.0
0 0.0
0 0.0
0.5180769416421297
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 1
Testing:	 Loss = 1.3531,	 Acc1 = 0.2948,	 Acc2 = 0.2992

 ===== Epoch 290	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   2.0850818   1.192497
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 2 1
train:	 Loss = 1.2670,	 Acc = 0.5158
45197 0.297
88647 0.57
46209 0.616
5475 0.599
880 0.497
88 0.432
0 0.0
0 0.0
0.5857649381807373
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2961,	 Acc = 0.5133
5196 0.383
27109 0.524
13039 0.543
1257 0.537
29 0.0
0 0.0
0 0.0
0 0.0
0.5297340348506058
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3697,	 Acc1 = 0.2647,	 Acc2 = 0.2629

 ===== Epoch 291	 =====
[ 2.8461983   3.312312   -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
  2.5881162   3.512244  ] [-0.08321133 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.2351812  -0.01411033] 0 0
train:	 Loss = 1.2672,	 Acc = 0.5154
45196 0.297
88644 0.569
46213 0.614
5475 0.612
880 0.499
88 0.477
0 0.0
0 0.0
0.5851663128096249
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.3442,	 Acc = 0.4836
5196 0.4
27109 0.504
13039 0.472
1257 0.489
29 0.966
0 0.0
0 0.0
0 0.0
0.4940869817058454
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.4014,	 Acc1 = 0.2987,	 Acc2 = 0.3039

 ===== Epoch 292	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  1.370382    2.6012838
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762  0.00293265 -0.03686245
 -0.01438999 -0.01411033] 2 0
train:	 Loss = 1.2669,	 Acc = 0.5157
45197 0.297
88652 0.569
46205 0.616
5474 0.61
880 0.49
88 0.5
0 0.0
0 0.0
0.5855455452621745
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2646,	 Acc = 0.5168
5196 0.39
27109 0.528
13039 0.544
1257 0.53
29 0.0
0 0.0
0 0.0
0 0.0
0.5327026113819568
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3329,	 Acc1 = 0.2857,	 Acc2 = 0.2883

 ===== Epoch 293	 =====
[ 0.7226418   3.4835472  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.99910355 -3.6226375   0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762  1.855446    2.2055314
 -0.01438999 -0.01411033] 6 6
train:	 Loss = 1.2677,	 Acc = 0.5163
45192 0.297
88653 0.572
46208 0.615
5475 0.604
880 0.477
88 0.443
0 0.0
0 0.0
0.586550982279341
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2647,	 Acc = 0.5207
5196 0.372
27109 0.529
13039 0.562
1257 0.541
29 0.138
0 0.0
0 0.0
0 0.0
0.539363807501086
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 0
Testing:	 Loss = 1.3418,	 Acc1 = 0.2639,	 Acc2 = 0.2619

 ===== Epoch 294	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.1922565   1.9611787
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762  0.10067776 -0.60719836
 -0.01438999 -0.01411033] 5 5
train:	 Loss = 1.2673,	 Acc = 0.5164
45195 0.296
88650 0.571
46209 0.616
5474 0.605
880 0.486
88 0.466
0 0.0
0 0.0
0.586782825316169
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2677,	 Acc = 0.5308
5196 0.382
27109 0.541
13039 0.567
1257 0.565
29 0.0
0 0.0
0 0.0
0 0.0
0.5493556016797799
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3426,	 Acc1 = 0.2734,	 Acc2 = 0.2734

 ===== Epoch 295	 =====
[-0.3681874  -0.38585573  1.6322727   2.725328    2.7456455   0.977759
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-1.3513822e-02 -1.6987352e-02 -2.4858766e+00 -2.7886055e+00
  7.2206490e-02  8.5421407e-01 -1.3048708e-02 -8.5879778e-03
  1.9501297e-02  3.3417623e-02 -7.8236684e-03  2.0873190e-03
 -1.4389989e-02 -1.4110334e-02] 6 6
train:	 Loss = 1.2675,	 Acc = 0.5159
45197 0.297
88647 0.57
46211 0.616
5474 0.608
879 0.493
88 0.5
0 0.0
0 0.0
0.5858923276173221
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2768,	 Acc = 0.5094
5196 0.357
27109 0.527
13039 0.532
1257 0.523
29 0.0
0 0.0
0 0.0
0 0.0
0.5284548921175846
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3394,	 Acc1 = 0.2860,	 Acc2 = 0.2885

 ===== Epoch 296	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 3 1
train:	 Loss = 1.2683,	 Acc = 0.5158
45197 0.296
88649 0.571
46208 0.614
5474 0.609
880 0.482
88 0.477
0 0.0
0 0.0
0.5859984854811429
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2931,	 Acc = 0.5095
5196 0.396
27109 0.526
13039 0.524
1257 0.483
29 0.069
0 0.0
0 0.0
0 0.0
0.5237968817879036
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3634,	 Acc1 = 0.2680,	 Acc2 = 0.2669

 ===== Epoch 297	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 3 1
train:	 Loss = 1.2668,	 Acc = 0.5154
45197 0.296
88650 0.571
46208 0.614
5473 0.6
880 0.5
88 0.489
0 0.0
0 0.0
0.5856375487441525
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2848,	 Acc = 0.5155
5196 0.408
27109 0.528
13039 0.536
1257 0.498
29 0.0
0 0.0
0 0.0
0 0.0
0.528937587488536
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 4
Testing:	 Loss = 1.3485,	 Acc1 = 0.2851,	 Acc2 = 0.2875

 ===== Epoch 298	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [ 2.9720144e+00  9.5829964e+00  2.0342596e-02  2.5701299e-02
  7.4981754e-03  5.3155841e-03 -1.3048708e-02 -8.5879778e-03
  1.9501297e-02  3.3417623e-02 -7.8236684e-03  2.0873190e-03
 -1.4389989e-02 -1.4110334e-02] 2 5
train:	 Loss = 1.2674,	 Acc = 0.5153
45196 0.296
88651 0.571
46208 0.613
5473 0.6
880 0.505
88 0.443
0 0.0
0 0.0
0.5854140127388535
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.3506,	 Acc = 0.4799
5196 0.377
27109 0.493
13039 0.492
1257 0.484
29 0.966
0 0.0
0 0.0
0 0.0
0.4927837042042767
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 0
Testing:	 Loss = 1.4172,	 Acc1 = 0.2732,	 Acc2 = 0.2731

 ===== Epoch 299	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 1 1
train:	 Loss = 1.2666,	 Acc = 0.5162
45192 0.297
88651 0.571
46210 0.616
5475 0.605
880 0.501
88 0.432
0 0.0
0 0.0
0.5863528279454226
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.2862,	 Acc = 0.5072
5196 0.366
27109 0.523
13039 0.528
1257 0.551
29 0.0
0 0.0
0 0.0
0 0.0
0.524955350678187
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 0
Testing:	 Loss = 1.3563,	 Acc1 = 0.2725,	 Acc2 = 0.2724

 ===== Epoch 300	 =====
[-0.3681874  -0.38585573 -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  1.5656106   3.241389
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762  0.16615248  0.03269071
 -0.01438999 -0.01411033] 4 0
train:	 Loss = 1.2665,	 Acc = 0.5155
45199 0.298
88646 0.57
46210 0.613
5473 0.603
880 0.498
88 0.42
0 0.0
0 0.0
0.5849734955448452
0.5663223439687214
[ 2.0165598   3.4605455  -0.42115992 -0.34173006 -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478  2.22824     1.5081811
 -0.39521116 -0.38799495] [-0.07568967 -0.44328284  0.0203426   0.0257013   0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.08031458 -0.4430529
 -0.01438999 -0.01411033] 5 5
val:	 Loss = 1.3013,	 Acc = 0.5140
5196 0.382
27109 0.513
13039 0.567
1257 0.547
29 0.448
0 0.0
0 0.0
0 0.0
0.5305787517497708
0.5663223439687214
[-0.3681874  -0.38585573  1.0960498   2.3507953  -0.4351751  -0.3592861
 -0.35938287 -0.366167   -0.41133043 -0.41984478 -0.42690593 -0.4244443
 -0.39521116 -0.38799495] [-0.01351382 -0.01698735 -0.00430691  0.21639518  0.00749818  0.00531558
 -0.01304871 -0.00858798  0.0195013   0.03341762 -0.00782367  0.00208732
 -0.01438999 -0.01411033] 4 0
Testing:	 Loss = 1.3879,	 Acc1 = 0.2414,	 Acc2 = 0.2348
