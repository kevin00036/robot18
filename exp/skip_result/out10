(0.24300000000005184, array([-1.000000e+00, -1.000000e+00,  1.189783e+03,  1.790000e-01,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 4, array([  0.   ,   0.   , -15.851,   0.091,   0.   ,   0.   ,   0.   ,
         0.   ,   0.   ,   0.   ,   0.   ,   0.   ,   0.   ,   0.   ]), 1)
((0.236, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.364685e+03, -3.300000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), array([0., 0., 0.])), (0.24, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.543573e+03, -3.290000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), array([0., 0., 0.])))
(0.236, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.364685e+03, -3.300000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([0.   , 0.   , 0.   , 0.   , 5.823, 3.129, 0.   , 0.   , 0.   ,
       0.   , 0.   , 0.   , 0.   , 0.   ]))
(0.236, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.364685e+03, -3.300000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([ 0.   ,  0.   ,  0.   ,  0.   , -8.411, -0.011,  0.   ,  0.   ,
        0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ]))
(0.236, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.364685e+03, -3.300000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  6.2433e+01,
       -3.0000e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
        0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]))
(0.236, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.364685e+03, -3.300000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([ 0.00000e+00,  0.00000e+00,  0.00000e+00,  0.00000e+00,
       -1.19043e+02, -4.00000e-03,  0.00000e+00,  0.00000e+00,
        0.00000e+00,  0.00000e+00,  0.00000e+00,  0.00000e+00,
        0.00000e+00,  0.00000e+00]))
(0.236, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.364685e+03, -3.300000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  5.0464e+01,
       -1.3000e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
        0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]))
(0.236, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.364685e+03, -3.300000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([ 0.00000e+00,  0.00000e+00,  0.00000e+00,  0.00000e+00,
       -1.53656e+02, -1.00000e-02,  0.00000e+00,  0.00000e+00,
        0.00000e+00,  0.00000e+00,  0.00000e+00,  0.00000e+00,
        0.00000e+00,  0.00000e+00]))
(0.236, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.364685e+03, -3.300000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([ 0.000000e+00,  0.000000e+00,  2.399328e+03,  1.498000e+00,
       -1.455700e+01, -6.000000e-03,  0.000000e+00,  0.000000e+00,
        0.000000e+00,  0.000000e+00,  0.000000e+00,  0.000000e+00,
        0.000000e+00,  0.000000e+00]))
(0.236, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.364685e+03, -3.300000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  1.9086e+01,
       -9.0000e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
        0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]))
(0.236, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.364685e+03, -3.300000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([ 0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  7.052e+01,
       -2.000e-02,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,
        0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00]))
(0.24, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.543573e+03, -3.290000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([   0.   ,    0.   ,    0.   ,    0.   , -173.065,    3.128,
          0.   ,    0.   ,    0.   ,    0.   ,    0.   ,    0.   ,
          0.   ,    0.   ]))
14 1 14

 ===== Epoch 1	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  1.2792907   2.3134902  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [ 3.2990801e+00  3.0369763e+00  7.6247272e-03  8.5936040e-03
  3.9517106e-03  1.5506904e-03  1.4607789e-01  1.2675546e+00
  6.1251274e-03  1.2857955e-02  3.1886040e-03  7.7467631e-03
 -6.7966389e-03 -7.4470886e-03] 4 4
train:	 Loss = 1.4804,	 Acc = 0.4515
24284 0.273
47778 0.493
24635 0.536
3191 0.528
536 0.487
56 0.339
0 0.0
0 0.0
0.5083337708016169
0.0
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.3701,	 Acc = 0.4803
2967 0.377
14640 0.508
6803 0.47
700 0.439
20 0.55
0 0.0
0 0.0
0 0.0
0.4942020484591436
0.4942020484591436
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.4510,	 Acc1 = 0.2909,	 Acc2 = 0.2945

 ===== Epoch 2	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576   2.5771797   0.8595068  -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-1.7898616e-03 -1.3066782e-03  7.6247272e-03  8.5936040e-03
  1.9392498e+00  2.9001644e+00 -4.8065083e-03 -4.2258762e-03
 -4.2738004e+00 -1.6953213e+00  3.1886040e-03  7.7467631e-03
 -6.7966389e-03 -7.4470886e-03] 5 5
train:	 Loss = 1.3640,	 Acc = 0.5013
24288 0.287
47774 0.55
24636 0.602
3191 0.627
535 0.492
56 0.339
0 0.0
0 0.0
0.5695742335153297
0.4942020484591436
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.2905,	 Acc = 0.5225
2967 0.4
14640 0.556
6803 0.512
700 0.453
20 0.0
0 0.0
0 0.0
0 0.0
0.5389162117041917
0.5389162117041917
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3825,	 Acc1 = 0.3006,	 Acc2 = 0.3062

 ===== Epoch 3	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-1.7898616e-03 -1.3066782e-03  7.6247272e-03  8.5936040e-03
  3.9517106e-03  1.5506904e-03 -4.8065083e-03 -4.2258762e-03
  3.9689314e+00  2.2162097e+00  3.1886040e-03  7.7467631e-03
  6.2398701e+00  2.0468144e+00] 6 2
train:	 Loss = 1.3371,	 Acc = 0.5144
24286 0.292
47782 0.566
24630 0.62
3190 0.633
536 0.463
56 0.339
0 0.0
0 0.0
0.5852298081213744
0.5389162117041917
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.2965,	 Acc = 0.5168
2967 0.377
14640 0.548
6803 0.523
700 0.419
20 0.0
0 0.0
0 0.0
0 0.0
0.5355321932951316
0.5389162117041917
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3877,	 Acc1 = 0.3041,	 Acc2 = 0.3104

 ===== Epoch 4	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563  1.1874062   1.8832692
 -0.38958293 -0.38242146] [ 2.5224063e+00  1.7296257e+00  7.6247272e-03  8.5936040e-03
  3.9517106e-03  1.5506904e-03 -4.8065083e-03 -4.2258762e-03
  6.1251274e-03  1.2857955e-02 -2.5375266e+00 -3.2052488e+00
 -6.7966389e-03 -7.4470886e-03] 6 6
train:	 Loss = 1.3225,	 Acc = 0.5201
24284 0.291
47782 0.576
24632 0.627
3190 0.613
536 0.476
56 0.304
0 0.0
0 0.0
0.5932463646385637
0.5389162117041917
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2380,	 Acc = 0.5431
2967 0.41
14640 0.579
6803 0.539
700 0.416
20 0.0
0 0.0
0 0.0
0 0.0
0.561025131976718
0.561025131976718
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3348,	 Acc1 = 0.3233,	 Acc2 = 0.3335

 ===== Epoch 5	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495   3.0715652   1.6143
 -0.36261833 -0.3698576   2.1319566   1.364126   -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936  -0.39556217  0.63401794
 -0.00480651 -0.00422588 -0.12401696  0.9068272   0.0031886   0.00774676
 -0.00679664 -0.00744709] 6 6
train:	 Loss = 1.3094,	 Acc = 0.5257
24289 0.293
47777 0.582
24635 0.635
3187 0.631
536 0.457
56 0.286
0 0.0
0 0.0
0.600031499783439
0.561025131976718
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.2834,	 Acc = 0.5426
2967 0.385
14640 0.592
6803 0.519
700 0.426
20 0.0
0 0.0
0 0.0
0 0.0
0.5637323467039661
0.5637323467039661
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3597,	 Acc1 = 0.3431,	 Acc2 = 0.3574

 ===== Epoch 6	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495   2.5215974   2.2256708
 -0.36261833 -0.3698576   2.4033356   1.1884987  -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.46579096  0.05756255
 -0.00480651 -0.00422588  0.45503142  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 3 2
train:	 Loss = 1.3034,	 Acc = 0.5282
24292 0.292
47777 0.586
24629 0.637
3190 0.632
536 0.485
56 0.339
0 0.0
0 0.0
0.6034677377014753
0.5637323467039661
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.2679,	 Acc = 0.5475
2967 0.384
14640 0.599
6803 0.517
700 0.471
20 0.0
0 0.0
0 0.0
0 0.0
0.569372377385733
0.569372377385733
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3531,	 Acc1 = 0.3433,	 Acc2 = 0.3576

 ===== Epoch 7	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 1 1
train:	 Loss = 1.2959,	 Acc = 0.5303
24284 0.292
47779 0.589
24635 0.64
3190 0.629
536 0.493
56 0.304
0 0.0
0 0.0
0.6060685600293979
0.569372377385733
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 1
val:	 Loss = 1.2193,	 Acc = 0.5581
2967 0.401
14640 0.598
6803 0.556
700 0.436
20 0.0
0 0.0
0 0.0
0 0.0
0.5790732301583721
0.5790732301583721
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 1
Testing:	 Loss = 1.3117,	 Acc1 = 0.3287,	 Acc2 = 0.3400

 ===== Epoch 8	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495   1.804264    1.2141299
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936  -0.3107983   0.349291
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 1
train:	 Loss = 1.2887,	 Acc = 0.5299
24290 0.291
47779 0.587
24628 0.641
3191 0.636
536 0.513
56 0.357
0 0.0
0 0.0
0.6059981624885156
0.5790732301583721
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2168,	 Acc = 0.5741
2967 0.395
14640 0.612
6803 0.59
700 0.394
20 0.0
0 0.0
0 0.0
0 0.0
0.598068853494563
0.598068853494563
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3115,	 Acc1 = 0.3351,	 Acc2 = 0.3477

 ===== Epoch 9	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
  2.2116382   2.1936092 ] [ 2.4044945e+00  2.5977066e+00  7.6247272e-03  8.5936040e-03
  3.9517106e-03  1.5506904e-03 -4.8065083e-03 -4.2258762e-03
  6.1251274e-03  1.2857955e-02  3.1886040e-03  7.7467631e-03
  1.5844472e-01  1.8480614e-01] 4 2
train:	 Loss = 1.2866,	 Acc = 0.5312
24291 0.292
47779 0.588
24630 0.643
3189 0.643
535 0.533
56 0.446
0 0.0
0 0.0
0.6076205226476263
0.598068853494563
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2378,	 Acc = 0.5722
2967 0.377
14640 0.607
6803 0.599
700 0.416
20 0.0
0 0.0
0 0.0
0 0.0
0.5982944547218337
0.5982944547218337
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3360,	 Acc1 = 0.3439,	 Acc2 = 0.3583

 ===== Epoch 10	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 3 1
train:	 Loss = 1.2832,	 Acc = 0.5313
24292 0.291
47774 0.586
24633 0.647
3189 0.654
536 0.526
56 0.375
0 0.0
0 0.0
0.6081272641360844
0.5982944547218337
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2754,	 Acc = 0.5477
2967 0.401
14640 0.601
6803 0.519
700 0.356
20 0.0
0 0.0
0 0.0
0 0.0
0.5673419663402969
0.5982944547218337
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3697,	 Acc1 = 0.3433,	 Acc2 = 0.3576

 ===== Epoch 11	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 1 1
train:	 Loss = 1.2830,	 Acc = 0.5318
24289 0.29
47775 0.586
24634 0.649
3190 0.652
536 0.537
56 0.429
0 0.0
0 0.0
0.6087726896877584
0.5982944547218337
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2441,	 Acc = 0.5673
2967 0.395
14640 0.601
6803 0.589
700 0.401
20 0.0
0 0.0
0 0.0
0 0.0
0.59039841176736
0.5982944547218337
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3365,	 Acc1 = 0.3344,	 Acc2 = 0.3469

 ===== Epoch 12	 =====
[ 1.028692    2.7387292  -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.04460711  0.25493407  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 0 4
train:	 Loss = 1.2813,	 Acc = 0.5329
24289 0.292
47778 0.589
24633 0.647
3189 0.653
535 0.52
56 0.5
0 0.0
0 0.0
0.6096258088225643
0.5982944547218337
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2429,	 Acc = 0.5587
2967 0.395
14640 0.606
6803 0.546
700 0.404
20 0.0
0 0.0
0 0.0
0 0.0
0.5805621982583585
0.5982944547218337
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3301,	 Acc1 = 0.3361,	 Acc2 = 0.3489

 ===== Epoch 13	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 0 1
train:	 Loss = 1.2801,	 Acc = 0.5332
24287 0.29
47777 0.588
24633 0.651
3191 0.656
536 0.547
56 0.5
0 0.0
0 0.0
0.6107910175475437
0.5982944547218337
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2287,	 Acc = 0.5645
2967 0.394
14640 0.604
6803 0.573
700 0.396
20 0.0
0 0.0
0 0.0
0 0.0
0.5873302350764789
0.5982944547218337
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3170,	 Acc1 = 0.3421,	 Acc2 = 0.3561

 ===== Epoch 14	 =====
[ 1.9155846   2.1681018  -0.420647   -0.3362495  -0.4419852  -0.36654198
  0.7640757   1.7559844  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [ 0.27166542  0.43273374  0.00762473  0.0085936   0.00395171  0.00155069
 -0.10317219  0.66861206  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 6 4
train:	 Loss = 1.2808,	 Acc = 0.5320
24284 0.294
47777 0.585
24636 0.649
3191 0.647
536 0.55
56 0.429
0 0.0
0 0.0
0.6078271825292666
0.5982944547218337
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2238,	 Acc = 0.5635
2967 0.388
14640 0.607
6803 0.564
700 0.413
20 0.0
0 0.0
0 0.0
0 0.0
0.587059513603754
0.5982944547218337
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3192,	 Acc1 = 0.3460,	 Acc2 = 0.3608

 ===== Epoch 15	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563  0.9169874   2.4304914
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795 -0.01639137 -0.02288561
 -0.00679664 -0.00744709] 2 0
train:	 Loss = 1.2803,	 Acc = 0.5332
24291 0.289
47780 0.589
24630 0.65
3187 0.648
536 0.558
56 0.393
0 0.0
0 0.0
0.6109805877488876
0.5982944547218337
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.3016,	 Acc = 0.5294
2967 0.391
14640 0.572
6803 0.513
700 0.389
20 0.0
0 0.0
0 0.0
0 0.0
0.5478951405495646
0.5982944547218337
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3953,	 Acc1 = 0.3039,	 Acc2 = 0.3101

 ===== Epoch 16	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576   1.340769    2.979402   -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-1.7898616e-03 -1.3066782e-03  7.6247272e-03  8.5936040e-03
  3.9517106e-03  1.5506904e-03 -4.8065083e-03 -4.2258762e-03
 -1.6113939e-02 -2.0243285e+00  3.1886040e-03  7.7467631e-03
 -6.7966389e-03 -7.4470886e-03] 5 5
train:	 Loss = 1.2774,	 Acc = 0.5335
24289 0.291
47777 0.589
24631 0.651
3191 0.647
536 0.532
56 0.429
0 0.0
0 0.0
0.6107939257917602
0.5982944547218337
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.2479,	 Acc = 0.5539
2967 0.401
14640 0.595
6803 0.545
700 0.431
20 0.0
0 0.0
0 0.0
0 0.0
0.5743356043856879
0.5982944547218337
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3384,	 Acc1 = 0.3386,	 Acc2 = 0.3519

 ===== Epoch 17	 =====
[-0.36757618 -0.38323686  1.6308554   1.2397169  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.14953002  0.60469663  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
train:	 Loss = 1.2751,	 Acc = 0.5341
24288 0.294
47782 0.588
24627 0.651
3191 0.651
536 0.55
56 0.446
0 0.0
0 0.0
0.6107334103317934
0.5982944547218337
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2152,	 Acc = 0.5622
2967 0.41
14640 0.597
6803 0.569
700 0.431
20 0.0
0 0.0
0 0.0
0 0.0
0.5825474890583405
0.5982944547218337
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 6
Testing:	 Loss = 1.3098,	 Acc1 = 0.3386,	 Acc2 = 0.3519

 ===== Epoch 18	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576   1.2673309   2.9769287  -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 0 0
train:	 Loss = 1.2746,	 Acc = 0.5342
24285 0.292
47779 0.588
24635 0.654
3189 0.649
536 0.545
56 0.429
0 0.0
0 0.0
0.6114836931557189
0.5982944547218337
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2236,	 Acc = 0.5720
2967 0.384
14640 0.602
6803 0.606
700 0.423
20 0.0
0 0.0
0 0.0
0 0.0
0.5971213283400262
0.5982944547218337
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3300,	 Acc1 = 0.3115,	 Acc2 = 0.3193

 ===== Epoch 19	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563  1.4978025   2.2619274
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.33901766  0.65102655
 -0.00679664 -0.00744709] 4 4
train:	 Loss = 1.2718,	 Acc = 0.5372
24290 0.292
47782 0.593
24627 0.655
3190 0.653
535 0.551
56 0.429
0 0.0
0 0.0
0.615211970074813
0.5982944547218337
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2394,	 Acc = 0.5597
2967 0.392
14640 0.582
6803 0.596
700 0.457
20 0.55
0 0.0
0 0.0
0 0.0
0.5821865270947074
0.5982944547218337
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3386,	 Acc1 = 0.3285,	 Acc2 = 0.3397

 ===== Epoch 20	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563  0.90680444  2.3303304
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.01920841  0.14729424
 -0.00679664 -0.00744709] 2 4
train:	 Loss = 1.2718,	 Acc = 0.5364
24292 0.294
47778 0.592
24629 0.654
3190 0.648
535 0.538
56 0.357
0 0.0
0 0.0
0.6138368246968027
0.5982944547218337
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2394,	 Acc = 0.5618
2967 0.4
14640 0.604
6803 0.557
700 0.423
20 0.0
0 0.0
0 0.0
0 0.0
0.5833596534765149
0.5982944547218337
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3407,	 Acc1 = 0.3142,	 Acc2 = 0.3226

 ===== Epoch 21	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563  0.8611687   2.5257666
 -0.38958293 -0.38242146] [ 4.6193576e+00  2.7127533e+00  7.6247272e-03  8.5936040e-03
  3.9517106e-03  1.5506904e-03  3.1413357e+00  2.3604302e+00
  6.1251274e-03  1.2857955e-02 -2.0242910e+00 -4.1003947e+00
 -6.7966389e-03 -7.4470886e-03] 6 6
train:	 Loss = 1.2705,	 Acc = 0.5366
24294 0.29
47774 0.592
24631 0.658
3190 0.648
535 0.559
56 0.411
0 0.0
0 0.0
0.615283647914315
0.5982944547218337
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2002,	 Acc = 0.5869
2967 0.405
14640 0.618
6803 0.619
700 0.41
20 0.0
0 0.0
0 0.0
0 0.0
0.6111988449217164
0.6111988449217164
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.2977,	 Acc1 = 0.3421,	 Acc2 = 0.3561

 ===== Epoch 22	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
  2.2573276   2.9878416 ] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.23319052 -0.05373027] 2 3
train:	 Loss = 1.2699,	 Acc = 0.5368
24288 0.293
47779 0.593
24630 0.655
3191 0.643
536 0.55
56 0.482
0 0.0
0 0.0
0.6144739605207896
0.6111988449217164
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2299,	 Acc = 0.5554
2967 0.261
14640 0.6
6803 0.6
700 0.447
20 0.0
0 0.0
0 0.0
0 0.0
0.5949104363127735
0.6111988449217164
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3262,	 Acc1 = 0.3078,	 Acc2 = 0.3549

 ===== Epoch 23	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495   2.7682402   1.6054074
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936  -0.40553486  0.17892158
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 3 3
train:	 Loss = 1.2725,	 Acc = 0.5367
24289 0.29
47777 0.593
24634 0.656
3188 0.644
536 0.545
56 0.375
0 0.0
0 0.0
0.6152301452927511
0.6111988449217164
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2293,	 Acc = 0.5739
2967 0.406
14640 0.613
6803 0.583
700 0.397
20 0.0
0 0.0
0 0.0
0 0.0
0.5963542841673058
0.6111988449217164
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 0
Testing:	 Loss = 1.3328,	 Acc1 = 0.3398,	 Acc2 = 0.3534

 ===== Epoch 24	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576   2.2576065   2.526729   -0.42759606 -0.42288172
  3.6134908   1.801718  ] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588 -0.03753056 -1.0306602   0.0031886   0.00774676
  0.51104313 -1.0790808 ] 5 5
train:	 Loss = 1.2712,	 Acc = 0.5374
24286 0.291
47779 0.594
24634 0.656
3189 0.655
536 0.56
56 0.446
0 0.0
0 0.0
0.6160327584849201
0.6111988449217164
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2486,	 Acc = 0.5512
2967 0.404
14640 0.592
6803 0.545
700 0.389
20 0.0
0 0.0
0 0.0
0 0.0
0.5709064657311735
0.6111988449217164
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 0
Testing:	 Loss = 1.3539,	 Acc1 = 0.3198,	 Acc2 = 0.3293

 ===== Epoch 25	 =====
[-0.36757618 -0.38323686  2.0159125   0.8627566  -0.4419852  -0.36654198
  5.348068    2.2628078  -0.40402457 -0.41193563  4.631188   -5.3771977
 -0.38958293 -0.38242146] [-1.7898616e-03 -1.3066782e-03  1.7642992e-02  1.2690413e-01
  3.9517106e-03  1.5506904e-03 -6.7586863e-01  2.3301759e-01
  6.1251274e-03  1.2857955e-02  1.0405434e-01  1.0912871e+01
 -6.7966389e-03 -7.4470886e-03] 3 2
train:	 Loss = 1.2703,	 Acc = 0.5375
24289 0.294
47779 0.592
24630 0.658
3190 0.643
536 0.563
56 0.375
0 0.0
0 0.0
0.6151251460146212
0.6111988449217164
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2375,	 Acc = 0.5676
2967 0.405
14640 0.616
6803 0.552
700 0.41
20 0.0
0 0.0
0 0.0
0 0.0
0.5894057663673691
0.6111988449217164
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3307,	 Acc1 = 0.3361,	 Acc2 = 0.3489

 ===== Epoch 26	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495   1.8912014   1.7121193
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.05630632  0.10657293
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 3 3
train:	 Loss = 1.2696,	 Acc = 0.5378
24289 0.293
47782 0.592
24628 0.658
3189 0.658
536 0.547
56 0.411
0 0.0
0 0.0
0.6157945164126997
0.6111988449217164
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.2685,	 Acc = 0.5418
2967 0.408
14640 0.59
6803 0.509
700 0.437
20 0.0
0 0.0
0 0.0
0 0.0
0.559716644858548
0.6111988449217164
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3445,	 Acc1 = 0.3487,	 Acc2 = 0.3641

 ===== Epoch 27	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495   2.700068    2.676974
 -0.36261833 -0.3698576   1.6000998   2.2150524  -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-1.7898616e-03 -1.3066782e-03  7.6247272e-03  8.5936040e-03
 -5.1093769e+00 -3.1934590e+00 -4.8065083e-03 -4.2258762e-03
  1.2226531e-01  6.6090256e-01  3.1886040e-03  7.7467631e-03
 -6.7966389e-03 -7.4470886e-03] 6 4
train:	 Loss = 1.2701,	 Acc = 0.5376
24288 0.294
47778 0.594
24634 0.655
3188 0.641
536 0.539
56 0.411
0 0.0
0 0.0
0.6154058168836624
0.6111988449217164
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2018,	 Acc = 0.5804
2967 0.405
14640 0.609
6803 0.614
700 0.426
20 0.0
0 0.0
0 0.0
0 0.0
0.6037991246672382
0.6111988449217164
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3095,	 Acc1 = 0.3377,	 Acc2 = 0.3509

 ===== Epoch 28	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563  1.4566929   3.2586536
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00093957
 -0.00679664 -0.00744709] 0 0
train:	 Loss = 1.2678,	 Acc = 0.5398
24287 0.295
47778 0.597
24632 0.657
3191 0.648
536 0.558
56 0.411
0 0.0
0 0.0
0.6179045319123804
0.6111988449217164
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2355,	 Acc = 0.5594
2967 0.406
14640 0.6
6803 0.552
700 0.446
20 0.0
0 0.0
0 0.0
0 0.0
0.5798402743310923
0.6111988449217164
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3325,	 Acc1 = 0.3386,	 Acc2 = 0.3519

 ===== Epoch 29	 =====
[ 3.1290596   1.8688396  -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576   1.7527156   3.3529193  -0.42759606 -0.42288172
  2.8249216   2.055141  ] [-3.4430428e+00 -2.3231616e+00  7.6247272e-03  8.5936040e-03
  3.9517106e-03  1.5506904e-03 -4.8065083e-03 -4.2258762e-03
 -3.0901699e+00 -5.0452132e+00  4.0147829e+00  2.0805373e+00
 -2.6376477e-01  2.0539348e+00] 6 6
train:	 Loss = 1.2713,	 Acc = 0.5375
24290 0.293
47774 0.594
24636 0.657
3188 0.637
536 0.562
56 0.357
0 0.0
0 0.0
0.6153825961412259
0.6111988449217164
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.3056,	 Acc = 0.5271
2967 0.405
14640 0.573
6803 0.497
700 0.386
20 0.0
0 0.0
0 0.0
0 0.0
0.5433379957586969
0.6111988449217164
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3916,	 Acc1 = 0.3287,	 Acc2 = 0.3400

 ===== Epoch 30	 =====
[ 3.7811766   3.458988   -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563  2.6654675   1.7782221
 -0.38958293 -0.38242146] [-4.0848312e+00 -3.9625793e+00  7.6247272e-03  8.5936040e-03
  3.9517106e-03  1.5506904e-03 -4.8065083e-03 -4.2258762e-03
  6.1251274e-03  1.2857955e-02 -6.3051600e-03  6.5607913e-02
 -6.7966389e-03 -7.4470886e-03] 3 2
train:	 Loss = 1.2691,	 Acc = 0.5388
24290 0.293
47776 0.596
24635 0.657
3188 0.65
535 0.538
56 0.446
0 0.0
0 0.0
0.6172594828717679
0.6111988449217164
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.2588,	 Acc = 0.5509
2967 0.406
14640 0.599
6803 0.526
700 0.411
20 0.0
0 0.0
0 0.0
0 0.0
0.5703199025402699
0.6111988449217164
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3489,	 Acc1 = 0.3348,	 Acc2 = 0.3474

 ===== Epoch 31	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 1 1
train:	 Loss = 1.2688,	 Acc = 0.5397
24285 0.295
47784 0.596
24629 0.657
3190 0.657
536 0.567
56 0.446
0 0.0
0 0.0
0.6177964433361769
0.6111988449217164
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.2825,	 Acc = 0.5358
2967 0.405
14640 0.587
6803 0.499
700 0.39
20 0.0
0 0.0
0 0.0
0 0.0
0.5533095700040608
0.6111988449217164
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3672,	 Acc1 = 0.3423,	 Acc2 = 0.3564

 ===== Epoch 32	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  3.3469827   1.4603376  -0.40402457 -0.41193563  3.3560336   2.0274036
 -0.38958293 -0.38242146] [-1.78986159e-03 -1.30667817e-03  7.62472721e-03  8.59360397e-03
  3.95171065e-03  1.55069039e-03  1.05388653e+00  1.24118954e-01
  6.12512743e-03  1.28579549e-02 -5.94920254e+00 -3.40606093e+00
 -6.79663895e-03 -7.44708860e-03] 2 2
train:	 Loss = 1.2724,	 Acc = 0.5368
24287 0.291
47776 0.593
24634 0.655
3191 0.645
536 0.571
56 0.357
0 0.0
0 0.0
0.6152008714711326
0.6111988449217164
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2614,	 Acc = 0.5541
2967 0.408
14640 0.598
6803 0.542
700 0.4
20 0.0
0 0.0
0 0.0
0 0.0
0.57370392094933
0.6111988449217164
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 0
Testing:	 Loss = 1.3612,	 Acc1 = 0.3322,	 Acc2 = 0.3442

 ===== Epoch 33	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576   2.3763695   1.7524848  -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.04648595 -0.75814897  0.0031886   0.00774676
 -0.00679664 -0.00744709] 2 5
train:	 Loss = 1.2704,	 Acc = 0.5383
24293 0.294
47776 0.594
24629 0.657
3190 0.649
536 0.554
56 0.393
0 0.0
0 0.0
0.6162862430598396
0.6111988449217164
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2233,	 Acc = 0.5748
2967 0.411
14640 0.605
6803 0.6
700 0.399
20 0.0
0 0.0
0 0.0
0 0.0
0.596715246130939
0.6111988449217164
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 0
Testing:	 Loss = 1.3325,	 Acc1 = 0.3084,	 Acc2 = 0.3156

 ===== Epoch 34	 =====
[ 4.752665    2.7209766  -0.420647   -0.3362495  -0.4419852  -0.36654198
  1.1580312   2.7048702  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-5.0409322e+00 -3.2017012e+00  7.6247272e-03  8.5936040e-03
  3.9517106e-03  1.5506904e-03  2.3610707e-01  2.8746691e-01
  6.1251274e-03  1.2857955e-02  3.1886040e-03  7.7467631e-03
 -6.7966389e-03 -7.4470886e-03] 2 4
train:	 Loss = 1.2689,	 Acc = 0.5387
24294 0.294
47773 0.594
24631 0.658
3191 0.648
535 0.557
56 0.464
0 0.0
0 0.0
0.6167799858241672
0.6111988449217164
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.2907,	 Acc = 0.5281
2967 0.384
14640 0.57
6803 0.511
700 0.443
20 0.0
0 0.0
0 0.0
0 0.0
0.5474439380950232
0.6111988449217164
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3932,	 Acc1 = 0.3035,	 Acc2 = 0.3096

 ===== Epoch 35	 =====
[-0.36757618 -0.38323686  2.5459297  -4.8688564  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-1.7898616e-03 -1.3066782e-03  3.9393759e-01  7.3438458e+00
  3.9517106e-03  1.5506904e-03 -4.8065083e-03 -4.2258762e-03
  6.1251274e-03  1.2857955e-02  3.1886040e-03  7.7467631e-03
 -6.7966389e-03 -7.4470886e-03] 2 1
train:	 Loss = 1.2692,	 Acc = 0.5384
24290 0.292
47780 0.594
24631 0.658
3188 0.657
535 0.54
56 0.357
0 0.0
0 0.0
0.6168394802467515
0.6111988449217164
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2311,	 Acc = 0.5651
2967 0.409
14640 0.606
6803 0.562
700 0.417
20 0.0
0 0.0
0 0.0
0 0.0
0.5859766277128547
0.6111988449217164
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3246,	 Acc1 = 0.3462,	 Acc2 = 0.3611

 ===== Epoch 36	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 1 1
train:	 Loss = 1.2687,	 Acc = 0.5383
24288 0.294
47782 0.595
24630 0.655
3189 0.646
535 0.553
56 0.411
0 0.0
0 0.0
0.6162326753464931
0.6111988449217164
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2331,	 Acc = 0.5717
2967 0.407
14640 0.614
6803 0.569
700 0.419
20 0.0
0 0.0
0 0.0
0 0.0
0.5936921896855119
0.6111988449217164
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3239,	 Acc1 = 0.3485,	 Acc2 = 0.3638

 ===== Epoch 37	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  3.1386194   1.1196398  -0.40402457 -0.41193563  3.347737    1.7928798
 -0.38958293 -0.38242146] [-1.7898616e-03 -1.3066782e-03  7.6247272e-03  8.5936040e-03
  3.9517106e-03  1.5506904e-03  2.6945311e-01  1.3967590e-01
  6.1251274e-03  1.2857955e-02  1.6338854e-01  1.0817571e+01
 -6.7966389e-03 -7.4470886e-03] 2 2
train:	 Loss = 1.2686,	 Acc = 0.5388
24293 0.293
47775 0.595
24631 0.659
3189 0.642
536 0.569
56 0.411
0 0.0
0 0.0
0.6171656581831546
0.6111988449217164
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2113,	 Acc = 0.5660
2967 0.41
14640 0.609
6803 0.556
700 0.451
20 0.0
0 0.0
0 0.0
0 0.0
0.5868790326219374
0.6111988449217164
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3030,	 Acc1 = 0.3441,	 Acc2 = 0.3586

 ===== Epoch 38	 =====
[ 1.5835727   2.5383756  -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [ 0.11013643 -0.77787304  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 5 5
train:	 Loss = 1.2689,	 Acc = 0.5384
24288 0.292
47781 0.595
24628 0.659
3191 0.642
536 0.547
56 0.411
0 0.0
0 0.0
0.6169545359092818
0.6111988449217164
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2380,	 Acc = 0.5652
2967 0.409
14640 0.601
6803 0.572
700 0.429
20 0.0
0 0.0
0 0.0
0 0.0
0.5860668682037631
0.6111988449217164
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3358,	 Acc1 = 0.3318,	 Acc2 = 0.3437

 ===== Epoch 39	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576   2.1893313   3.3133411  -0.42759606 -0.42288172
  3.151332    2.4627078 ] [ 5.1276932e+00  2.4800451e+00  7.6247272e-03  8.5936040e-03
  3.9517106e-03  1.5506904e-03 -4.8065083e-03 -4.2258762e-03
  1.2473519e-01 -5.3608160e-02  3.1886040e-03  7.7467631e-03
  3.7440807e-02 -1.8545933e-01] 5 5
train:	 Loss = 1.2702,	 Acc = 0.5389
24289 0.292
47777 0.596
24633 0.659
3189 0.638
536 0.556
56 0.321
0 0.0
0 0.0
0.6175663792311428
0.6111988449217164
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2736,	 Acc = 0.5573
2967 0.405
14640 0.586
6803 0.579
700 0.413
20 0.0
0 0.0
0 0.0
0 0.0
0.5775842620583856
0.6111988449217164
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 0
Testing:	 Loss = 1.3981,	 Acc1 = 0.2767,	 Acc2 = 0.2773

 ===== Epoch 40	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563  1.3846565   0.91830134
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.29629755  0.05880072
 -0.00679664 -0.00744709] 3 2
train:	 Loss = 1.2713,	 Acc = 0.5380
24291 0.294
47775 0.593
24633 0.659
3189 0.634
536 0.563
56 0.321
0 0.0
0 0.0
0.615797556077649
0.6111988449217164
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2033,	 Acc = 0.5823
2967 0.407
14640 0.605
6803 0.627
700 0.434
20 0.0
0 0.0
0 0.0
0 0.0
0.6056490547308577
0.6111988449217164
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 0
Testing:	 Loss = 1.3114,	 Acc1 = 0.3157,	 Acc2 = 0.3243

 ===== Epoch 41	 =====
[-0.36757618 -0.38323686  1.0922636   1.5417393  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.00364388  0.01769441  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 0 0
train:	 Loss = 1.2694,	 Acc = 0.5374
24292 0.293
47783 0.593
24623 0.656
3190 0.64
536 0.573
56 0.393
0 0.0
0 0.0
0.6151887436341681
0.6111988449217164
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2164,	 Acc = 0.5625
2967 0.408
14640 0.6
6803 0.571
700 0.379
20 0.0
0 0.0
0 0.0
0 0.0
0.5832242927401525
0.6111988449217164
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 6
Testing:	 Loss = 1.3043,	 Acc1 = 0.3596,	 Acc2 = 0.3772

 ===== Epoch 42	 =====
[-0.36757618 -0.38323686  1.7901056   3.024601   -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.0082499   0.0063184   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 0 0
train:	 Loss = 1.2671,	 Acc = 0.5394
24289 0.292
47779 0.597
24631 0.658
3189 0.644
536 0.56
56 0.357
0 0.0
0 0.0
0.6182751243585201
0.6111988449217164
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2539,	 Acc = 0.5542
2967 0.404
14640 0.605
6803 0.523
700 0.446
20 0.0
0 0.0
0 0.0
0 0.0
0.5742904841402338
0.6111988449217164
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3470,	 Acc1 = 0.3410,	 Acc2 = 0.3549

 ===== Epoch 43	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  3.5451      1.2547926  -0.40402457 -0.41193563  3.409967    2.227726
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.08400208  0.32247004  0.00612513  0.01285795 -0.70822     0.3208777
 -0.00679664 -0.00744709] 4 4
train:	 Loss = 1.2687,	 Acc = 0.5382
24291 0.292
47779 0.595
24627 0.657
3191 0.645
536 0.576
56 0.393
0 0.0
0 0.0
0.6166244470986625
0.6111988449217164
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.2183,	 Acc = 0.5784
2967 0.407
14640 0.625
6803 0.572
700 0.417
20 0.0
0 0.0
0 0.0
0 0.0
0.6013175111672607
0.6111988449217164
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3085,	 Acc1 = 0.3313,	 Acc2 = 0.3432

 ===== Epoch 44	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495   1.6606435   1.2408079
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 0 0
train:	 Loss = 1.2683,	 Acc = 0.5373
24288 0.292
47782 0.593
24631 0.657
3187 0.645
536 0.55
56 0.393
0 0.0
0 0.0
0.6153533179336413
0.6111988449217164
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2228,	 Acc = 0.5826
2967 0.406
14640 0.607
6803 0.625
700 0.434
20 0.0
0 0.0
0 0.0
0 0.0
0.6061904976763074
0.6111988449217164
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3280,	 Acc1 = 0.3231,	 Acc2 = 0.3333

 ===== Epoch 45	 =====
[-0.36757618 -0.38323686  2.2850046   1.017174   -0.4419852  -0.36654198
  1.4574808   3.251113   -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-1.7898616e-03 -1.3066782e-03 -4.1410704e+00 -1.3474270e+00
  3.9517106e-03  1.5506904e-03  4.0210649e-02 -1.9410659e+00
  6.1251274e-03  1.2857955e-02  3.1886040e-03  7.7467631e-03
 -6.7966389e-03 -7.4470886e-03] 5 5
train:	 Loss = 1.2680,	 Acc = 0.5390
24290 0.293
47777 0.594
24632 0.661
3189 0.641
536 0.567
56 0.429
0 0.0
0 0.0
0.6173513584459903
0.6111988449217164
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2506,	 Acc = 0.5661
2967 0.411
14640 0.605
6803 0.563
700 0.456
20 0.0
0 0.0
0 0.0
0 0.0
0.5869241528673916
0.6111988449217164
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3464,	 Acc1 = 0.3233,	 Acc2 = 0.3335

 ===== Epoch 46	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  3.4910152   1.4913101  -0.40402457 -0.41193563  2.9577613   2.452478
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
  0.05938334  0.6569444   0.00612513  0.01285795  0.9804115   0.45021433
 -0.00679664 -0.00744709] 4 2
train:	 Loss = 1.2682,	 Acc = 0.5379
24290 0.292
47781 0.594
24632 0.658
3186 0.643
535 0.544
56 0.429
0 0.0
0 0.0
0.6163276020475128
0.6111988449217164
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2370,	 Acc = 0.5604
2967 0.408
14640 0.605
6803 0.549
700 0.409
20 0.0
0 0.0
0 0.0
0 0.0
0.5807877994856292
0.6111988449217164
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3128,	 Acc1 = 0.3689,	 Acc2 = 0.3884

 ===== Epoch 47	 =====
[-0.36757618 -0.38323686  1.7488631   0.828694   -0.4419852  -0.36654198
  3.9282374   2.9639132  -0.40402457 -0.41193563  3.9587247   3.2268953
 -0.38958293 -0.38242146] [-1.7898616e-03 -1.3066782e-03 -1.3512780e-01  6.3427430e-01
  3.9517106e-03  1.5506904e-03 -3.6492831e-01  1.0847605e+00
  6.1251274e-03  1.2857955e-02 -6.8973541e+00 -5.0772271e+00
 -6.7966389e-03 -7.4470886e-03] 6 6
train:	 Loss = 1.2664,	 Acc = 0.5380
24291 0.294
47776 0.595
24632 0.654
3189 0.642
536 0.556
56 0.339
0 0.0
0 0.0
0.615863182349158
0.6111988449217164
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.2568,	 Acc = 0.5354
2967 0.263
14640 0.605
6803 0.52
700 0.397
20 0.0
0 0.0
0 0.0
0 0.0
0.5718991111311645
0.6111988449217164
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3357,	 Acc1 = 0.3299,	 Acc2 = 0.3815

 ===== Epoch 48	 =====
[ 1.4996479   2.2619383  -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [ 0.30308223  0.07974907  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 2 2
train:	 Loss = 1.2668,	 Acc = 0.5391
24289 0.293
47780 0.597
24630 0.655
3189 0.651
536 0.535
56 0.464
0 0.0
0 0.0
0.6175270045018441
0.6111988449217164
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2213,	 Acc = 0.5544
2967 0.407
14640 0.596
6803 0.545
700 0.42
20 0.0
0 0.0
0 0.0
0 0.0
0.5742002436493254
0.6111988449217164
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 1
Testing:	 Loss = 1.3181,	 Acc1 = 0.3396,	 Acc2 = 0.3531

 ===== Epoch 49	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [ 5.8205914e-01  2.1479781e+00  7.6247272e-03  8.5936040e-03
  3.9517106e-03  1.5506904e-03 -4.8065083e-03 -4.2258762e-03
  6.1251274e-03  1.2857955e-02  3.1886040e-03  7.7467631e-03
 -6.7966389e-03 -7.4470886e-03] 6 6
train:	 Loss = 1.2660,	 Acc = 0.5373
24285 0.292
47776 0.594
24636 0.656
3191 0.643
536 0.539
56 0.339
0 0.0
0 0.0
0.6155259531465319
0.6111988449217164
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2165,	 Acc = 0.5762
2967 0.397
14640 0.604
6803 0.609
700 0.446
20 0.0
0 0.0
0 0.0
0 0.0
0.6000992645399991
0.6111988449217164
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3172,	 Acc1 = 0.3097,	 Acc2 = 0.3171

 ===== Epoch 50	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495   1.4963431   1.7210121
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.10554352  0.22093047
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 6 4
train:	 Loss = 1.2677,	 Acc = 0.5379
24287 0.291
47783 0.594
24629 0.658
3189 0.642
536 0.563
56 0.393
0 0.0
0 0.0
0.6165920753875028
0.6111988449217164
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2176,	 Acc = 0.5681
2967 0.408
14640 0.607
6803 0.567
700 0.454
20 0.0
0 0.0
0 0.0
0 0.0
0.5894508866128232
0.6111988449217164
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3148,	 Acc1 = 0.3431,	 Acc2 = 0.3574

 ===== Epoch 51	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563  1.6935455   2.6821158
 -0.38958293 -0.38242146] [-1.7898616e-03 -1.3066782e-03  7.6247272e-03  8.5936040e-03
  3.9517106e-03  1.5506904e-03  3.0404680e+00  1.9870638e+00
  6.1251274e-03  1.2857955e-02  3.4138927e-01 -2.4640281e-03
 -6.7966389e-03 -7.4470886e-03] 2 2
train:	 Loss = 1.2670,	 Acc = 0.5393
24288 0.293
47776 0.594
24637 0.662
3188 0.647
535 0.553
56 0.357
0 0.0
0 0.0
0.6179388912221756
0.6111988449217164
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2485,	 Acc = 0.5871
2967 0.402
14640 0.602
6803 0.65
700 0.479
20 0.0
0 0.0
0 0.0
0 0.0
0.6119207688489825
0.6119207688489825
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 1
Testing:	 Loss = 1.3783,	 Acc1 = 0.2697,	 Acc2 = 0.2689

 ===== Epoch 52	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495   3.3622513  -4.4082603
 -0.36261833 -0.3698576   3.2691088   1.8167992  -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-1.7898616e-03 -1.3066782e-03  7.6247272e-03  8.5936040e-03
 -8.0816239e-01  7.6284986e+00 -4.8065083e-03 -4.2258762e-03
  2.1774465e-02  5.1467711e-01  3.1886040e-03  7.7467631e-03
 -6.7966389e-03 -7.4470886e-03] 4 4
train:	 Loss = 1.2668,	 Acc = 0.5398
24292 0.294
47779 0.596
24630 0.658
3187 0.649
536 0.537
56 0.429
0 0.0
0 0.0
0.6180107103480863
0.6119207688489825
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2586,	 Acc = 0.5523
2967 0.402
14640 0.593
6803 0.543
700 0.437
20 0.0
0 0.0
0 0.0
0 0.0
0.5724405540766142
0.6119207688489825
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3479,	 Acc1 = 0.3208,	 Acc2 = 0.3305

 ===== Epoch 53	 =====
[ 2.7782948   2.1376686  -0.420647   -0.3362495  -0.4419852  -0.36654198
  0.5864541   2.1417334  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-3.0978336e+00 -2.6003199e+00  7.6247272e-03  8.5936040e-03
  3.9517106e-03  1.5506904e-03 -1.3945197e+00 -3.4734254e+00
  6.1251274e-03  1.2857955e-02  3.1886040e-03  7.7467631e-03
 -6.7966389e-03 -7.4470886e-03] 1 6
train:	 Loss = 1.2673,	 Acc = 0.5384
24293 0.294
47774 0.594
24632 0.657
3189 0.646
536 0.558
56 0.411
0 0.0
0 0.0
0.6163124942575505
0.6119207688489825
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2302,	 Acc = 0.5793
2967 0.409
14640 0.618
6803 0.588
700 0.413
20 0.0
0 0.0
0 0.0
0 0.0
0.6020845553399811
0.6119207688489825
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3222,	 Acc1 = 0.3528,	 Acc2 = 0.3690

 ===== Epoch 54	 =====
[ 0.28582633  1.234809    1.0608224   3.0473094  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-6.4484322e-01 -1.6694862e+00 -3.8707808e-02 -2.5760508e-01
  3.9517106e-03  1.5506904e-03 -4.8065083e-03 -4.2258762e-03
  6.1251274e-03  1.2857955e-02  3.1886040e-03  7.7467631e-03
 -6.7966389e-03 -7.4470886e-03] 5 5
train:	 Loss = 1.2660,	 Acc = 0.5382
24289 0.294
47783 0.594
24631 0.657
3189 0.647
534 0.552
54 0.444
0 0.0
0 0.0
0.6161488889763883
0.6119207688489825
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2207,	 Acc = 0.5651
2967 0.408
14640 0.599
6803 0.573
700 0.447
20 0.0
0 0.0
0 0.0
0 0.0
0.5860668682037631
0.6119207688489825
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3096,	 Acc1 = 0.3394,	 Acc2 = 0.3529

 ===== Epoch 55	 =====
[-0.36757618 -0.38323686  2.4279208   3.0813723   2.101461    0.85620004
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.04622151 -0.10289131  0.31246883 -0.00778462
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 2 2
train:	 Loss = 1.2669,	 Acc = 0.5365
24293 0.293
47774 0.593
24633 0.654
3188 0.637
536 0.562
56 0.411
0 0.0
0 0.0
0.614304277632667
0.6119207688489825
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.2264,	 Acc = 0.5620
2967 0.406
14640 0.602
6803 0.561
700 0.426
20 0.0
0 0.0
0 0.0
0 0.0
0.5829535712674276
0.6119207688489825
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 1
Testing:	 Loss = 1.3173,	 Acc1 = 0.3328,	 Acc2 = 0.3449

 ===== Epoch 56	 =====
[ 1.835966    1.3869764  -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-2.1704307e+00 -1.8263682e+00  7.6247272e-03  8.5936040e-03
  3.9517106e-03  1.5506904e-03 -4.8065083e-03 -4.2258762e-03
  6.1251274e-03  1.2857955e-02  3.1886040e-03  7.7467631e-03
 -6.7966389e-03 -7.4470886e-03] 0 5
train:	 Loss = 1.2659,	 Acc = 0.5389
24288 0.293
47780 0.596
24629 0.657
3191 0.645
536 0.571
56 0.482
0 0.0
0 0.0
0.6173614027719445
0.6119207688489825
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2446,	 Acc = 0.5448
2967 0.409
14640 0.587
6803 0.528
700 0.423
20 0.0
0 0.0
0 0.0
0 0.0
0.5630104227766999
0.6119207688489825
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3394,	 Acc1 = 0.3437,	 Acc2 = 0.3581

 ===== Epoch 57	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563  2.1544256   2.1055782
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795 -0.02232497  0.03157194
 -0.00679664 -0.00744709] 2 3
train:	 Loss = 1.2673,	 Acc = 0.5379
24288 0.294
47782 0.594
24628 0.657
3190 0.645
536 0.537
56 0.429
0 0.0
0 0.0
0.6158389332213355
0.6119207688489825
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.2434,	 Acc = 0.5581
2967 0.404
14640 0.601
6803 0.545
700 0.449
20 0.0
0 0.0
0 0.0
0 0.0
0.578712268194739
0.6119207688489825
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3302,	 Acc1 = 0.3417,	 Acc2 = 0.3556

 ===== Epoch 58	 =====
[ 2.7677336   3.4640603  -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.13621527  0.02745504  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 3 3
train:	 Loss = 1.2667,	 Acc = 0.5382
24289 0.292
47776 0.595
24633 0.656
3190 0.649
536 0.55
56 0.464
0 0.0
0 0.0
0.6165688860889081
0.6119207688489825
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2041,	 Acc = 0.5844
2967 0.415
14640 0.626
6803 0.589
700 0.404
20 0.0
0 0.0
0 0.0
0 0.0
0.6070929025853901
0.6119207688489825
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.2998,	 Acc1 = 0.3367,	 Acc2 = 0.3497

 ===== Epoch 59	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563  1.4566929   3.2586536
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 0 0
train:	 Loss = 1.2680,	 Acc = 0.5369
24289 0.291
47777 0.593
24631 0.656
3191 0.651
536 0.563
56 0.429
0 0.0
0 0.0
0.6155057683978422
0.6119207688489825
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2275,	 Acc = 0.5618
2967 0.406
14640 0.6
6803 0.563
700 0.436
20 0.0
0 0.0
0 0.0
0 0.0
0.5825474890583405
0.6119207688489825
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3230,	 Acc1 = 0.3363,	 Acc2 = 0.3492

 ===== Epoch 60	 =====
[ 1.028692    2.7387292  -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.18083662  0.3229163   0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 0 1
train:	 Loss = 1.2663,	 Acc = 0.5383
24291 0.295
47771 0.594
24635 0.656
3191 0.648
536 0.552
56 0.464
0 0.0
0 0.0
0.6159419338749689
0.6119207688489825
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2229,	 Acc = 0.5772
2967 0.399
14640 0.61
6803 0.602
700 0.426
20 0.0
0 0.0
0 0.0
0 0.0
0.601046789694536
0.6119207688489825
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3210,	 Acc1 = 0.3410,	 Acc2 = 0.3549

 ===== Epoch 61	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 1 1
train:	 Loss = 1.2688,	 Acc = 0.5362
24290 0.292
47776 0.593
24631 0.654
3191 0.64
536 0.543
56 0.464
0 0.0
0 0.0
0.6139519621997638
0.6119207688489825
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.2400,	 Acc = 0.5563
2967 0.411
14640 0.595
6803 0.551
700 0.433
20 0.0
0 0.0
0 0.0
0 0.0
0.5758696927311284
0.6119207688489825
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3354,	 Acc1 = 0.3388,	 Acc2 = 0.3521

 ===== Epoch 62	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563  2.0126162   2.1055782
 -0.38958293 -0.38242146] [-1.7898616e-03 -1.3066782e-03  7.6247272e-03  8.5936040e-03
  3.9517106e-03  1.5506904e-03 -4.8065083e-03 -4.2258762e-03
  6.1251274e-03  1.2857955e-02 -3.8357439e+00 -3.5149760e+00
 -6.7966389e-03 -7.4470886e-03] 2 1
train:	 Loss = 1.2689,	 Acc = 0.5353
24288 0.291
47782 0.591
24631 0.656
3188 0.641
535 0.546
56 0.411
0 0.0
0 0.0
0.6132664846703066
0.6119207688489825
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2457,	 Acc = 0.5491
2967 0.41
14640 0.594
6803 0.529
700 0.42
20 0.0
0 0.0
0 0.0
0 0.0
0.56770292830393
0.6119207688489825
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3405,	 Acc1 = 0.3363,	 Acc2 = 0.3492

 ===== Epoch 63	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [ 2.6306534e+00  2.9350030e+00  7.6247272e-03  8.5936040e-03
  3.9517106e-03  1.5506904e-03 -4.8065083e-03 -4.2258762e-03
  6.1251274e-03  1.2857955e-02  3.1886040e-03  7.7467631e-03
  2.7561603e+00  4.5176239e+00] 5 5
train:	 Loss = 1.2667,	 Acc = 0.5369
24290 0.293
47779 0.591
24629 0.658
3191 0.646
535 0.561
56 0.411
0 0.0
0 0.0
0.6148313426958919
0.6119207688489825
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2312,	 Acc = 0.5620
2967 0.405
14640 0.608
6803 0.549
700 0.407
20 0.0
0 0.0
0 0.0
0 0.0
0.5829986915128819
0.6119207688489825
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3254,	 Acc1 = 0.3196,	 Acc2 = 0.3290

 ===== Epoch 64	 =====
[ 1.7506367   0.8924325  -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576   1.2288914   3.315815   -0.42759606 -0.42288172
  2.0877516   1.0858638 ] [ 1.19769685e-01  1.24198988e-01  7.62472721e-03  8.59360397e-03
  3.95171065e-03  1.55069039e-03 -4.80650831e-03 -4.22587618e-03
 -2.33814859e+00 -4.99536371e+00  3.18860402e-03  7.74676306e-03
 -3.62648994e-01  1.34962708e-01] 1 2
train:	 Loss = 1.2660,	 Acc = 0.5376
24289 0.295
47778 0.593
24632 0.657
3189 0.637
536 0.556
56 0.446
0 0.0
0 0.0
0.6150595214657899
0.6119207688489825
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.2530,	 Acc = 0.5432
2967 0.266
14640 0.606
6803 0.544
700 0.407
20 0.0
0 0.0
0 0.0
0 0.0
0.5802463565401795
0.6119207688489825
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3298,	 Acc1 = 0.3326,	 Acc2 = 0.3847

 ===== Epoch 65	 =====
[ 2.9687445   1.8003643  -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576   1.9420488   3.3207622  -0.42759606 -0.42288172
  2.8060308   1.9924384 ] [ 0.01309885  0.00392273  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588 -0.00293448  0.00288804  0.0031886   0.00774676
 -0.00679664 -0.01456758] 0 0
train:	 Loss = 1.2649,	 Acc = 0.5390
24286 0.291
47777 0.596
24635 0.658
3190 0.647
536 0.556
56 0.464
0 0.0
0 0.0
0.6179620442554532
0.6119207688489825
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2238,	 Acc = 0.5555
2967 0.408
14640 0.595
6803 0.549
700 0.437
20 0.0
0 0.0
0 0.0
0 0.0
0.5752380092947705
0.6119207688489825
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 0
Testing:	 Loss = 1.3239,	 Acc1 = 0.3318,	 Acc2 = 0.3437

 ===== Epoch 66	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 3 1
train:	 Loss = 1.2648,	 Acc = 0.5387
24290 0.295
47779 0.596
24629 0.656
3190 0.642
536 0.543
56 0.393
0 0.0
0 0.0
0.616590103688148
0.6119207688489825
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2371,	 Acc = 0.5618
2967 0.408
14640 0.6
6803 0.566
700 0.399
20 0.0
0 0.0
0 0.0
0 0.0
0.5824572485674322
0.6119207688489825
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3374,	 Acc1 = 0.3171,	 Acc2 = 0.3260

 ===== Epoch 67	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  1.2263464   2.490878   -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
  0.09189186  0.3691409   0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
train:	 Loss = 1.2662,	 Acc = 0.5382
24291 0.292
47775 0.595
24635 0.656
3188 0.653
535 0.572
56 0.429
0 0.0
0 0.0
0.6164800693013427
0.6119207688489825
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2220,	 Acc = 0.5790
2967 0.409
14640 0.611
6803 0.597
700 0.463
20 0.0
0 0.0
0 0.0
0 0.0
0.6017687136218021
0.6119207688489825
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3278,	 Acc1 = 0.3206,	 Acc2 = 0.3303

 ===== Epoch 68	 =====
[ 2.3667204   2.4496114  -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
  2.0859945   2.6978424 ] [-0.01562564 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.05818946 -0.04304954] 3 0
train:	 Loss = 1.2670,	 Acc = 0.5380
24294 0.293
47774 0.595
24631 0.655
3190 0.652
535 0.555
56 0.429
0 0.0
0 0.0
0.6162287034363269
0.6119207688489825
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2161,	 Acc = 0.5741
2967 0.408
14640 0.601
6803 0.604
700 0.436
20 0.0
0 0.0
0 0.0
0 0.0
0.5962640436763976
0.6119207688489825
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3279,	 Acc1 = 0.3037,	 Acc2 = 0.3099

 ===== Epoch 69	 =====
[-0.36757618 -0.38323686  1.7933713   3.0223303  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.0032427   0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 0 0
train:	 Loss = 1.2650,	 Acc = 0.5392
24291 0.294
47775 0.595
24634 0.657
3188 0.659
536 0.554
56 0.446
0 0.0
0 0.0
0.6174119623567706
0.6119207688489825
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.2234,	 Acc = 0.5799
2967 0.406
14640 0.614
6803 0.6
700 0.433
20 0.0
0 0.0
0 0.0
0 0.0
0.6031674412308803
0.6119207688489825
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3094,	 Acc1 = 0.3511,	 Acc2 = 0.3670

 ===== Epoch 70	 =====
[-0.36757618 -0.38323686  1.4344473   2.2457013  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-1.7898616e-03 -1.3066782e-03 -2.8368733e+00 -2.5783114e+00
  3.9517106e-03  1.5506904e-03 -4.8065083e-03 -4.2258762e-03
  6.1251274e-03  1.2857955e-02  3.1886040e-03  7.7467631e-03
 -6.7966389e-03 -7.4470886e-03] 6 5
train:	 Loss = 1.2680,	 Acc = 0.5382
24295 0.292
47772 0.594
24631 0.658
3191 0.661
535 0.551
56 0.375
0 0.0
0 0.0
0.6166962000393779
0.6119207688489825
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2296,	 Acc = 0.5731
2967 0.408
14640 0.606
6803 0.584
700 0.497
20 0.0
0 0.0
0 0.0
0 0.0
0.5952262780309525
0.6119207688489825
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3278,	 Acc1 = 0.3342,	 Acc2 = 0.3467

 ===== Epoch 71	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576   2.7424166   2.1903162  -0.42759606 -0.42288172
  3.9636254   1.9375737 ] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.01765461 -0.00043527  0.0031886   0.00774676
  0.16299577  0.05663731] 3 0
train:	 Loss = 1.2667,	 Acc = 0.5392
24287 0.292
47779 0.596
24632 0.658
3191 0.653
535 0.553
56 0.464
0 0.0
0 0.0
0.6179964038691218
0.6119207688489825
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2650,	 Acc = 0.5488
2967 0.396
14640 0.593
6803 0.539
700 0.383
20 0.0
0 0.0
0 0.0
0 0.0
0.5691918964039164
0.6119207688489825
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3531,	 Acc1 = 0.3464,	 Acc2 = 0.3613

 ===== Epoch 72	 =====
[ 2.742363    3.286532   -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
  2.3970287   3.510363  ] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
  0.27554312 -0.00744709] 0 0
train:	 Loss = 1.2668,	 Acc = 0.5371
24286 0.292
47781 0.593
24630 0.655
3191 0.651
536 0.571
56 0.464
0 0.0
0 0.0
0.6150878021891487
0.6119207688489825
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.2682,	 Acc = 0.5602
2967 0.406
14640 0.605
6803 0.55
700 0.384
20 0.0
0 0.0
0 0.0
0 0.0
0.5808780399765374
0.6119207688489825
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3478,	 Acc1 = 0.3580,	 Acc2 = 0.3752

 ===== Epoch 73	 =====
[ 0.16092055  1.1029307  -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [ 0.05038917  0.44842198  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 0 6
train:	 Loss = 1.2657,	 Acc = 0.5377
24287 0.293
47776 0.593
24636 0.655
3189 0.657
536 0.578
56 0.464
0 0.0
0 0.0
0.6156996049505861
0.6119207688489825
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2192,	 Acc = 0.5854
2967 0.407
14640 0.631
6803 0.587
700 0.4
20 0.0
0 0.0
0 0.0
0 0.0
0.6093037946126427
0.6119207688489825
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3087,	 Acc1 = 0.3357,	 Acc2 = 0.3484

 ===== Epoch 74	 =====
[-0.36757618 -0.38323686  2.097171    0.90363187 -0.4419852  -0.36654198
  1.3328053   3.5861797  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.12323207  0.25431547  0.00395171  0.00155069
  0.37865755  0.40414402  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
train:	 Loss = 1.2655,	 Acc = 0.5372
24291 0.293
47775 0.592
24632 0.656
3190 0.654
536 0.545
56 0.446
0 0.0
0 0.0
0.6151675438711625
0.6119207688489825
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2037,	 Acc = 0.5840
2967 0.407
14640 0.617
6803 0.603
700 0.476
20 0.0
0 0.0
0 0.0
0 0.0
0.6076343455308397
0.6119207688489825
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3038,	 Acc1 = 0.3221,	 Acc2 = 0.3320

 ===== Epoch 75	 =====
[-0.36757618 -0.38323686  3.0734956   1.51676    -0.4419852  -0.36654198
  2.149749    3.0427523  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.04872607  0.42723083  0.00395171  0.00155069
 -0.11817791  0.72306144  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 6 4
train:	 Loss = 1.2684,	 Acc = 0.5378
24293 0.292
47775 0.594
24630 0.656
3190 0.646
536 0.571
56 0.464
0 0.0
0 0.0
0.6161418614724297
0.6119207688489825
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.2992,	 Acc = 0.5329
2967 0.408
14640 0.579
6803 0.5
700 0.427
20 0.0
0 0.0
0 0.0
0 0.0
0.5495194693859135
0.6119207688489825
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3758,	 Acc1 = 0.3627,	 Acc2 = 0.3810

 ===== Epoch 76	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  1.3049093   2.6654508  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [ 3.9492364e+00  3.0866556e+00  7.6247272e-03  8.5936040e-03
  3.9517106e-03  1.5506904e-03  1.2857251e-01  8.7085241e-01
  6.1251274e-03  1.2857955e-02  3.1886040e-03  7.7467631e-03
 -6.7966389e-03 -7.4470886e-03] 4 4
train:	 Loss = 1.2663,	 Acc = 0.5387
24286 0.291
47782 0.595
24630 0.659
3190 0.65
536 0.535
56 0.429
0 0.0
0 0.0
0.6175158148935612
0.6119207688489825
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2088,	 Acc = 0.5648
2967 0.409
14640 0.593
6803 0.584
700 0.453
20 0.0
0 0.0
0 0.0
0 0.0
0.5856607859946759
0.6119207688489825
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3128,	 Acc1 = 0.3235,	 Acc2 = 0.3337

 ===== Epoch 77	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576   2.3643212   1.8069046  -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00118283  0.04941432  0.0031886   0.00774676
 -0.00679664 -0.00744709] 2 3
train:	 Loss = 1.2658,	 Acc = 0.5375
24287 0.293
47779 0.592
24632 0.659
3190 0.649
536 0.53
56 0.464
0 0.0
0 0.0
0.6152533697321276
0.6119207688489825
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2236,	 Acc = 0.5617
2967 0.409
14640 0.605
6803 0.55
700 0.436
20 0.0
0 0.0
0 0.0
0 0.0
0.5821865270947074
0.6119207688489825
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3201,	 Acc1 = 0.3445,	 Acc2 = 0.3591

 ===== Epoch 78	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495   1.870138    0.88510114
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936  -0.2559505   0.0365581
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 6 1
train:	 Loss = 1.2666,	 Acc = 0.5378
24286 0.291
47785 0.593
24628 0.659
3189 0.65
536 0.543
56 0.429
0 0.0
0 0.0
0.6163871170958343
0.6119207688489825
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2642,	 Acc = 0.5557
2967 0.409
14640 0.596
6803 0.543
700 0.477
20 0.0
0 0.0
0 0.0
0 0.0
0.575373370031133
0.6119207688489825
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3483,	 Acc1 = 0.3557,	 Acc2 = 0.3725

 ===== Epoch 79	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 6 1
train:	 Loss = 1.2657,	 Acc = 0.5375
24286 0.293
47782 0.594
24630 0.657
3191 0.637
536 0.549
55 0.473
0 0.0
0 0.0
0.615520907158044
0.6119207688489825
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2242,	 Acc = 0.5832
2967 0.409
14640 0.614
6803 0.609
700 0.443
20 0.0
0 0.0
0 0.0
0 0.0
0.6065063393944863
0.6119207688489825
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3295,	 Acc1 = 0.3210,	 Acc2 = 0.3308

 ===== Epoch 80	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576   1.3866684   2.9150877  -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.01353985  0.00953465  0.0031886   0.00774676
 -0.00679664 -0.00744709] 0 0
train:	 Loss = 1.2678,	 Acc = 0.5382
24289 0.294
47776 0.595
24632 0.653
3191 0.657
536 0.563
56 0.446
0 0.0
0 0.0
0.6160701395177908
0.6119207688489825
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.2574,	 Acc = 0.5486
2967 0.403
14640 0.599
6803 0.516
700 0.436
20 0.0
0 0.0
0 0.0
0 0.0
0.5681541307584713
0.6119207688489825
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3412,	 Acc1 = 0.3458,	 Acc2 = 0.3606

 ===== Epoch 81	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563  1.1063178   1.8173094
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.13728252  0.4978647
 -0.00679664 -0.00744709] 2 4
train:	 Loss = 1.2648,	 Acc = 0.5388
24292 0.293
47777 0.596
24631 0.657
3189 0.648
535 0.557
56 0.429
0 0.0
0 0.0
0.6173281881661153
0.6119207688489825
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2643,	 Acc = 0.5427
2967 0.404
14640 0.586
6803 0.525
700 0.416
20 0.0
0 0.0
0 0.0
0 0.0
0.5612507332039887
0.6119207688489825
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3483,	 Acc1 = 0.3501,	 Acc2 = 0.3658

 ===== Epoch 82	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
  1.920812    1.2321697 ] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.49406016  0.10648075] 1 1
train:	 Loss = 1.2681,	 Acc = 0.5379
24287 0.295
47778 0.594
24632 0.653
3191 0.652
536 0.547
56 0.446
0 0.0
0 0.0
0.6153058679931227
0.6119207688489825
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2778,	 Acc = 0.5332
2967 0.377
14640 0.585
6803 0.501
700 0.439
20 0.0
0 0.0
0 0.0
0 0.0
0.554031493931327
0.6119207688489825
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3617,	 Acc1 = 0.3301,	 Acc2 = 0.3417

 ===== Epoch 83	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563  1.3194094   2.9386263
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.67662406  0.3208777
 -0.00679664 -0.00744709] 4 2
train:	 Loss = 1.2651,	 Acc = 0.5380
24285 0.293
47779 0.594
24633 0.656
3191 0.654
536 0.55
56 0.429
0 0.0
0 0.0
0.6160771704180065
0.6119207688489825
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.1977,	 Acc = 0.5873
2967 0.41
14640 0.614
6803 0.62
700 0.471
20 0.0
0 0.0
0 0.0
0 0.0
0.6111086044308081
0.6119207688489825
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.2980,	 Acc1 = 0.3355,	 Acc2 = 0.3482

 ===== Epoch 84	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 5 1
train:	 Loss = 1.2662,	 Acc = 0.5373
24292 0.291
47777 0.593
24630 0.657
3190 0.648
535 0.563
56 0.518
0 0.0
0 0.0
0.6157925132566808
0.6119207688489825
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2048,	 Acc = 0.5853
2967 0.409
14640 0.609
6803 0.624
700 0.479
20 0.0
0 0.0
0 0.0
0 0.0
0.6088977124035555
0.6119207688489825
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3067,	 Acc1 = 0.3287,	 Acc2 = 0.3400

 ===== Epoch 85	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576   1.3625723   1.3245479  -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.12473519  0.00621134  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 0
train:	 Loss = 1.2656,	 Acc = 0.5372
24291 0.292
47771 0.592
24637 0.657
3189 0.653
536 0.556
56 0.429
0 0.0
0 0.0
0.6153644226856896
0.6119207688489825
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2204,	 Acc = 0.5544
2967 0.265
14640 0.609
6803 0.578
700 0.436
20 0.0
0 0.0
0 0.0
0 0.0
0.5931958669855164
0.6119207688489825
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3088,	 Acc1 = 0.3229,	 Acc2 = 0.3730

 ===== Epoch 86	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 5
train:	 Loss = 1.2703,	 Acc = 0.5367
24287 0.29
47780 0.594
24630 0.653
3191 0.647
536 0.575
56 0.482
0 0.0
0 0.0
0.6154371136456105
0.6119207688489825
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2333,	 Acc = 0.5620
2967 0.403
14640 0.59
6803 0.585
700 0.453
20 0.0
0 0.0
0 0.0
0 0.0
0.5833145332310608
0.6119207688489825
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3254,	 Acc1 = 0.3375,	 Acc2 = 0.3506

 ===== Epoch 87	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  0.96902305  2.4261174  -0.40402457 -0.41193563  2.3720434   1.8637255
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.12317982  0.15912208  0.00612513  0.01285795 -0.26796693  0.0758187
 -0.00679664 -0.00744709] 1 1
train:	 Loss = 1.2657,	 Acc = 0.5376
24289 0.292
47776 0.594
24632 0.656
3191 0.644
536 0.56
56 0.411
0 0.0
0 0.0
0.6159388904201284
0.6119207688489825
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2312,	 Acc = 0.5823
2967 0.407
14640 0.616
6803 0.598
700 0.487
20 0.0
0 0.0
0 0.0
0 0.0
0.6056490547308577
0.6119207688489825
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 1
Testing:	 Loss = 1.3351,	 Acc1 = 0.3163,	 Acc2 = 0.3250

 ===== Epoch 88	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563  0.7370855   2.388961
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795 -0.01639137 -0.89420635
 -0.00679664 -0.00744709] 2 5
train:	 Loss = 1.2671,	 Acc = 0.5380
24296 0.293
47775 0.596
24626 0.655
3191 0.641
536 0.53
56 0.5
0 0.0
0 0.0
0.6162055024677098
0.6119207688489825
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.2086,	 Acc = 0.5703
2967 0.404
14640 0.605
6803 0.589
700 0.39
20 0.0
0 0.0
0 0.0
0 0.0
0.5925190633037044
0.6119207688489825
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3056,	 Acc1 = 0.3375,	 Acc2 = 0.3506

 ===== Epoch 89	 =====
[ 2.496404    1.9550676  -0.420647   -0.3362495  -0.4419852  -0.36654198
  0.9502363   1.9756079  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [ 0.13103704  0.12158428  0.00762473  0.0085936   0.00395171  0.00155069
 -0.13985282  0.22523911  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 1 1
train:	 Loss = 1.2680,	 Acc = 0.5368
24291 0.293
47778 0.593
24628 0.655
3191 0.645
536 0.558
56 0.429
0 0.0
0 0.0
0.614471905393167
0.6119207688489825
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2470,	 Acc = 0.5622
2967 0.393
14640 0.6
6803 0.568
700 0.446
20 0.0
0 0.0
0 0.0
0 0.0
0.5848035013310472
0.6119207688489825
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 1
Testing:	 Loss = 1.3400,	 Acc1 = 0.3338,	 Acc2 = 0.3462

 ===== Epoch 90	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563  1.5400442   1.6169869
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.62797034  0.46042514
 -0.00679664 -0.00744709] 2 2
train:	 Loss = 1.2657,	 Acc = 0.5384
24287 0.293
47778 0.593
24633 0.659
3190 0.657
536 0.547
56 0.482
0 0.0
0 0.0
0.6166839473442443
0.6119207688489825
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2517,	 Acc = 0.5674
2967 0.409
14640 0.595
6803 0.597
700 0.394
20 0.0
0 0.0
0 0.0
0 0.0
0.5885936019491946
0.6119207688489825
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 1
Testing:	 Loss = 1.3597,	 Acc1 = 0.3089,	 Acc2 = 0.3161

 ===== Epoch 91	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 2 1
train:	 Loss = 1.2654,	 Acc = 0.5380
24289 0.292
47781 0.595
24627 0.654
3191 0.651
536 0.56
56 0.429
0 0.0
0 0.0
0.6164507619010119
0.6119207688489825
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2268,	 Acc = 0.5687
2967 0.407
14640 0.609
6803 0.569
700 0.42
20 0.0
0 0.0
0 0.0
0 0.0
0.5903081712764517
0.6119207688489825
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3163,	 Acc1 = 0.3516,	 Acc2 = 0.3675

 ===== Epoch 92	 =====
[ 4.586121    2.956836   -0.420647   -0.3362495  -0.4419852  -0.36654198
  1.8058921   2.099498   -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-4.8770261e+00 -3.4448686e+00  4.8231330e+00  2.0266981e+00
  3.9517106e-03  1.5506904e-03  1.6441822e-01  1.9637281e+00
  6.1251274e-03  1.2857955e-02  3.1886040e-03  7.7467631e-03
 -6.7966389e-03 -7.4470886e-03] 6 6
train:	 Loss = 1.2662,	 Acc = 0.5374
24291 0.291
47779 0.595
24633 0.654
3186 0.657
535 0.55
56 0.446
0 0.0
0 0.0
0.615928808620667
0.6119207688489825
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.2551,	 Acc = 0.5686
2967 0.397
14640 0.606
6803 0.575
700 0.454
20 0.0
0 0.0
0 0.0
0 0.0
0.5915715381491675
0.6119207688489825
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3421,	 Acc1 = 0.3361,	 Acc2 = 0.3489

 ===== Epoch 93	 =====
[-0.36757618 -0.38323686  2.2792876   1.4463638  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.21214223  1.1166172   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 6 6
train:	 Loss = 1.2661,	 Acc = 0.5376
24290 0.293
47782 0.594
24628 0.655
3188 0.65
536 0.556
56 0.464
0 0.0
0 0.0
0.6156844730279564
0.6119207688489825
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2288,	 Acc = 0.5680
2967 0.406
14640 0.609
6803 0.567
700 0.424
20 0.0
0 0.0
0 0.0
0 0.0
0.5896764878400939
0.6119207688489825
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3161,	 Acc1 = 0.3547,	 Acc2 = 0.3713

 ===== Epoch 94	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576   1.2673309   2.9769287  -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 0 0
train:	 Loss = 1.2692,	 Acc = 0.5357
24290 0.29
47776 0.593
24632 0.652
3190 0.648
536 0.556
56 0.429
0 0.0
0 0.0
0.6140832130200814
0.6119207688489825
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2325,	 Acc = 0.5507
2967 0.41
14640 0.596
6803 0.53
700 0.414
20 0.0
0 0.0
0 0.0
0 0.0
0.5695528583675495
0.6119207688489825
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3199,	 Acc1 = 0.3573,	 Acc2 = 0.3745

 ===== Epoch 95	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495   1.792008    2.719214
 -0.36261833 -0.3698576   1.4193721   0.82734966 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-1.7898616e-03 -1.3066782e-03  7.6247272e-03  8.5936040e-03
  4.4464737e-02 -3.5319108e-01 -4.8065083e-03 -4.2258762e-03
 -2.6116097e+00 -1.6521182e+00  3.1886040e-03  7.7467631e-03
 -6.7966389e-03 -7.4470886e-03] 5 5
train:	 Loss = 1.2696,	 Acc = 0.5374
24292 0.295
47778 0.593
24628 0.655
3190 0.635
536 0.558
56 0.464
0 0.0
0 0.0
0.6146112248648081
0.6119207688489825
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2029,	 Acc = 0.5891
2967 0.407
14640 0.619
6803 0.619
700 0.467
20 0.0
0 0.0
0 0.0
0 0.0
0.6134999774398773
0.6134999774398773
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 1
Testing:	 Loss = 1.3128,	 Acc1 = 0.3185,	 Acc2 = 0.3278

 ===== Epoch 96	 =====
[ 2.2805672   2.1249878  -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
  2.1751757   2.3425276 ] [ 0.03456217 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00744822 -0.00744709] 0 0
train:	 Loss = 1.2690,	 Acc = 0.5371
24290 0.292
47777 0.593
24632 0.657
3190 0.64
536 0.563
55 0.509
0 0.0
0 0.0
0.6151332195826223
0.6134999774398773
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.1890,	 Acc = 0.5838
2967 0.411
14640 0.603
6803 0.63
700 0.49
20 0.0
0 0.0
0 0.0
0 0.0
0.6069124216035735
0.6134999774398773
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.2926,	 Acc1 = 0.3291,	 Acc2 = 0.3405

 ===== Epoch 97	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563  3.0286653   1.7147052
 -0.38958293 -0.38242146] [-1.7898616e-03 -1.3066782e-03  7.6247272e-03  8.5936040e-03
  3.9517106e-03  1.5506904e-03  5.1220059e+00  2.0531807e+00
  6.1251274e-03  1.2857955e-02  5.0515121e-01  1.1666187e-01
 -6.7966389e-03 -7.4470886e-03] 2 2
train:	 Loss = 1.2697,	 Acc = 0.5357
24294 0.294
47779 0.591
24626 0.653
3190 0.644
535 0.564
56 0.464
0 0.0
0 0.0
0.612881631795868
0.6134999774398773
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2425,	 Acc = 0.5462
2967 0.268
14640 0.59
6803 0.584
700 0.46
20 0.0
0 0.0
0 0.0
0 0.0
0.5834047737219691
0.6134999774398773
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3246,	 Acc1 = 0.3297,	 Acc2 = 0.3812

 ===== Epoch 98	 =====
[-0.36757618 -0.38323686  1.203331    1.0671327  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.03517302 -0.20072502  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 6 5
train:	 Loss = 1.2694,	 Acc = 0.5367
24285 0.294
47780 0.591
24634 0.656
3189 0.643
536 0.545
56 0.464
0 0.0
0 0.0
0.6139904193188529
0.6134999774398773
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2135,	 Acc = 0.5816
2967 0.404
14640 0.609
6803 0.615
700 0.466
20 0.0
0 0.0
0 0.0
0 0.0
0.605378333258133
0.6134999774398773
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 1
Testing:	 Loss = 1.3089,	 Acc1 = 0.3276,	 Acc2 = 0.3387

 ===== Epoch 99	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  1.7142357   1.1111926  -0.40402457 -0.41193563  1.8515717   3.0510023
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.13068396 -0.00422588  0.00612513  0.01285795  0.01980269 -0.00246403
 -0.00679664 -0.00744709] 0 0
train:	 Loss = 1.2689,	 Acc = 0.5354
24292 0.292
47774 0.589
24633 0.658
3189 0.632
536 0.537
56 0.482
0 0.0
0 0.0
0.6129049194098808
0.6134999774398773
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.2470,	 Acc = 0.5663
2967 0.402
14640 0.596
6803 0.584
700 0.481
20 0.0
0 0.0
0 0.0
0 0.0
0.5882777602310156
0.6134999774398773
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3483,	 Acc1 = 0.3113,	 Acc2 = 0.3191

 ===== Epoch 100	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495   1.7284331   1.8210546
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936  -0.0110073   0.7693799
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 6 4
train:	 Loss = 1.2718,	 Acc = 0.5342
24288 0.29
47782 0.59
24629 0.652
3190 0.647
535 0.579
56 0.411
0 0.0
0 0.0
0.6120852582948341
0.6134999774398773
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2474,	 Acc = 0.5407
2967 0.203
14640 0.592
6803 0.588
700 0.446
20 0.0
0 0.0
0 0.0
0 0.0
0.5860217479583089
0.6134999774398773
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3422,	 Acc1 = 0.3113,	 Acc2 = 0.3529

 ===== Epoch 101	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 3 6
train:	 Loss = 1.2689,	 Acc = 0.5359
24288 0.293
47778 0.591
24634 0.656
3188 0.627
536 0.535
56 0.464
0 0.0
0 0.0
0.6132402351952961
0.6134999774398773
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2347,	 Acc = 0.5842
2967 0.406
14640 0.61
6803 0.617
700 0.489
20 0.0
0 0.0
0 0.0
0 0.0
0.6079501872490186
0.6134999774398773
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3334,	 Acc1 = 0.3320,	 Acc2 = 0.3439

 ===== Epoch 102	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495   1.4320011   0.922895
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00332938  0.03422428
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 3 3
train:	 Loss = 1.2697,	 Acc = 0.5363
24290 0.293
47776 0.591
24633 0.656
3190 0.639
535 0.553
56 0.429
0 0.0
0 0.0
0.6137288358052237
0.6134999774398773
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.2285,	 Acc = 0.5753
2967 0.405
14640 0.61
6803 0.587
700 0.464
20 0.0
0 0.0
0 0.0
0 0.0
0.598068853494563
0.6134999774398773
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3070,	 Acc1 = 0.3625,	 Acc2 = 0.3807

 ===== Epoch 103	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563  1.3541076   2.6112702
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.21856828  0.12346906
 -0.00679664 -0.00744709] 2 2
train:	 Loss = 1.2677,	 Acc = 0.5373
24289 0.293
47775 0.594
24634 0.656
3190 0.639
536 0.541
56 0.5
0 0.0
0 0.0
0.6152695200220498
0.6134999774398773
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2518,	 Acc = 0.5412
2967 0.406
14640 0.59
6803 0.511
700 0.406
20 0.0
0 0.0
0 0.0
0 0.0
0.5592654424040067
0.6134999774398773
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3492,	 Acc1 = 0.3285,	 Acc2 = 0.3397

 ===== Epoch 104	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576   2.5157878   1.3171271  -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.0942615  -0.6019536   0.0031886   0.00774676
 -0.00679664 -0.00744709] 5 5
train:	 Loss = 1.2693,	 Acc = 0.5350
24287 0.293
47782 0.589
24631 0.655
3189 0.64
535 0.557
56 0.446
0 0.0
0 0.0
0.6121297232029189
0.6134999774398773
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2069,	 Acc = 0.5822
2967 0.399
14640 0.604
6803 0.628
700 0.471
20 0.0
0 0.0
0 0.0
0 0.0
0.6066868203763028
0.6134999774398773
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3074,	 Acc1 = 0.3235,	 Acc2 = 0.3337

 ===== Epoch 105	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [ 3.6401341e+00  3.0212882e+00  7.6247272e-03  8.5936040e-03
  3.9517106e-03  1.5506904e-03 -4.8065083e-03 -4.2258762e-03
  6.1251274e-03  1.2857955e-02  3.1886040e-03  7.7467631e-03
 -6.7966389e-03 -7.4470886e-03] 0 6
train:	 Loss = 1.2699,	 Acc = 0.5359
24286 0.294
47781 0.59
24632 0.654
3190 0.644
535 0.579
56 0.464
0 0.0
0 0.0
0.6129485261306664
0.6134999774398773
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2473,	 Acc = 0.5580
2967 0.409
14640 0.598
6803 0.553
700 0.423
20 0.0
0 0.0
0 0.0
0 0.0
0.5779452240220186
0.6134999774398773
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3424,	 Acc1 = 0.3400,	 Acc2 = 0.3536

 ===== Epoch 106	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 5 1
train:	 Loss = 1.2676,	 Acc = 0.5364
24287 0.292
47781 0.591
24633 0.656
3187 0.645
536 0.56
56 0.482
0 0.0
0 0.0
0.614177155381728
0.6134999774398773
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2066,	 Acc = 0.5717
2967 0.409
14640 0.608
6803 0.582
700 0.421
20 0.0
0 0.0
0 0.0
0 0.0
0.5935117087036953
0.6134999774398773
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3097,	 Acc1 = 0.3464,	 Acc2 = 0.3613

 ===== Epoch 107	 =====
[ 0.6174713   2.081874   -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.01777105  0.5033307   0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 6 6
train:	 Loss = 1.2670,	 Acc = 0.5365
24287 0.292
47781 0.593
24629 0.654
3191 0.646
536 0.545
56 0.482
0 0.0
0 0.0
0.6144921449476987
0.6134999774398773
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2367,	 Acc = 0.5660
2967 0.407
14640 0.593
6803 0.587
700 0.486
20 0.0
0 0.0
0 0.0
0 0.0
0.5872399945855705
0.6134999774398773
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 1
Testing:	 Loss = 1.3315,	 Acc1 = 0.3214,	 Acc2 = 0.3313

 ===== Epoch 108	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 5 1
train:	 Loss = 1.2698,	 Acc = 0.5356
24291 0.291
47777 0.591
24630 0.655
3190 0.641
536 0.56
56 0.554
0 0.0
0 0.0
0.6135793881006445
0.6134999774398773
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2021,	 Acc = 0.5810
2967 0.408
14640 0.607
6803 0.609
700 0.503
20 0.0
0 0.0
0 0.0
0 0.0
0.6041600866308713
0.6134999774398773
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3019,	 Acc1 = 0.3305,	 Acc2 = 0.3422

 ===== Epoch 109	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 1
train:	 Loss = 1.2694,	 Acc = 0.5360
24289 0.292
47775 0.59
24634 0.656
3190 0.648
536 0.569
56 0.536
0 0.0
0 0.0
0.6136814059403342
0.6134999774398773
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.2318,	 Acc = 0.5565
2967 0.407
14640 0.603
6803 0.538
700 0.41
20 0.0
0 0.0
0 0.0
0 0.0
0.5764111356765781
0.6134999774398773
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3203,	 Acc1 = 0.3588,	 Acc2 = 0.3762

 ===== Epoch 110	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  4.000539    1.8291923  -0.40402457 -0.41193563  4.449023    1.9541149
  4.6265492   3.5025253 ] [-1.7898616e-03 -1.3066782e-03  7.6247272e-03  8.5936040e-03
  3.9517106e-03  1.5506904e-03  4.3450829e-01  1.7078981e-01
  6.1251274e-03  1.2857955e-02  4.4721983e-02  1.6771582e-01
 -7.4348664e+00 -5.3015313e+00] 2 2
train:	 Loss = 1.2665,	 Acc = 0.5373
24292 0.293
47773 0.592
24632 0.657
3191 0.646
536 0.554
56 0.446
0 0.0
0 0.0
0.6151756182075917
0.6134999774398773
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2368,	 Acc = 0.5610
2967 0.407
14640 0.602
6803 0.556
700 0.417
20 0.0
0 0.0
0 0.0
0 0.0
0.5816450841492578
0.6134999774398773
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3207,	 Acc1 = 0.3518,	 Acc2 = 0.3678

 ===== Epoch 111	 =====
[ 0.21102291  1.5366075  -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00141824 -0.00392138  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 0 0
train:	 Loss = 1.2687,	 Acc = 0.5354
24288 0.293
47778 0.59
24634 0.654
3189 0.644
535 0.536
56 0.482
0 0.0
0 0.0
0.6125446241075179
0.6134999774398773
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2354,	 Acc = 0.5731
2967 0.408
14640 0.597
6803 0.607
700 0.459
20 0.0
0 0.0
0 0.0
0 0.0
0.5951360375400442
0.6134999774398773
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3483,	 Acc1 = 0.3029,	 Acc2 = 0.3089

 ===== Epoch 112	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 1 1
train:	 Loss = 1.2696,	 Acc = 0.5352
24293 0.294
47776 0.589
24629 0.654
3190 0.638
536 0.552
56 0.464
0 0.0
0 0.0
0.6119285442398309
0.6134999774398773
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2258,	 Acc = 0.5553
2967 0.407
14640 0.586
6803 0.565
700 0.459
20 0.0
0 0.0
0 0.0
0 0.0
0.5751026485584082
0.6134999774398773
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 1
Testing:	 Loss = 1.3283,	 Acc1 = 0.3243,	 Acc2 = 0.3347

 ===== Epoch 113	 =====
[ 0.47362894  1.9297065   1.5030475   2.9178715  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-8.2967126e-01 -2.3859143e+00 -2.9420612e+00 -3.2517715e+00
  3.9517106e-03  1.5506904e-03 -4.8065083e-03 -4.2258762e-03
  6.1251274e-03  1.2857955e-02  3.1886040e-03  7.7467631e-03
 -6.7966389e-03 -7.4470886e-03] 6 5
train:	 Loss = 1.2685,	 Acc = 0.5367
24288 0.292
47779 0.592
24631 0.656
3190 0.65
536 0.56
56 0.5
0 0.0
0 0.0
0.6145133347333054
0.6134999774398773
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.2504,	 Acc = 0.5547
2967 0.41
14640 0.59
6803 0.554
700 0.461
20 0.0
0 0.0
0 0.0
0 0.0
0.5740648829129631
0.6134999774398773
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3320,	 Acc1 = 0.3520,	 Acc2 = 0.3680

 ===== Epoch 114	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 6 1
train:	 Loss = 1.2678,	 Acc = 0.5356
24288 0.294
47775 0.59
24634 0.655
3191 0.641
536 0.547
56 0.429
0 0.0
0 0.0
0.6124921251574968
0.6134999774398773
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2070,	 Acc = 0.5795
2967 0.409
14640 0.61
6803 0.605
700 0.433
20 0.0
0 0.0
0 0.0
0 0.0
0.6023101565672517
0.6134999774398773
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3067,	 Acc1 = 0.3559,	 Acc2 = 0.3728

 ===== Epoch 115	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495   0.73190236  2.5458071
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936  -0.03469047 -0.50722367
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 5 5
train:	 Loss = 1.2667,	 Acc = 0.5374
24290 0.293
47776 0.593
24631 0.656
3191 0.644
536 0.539
56 0.446
0 0.0
0 0.0
0.6152907205670035
0.6134999774398773
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2179,	 Acc = 0.5891
2967 0.407
14640 0.615
6803 0.627
700 0.47
20 0.0
0 0.0
0 0.0
0 0.0
0.6134548571944232
0.6134999774398773
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3178,	 Acc1 = 0.3260,	 Acc2 = 0.3367

 ===== Epoch 116	 =====
[ 0.50224924  2.7184405  -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [ 0.00469249 -0.628835    0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 3 5
train:	 Loss = 1.2678,	 Acc = 0.5375
24292 0.295
47774 0.593
24631 0.654
3191 0.647
536 0.563
56 0.5
0 0.0
0 0.0
0.6147949808368772
0.6134999774398773
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2463,	 Acc = 0.5643
2967 0.405
14640 0.598
6803 0.574
700 0.466
20 0.0
0 0.0
0 0.0
0 0.0
0.5855705455037675
0.6134999774398773
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3507,	 Acc1 = 0.3169,	 Acc2 = 0.3258

 ===== Epoch 117	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [ 1.9606649e+00  2.2081161e+00  7.6247272e-03  8.5936040e-03
  3.9517106e-03  1.5506904e-03 -4.8065083e-03 -4.2258762e-03
  6.1251274e-03  1.2857955e-02  3.1886040e-03  7.7467631e-03
 -6.7966389e-03 -7.4470886e-03] 6 6
train:	 Loss = 1.2718,	 Acc = 0.5348
24284 0.293
47783 0.59
24632 0.653
3189 0.633
536 0.547
56 0.482
0 0.0
0 0.0
0.6118431413722505
0.6134999774398773
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2131,	 Acc = 0.5901
2967 0.408
14640 0.611
6803 0.639
700 0.464
20 0.0
0 0.0
0 0.0
0 0.0
0.6144475025944142
0.6144475025944142
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3059,	 Acc1 = 0.3526,	 Acc2 = 0.3688

 ===== Epoch 118	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
  1.894892    2.6534278 ] [-1.7898616e-03 -1.3066782e-03  7.6247272e-03  8.5936040e-03
  3.9517106e-03  1.5506904e-03 -4.8065083e-03 -4.2258762e-03
  6.1251274e-03  1.2857955e-02  4.5499697e+00  1.8048459e+00
  2.3260684e-01  7.4376458e-01] 4 6
train:	 Loss = 1.2691,	 Acc = 0.5362
24288 0.292
47781 0.591
24631 0.656
3188 0.643
536 0.563
56 0.518
0 0.0
0 0.0
0.6140145947081058
0.6144475025944142
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2383,	 Acc = 0.5618
2967 0.405
14640 0.596
6803 0.57
700 0.434
20 0.0
0 0.0
0 0.0
0 0.0
0.582727970040157
0.6144475025944142
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3285,	 Acc1 = 0.3443,	 Acc2 = 0.3588

 ===== Epoch 119	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576   1.266758    2.9744549  -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01618126  0.0031886   0.00774676
 -0.00679664 -0.00744709] 0 0
train:	 Loss = 1.2697,	 Acc = 0.5361
24292 0.294
47776 0.591
24630 0.653
3190 0.644
536 0.567
56 0.5
0 0.0
0 0.0
0.6132593059274426
0.6144475025944142
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.2798,	 Acc = 0.5480
2967 0.407
14640 0.591
6803 0.526
700 0.47
20 0.0
0 0.0
0 0.0
0 0.0
0.5668456436403014
0.6144475025944142
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3693,	 Acc1 = 0.3437,	 Acc2 = 0.3581

 ===== Epoch 120	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576   1.0263609   2.9200354  -0.42759606 -0.42288172
  2.7770362   2.1962218 ] [-1.78986159e-03 -1.30667817e-03  7.62472721e-03  8.59360397e-03
  3.95171065e-03  1.55069039e-03 -4.80650831e-03 -4.22587618e-03
 -2.04738832e+00 -4.46363497e+00  3.18860402e-03  7.74676306e-03
  1.23313375e-01  5.40830612e-01] 4 4
train:	 Loss = 1.2692,	 Acc = 0.5352
24290 0.294
47783 0.59
24625 0.652
3191 0.641
535 0.561
56 0.411
0 0.0
0 0.0
0.6121932012075075
0.6144475025944142
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2627,	 Acc = 0.5696
2967 0.409
14640 0.595
6803 0.602
700 0.416
20 0.0
0 0.0
0 0.0
0 0.0
0.5911654559400803
0.6144475025944142
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 6
Testing:	 Loss = 1.3604,	 Acc1 = 0.3414,	 Acc2 = 0.3554

 ===== Epoch 121	 =====
[ 2.4886963   1.7876836  -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-2.8128223e+00 -2.2394910e+00  7.6247272e-03  8.5936040e-03
  3.9517106e-03  1.5506904e-03 -4.8065083e-03 -4.2258762e-03
  6.1251274e-03  1.2857955e-02  3.1886040e-03  7.7467631e-03
 -6.7966389e-03 -7.4470886e-03] 6 5
train:	 Loss = 1.2665,	 Acc = 0.5376
24287 0.294
47782 0.592
24628 0.657
3191 0.651
536 0.554
56 0.482
0 0.0
0 0.0
0.6152139960363813
0.6144475025944142
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2036,	 Acc = 0.5811
2967 0.408
14640 0.606
6803 0.619
700 0.431
20 0.0
0 0.0
0 0.0
0 0.0
0.6042052068763254
0.6144475025944142
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3040,	 Acc1 = 0.3476,	 Acc2 = 0.3628

 ===== Epoch 122	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  0.6923446   2.8034194  -0.40402457 -0.41193563  1.9541578   2.1055782
 -0.38958293 -0.38242146] [-1.7898616e-03 -1.3066782e-03  7.6247272e-03  8.5936040e-03
  3.9517106e-03  1.5506904e-03 -1.5495737e+00 -4.3873963e+00
  6.1251274e-03  1.2857955e-02 -4.8394087e-01  4.1617841e-01
 -6.7966389e-03 -7.4470886e-03] 1 1
train:	 Loss = 1.2680,	 Acc = 0.5361
24291 0.295
47781 0.589
24627 0.656
3189 0.65
536 0.562
56 0.446
0 0.0
0 0.0
0.6130806284371759
0.6144475025944142
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2281,	 Acc = 0.5718
2967 0.407
14640 0.607
6803 0.58
700 0.479
20 0.0
0 0.0
0 0.0
0 0.0
0.5938726706673284
0.6144475025944142
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3135,	 Acc1 = 0.3625,	 Acc2 = 0.3807

 ===== Epoch 123	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  1.8075994   1.8122983  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-1.7898616e-03 -1.3066782e-03  4.1607027e+00  1.2099005e+00
  3.9517106e-03  1.5506904e-03  8.1058264e-02  2.1426330e+00
  6.1251274e-03  1.2857955e-02  3.1886040e-03  7.7467631e-03
 -6.7966389e-03 -7.4470886e-03] 6 6
train:	 Loss = 1.2681,	 Acc = 0.5360
24292 0.293
47774 0.59
24634 0.658
3188 0.642
536 0.575
56 0.446
0 0.0
0 0.0
0.6135743161652754
0.6144475025944142
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2486,	 Acc = 0.5435
2967 0.406
14640 0.591
6803 0.511
700 0.466
20 0.0
0 0.0
0 0.0
0 0.0
0.5619275368858007
0.6144475025944142
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3399,	 Acc1 = 0.3332,	 Acc2 = 0.3454

 ===== Epoch 124	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 1 1
train:	 Loss = 1.2669,	 Acc = 0.5367
24290 0.294
47784 0.592
24626 0.655
3190 0.639
534 0.551
56 0.482
0 0.0
0 0.0
0.6141357133482084
0.6144475025944142
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.2438,	 Acc = 0.5637
2967 0.407
14640 0.606
6803 0.556
700 0.436
20 0.0
0 0.0
0 0.0
0 0.0
0.5846681405946849
0.6144475025944142
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3299,	 Acc1 = 0.3489,	 Acc2 = 0.3643

 ===== Epoch 125	 =====
[ 0.8355466   3.0760336  -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [ 0.01245312 -0.00392138  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 0 0
train:	 Loss = 1.2679,	 Acc = 0.5371
24287 0.295
47780 0.592
24631 0.656
3190 0.636
536 0.567
56 0.393
0 0.0
0 0.0
0.6143084010342157
0.6144475025944142
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.2613,	 Acc = 0.5534
2967 0.407
14640 0.591
6803 0.549
700 0.451
20 0.0
0 0.0
0 0.0
0 0.0
0.5729368767766096
0.6144475025944142
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3559,	 Acc1 = 0.3355,	 Acc2 = 0.3482

 ===== Epoch 126	 =====
[ 2.6279244   1.6456609  -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
  2.5982351   1.8095559 ] [-7.41367862e-02  6.40608594e-02  7.62472721e-03  8.59360397e-03
  3.95171065e-03  1.55069039e-03 -4.80650831e-03 -4.22587618e-03
  3.57521296e+00  5.03437328e+00  3.18860402e-03  7.74676306e-03
 -1.16090104e-01  7.79987872e-02] 3 3
train:	 Loss = 1.2690,	 Acc = 0.5369
24293 0.295
47775 0.591
24629 0.656
3191 0.641
536 0.554
56 0.5
0 0.0
0 0.0
0.6141598960452571
0.6144475025944142
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2444,	 Acc = 0.5475
2967 0.406
14640 0.598
6803 0.517
700 0.411
20 0.0
0 0.0
0 0.0
0 0.0
0.5664395614312142
0.6144475025944142
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 1
Testing:	 Loss = 1.3415,	 Acc1 = 0.3272,	 Acc2 = 0.3382

 ===== Epoch 127	 =====
[ 1.5244415   0.98119676 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
  1.6001124   1.1746924 ] [ 1.4370918e-01  1.9675636e+00  7.6247272e-03  8.5936040e-03
  3.9517106e-03  1.5506904e-03 -4.8065083e-03 -4.2258762e-03
  6.1251274e-03  1.2857955e-02  3.1886040e-03  7.7467631e-03
 -1.3040087e-01  2.6627367e+00] 6 6
train:	 Loss = 1.2685,	 Acc = 0.5364
24287 0.292
47777 0.591
24636 0.656
3189 0.648
535 0.564
56 0.464
0 0.0
0 0.0
0.6142034045122255
0.6144475025944142
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2632,	 Acc = 0.5379
2967 0.412
14640 0.576
6803 0.522
700 0.446
20 0.0
0 0.0
0 0.0
0 0.0
0.5548436583495014
0.6144475025944142
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3566,	 Acc1 = 0.3363,	 Acc2 = 0.3492

 ===== Epoch 128	 =====
[ 0.21979222  1.5416797  -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00755005 -0.00392138  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 0 0
train:	 Loss = 1.2673,	 Acc = 0.5356
24292 0.293
47777 0.589
24630 0.657
3189 0.645
536 0.571
56 0.411
0 0.0
0 0.0
0.6128261668504227
0.6144475025944142
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.2355,	 Acc = 0.5485
2967 0.267
14640 0.597
6803 0.583
700 0.413
20 0.0
0 0.0
0 0.0
0 0.0
0.5862924694310337
0.6144475025944142
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3219,	 Acc1 = 0.3320,	 Acc2 = 0.3839

 ===== Epoch 129	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 5
train:	 Loss = 1.2681,	 Acc = 0.5366
24287 0.291
47779 0.593
24632 0.656
3190 0.644
536 0.552
56 0.429
0 0.0
0 0.0
0.6148990064704106
0.6144475025944142
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2522,	 Acc = 0.5577
2967 0.41
14640 0.599
6803 0.551
700 0.396
20 0.0
0 0.0
0 0.0
0 0.0
0.5775391418129314
0.6144475025944142
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 1
Testing:	 Loss = 1.3510,	 Acc1 = 0.3328,	 Acc2 = 0.3449

 ===== Epoch 130	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 3 1
train:	 Loss = 1.2696,	 Acc = 0.5347
24288 0.292
47783 0.589
24628 0.655
3190 0.638
535 0.574
56 0.429
0 0.0
0 0.0
0.6122427551448971
0.6144475025944142
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2435,	 Acc = 0.5806
2967 0.401
14640 0.596
6803 0.64
700 0.453
20 0.55
0 0.0
0 0.0
0 0.0
0.6046112890854126
0.6144475025944142
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 1
Testing:	 Loss = 1.3672,	 Acc1 = 0.2835,	 Acc2 = 0.2855

 ===== Epoch 131	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-1.7898616e-03 -1.3066782e-03  7.6247272e-03  8.5936040e-03
  3.9517106e-03  1.5506904e-03 -4.8065083e-03 -4.2258762e-03
  6.1251274e-03  1.2857955e-02  1.3958005e+00  2.9416473e+00
 -6.7966389e-03 -7.4470886e-03] 4 6
train:	 Loss = 1.2682,	 Acc = 0.5365
24286 0.295
47774 0.591
24638 0.656
3191 0.645
535 0.544
56 0.5
0 0.0
0 0.0
0.6135259994225267
0.6144475025944142
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2364,	 Acc = 0.5786
2967 0.409
14640 0.606
6803 0.604
700 0.501
20 0.0
0 0.0
0 0.0
0 0.0
0.6012272706763525
0.6144475025944142
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3304,	 Acc1 = 0.3384,	 Acc2 = 0.3516

 ===== Epoch 132	 =====
[-0.36757618 -0.38323686  2.4687548   2.1980135  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00574534 -0.00050721  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 0 0
train:	 Loss = 1.2698,	 Acc = 0.5358
24289 0.291
47777 0.591
24632 0.656
3191 0.644
535 0.559
56 0.429
0 0.0
0 0.0
0.6138914044965941
0.6144475025944142
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2673,	 Acc = 0.5545
2967 0.41
14640 0.606
6803 0.523
700 0.406
20 0.0
0 0.0
0 0.0
0 0.0
0.5738844019311465
0.6144475025944142
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3544,	 Acc1 = 0.3478,	 Acc2 = 0.3631

 ===== Epoch 133	 =====
[ 0.21914142  0.50694203 -0.420647   -0.3362495   3.2197804  -4.0814548
 -0.36261833 -0.3698576   2.549639    2.8656154  -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-5.7921457e-01 -9.1906697e-01  6.2218733e+00  3.0641904e+00
  7.3878324e-01  5.5887337e+00 -4.8065083e-03 -4.2258762e-03
  1.5191655e-01 -2.4497118e+00  3.1886040e-03  7.7467631e-03
 -6.7966389e-03 -7.4470886e-03] 5 5
train:	 Loss = 1.2672,	 Acc = 0.5371
24290 0.294
47777 0.592
24632 0.656
3189 0.643
536 0.56
56 0.482
0 0.0
0 0.0
0.6147000918755742
0.6144475025944142
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2102,	 Acc = 0.5712
2967 0.411
14640 0.605
6803 0.584
700 0.433
20 0.0
0 0.0
0 0.0
0 0.0
0.5925641835491585
0.6144475025944142
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 1
Testing:	 Loss = 1.3066,	 Acc1 = 0.3398,	 Acc2 = 0.3534

 ===== Epoch 134	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563  2.662828    1.8050946
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.4778566  -0.0330964
 -0.00679664 -0.00744709] 2 2
train:	 Loss = 1.2695,	 Acc = 0.5372
24288 0.292
47779 0.593
24631 0.656
3190 0.644
536 0.578
56 0.464
0 0.0
0 0.0
0.6152483200335993
0.6144475025944142
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2213,	 Acc = 0.5743
2967 0.408
14640 0.601
6803 0.608
700 0.404
20 0.0
0 0.0
0 0.0
0 0.0
0.5965347651491224
0.6144475025944142
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 1
Testing:	 Loss = 1.3273,	 Acc1 = 0.3365,	 Acc2 = 0.3494

 ===== Epoch 135	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 1 1
train:	 Loss = 1.2685,	 Acc = 0.5357
24291 0.29
47780 0.59
24627 0.657
3190 0.647
536 0.56
56 0.446
0 0.0
0 0.0
0.6139468952210949
0.6144475025944142
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.1983,	 Acc = 0.5981
2967 0.408
14640 0.622
6803 0.649
700 0.416
20 0.0
0 0.0
0 0.0
0 0.0
0.6234715516852412
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3029,	 Acc1 = 0.3231,	 Acc2 = 0.3333

 ===== Epoch 136	 =====
[-0.36757618 -0.38323686  1.2470225   1.3532592  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.32859877  0.1997106   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 0 1
train:	 Loss = 1.2690,	 Acc = 0.5373
24293 0.294
47771 0.592
24633 0.656
3191 0.643
536 0.575
56 0.482
0 0.0
0 0.0
0.6149080551800176
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.2312,	 Acc = 0.5698
2967 0.402
14640 0.604
6803 0.582
700 0.467
20 0.0
0 0.0
0 0.0
0 0.0
0.5922032215855254
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3183,	 Acc1 = 0.3501,	 Acc2 = 0.3658

 ===== Epoch 137	 =====
[ 0.21255343  1.5340713  -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00096095  0.00392273  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 0 0
train:	 Loss = 1.2689,	 Acc = 0.5370
24288 0.293
47780 0.592
24629 0.659
3191 0.639
536 0.556
56 0.554
0 0.0
0 0.0
0.6149070768584628
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.2682,	 Acc = 0.5528
2967 0.409
14640 0.599
6803 0.53
700 0.431
20 0.0
0 0.0
0 0.0
0 0.0
0.5720795921129811
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3562,	 Acc1 = 0.3499,	 Acc2 = 0.3656

 ===== Epoch 138	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495   1.7403055   0.9384572
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936  -0.22478785  0.09256996
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 5 3
train:	 Loss = 1.2695,	 Acc = 0.5365
24293 0.294
47775 0.591
24631 0.657
3189 0.638
536 0.563
56 0.536
0 0.0
0 0.0
0.6138842584692926
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2315,	 Acc = 0.5771
2967 0.412
14640 0.614
6803 0.585
700 0.449
20 0.0
0 0.0
0 0.0
0 0.0
0.5992419798763705
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3247,	 Acc1 = 0.3417,	 Acc2 = 0.3556

 ===== Epoch 139	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563  0.9030326   1.5021679
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795 -0.4180788   0.24940217
 -0.00679664 -0.00744709] 1 1
train:	 Loss = 1.2677,	 Acc = 0.5369
24290 0.293
47775 0.591
24633 0.659
3190 0.645
536 0.562
56 0.518
0 0.0
0 0.0
0.6146213413833836
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2246,	 Acc = 0.5841
2967 0.408
14640 0.613
6803 0.616
700 0.426
20 0.0
0 0.0
0 0.0
0 0.0
0.6076343455308397
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3259,	 Acc1 = 0.3119,	 Acc2 = 0.3198

 ===== Epoch 140	 =====
[ 2.5856955   2.0565126  -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576   0.70736223  2.0913713  -0.42759606 -0.42288172
  2.3311315   2.2249606 ] [-2.9082849e+00 -2.5166495e+00  7.6247272e-03  8.5936040e-03
  3.9517106e-03  1.5506904e-03 -4.8065083e-03 -4.2258762e-03
  1.1320316e-01 -1.5391259e+00  3.1886040e-03  7.7467631e-03
  5.8260500e-01 -1.7341659e+00] 5 5
train:	 Loss = 1.2680,	 Acc = 0.5362
24293 0.295
47776 0.59
24629 0.656
3190 0.637
536 0.558
56 0.464
0 0.0
0 0.0
0.6130704713402549
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.2842,	 Acc = 0.5509
2967 0.405
14640 0.602
6803 0.515
700 0.456
20 0.0
0 0.0
0 0.0
0 0.0
0.5704101430311781
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3582,	 Acc1 = 0.3594,	 Acc2 = 0.3770

 ===== Epoch 141	 =====
[ 1.462838    2.317733   -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
  1.509614    2.5567615 ] [ 0.20433675  0.5896158   0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
  0.00361265  0.779367  ] 6 4
train:	 Loss = 1.2704,	 Acc = 0.5360
24286 0.292
47783 0.591
24628 0.656
3191 0.637
536 0.55
56 0.482
0 0.0
0 0.0
0.6137491141034727
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.2441,	 Acc = 0.5572
2967 0.394
14640 0.601
6803 0.549
700 0.413
20 0.0
0 0.0
0 0.0
0 0.0
0.5789829896674638
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3376,	 Acc1 = 0.3264,	 Acc2 = 0.3372

 ===== Epoch 142	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 2 1
train:	 Loss = 1.2704,	 Acc = 0.5358
24290 0.293
47776 0.591
24631 0.655
3191 0.641
536 0.558
56 0.518
0 0.0
0 0.0
0.6132957080981756
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2215,	 Acc = 0.5768
2967 0.408
14640 0.602
6803 0.609
700 0.466
20 0.0
0 0.0
0 0.0
0 0.0
0.5994675811036412
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3269,	 Acc1 = 0.3258,	 Acc2 = 0.3365

 ===== Epoch 143	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495   1.8352854   1.0007058
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936  -0.28025794  0.20225987
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 3
train:	 Loss = 1.2677,	 Acc = 0.5359
24289 0.293
47778 0.591
24631 0.654
3191 0.645
535 0.553
56 0.464
0 0.0
0 0.0
0.6132089091887494
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2391,	 Acc = 0.5780
2967 0.41
14640 0.61
6803 0.601
700 0.424
20 0.0
0 0.0
0 0.0
0 0.0
0.6005053467490863
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 1
Testing:	 Loss = 1.3375,	 Acc1 = 0.3274,	 Acc2 = 0.3385

 ===== Epoch 144	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 1 1
train:	 Loss = 1.2682,	 Acc = 0.5365
24287 0.293
47775 0.593
24635 0.653
3191 0.649
536 0.567
56 0.482
0 0.0
0 0.0
0.6142296536427231
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2413,	 Acc = 0.5622
2967 0.403
14640 0.606
6803 0.555
700 0.403
20 0.0
0 0.0
0 0.0
0 0.0
0.5834950142128773
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3248,	 Acc1 = 0.3615,	 Acc2 = 0.3795

 ===== Epoch 145	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563  1.8881563   0.87432814
  2.907073    3.353607  ] [-1.7898616e-03 -1.3066782e-03  7.6247272e-03  8.5936040e-03
  3.9517106e-03  1.5506904e-03 -4.8065083e-03 -4.2258762e-03
  6.1251274e-03  1.2857955e-02  3.0934966e-01  5.0126827e-01
 -4.8886042e+00 -5.0985970e+00] 6 2
train:	 Loss = 1.2720,	 Acc = 0.5350
24292 0.293
47774 0.588
24632 0.656
3190 0.647
536 0.547
56 0.411
0 0.0
0 0.0
0.6120648921089935
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.2590,	 Acc = 0.5520
2967 0.403
14640 0.597
6803 0.531
700 0.459
20 0.0
0 0.0
0 0.0
0 0.0
0.5719442313766187
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3441,	 Acc1 = 0.3582,	 Acc2 = 0.3755

 ===== Epoch 146	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576   3.3333666   1.4432819  -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 0 0
train:	 Loss = 1.2687,	 Acc = 0.5367
24291 0.295
47776 0.59
24632 0.659
3190 0.632
535 0.555
56 0.5
0 0.0
0 0.0
0.6138418931866805
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.2690,	 Acc = 0.5645
2967 0.401
14640 0.595
6803 0.58
700 0.494
20 0.0
0 0.0
0 0.0
0 0.0
0.5863375896764879
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 1
Testing:	 Loss = 1.3636,	 Acc1 = 0.3086,	 Acc2 = 0.3159

 ===== Epoch 147	 =====
[-0.36757618 -0.38323686  1.7272223   1.559906   -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.08964442  1.0028571   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 6
train:	 Loss = 1.2697,	 Acc = 0.5367
24291 0.294
47782 0.592
24626 0.655
3189 0.641
536 0.56
56 0.446
0 0.0
0 0.0
0.6141700245442255
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.2802,	 Acc = 0.5473
2967 0.403
14640 0.595
6803 0.521
700 0.429
20 0.0
0 0.0
0 0.0
0 0.0
0.5666651626584849
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3628,	 Acc1 = 0.3582,	 Acc2 = 0.3755

 ===== Epoch 148	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 3 1
train:	 Loss = 1.2700,	 Acc = 0.5342
24287 0.293
47776 0.587
24635 0.655
3190 0.638
536 0.558
56 0.411
0 0.0
0 0.0
0.6109616368957779
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.2488,	 Acc = 0.5556
2967 0.409
14640 0.597
6803 0.542
700 0.46
20 0.0
0 0.0
0 0.0
0 0.0
0.5752380092947705
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3435,	 Acc1 = 0.3421,	 Acc2 = 0.3561

 ===== Epoch 149	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576   1.5054332   2.2991557  -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.13956207 -0.16992386  0.0031886   0.00774676
 -0.00679664 -0.00744709] 3 5
train:	 Loss = 1.2697,	 Acc = 0.5378
24290 0.293
47774 0.593
24633 0.658
3191 0.652
536 0.552
56 0.411
0 0.0
0 0.0
0.6157894736842106
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2576,	 Acc = 0.5499
2967 0.409
14640 0.592
6803 0.537
700 0.41
20 0.0
0 0.0
0 0.0
0 0.0
0.568695573703921
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3483,	 Acc1 = 0.3474,	 Acc2 = 0.3626

 ===== Epoch 150	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 3 1
train:	 Loss = 1.2692,	 Acc = 0.5355
24289 0.292
47782 0.589
24626 0.658
3191 0.646
536 0.539
56 0.411
0 0.0
0 0.0
0.6130776600910869
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.2868,	 Acc = 0.5469
2967 0.408
14640 0.599
6803 0.513
700 0.4
20 0.0
0 0.0
0 0.0
0 0.0
0.5655371565221314
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3624,	 Acc1 = 0.3551,	 Acc2 = 0.3718

 ===== Epoch 151	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495   0.8107972   1.6165233
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-1.7898616e-03 -1.3066782e-03  4.6014886e+00  3.3030865e+00
 -2.0348070e+00 -2.0802236e+00 -4.8065083e-03 -4.2258762e-03
  6.1251274e-03  1.2857955e-02  3.1886040e-03  7.7467631e-03
 -6.7966389e-03 -7.4470886e-03] 5 5
train:	 Loss = 1.2687,	 Acc = 0.5358
24292 0.294
47777 0.59
24628 0.653
3191 0.647
536 0.573
56 0.518
0 0.0
0 0.0
0.6128524177035753
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.2488,	 Acc = 0.5591
2967 0.408
14640 0.593
6803 0.566
700 0.439
20 0.0
0 0.0
0 0.0
0 0.0
0.5792988313856428
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 0
Testing:	 Loss = 1.3415,	 Acc1 = 0.3381,	 Acc2 = 0.3514

 ===== Epoch 152	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495   2.3948288   1.0940789
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936  -0.00103463  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 0 0
train:	 Loss = 1.2697,	 Acc = 0.5360
24292 0.293
47778 0.591
24629 0.656
3189 0.636
536 0.55
56 0.429
0 0.0
0 0.0
0.6135349398855463
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2363,	 Acc = 0.5612
2967 0.411
14640 0.58
6803 0.601
700 0.441
20 0.0
0 0.0
0 0.0
0 0.0
0.5814194829219871
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3201,	 Acc1 = 0.3625,	 Acc2 = 0.3807

 ===== Epoch 153	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-1.7898616e-03 -1.3066782e-03  7.6247272e-03  8.5936040e-03
  3.9517106e-03  1.5506904e-03 -4.8065083e-03 -4.2258762e-03
  2.6032681e+00  1.7110671e+00  3.1886040e-03  7.7467631e-03
 -6.7966389e-03 -7.4470886e-03] 2 6
train:	 Loss = 1.2692,	 Acc = 0.5351
24291 0.294
47777 0.59
24631 0.653
3189 0.645
536 0.543
56 0.464
0 0.0
0 0.0
0.6121356101274462
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2374,	 Acc = 0.5647
2967 0.405
14640 0.589
6803 0.599
700 0.43
20 0.0
0 0.0
0 0.0
0 0.0
0.5861571086946713
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 1
Testing:	 Loss = 1.3488,	 Acc1 = 0.3194,	 Acc2 = 0.3288

 ===== Epoch 154	 =====
[-0.36757618 -0.38323686  1.2470225   1.3532592  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.11947715  0.03362083  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 0 1
train:	 Loss = 1.2688,	 Acc = 0.5376
24292 0.293
47775 0.593
24631 0.657
3190 0.638
536 0.562
56 0.482
0 0.0
0 0.0
0.61568750984407
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.2506,	 Acc = 0.5571
2967 0.407
14640 0.6
6803 0.547
700 0.411
20 0.0
0 0.0
0 0.0
0 0.0
0.5771781798492984
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3342,	 Acc1 = 0.3526,	 Acc2 = 0.3688

 ===== Epoch 155	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495   0.4147894   2.9526467
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-1.7898616e-03 -1.3066782e-03  7.6247272e-03  8.5936040e-03
 -1.3903500e+00 -3.4828537e+00 -4.8065083e-03 -4.2258762e-03
  6.1251274e-03  1.2857955e-02  3.1886040e-03  7.7467631e-03
 -6.7966389e-03 -7.4470886e-03] 6 6
train:	 Loss = 1.2675,	 Acc = 0.5358
24287 0.292
47780 0.591
24631 0.656
3191 0.63
535 0.564
56 0.464
0 0.0
0 0.0
0.6134290551625478
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2241,	 Acc = 0.5859
2967 0.407
14640 0.608
6803 0.631
700 0.467
20 0.0
0 0.0
0 0.0
0 0.0
0.6098452375580923
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 1
Testing:	 Loss = 1.3273,	 Acc1 = 0.3185,	 Acc2 = 0.3278

 ===== Epoch 156	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.0637841   2.2571764  -0.40402457 -0.41193563  2.7378807   2.5917265
 -0.38958293 -0.38242146] [-1.7898616e-03 -1.3066782e-03  7.6247272e-03  8.5936040e-03
  3.9517106e-03  1.5506904e-03 -1.2308080e-02  1.3881210e+00
  6.1251274e-03  1.2857955e-02 -4.9767270e+00 -4.1922917e+00
 -6.7966389e-03 -7.4470886e-03] 6 6
train:	 Loss = 1.2690,	 Acc = 0.5366
24291 0.294
47780 0.591
24628 0.656
3190 0.641
535 0.561
56 0.482
0 0.0
0 0.0
0.6138418931866805
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.2797,	 Acc = 0.5408
2967 0.406
14640 0.584
6803 0.519
700 0.444
20 0.0
0 0.0
0 0.0
0 0.0
0.5588593601949194
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3669,	 Acc1 = 0.3423,	 Acc2 = 0.3564

 ===== Epoch 157	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  1.7148042   1.116824   -0.40402457 -0.41193563  2.0028098   3.0558882
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00563887 -0.00422588  0.00612513  0.01285795 -0.23473877  0.00434317
 -0.00679664 -0.00744709] 0 0
train:	 Loss = 1.2701,	 Acc = 0.5369
24288 0.292
47780 0.592
24629 0.658
3191 0.643
536 0.545
56 0.446
0 0.0
0 0.0
0.614867702645947
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2465,	 Acc = 0.5746
2967 0.408
14640 0.605
6803 0.595
700 0.469
20 0.0
0 0.0
0 0.0
0 0.0
0.5968506068673014
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3476,	 Acc1 = 0.3313,	 Acc2 = 0.3432

 ===== Epoch 158	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 1 1
train:	 Loss = 1.2696,	 Acc = 0.5362
24290 0.292
47775 0.592
24634 0.656
3189 0.637
536 0.552
56 0.446
0 0.0
0 0.0
0.6138994618716367
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.2957,	 Acc = 0.5410
2967 0.407
14640 0.588
6803 0.512
700 0.419
20 0.0
0 0.0
0 0.0
0 0.0
0.5589496006858278
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 0
Testing:	 Loss = 1.3827,	 Acc1 = 0.3322,	 Acc2 = 0.3442

 ===== Epoch 159	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576   1.5157609   3.2391326  -0.42759606 -0.42288172
  2.8249216   2.3555908 ] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.53822297 -0.01372849  0.0031886   0.00774676
  0.5090924   0.33433643] 2 2
train:	 Loss = 1.2685,	 Acc = 0.5366
24293 0.295
47774 0.59
24631 0.658
3190 0.63
536 0.56
56 0.411
0 0.0
0 0.0
0.6136479976898946
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2105,	 Acc = 0.5650
2967 0.41
14640 0.602
6803 0.571
700 0.409
20 0.0
0 0.0
0 0.0
0 0.0
0.5857510264855841
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 1
Testing:	 Loss = 1.3120,	 Acc1 = 0.3330,	 Acc2 = 0.3452

 ===== Epoch 160	 =====
[ 2.3445542   3.458988   -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.32521552  0.04052855  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 3 1
train:	 Loss = 1.2695,	 Acc = 0.5354
24293 0.293
47776 0.59
24631 0.656
3188 0.63
536 0.565
56 0.482
0 0.0
0 0.0
0.6127292057700132
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.2534,	 Acc = 0.5474
2967 0.408
14640 0.588
6803 0.534
700 0.43
20 0.0
0 0.0
0 0.0
0 0.0
0.5661237197130352
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3467,	 Acc1 = 0.3334,	 Acc2 = 0.3457

 ===== Epoch 161	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  1.3646857   1.0436162  -0.40402457 -0.41193563  1.6878873   2.960613
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.19737089  0.22134988  0.00612513  0.01285795  0.13669008  0.30726328
 -0.00679664 -0.00744709] 6 4
train:	 Loss = 1.2704,	 Acc = 0.5354
24292 0.292
47773 0.59
24636 0.657
3187 0.638
536 0.556
56 0.482
0 0.0
0 0.0
0.613128051661679
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.3035,	 Acc = 0.5431
2967 0.387
14640 0.579
6803 0.542
700 0.476
20 0.0
0 0.0
0 0.0
0 0.0
0.5640030681766909
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3918,	 Acc1 = 0.3367,	 Acc2 = 0.3497

 ===== Epoch 162	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495   1.2539127   1.8343936
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.11987827 -1.1420248
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 5 5
train:	 Loss = 1.2700,	 Acc = 0.5346
24290 0.29
47776 0.59
24633 0.655
3189 0.641
536 0.545
56 0.482
0 0.0
0 0.0
0.6125344533403334
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.1992,	 Acc = 0.5790
2967 0.407
14640 0.605
6803 0.612
700 0.464
20 0.0
0 0.0
0 0.0
0 0.0
0.6019943148490727
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.2944,	 Acc1 = 0.3456,	 Acc2 = 0.3603

 ===== Epoch 163	 =====
[-0.36757618 -0.38323686  2.0808384   0.9785697  -0.4419852  -0.36654198
  3.4141612   3.2426658  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02368235  0.06092326  0.00395171  0.00155069
  0.02020045  0.06578039  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 3 3
train:	 Loss = 1.2685,	 Acc = 0.5369
24291 0.294
47774 0.593
24633 0.656
3190 0.635
536 0.558
56 0.393
0 0.0
0 0.0
0.6144587801388652
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.2419,	 Acc = 0.5691
2967 0.406
14640 0.608
6803 0.567
700 0.474
20 0.0
0 0.0
0 0.0
0 0.0
0.5908496142219014
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3305,	 Acc1 = 0.3466,	 Acc2 = 0.3616

 ===== Epoch 164	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [ 9.6654725e-01  1.6015053e+00  7.6247272e-03  8.5936040e-03
  3.9517106e-03  1.5506904e-03 -4.8065083e-03 -4.2258762e-03
  6.1251274e-03  1.2857955e-02  3.1886040e-03  7.7467631e-03
  1.3665793e+00  2.5844111e+00] 6 6
train:	 Loss = 1.2699,	 Acc = 0.5355
24288 0.292
47778 0.591
24634 0.654
3188 0.634
536 0.554
56 0.411
0 0.0
0 0.0
0.6129908651826963
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2770,	 Acc = 0.5406
2967 0.404
14640 0.579
6803 0.536
700 0.377
20 0.0
0 0.0
0 0.0
0 0.0
0.5589044804403736
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 0
Testing:	 Loss = 1.3791,	 Acc1 = 0.3221,	 Acc2 = 0.3320

 ===== Epoch 165	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495   1.5901746   1.9744531
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 0 0
train:	 Loss = 1.2684,	 Acc = 0.5380
24287 0.295
47781 0.593
24631 0.656
3189 0.642
536 0.569
56 0.5
0 0.0
0 0.0
0.6154502382108593
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2299,	 Acc = 0.5730
2967 0.41
14640 0.609
6803 0.582
700 0.45
20 0.0
0 0.0
0 0.0
0 0.0
0.5948653160673194
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3169,	 Acc1 = 0.3487,	 Acc2 = 0.3641

 ===== Epoch 166	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495   2.6276836   2.9659858
 -0.36261833 -0.3698576   1.9322957   2.2942083  -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.0288834  -0.00078314
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 0 0
train:	 Loss = 1.2698,	 Acc = 0.5365
24290 0.292
47782 0.592
24627 0.656
3189 0.637
536 0.569
56 0.429
0 0.0
0 0.0
0.6144507153169707
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2458,	 Acc = 0.5683
2967 0.41
14640 0.602
6803 0.576
700 0.479
20 0.0
0 0.0
0 0.0
0 0.0
0.5894508866128232
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 1
Testing:	 Loss = 1.3427,	 Acc1 = 0.3406,	 Acc2 = 0.3544

 ===== Epoch 167	 =====
[-0.36757618 -0.38323686  2.3360465   1.1852167  -0.4419852  -0.36654198
  4.8522086   2.8343916  -0.40402457 -0.41193563  4.985711    2.9166398
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.03119411 -0.17114739  0.00395171  0.00155069
 -0.01814235 -0.2803617   0.00612513  0.01285795 -0.49105933 -0.21008345
 -0.00679664 -0.00744709] 5 3
train:	 Loss = 1.2691,	 Acc = 0.5374
24283 0.292
47783 0.593
24633 0.658
3189 0.638
536 0.554
56 0.429
0 0.0
0 0.0
0.6156804073651193
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2541,	 Acc = 0.5577
2967 0.408
14640 0.594
6803 0.56
700 0.431
20 0.0
0 0.0
0 0.0
0 0.0
0.5776745025492939
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3385,	 Acc1 = 0.3437,	 Acc2 = 0.3581

 ===== Epoch 168	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563  1.5947307   2.6845589
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.24408369 -0.1488187
 -0.00679664 -0.00744709] 2 2
train:	 Loss = 1.2662,	 Acc = 0.5373
24286 0.293
47781 0.592
24632 0.658
3189 0.64
536 0.55
56 0.393
0 0.0
0 0.0
0.615219046119117
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2113,	 Acc = 0.5797
2967 0.41
14640 0.617
6803 0.59
700 0.444
20 0.0
0 0.0
0 0.0
0 0.0
0.6024455173036142
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3032,	 Acc1 = 0.3514,	 Acc2 = 0.3673

 ===== Epoch 169	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495   1.8739669   2.5369143
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936  -0.23787409 -0.70793283
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 5 5
train:	 Loss = 1.2677,	 Acc = 0.5382
24287 0.294
47780 0.594
24631 0.659
3191 0.635
535 0.546
56 0.375
0 0.0
0 0.0
0.615935847125064
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.3137,	 Acc = 0.5238
2967 0.409
14640 0.566
6803 0.491
700 0.454
20 0.0
0 0.0
0 0.0
0 0.0
0.5391418129314623
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3940,	 Acc1 = 0.3406,	 Acc2 = 0.3544

 ===== Epoch 170	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495   1.8368174   1.0051522
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-1.7898616e-03 -1.3066782e-03  7.6247272e-03  8.5936040e-03
 -4.7035339e-01  9.5141852e-01 -4.8065083e-03 -4.2258762e-03
  6.1251274e-03  1.2857955e-02  9.7636061e+00  1.8184603e+00
 -6.7966389e-03 -7.4470886e-03] 4 6
train:	 Loss = 1.2690,	 Acc = 0.5374
24289 0.294
47779 0.593
24631 0.656
3189 0.637
536 0.563
56 0.429
0 0.0
0 0.0
0.6150988961950886
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2447,	 Acc = 0.5686
2967 0.409
14640 0.609
6803 0.568
700 0.431
20 0.0
0 0.0
0 0.0
0 0.0
0.5899472093128186
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3331,	 Acc1 = 0.3483,	 Acc2 = 0.3636

 ===== Epoch 171	 =====
[ 2.5950406   2.3532388  -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
  2.2393155   2.5907254 ] [ 1.020164    0.39351323  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
  0.4355778   0.4838667 ] 4 2
train:	 Loss = 1.2688,	 Acc = 0.5371
24288 0.294
47775 0.592
24634 0.655
3191 0.642
536 0.56
56 0.446
0 0.0
0 0.0
0.6144214615707686
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.2435,	 Acc = 0.5575
2967 0.406
14640 0.596
6803 0.554
700 0.44
20 0.0
0 0.0
0 0.0
0 0.0
0.5776745025492939
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3330,	 Acc1 = 0.3392,	 Acc2 = 0.3526

 ===== Epoch 172	 =====
[ 1.4354576   2.7336571  -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
  1.2447076   3.0479317 ] [ 0.05093211 -0.11373885  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.0738034  -0.18189909] 3 5
train:	 Loss = 1.2676,	 Acc = 0.5386
24288 0.295
47774 0.593
24635 0.66
3191 0.629
536 0.59
56 0.464
0 0.0
0 0.0
0.6164164216715665
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2354,	 Acc = 0.5628
2967 0.409
14640 0.6
6803 0.567
700 0.409
20 0.0
0 0.0
0 0.0
0 0.0
0.5833145332310608
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3289,	 Acc1 = 0.3443,	 Acc2 = 0.3588

 ===== Epoch 173	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495   1.7353259   1.3386273
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-1.7898616e-03 -1.3066782e-03  7.6247272e-03  8.5936040e-03
 -1.0948168e-01  9.3041402e-01  8.8682766e+00  2.0648484e+00
  6.1251274e-03  1.2857955e-02  9.3631039e+00  2.2609279e+00
 -6.7966389e-03 -7.4470886e-03] 4 6
train:	 Loss = 1.2691,	 Acc = 0.5364
24290 0.291
47776 0.592
24632 0.657
3190 0.64
536 0.547
56 0.429
0 0.0
0 0.0
0.6145294658091613
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.2646,	 Acc = 0.5407
2967 0.408
14640 0.582
6803 0.523
700 0.41
20 0.0
0 0.0
0 0.0
0 0.0
0.558363037494924
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3529,	 Acc1 = 0.3464,	 Acc2 = 0.3613

 ===== Epoch 174	 =====
[-0.36757618 -0.38323686  2.1678123   1.2101959  -0.4419852  -0.36654198
  1.8918556   3.7269638  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-1.7898616e-03 -1.3066782e-03 -3.9613748e+00 -1.5408192e+00
  3.9517106e-03  1.5506904e-03 -1.9987056e-01 -1.1701375e-01
  6.1251274e-03  1.2857955e-02  3.1886040e-03  7.7467631e-03
 -6.7966389e-03 -7.4470886e-03] 1 5
train:	 Loss = 1.2680,	 Acc = 0.5373
24287 0.291
47782 0.594
24630 0.656
3189 0.645
536 0.56
56 0.429
0 0.0
0 0.0
0.6158833488640689
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.2620,	 Acc = 0.5450
2967 0.407
14640 0.589
6803 0.527
700 0.409
20 0.0
0 0.0
0 0.0
0 0.0
0.5634616252312412
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3433,	 Acc1 = 0.3573,	 Acc2 = 0.3745

 ===== Epoch 175	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 1 1
train:	 Loss = 1.2670,	 Acc = 0.5374
24288 0.294
47780 0.591
24631 0.658
3189 0.643
536 0.554
56 0.411
0 0.0
0 0.0
0.6148939521209575
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2301,	 Acc = 0.5740
2967 0.401
14640 0.605
6803 0.595
700 0.481
20 0.0
0 0.0
0 0.0
0 0.0
0.5972115688309344
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3298,	 Acc1 = 0.3278,	 Acc2 = 0.3390

 ===== Epoch 176	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495   0.4400675   1.5631671
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [ 3.7536271e+00  1.0355526e+01  7.6247272e-03  8.5936040e-03
  2.9503800e-02  8.1372261e-01 -4.8065083e-03 -4.2258762e-03
  6.1251274e-03  1.2857955e-02  3.1886040e-03  7.7467631e-03
 -6.7966389e-03 -7.4470886e-03] 6 4
train:	 Loss = 1.2682,	 Acc = 0.5371
24291 0.294
47782 0.591
24627 0.66
3188 0.636
536 0.535
56 0.339
0 0.0
0 0.0
0.6146556589533922
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.2932,	 Acc = 0.5403
2967 0.411
14640 0.585
6803 0.513
700 0.433
20 0.0
0 0.0
0 0.0
0 0.0
0.5576411135676578
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3788,	 Acc1 = 0.3445,	 Acc2 = 0.3591

 ===== Epoch 177	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 5 1
train:	 Loss = 1.2666,	 Acc = 0.5363
24291 0.291
47775 0.591
24633 0.659
3189 0.64
536 0.541
56 0.375
0 0.0
0 0.0
0.6144456548845634
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2268,	 Acc = 0.5589
2967 0.268
14640 0.609
6803 0.59
700 0.459
20 0.0
0 0.0
0 0.0
0 0.0
0.5977981320218382
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3146,	 Acc1 = 0.3173,	 Acc2 = 0.3663

 ===== Epoch 178	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495   2.0535872   2.1945467
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936  -0.10698851  0.66902536
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 6 4
train:	 Loss = 1.2658,	 Acc = 0.5373
24290 0.294
47777 0.593
24631 0.657
3190 0.636
536 0.545
56 0.446
0 0.0
0 0.0
0.6148182176138601
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2679,	 Acc = 0.5481
2967 0.411
14640 0.592
6803 0.53
700 0.404
20 0.0
0 0.0
0 0.0
0 0.0
0.5665298019221224
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3613,	 Acc1 = 0.3332,	 Acc2 = 0.3454

 ===== Epoch 179	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [ 1.2882638e+00  2.6944506e+00  7.6247272e-03  8.5936040e-03
  3.9517106e-03  1.5506904e-03 -4.8065083e-03 -4.2258762e-03
  6.1251274e-03  1.2857955e-02  3.1886040e-03  7.7467631e-03
  1.7647183e+00  4.0654731e+00] 6 5
train:	 Loss = 1.2700,	 Acc = 0.5363
24285 0.292
47782 0.592
24634 0.656
3187 0.634
536 0.56
56 0.393
0 0.0
0 0.0
0.6143316490583371
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2412,	 Acc = 0.5559
2967 0.406
14640 0.597
6803 0.552
700 0.39
20 0.0
0 0.0
0 0.0
0 0.0
0.5760050534674909
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3388,	 Acc1 = 0.3299,	 Acc2 = 0.3415

 ===== Epoch 180	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 0 1
train:	 Loss = 1.2690,	 Acc = 0.5367
24293 0.295
47772 0.592
24633 0.656
3190 0.635
536 0.539
56 0.393
0 0.0
0 0.0
0.6138448816727263
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2505,	 Acc = 0.5634
2967 0.402
14640 0.599
6803 0.572
700 0.443
20 0.0
0 0.0
0 0.0
0 0.0
0.585029102558318
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 1
Testing:	 Loss = 1.3432,	 Acc1 = 0.3369,	 Acc2 = 0.3499

 ===== Epoch 181	 =====
[ 1.4890282   2.624604   -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
  1.1994582   2.9199138 ] [-0.29090744 -0.06667421  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
  0.11030177 -0.05729052] 3 3
train:	 Loss = 1.2694,	 Acc = 0.5375
24290 0.292
47775 0.594
24632 0.657
3191 0.639
536 0.55
56 0.393
0 0.0
0 0.0
0.6157500984381152
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.2449,	 Acc = 0.5587
2967 0.41
14640 0.605
6803 0.537
700 0.447
20 0.0
0 0.0
0 0.0
0 0.0
0.5786220277038308
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3264,	 Acc1 = 0.3557,	 Acc2 = 0.3725

 ===== Epoch 182	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  1.3891655   1.2801337  -0.40402457 -0.41193563  1.7934911   3.1878078
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.06566175 -0.268694    0.00612513  0.01285795 -0.14218006 -0.32580572
 -0.00679664 -0.00744709] 5 5
train:	 Loss = 1.2675,	 Acc = 0.5358
24288 0.293
47779 0.591
24632 0.657
3189 0.633
536 0.521
56 0.446
0 0.0
0 0.0
0.6132139857202856
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.2599,	 Acc = 0.5535
2967 0.41
14640 0.592
6803 0.547
700 0.439
20 0.0
0 0.0
0 0.0
0 0.0
0.5728015160402472
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3485,	 Acc1 = 0.3464,	 Acc2 = 0.3613

 ===== Epoch 183	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 1 1
train:	 Loss = 1.2691,	 Acc = 0.5376
24290 0.295
47777 0.592
24630 0.657
3191 0.645
536 0.563
56 0.411
0 0.0
0 0.0
0.614936343352146
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.2596,	 Acc = 0.5430
2967 0.411
14640 0.589
6803 0.519
700 0.39
20 0.0
0 0.0
0 0.0
0 0.0
0.560709290258539
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3403,	 Acc1 = 0.3563,	 Acc2 = 0.3733

 ===== Epoch 184	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 1 1
train:	 Loss = 1.2664,	 Acc = 0.5377
24287 0.291
47777 0.594
24633 0.658
3191 0.642
536 0.558
56 0.357
0 0.0
0 0.0
0.6162770858215322
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2266,	 Acc = 0.5669
2967 0.404
14640 0.594
6803 0.596
700 0.421
20 0.0
0 0.0
0 0.0
0 0.0
0.588728962685557
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 0
Testing:	 Loss = 1.3327,	 Acc1 = 0.3196,	 Acc2 = 0.3290

 ===== Epoch 185	 =====
[-0.36757618 -0.38323686  1.128197    1.51676    -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.20860744 -0.04828646  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 3 2
train:	 Loss = 1.2684,	 Acc = 0.5367
24290 0.294
47775 0.592
24633 0.656
3190 0.636
536 0.556
56 0.429
0 0.0
0 0.0
0.6142013387583672
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.2607,	 Acc = 0.5658
2967 0.406
14640 0.592
6803 0.593
700 0.453
20 0.0
0 0.0
0 0.0
0 0.0
0.5871046338492082
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3509,	 Acc1 = 0.3474,	 Acc2 = 0.3626

 ===== Epoch 186	 =====
[ 2.6501644   2.2594023  -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
  1.9827558   2.4705455 ] [-2.9717326e+00 -2.7258255e+00  7.6247272e-03  8.5936040e-03
  3.9517106e-03  1.5506904e-03 -4.8065083e-03 -4.2258762e-03
  6.1251274e-03  1.2857955e-02  3.1886040e-03  7.7467631e-03
  5.6306630e-02  1.0648075e-01] 1 4
train:	 Loss = 1.2701,	 Acc = 0.5357
24285 0.293
47776 0.591
24637 0.654
3190 0.634
536 0.56
56 0.411
0 0.0
0 0.0
0.6131110965286436
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2686,	 Acc = 0.5456
2967 0.409
14640 0.581
6803 0.543
700 0.426
20 0.0
0 0.0
0 0.0
0 0.0
0.5638225871948743
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3574,	 Acc1 = 0.3324,	 Acc2 = 0.3444

 ===== Epoch 187	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563  0.7472685   0.8083683
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.13431662  0.3208777
 -0.00679664 -0.00744709] 2 2
train:	 Loss = 1.2693,	 Acc = 0.5375
24290 0.294
47779 0.592
24630 0.659
3189 0.646
536 0.545
56 0.446
0 0.0
0 0.0
0.6150413440084
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2399,	 Acc = 0.5779
2967 0.403
14640 0.609
6803 0.603
700 0.43
20 0.0
0 0.0
0 0.0
0 0.0
0.6012723909218066
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3520,	 Acc1 = 0.3055,	 Acc2 = 0.3121

 ===== Epoch 188	 =====
[-0.36757618 -0.38323686  1.5161133   1.5712602  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.10968096  0.25886586  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
train:	 Loss = 1.2677,	 Acc = 0.5376
24295 0.293
47775 0.593
24628 0.659
3190 0.635
536 0.569
56 0.429
0 0.0
0 0.0
0.6157248802257662
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2435,	 Acc = 0.5705
2967 0.41
14640 0.605
6803 0.58
700 0.454
20 0.0
0 0.0
0 0.0
0 0.0
0.5919776203582547
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3371,	 Acc1 = 0.3251,	 Acc2 = 0.3357

 ===== Epoch 189	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9752316   1.0774044  -0.40402457 -0.41193563  3.330011    1.7782221
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
  0.4086664   0.3185808   0.00612513  0.01285795  0.01861414  0.20855899
 -0.00679664 -0.00744709] 2 2
train:	 Loss = 1.2692,	 Acc = 0.5366
24289 0.294
47780 0.593
24630 0.655
3189 0.635
536 0.547
56 0.411
0 0.0
0 0.0
0.6139701539551916
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2481,	 Acc = 0.5739
2967 0.411
14640 0.61
6803 0.587
700 0.396
20 0.0
0 0.0
0 0.0
0 0.0
0.5956323602400397
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3369,	 Acc1 = 0.3483,	 Acc2 = 0.3636

 ===== Epoch 190	 =====
[ 1.7282012   0.90257704 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
  1.3655181   1.151179  ] [-2.0643725e+00 -1.3269603e+00  7.6247272e-03  8.5936040e-03
  3.9517106e-03  1.5506904e-03 -4.8065083e-03 -4.2258762e-03
  6.1251274e-03  1.2857955e-02  3.1886040e-03  7.7467631e-03
  2.8334696e-02 -3.2368802e-02] 3 3
train:	 Loss = 1.2679,	 Acc = 0.5373
24282 0.295
47779 0.592
24636 0.657
3191 0.643
536 0.575
56 0.429
0 0.0
0 0.0
0.6145830599228326
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2280,	 Acc = 0.5769
2967 0.412
14640 0.608
6803 0.591
700 0.496
20 0.0
0 0.0
0 0.0
0 0.0
0.5989712584036457
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3217,	 Acc1 = 0.3373,	 Acc2 = 0.3504

 ===== Epoch 191	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 1 1
train:	 Loss = 1.2680,	 Acc = 0.5363
24286 0.29
47777 0.592
24635 0.657
3191 0.642
535 0.566
56 0.375
0 0.0
0 0.0
0.6146546972202536
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.3196,	 Acc = 0.5326
2967 0.408
14640 0.574
6803 0.508
700 0.447
20 0.0
0 0.0
0 0.0
0 0.0
0.5492487479131887
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.4102,	 Acc1 = 0.3309,	 Acc2 = 0.3427

 ===== Epoch 192	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495   2.7996452   2.0278091
 -0.36261833 -0.3698576   2.6580758   0.9782407  -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.42527986  0.09723762
 -0.00480651 -0.00422588  0.71449065  0.34851187  0.0031886   0.00774676
 -0.00679664 -0.00744709] 2 2
train:	 Loss = 1.2692,	 Acc = 0.5365
24288 0.291
47779 0.592
24632 0.658
3189 0.648
536 0.53
56 0.411
0 0.0
0 0.0
0.6149595758084838
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.2603,	 Acc = 0.5524
2967 0.404
14640 0.601
6803 0.525
700 0.457
20 0.0
0 0.0
0 0.0
0 0.0
0.5723051933402518
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3468,	 Acc1 = 0.3363,	 Acc2 = 0.3492

 ===== Epoch 193	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495   3.463743    2.0189166
 -0.36261833 -0.3698576   2.3585837   1.7673266  -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936  -0.52395463  0.08090083
 -0.00480651 -0.00422588  0.5736415   0.10923383  0.0031886   0.00774676
 -0.00679664 -0.00744709] 3 2
train:	 Loss = 1.2668,	 Acc = 0.5378
24291 0.293
47775 0.594
24633 0.656
3189 0.634
536 0.567
56 0.393
0 0.0
0 0.0
0.6157844308233472
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.2296,	 Acc = 0.5685
2967 0.41
14640 0.617
6803 0.551
700 0.411
20 0.0
0 0.0
0 0.0
0 0.0
0.5897667283310021
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3240,	 Acc1 = 0.3433,	 Acc2 = 0.3576

 ===== Epoch 194	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563  2.048069    1.4313222
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.33130306  0.01455396
 -0.00679664 -0.00744709] 3 2
train:	 Loss = 1.2681,	 Acc = 0.5364
24289 0.293
47781 0.591
24628 0.657
3190 0.645
536 0.532
56 0.375
0 0.0
0 0.0
0.6138520297672954
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.2462,	 Acc = 0.5604
2967 0.409
14640 0.599
6803 0.558
700 0.443
20 0.0
0 0.0
0 0.0
0 0.0
0.5806073185038126
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3344,	 Acc1 = 0.3518,	 Acc2 = 0.3678

 ===== Epoch 195	 =====
[-0.36757618 -0.38323686  1.0934892   1.5417393  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.00364582  0.01769441  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 0 0
train:	 Loss = 1.2690,	 Acc = 0.5363
24288 0.292
47780 0.592
24631 0.657
3189 0.63
536 0.539
56 0.446
0 0.0
0 0.0
0.6141983410331794
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2675,	 Acc = 0.5575
2967 0.404
14640 0.606
6803 0.538
700 0.4
20 0.0
0 0.0
0 0.0
0 0.0
0.5781257050038352
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3597,	 Acc1 = 0.3460,	 Acc2 = 0.3608

 ===== Epoch 196	 =====
[-0.36757618 -0.38323686  2.683946   -4.0422688   2.1991224   1.029607
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-1.7898616e-03 -1.3066782e-03 -1.2886831e-01  7.1277018e+00
  6.7396277e-01  1.2524354e-01 -4.8065083e-03 -4.2258762e-03
  6.1251274e-03  1.2857955e-02  3.1886040e-03  7.7467631e-03
 -6.7966389e-03 -7.4470886e-03] 2 2
train:	 Loss = 1.2707,	 Acc = 0.5361
24289 0.291
47779 0.592
24633 0.657
3188 0.63
535 0.548
56 0.464
0 0.0
0 0.0
0.6143245265188801
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2346,	 Acc = 0.5641
2967 0.407
14640 0.605
6803 0.555
700 0.48
20 0.0
0 0.0
0 0.0
0 0.0
0.5851644632946803
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3193,	 Acc1 = 0.3439,	 Acc2 = 0.3583

 ===== Epoch 197	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 2 1
train:	 Loss = 1.2673,	 Acc = 0.5369
24285 0.294
47779 0.592
24634 0.656
3190 0.636
536 0.543
56 0.357
0 0.0
0 0.0
0.6141741584093444
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.3162,	 Acc = 0.5370
2967 0.384
14640 0.581
6803 0.52
700 0.439
20 0.0
0 0.0
0 0.0
0 0.0
0.5574155123403871
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.4076,	 Acc1 = 0.3282,	 Acc2 = 0.3395

 ===== Epoch 198	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563  1.1930631   1.8637255
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795 -0.0395315  -0.5640575
 -0.00679664 -0.00744709] 5 5
train:	 Loss = 1.2724,	 Acc = 0.5358
24287 0.292
47784 0.591
24630 0.654
3187 0.638
536 0.569
56 0.429
0 0.0
0 0.0
0.6134815534235428
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.2398,	 Acc = 0.5669
2967 0.405
14640 0.609
6803 0.558
700 0.467
20 0.0
0 0.0
0 0.0
0 0.0
0.5885033614582863
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3286,	 Acc1 = 0.3414,	 Acc2 = 0.3554

 ===== Epoch 199	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
  2.3495834   1.9950509 ] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.3255689   0.4375835 ] 6 4
train:	 Loss = 1.2694,	 Acc = 0.5357
24292 0.29
47780 0.592
24629 0.657
3189 0.629
535 0.551
55 0.418
0 0.0
0 0.0
0.6139680789625663
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2327,	 Acc = 0.5732
2967 0.411
14640 0.61
6803 0.58
700 0.449
20 0.0
0 0.0
0 0.0
0 0.0
0.5949555565582276
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3222,	 Acc1 = 0.3538,	 Acc2 = 0.3703

 ===== Epoch 200	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495   3.1255665   2.2990355
 -0.36261833 -0.3698576   2.7016802   1.9305859  -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-1.7898616e-03 -1.3066782e-03  7.6247272e-03  8.5936040e-03
  1.8781422e-01 -7.4340239e+00 -4.8065083e-03 -4.2258762e-03
  1.7086074e-01 -2.2974336e-01  3.1886040e-03  7.7467631e-03
 -6.7966389e-03 -7.4470886e-03] 3 3
train:	 Loss = 1.2680,	 Acc = 0.5375
24290 0.291
47771 0.593
24636 0.659
3191 0.636
536 0.55
56 0.5
0 0.0
0 0.0
0.6160519753248458
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.2485,	 Acc = 0.5563
2967 0.409
14640 0.592
6803 0.557
700 0.437
20 0.0
0 0.0
0 0.0
0 0.0
0.5760050534674909
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 0
Testing:	 Loss = 1.3473,	 Acc1 = 0.3280,	 Acc2 = 0.3392

 ===== Epoch 201	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495   1.6633251   2.3724003
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.16973971 -0.9926598
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 5 5
train:	 Loss = 1.2698,	 Acc = 0.5372
24295 0.295
47773 0.593
24629 0.655
3191 0.645
536 0.535
56 0.429
0 0.0
0 0.0
0.6145960490910284
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2241,	 Acc = 0.5713
2967 0.411
14640 0.602
6803 0.59
700 0.444
20 0.0
0 0.0
0 0.0
0 0.0
0.5927446645309751
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3231,	 Acc1 = 0.3204,	 Acc2 = 0.3300

 ===== Epoch 202	 =====
[-0.36757618 -0.38323686  1.4781386   3.0404968  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-1.7898616e-03 -1.3066782e-03 -2.9038675e+00 -3.3746324e+00
  3.9517106e-03  1.5506904e-03 -4.8065083e-03 -4.2258762e-03
  6.1251274e-03  1.2857955e-02  3.1886040e-03  7.7467631e-03
 -6.7966389e-03 -7.4470886e-03] 6 6
train:	 Loss = 1.2684,	 Acc = 0.5368
24286 0.291
47779 0.592
24632 0.659
3191 0.631
536 0.554
56 0.411
0 0.0
0 0.0
0.6150746777961519
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.2686,	 Acc = 0.5482
2967 0.409
14640 0.595
6803 0.522
700 0.433
20 0.0
0 0.0
0 0.0
0 0.0
0.5668456436403014
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3450,	 Acc1 = 0.3586,	 Acc2 = 0.3760

 ===== Epoch 203	 =====
[ 3.7011108   2.9086494  -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
  2.3702314   3.1236973 ] [-0.06306448  0.11635489  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
  0.09924291  0.15632418] 4 4
train:	 Loss = 1.2685,	 Acc = 0.5382
24289 0.294
47779 0.594
24630 0.659
3190 0.639
536 0.55
56 0.375
0 0.0
0 0.0
0.6161620138861545
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2511,	 Acc = 0.5694
2967 0.402
14640 0.596
6803 0.6
700 0.447
20 0.0
0 0.0
0 0.0
0 0.0
0.5917971393764382
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 1
Testing:	 Loss = 1.3542,	 Acc1 = 0.3031,	 Acc2 = 0.3091

 ===== Epoch 204	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495   0.36193737  1.985569
 -0.36261833 -0.3698576  -0.40402457 -0.41193563  4.4097986   1.0209055
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.07126534  0.07389934
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.5158317   0.21876979
 -0.00679664 -0.00744709] 6 2
train:	 Loss = 1.2710,	 Acc = 0.5356
24289 0.29
47782 0.591
24630 0.658
3188 0.637
535 0.551
56 0.411
0 0.0
0 0.0
0.6139176543161265
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.2457,	 Acc = 0.5646
2967 0.411
14640 0.606
6803 0.559
700 0.424
20 0.0
0 0.0
0 0.0
0 0.0
0.5851644632946803
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3339,	 Acc1 = 0.3528,	 Acc2 = 0.3690

 ===== Epoch 205	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495   1.6713675  -4.421599
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-1.7898616e-03 -1.3066782e-03  7.6247272e-03  8.5936040e-03
 -2.6529893e-01  7.5048060e+00 -4.8065083e-03 -4.2258762e-03
  7.5346503e+00  4.1536975e+00  3.1886040e-03  7.7467631e-03
  8.4816751e+00  4.2470455e+00] 1 6
train:	 Loss = 1.2681,	 Acc = 0.5376
24289 0.294
47774 0.592
24635 0.66
3190 0.634
536 0.534
56 0.464
0 0.0
0 0.0
0.6152038954732186
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2202,	 Acc = 0.5775
2967 0.409
14640 0.602
6803 0.61
700 0.48
20 0.0
0 0.0
0 0.0
0 0.0
0.600054144294545
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3162,	 Acc1 = 0.3398,	 Acc2 = 0.3534

 ===== Epoch 206	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495   0.96705633  0.749488
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936  -0.6504781   0.05522872
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 1 1
train:	 Loss = 1.2689,	 Acc = 0.5373
24294 0.291
47780 0.592
24626 0.659
3188 0.646
536 0.562
56 0.446
0 0.0
0 0.0
0.6158480560732943
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.2805,	 Acc = 0.5531
2967 0.405
14640 0.597
6803 0.533
700 0.471
20 0.0
0 0.0
0 0.0
0 0.0
0.5729819970220638
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3712,	 Acc1 = 0.3402,	 Acc2 = 0.3539

 ===== Epoch 207	 =====
[ 2.8426468   1.9855012  -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-3.1611667e+00 -2.4434378e+00  7.6247272e-03  8.5936040e-03
  3.9517106e-03  1.5506904e-03 -4.8065083e-03 -4.2258762e-03
  6.1251274e-03  1.2857955e-02  3.1886040e-03  7.7467631e-03
 -6.7966389e-03 -7.4470886e-03] 6 5
train:	 Loss = 1.2672,	 Acc = 0.5374
24290 0.293
47778 0.593
24630 0.657
3190 0.641
536 0.537
56 0.464
0 0.0
0 0.0
0.6152513453209083
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2196,	 Acc = 0.5829
2967 0.406
14640 0.605
6803 0.624
700 0.49
20 0.0
0 0.0
0 0.0
0 0.0
0.6065514596399404
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3201,	 Acc1 = 0.3454,	 Acc2 = 0.3601

 ===== Epoch 208	 =====
[ 1.5893627   2.7438018  -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
  1.2407529   3.0427065 ] [-0.3359518  -0.11635354  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.03932364 -0.12137493] 3 5
train:	 Loss = 1.2689,	 Acc = 0.5368
24292 0.293
47778 0.593
24629 0.656
3191 0.647
534 0.536
56 0.321
0 0.0
0 0.0
0.6145980994382317
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.2709,	 Acc = 0.5595
2967 0.408
14640 0.596
6803 0.559
700 0.47
20 0.0
0 0.0
0 0.0
0 0.0
0.5798402743310923
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3609,	 Acc1 = 0.3369,	 Acc2 = 0.3499

 ===== Epoch 209	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.6040492   1.0492476  -0.40402457 -0.41193563  2.7028058   1.7904369
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
  0.78712857  0.01522031  0.00612513  0.01285795  0.97744566 -0.01267482
 -0.00679664 -0.00744709] 2 2
train:	 Loss = 1.2700,	 Acc = 0.5365
24290 0.294
47778 0.592
24632 0.655
3188 0.64
536 0.539
56 0.464
0 0.0
0 0.0
0.6137682110513191
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2890,	 Acc = 0.5477
2967 0.408
14640 0.59
6803 0.53
700 0.437
20 0.0
0 0.0
0 0.0
0 0.0
0.5663493209403059
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3856,	 Acc1 = 0.3338,	 Acc2 = 0.3462

 ===== Epoch 210	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495   2.570619    1.1252033
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936  -0.24348466 -0.03112289
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 0 3
train:	 Loss = 1.2685,	 Acc = 0.5372
24287 0.294
47776 0.593
24635 0.655
3190 0.636
536 0.567
56 0.411
0 0.0
0 0.0
0.6146496397306839
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2164,	 Acc = 0.5736
2967 0.41
14640 0.599
6803 0.602
700 0.467
20 0.05
0 0.0
0 0.0
0 0.0
0.5954518792582232
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3134,	 Acc1 = 0.3485,	 Acc2 = 0.3638

 ===== Epoch 211	 =====
[-0.36757618 -0.38323686  0.5495894   1.4826972  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.05458437  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 0 0
train:	 Loss = 1.2680,	 Acc = 0.5361
24284 0.294
47782 0.59
24632 0.657
3191 0.636
535 0.55
56 0.375
0 0.0
0 0.0
0.6132342905139377
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2205,	 Acc = 0.5788
2967 0.409
14640 0.596
6803 0.63
700 0.456
20 0.0
0 0.0
0 0.0
0 0.0
0.6014528719036232
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 1
Testing:	 Loss = 1.3422,	 Acc1 = 0.3012,	 Acc2 = 0.3069

 ===== Epoch 212	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576   1.0682428   2.7048297  -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-1.7898616e-03 -1.3066782e-03  7.6247272e-03  8.5936040e-03
  3.9517106e-03  1.5506904e-03 -4.8065083e-03 -4.2258762e-03
 -2.1075156e+00 -4.1745071e+00  3.1886040e-03  7.7467631e-03
 -6.7966389e-03 -7.4470886e-03] 1 1
train:	 Loss = 1.2693,	 Acc = 0.5362
24289 0.291
47780 0.592
24630 0.657
3189 0.632
536 0.547
56 0.411
0 0.0
0 0.0
0.6142195272407502
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2333,	 Acc = 0.5649
2967 0.408
14640 0.599
6803 0.574
700 0.44
20 0.0
0 0.0
0 0.0
0 0.0
0.5859315074674006
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3281,	 Acc1 = 0.3414,	 Acc2 = 0.3554

 ===== Epoch 213	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563  1.6709154   2.1642091
 -0.38958293 -0.38242146] [-1.7898616e-03 -1.3066782e-03  7.6247272e-03  8.5936040e-03
  3.9517106e-03  1.5506904e-03  2.4769430e+00  2.1854148e+00
  6.1251274e-03  1.2857955e-02  2.0907634e-01  1.4985223e+00
 -6.7966389e-03 -7.4470886e-03] 6 6
train:	 Loss = 1.2669,	 Acc = 0.5383
24290 0.294
47773 0.593
24635 0.66
3190 0.633
536 0.556
56 0.375
0 0.0
0 0.0
0.6161438508990681
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2595,	 Acc = 0.5563
2967 0.407
14640 0.593
6803 0.553
700 0.454
20 0.0
0 0.0
0 0.0
0 0.0
0.5762306546947615
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 1
Testing:	 Loss = 1.3542,	 Acc1 = 0.3303,	 Acc2 = 0.3419

 ===== Epoch 214	 =====
[ 2.6143777   1.8916646  -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
  2.1883543   2.1178436 ] [-0.44559827 -0.18695047  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
  0.17145632 -0.3029474 ] 4 2
train:	 Loss = 1.2671,	 Acc = 0.5374
24289 0.294
47780 0.592
24629 0.659
3191 0.636
535 0.564
56 0.411
0 0.0
0 0.0
0.6151645207439199
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.2772,	 Acc = 0.5587
2967 0.408
14640 0.598
6803 0.556
700 0.403
20 0.0
0 0.0
0 0.0
0 0.0
0.5787573884401931
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3580,	 Acc1 = 0.3608,	 Acc2 = 0.3787

 ===== Epoch 215	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576   3.3333666   1.4432819  -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 0 0
train:	 Loss = 1.2699,	 Acc = 0.5363
24284 0.294
47785 0.591
24630 0.655
3190 0.64
535 0.536
56 0.393
0 0.0
0 0.0
0.6135361436295869
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2421,	 Acc = 0.5821
2967 0.405
14640 0.614
6803 0.603
700 0.487
20 0.0
0 0.0
0 0.0
0 0.0
0.6057844154672202
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3381,	 Acc1 = 0.3379,	 Acc2 = 0.3511

 ===== Epoch 216	 =====
[-0.36757618 -0.38323686  1.5573549   1.3191966  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.19984339  0.52278936  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 6 2
train:	 Loss = 1.2723,	 Acc = 0.5352
24285 0.291
47788 0.591
24624 0.655
3191 0.635
536 0.567
56 0.411
0 0.0
0 0.0
0.6129798543211497
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2227,	 Acc = 0.5779
2967 0.411
14640 0.609
6803 0.598
700 0.456
20 0.0
0 0.0
0 0.0
0 0.0
0.6002797455218156
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3177,	 Acc1 = 0.3404,	 Acc2 = 0.3541

 ===== Epoch 217	 =====
[ 2.8000805   3.286532   -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
  2.5894485   3.510363  ] [-0.02533269 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
  0.00036075  0.00323365] 0 0
train:	 Loss = 1.2693,	 Acc = 0.5360
24296 0.292
47775 0.591
24629 0.658
3188 0.636
536 0.55
56 0.429
0 0.0
0 0.0
0.6138296755224194
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.2415,	 Acc = 0.5651
2967 0.408
14640 0.6
6803 0.57
700 0.474
20 0.0
0 0.0
0 0.0
0 0.0
0.5861571086946713
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3363,	 Acc1 = 0.3270,	 Acc2 = 0.3380

 ===== Epoch 218	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563  0.65335757  3.2610965
 -0.38958293 -0.38242146] [-1.7898616e-03 -1.3066782e-03  7.6247272e-03  8.5936040e-03
  3.9517106e-03  1.5506904e-03 -4.8065083e-03 -4.2258762e-03
  6.1251274e-03  1.2857955e-02 -1.6973635e+00 -5.1248775e+00
 -6.7966389e-03 -7.4470886e-03] 1 1
train:	 Loss = 1.2690,	 Acc = 0.5358
24287 0.294
47777 0.59
24634 0.656
3190 0.639
536 0.517
56 0.393
0 0.0
0 0.0
0.6127465777696114
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2789,	 Acc = 0.5487
2967 0.41
14640 0.589
6803 0.535
700 0.434
20 0.0
0 0.0
0 0.0
0 0.0
0.5673419663402969
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3765,	 Acc1 = 0.3227,	 Acc2 = 0.3328

 ===== Epoch 219	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-1.7898616e-03 -1.3066782e-03  7.6247272e-03  8.5936040e-03
  3.9517106e-03  1.5506904e-03  3.0046220e+00  5.2968044e+00
  6.1251274e-03  1.2857955e-02  3.1886040e-03  7.7467631e-03
 -6.7966389e-03 -7.4470886e-03] 5 5
train:	 Loss = 1.2691,	 Acc = 0.5365
24290 0.293
47779 0.591
24629 0.657
3190 0.636
536 0.556
56 0.446
0 0.0
0 0.0
0.6140438377739861
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2175,	 Acc = 0.5728
2967 0.411
14640 0.607
6803 0.585
700 0.446
20 0.0
0 0.0
0 0.0
0 0.0
0.5945043541036863
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 1
Testing:	 Loss = 1.3216,	 Acc1 = 0.3361,	 Acc2 = 0.3489

 ===== Epoch 220	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563  2.839335    1.1161808
  2.8798363   3.2935169 ] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795 -0.09293208  0.30726328
  0.22414829  0.25957128] 6 4
train:	 Loss = 1.2689,	 Acc = 0.5368
24291 0.293
47777 0.592
24630 0.658
3191 0.64
535 0.557
56 0.429
0 0.0
0 0.0
0.6145769074275814
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2357,	 Acc = 0.5659
2967 0.41
14640 0.598
6803 0.58
700 0.437
20 0.0
0 0.0
0 0.0
0 0.0
0.5867436718855751
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3244,	 Acc1 = 0.3414,	 Acc2 = 0.3554

 ===== Epoch 221	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
  1.952004    2.7500944 ] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
  0.1851155  -0.0003266 ] 3 0
train:	 Loss = 1.2686,	 Acc = 0.5363
24287 0.291
47785 0.592
24628 0.657
3189 0.636
535 0.55
56 0.446
0 0.0
0 0.0
0.6143871484257084
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2193,	 Acc = 0.5668
2967 0.399
14640 0.599
6803 0.586
700 0.441
20 0.0
0 0.0
0 0.0
0 0.0
0.5893155258764607
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 1
Testing:	 Loss = 1.3234,	 Acc1 = 0.3307,	 Acc2 = 0.3424

 ===== Epoch 222	 =====
[-0.36757618 -0.38323686  3.7501032   2.8815377   2.366488    1.2630396
 -0.36261833 -0.3698576   0.9196459   1.8217463  -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-1.7898616e-03 -1.3066782e-03  1.3159494e-01 -1.3838302e+00
 -4.5665140e+00 -1.7091448e+00  5.8931012e+00  4.9117699e+00
 -1.8941846e+00 -2.9880874e+00  3.1886040e-03  7.7467631e-03
 -6.7966389e-03 -7.4470886e-03] 5 5
train:	 Loss = 1.2682,	 Acc = 0.5382
24287 0.294
47779 0.595
24631 0.656
3191 0.641
536 0.535
56 0.393
0 0.0
0 0.0
0.6160802173428005
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2224,	 Acc = 0.5733
2967 0.403
14640 0.605
6803 0.59
700 0.483
20 0.0
0 0.0
0 0.0
0 0.0
0.5960835626945811
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3214,	 Acc1 = 0.3299,	 Acc2 = 0.3415

 ===== Epoch 223	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563  2.173283    1.839296
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.00793549 -0.04671079
 -0.00679664 -0.00744709] 3 3
train:	 Loss = 1.2692,	 Acc = 0.5377
24290 0.294
47775 0.593
24633 0.657
3190 0.643
536 0.547
56 0.375
0 0.0
0 0.0
0.6155138469615435
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.2347,	 Acc = 0.5682
2967 0.408
14640 0.605
6803 0.568
700 0.491
20 0.0
0 0.0
0 0.0
0 0.0
0.5895862473491856
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3290,	 Acc1 = 0.3379,	 Acc2 = 0.3511

 ===== Epoch 224	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 1 1
train:	 Loss = 1.2708,	 Acc = 0.5359
24288 0.294
47777 0.591
24632 0.656
3191 0.629
536 0.541
56 0.375
0 0.0
0 0.0
0.6131352372952541
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.2645,	 Acc = 0.5511
2967 0.408
14640 0.597
6803 0.528
700 0.441
20 0.0
0 0.0
0 0.0
0 0.0
0.5702296620493615
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3482,	 Acc1 = 0.3505,	 Acc2 = 0.3663

 ===== Epoch 225	 =====
[-0.36757618 -0.38323686  1.6876128   0.8196106  -0.4419852  -0.36654198
  1.8576978   3.8649325  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-1.7898616e-03 -1.3066782e-03 -3.2250636e+00 -1.1494844e+00
  3.9517106e-03  1.5506904e-03  1.9275978e-01 -1.2915634e+00
  6.1251274e-03  1.2857955e-02  3.1886040e-03  7.7467631e-03
 -6.7966389e-03 -7.4470886e-03] 5 5
train:	 Loss = 1.2689,	 Acc = 0.5368
24293 0.292
47773 0.593
24635 0.656
3189 0.638
534 0.545
56 0.429
0 0.0
0 0.0
0.6147374223948968
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2348,	 Acc = 0.5707
2967 0.407
14640 0.604
6803 0.582
700 0.469
20 0.0
0 0.0
0 0.0
0 0.0
0.5925641835491585
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3281,	 Acc1 = 0.3511,	 Acc2 = 0.3670

 ===== Epoch 226	 =====
[-0.36757618 -0.38323686  2.3940299   1.1193621  -0.4419852  -0.36654198
  2.2248962   3.3637407  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.05874434  0.07457447  0.00395171  0.00155069
 -0.26989725  0.2641315   0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 1 3
train:	 Loss = 1.2676,	 Acc = 0.5370
24284 0.293
47782 0.591
24631 0.66
3191 0.642
536 0.543
56 0.446
0 0.0
0 0.0
0.6148223003832222
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2727,	 Acc = 0.5686
2967 0.41
14640 0.598
6803 0.586
700 0.467
20 0.0
0 0.0
0 0.0
0 0.0
0.5898569688219104
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3714,	 Acc1 = 0.3396,	 Acc2 = 0.3531

 ===== Epoch 227	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576   0.8594017   2.9695077  -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-1.7898616e-03 -1.3066782e-03  7.6247272e-03  8.5936040e-03
  3.9517106e-03  1.5506904e-03 -4.8065083e-03 -4.2258762e-03
 -1.8076957e+00 -4.5301008e+00  3.1886040e-03  7.7467631e-03
 -6.7966389e-03 -7.4470886e-03] 1 1
train:	 Loss = 1.2685,	 Acc = 0.5368
24287 0.294
47779 0.591
24633 0.659
3190 0.631
536 0.545
55 0.382
0 0.0
0 0.0
0.6140852834249866
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.2349,	 Acc = 0.5639
2967 0.408
14640 0.606
6803 0.556
700 0.447
20 0.0
0 0.0
0 0.0
0 0.0
0.5848035013310472
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3231,	 Acc1 = 0.3505,	 Acc2 = 0.3663

 ===== Epoch 228	 =====
[-0.36757618 -0.38323686  0.6675981   1.2851337  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.20963767  0.192885    0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 0 1
train:	 Loss = 1.2697,	 Acc = 0.5354
24291 0.291
47775 0.59
24632 0.657
3190 0.635
536 0.552
56 0.393
0 0.0
0 0.0
0.6132643819974012
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.2834,	 Acc = 0.5501
2967 0.407
14640 0.594
6803 0.53
700 0.444
20 0.0
0 0.0
0 0.0
0 0.0
0.5692370166493705
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3655,	 Acc1 = 0.3443,	 Acc2 = 0.3588

 ===== Epoch 229	 =====
[-0.36757618 -0.38323686  3.445079    3.0609345   2.2761028   1.3830906
 -0.36261833 -0.3698576   0.9580856   1.5768577  -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-1.7898616e-03 -1.3066782e-03 -5.9198642e+00 -3.3951094e+00
 -7.2086081e-02  4.0763667e-01 -4.8065083e-03 -4.2258762e-03
 -1.6113939e-02  1.0995790e+00  3.1886040e-03  7.7467631e-03
 -6.7966389e-03 -7.4470886e-03] 4 4
train:	 Loss = 1.2682,	 Acc = 0.5374
24295 0.29
47775 0.594
24628 0.659
3190 0.641
536 0.558
56 0.375
0 0.0
0 0.0
0.616210540132572
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.3266,	 Acc = 0.5390
2967 0.409
14640 0.576
6803 0.523
700 0.487
20 0.0
0 0.0
0 0.0
0 0.0
0.5564228669403961
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.4072,	 Acc1 = 0.3293,	 Acc2 = 0.3407

 ===== Epoch 230	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563  1.2322875   2.4842365
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.19483571  0.18473381
 -0.00679664 -0.00744709] 2 4
train:	 Loss = 1.2682,	 Acc = 0.5369
24289 0.293
47779 0.593
24631 0.655
3189 0.644
536 0.543
56 0.482
0 0.0
0 0.0
0.6146788990825688
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2058,	 Acc = 0.5771
2967 0.409
14640 0.594
6803 0.625
700 0.491
20 0.0
0 0.0
0 0.0
0 0.0
0.5996480620854577
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3098,	 Acc1 = 0.3169,	 Acc2 = 0.3258

 ===== Epoch 231	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563  2.1789412   1.8637255
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.3140947   0.0690115
 -0.00679664 -0.00744709] 3 3
train:	 Loss = 1.2676,	 Acc = 0.5371
24292 0.293
47778 0.593
24628 0.657
3190 0.638
536 0.562
56 0.393
0 0.0
0 0.0
0.6150574893684044
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.2874,	 Acc = 0.5422
2967 0.409
14640 0.592
6803 0.506
700 0.43
20 0.0
0 0.0
0 0.0
0 0.0
0.560032486576727
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3721,	 Acc1 = 0.3501,	 Acc2 = 0.3658

 ===== Epoch 232	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  1.8286635   2.403592   -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [ 4.8244419e+00  2.4512835e+00  7.6247272e-03  8.5936040e-03
  3.9517106e-03  1.5506904e-03 -1.1150868e-01 -6.4983922e-01
  6.1251274e-03  1.2857955e-02  3.1886040e-03  7.7467631e-03
 -6.7966389e-03 -7.4470886e-03] 5 5
train:	 Loss = 1.2696,	 Acc = 0.5360
24294 0.292
47776 0.591
24628 0.656
3190 0.639
536 0.545
56 0.393
0 0.0
0 0.0
0.6136166749796551
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2359,	 Acc = 0.5638
2967 0.408
14640 0.6
6803 0.574
700 0.387
20 0.0
0 0.0
0 0.0
0 0.0
0.584713260840139
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3369,	 Acc1 = 0.3361,	 Acc2 = 0.3489

 ===== Epoch 233	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 1 1
train:	 Loss = 1.2694,	 Acc = 0.5370
24291 0.293
47781 0.591
24627 0.659
3190 0.639
535 0.544
56 0.375
0 0.0
0 0.0
0.6147475357335048
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2136,	 Acc = 0.5890
2967 0.391
14640 0.608
6803 0.648
700 0.466
20 0.0
0 0.0
0 0.0
0 0.0
0.6155303884853134
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 1
Testing:	 Loss = 1.3144,	 Acc1 = 0.3227,	 Acc2 = 0.3328

 ===== Epoch 234	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495   1.921458    2.2412333
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936  -0.09639543 -0.5632356
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 3 5
train:	 Loss = 1.2673,	 Acc = 0.5367
24287 0.293
47785 0.592
24626 0.657
3190 0.635
536 0.539
56 0.429
0 0.0
0 0.0
0.6144396466867035
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.3673,	 Acc = 0.5153
2967 0.409
14640 0.566
6803 0.467
700 0.394
20 0.0
0 0.0
0 0.0
0 0.0
0.5295312006497316
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.4420,	 Acc1 = 0.3373,	 Acc2 = 0.3504

 ===== Epoch 235	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576   1.077423    3.2317116  -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588 -0.04082288 -0.58866036  0.0031886   0.00774676
 -0.00679664 -0.00744709] 1 5
train:	 Loss = 1.2706,	 Acc = 0.5366
24286 0.291
47780 0.592
24632 0.657
3190 0.64
536 0.563
56 0.411
0 0.0
0 0.0
0.614772816757225
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.2349,	 Acc = 0.5712
2967 0.408
14640 0.598
6803 0.596
700 0.473
20 0.0
0 0.0
0 0.0
0 0.0
0.5929702657582457
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 1
Testing:	 Loss = 1.3222,	 Acc1 = 0.3501,	 Acc2 = 0.3658

 ===== Epoch 236	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  1.3162959   2.831576   -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.06482681  0.41581175  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 1
train:	 Loss = 1.2675,	 Acc = 0.5375
24286 0.294
47780 0.593
24632 0.657
3190 0.638
536 0.556
56 0.375
0 0.0
0 0.0
0.6151665485471297
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2282,	 Acc = 0.5780
2967 0.407
14640 0.606
6803 0.603
700 0.487
20 0.0
0 0.0
0 0.0
0 0.0
0.6008211884672653
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3368,	 Acc1 = 0.3206,	 Acc2 = 0.3303

 ===== Epoch 237	 =====
[-0.36757618 -0.38323686  1.4854889   1.5939686  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.07064006  0.6615767   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 6 4
train:	 Loss = 1.2676,	 Acc = 0.5385
24288 0.294
47782 0.595
24631 0.656
3188 0.643
535 0.553
56 0.482
0 0.0
0 0.0
0.6164689206215875
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.2884,	 Acc = 0.5528
2967 0.409
14640 0.595
6803 0.539
700 0.424
20 0.0
0 0.0
0 0.0
0 0.0
0.5720795921129811
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3756,	 Acc1 = 0.3503,	 Acc2 = 0.3661

 ===== Epoch 238	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-1.7898616e-03 -1.3066782e-03  7.6247272e-03  8.5936040e-03
  2.9470699e+00  3.3155856e+00  9.5735140e+00  2.5626707e+00
  8.6845074e+00  4.2467499e+00  9.1904440e+00  2.7272208e+00
  1.0083987e+01  4.3467326e+00] 1 1
train:	 Loss = 1.2696,	 Acc = 0.5374
24289 0.293
47782 0.594
24627 0.656
3190 0.638
536 0.55
56 0.411
0 0.0
0 0.0
0.6153613943904136
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2201,	 Acc = 0.5700
2967 0.408
14640 0.599
6803 0.594
700 0.437
20 0.0
0 0.0
0 0.0
0 0.0
0.5916166583946216
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 1
Testing:	 Loss = 1.3163,	 Acc1 = 0.3464,	 Acc2 = 0.3613

 ===== Epoch 239	 =====
[-0.36757618 -0.38323686  2.4099548   2.1503255  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.38494956  0.26569146  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 6 1
train:	 Loss = 1.2682,	 Acc = 0.5378
24292 0.292
47779 0.594
24627 0.658
3190 0.646
536 0.543
56 0.446
0 0.0
0 0.0
0.6160681472147845
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2324,	 Acc = 0.5905
2967 0.407
14640 0.622
6803 0.615
700 0.479
20 0.0
0 0.0
0 0.0
0 0.0
0.6149889455398637
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3338,	 Acc1 = 0.3256,	 Acc2 = 0.3362

 ===== Epoch 240	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563  0.518337    1.6365305
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.05540247  0.0792223
 -0.00679664 -0.00744709] 1 4
train:	 Loss = 1.2688,	 Acc = 0.5362
24288 0.293
47777 0.591
24632 0.657
3191 0.64
536 0.556
56 0.411
0 0.0
0 0.0
0.6138308483830324
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.1944,	 Acc = 0.5921
2967 0.407
14640 0.614
6803 0.639
700 0.469
20 0.0
0 0.0
0 0.0
0 0.0
0.6168388756034833
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 1
Testing:	 Loss = 1.3116,	 Acc1 = 0.3091,	 Acc2 = 0.3164

 ===== Epoch 241	 =====
[ 1.554413    2.497798   -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
  1.5210365   2.7500944 ] [ 0.48039338  0.03268444  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.23319052  0.0958    ] 1 3
train:	 Loss = 1.2675,	 Acc = 0.5369
24287 0.294
47782 0.592
24630 0.657
3190 0.638
535 0.561
56 0.482
0 0.0
0 0.0
0.6144002729909571
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2608,	 Acc = 0.5513
2967 0.398
14640 0.589
6803 0.555
700 0.396
20 0.0
0 0.0
0 0.0
0 0.0
0.571718630149348
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 1
Testing:	 Loss = 1.3526,	 Acc1 = 0.3340,	 Acc2 = 0.3464

 ===== Epoch 242	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.8027349   3.5129719  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-1.7898616e-03 -1.3066782e-03  2.9729631e+00  1.6057857e+00
  3.9517106e-03  1.5506904e-03 -4.6397882e+00 -5.3674836e+00
  6.1251274e-03  1.2857955e-02  3.1886040e-03  7.7467631e-03
 -6.7966389e-03 -7.4470886e-03] 6 6
train:	 Loss = 1.2687,	 Acc = 0.5370
24292 0.292
47775 0.592
24631 0.659
3190 0.633
536 0.55
56 0.375
0 0.0
0 0.0
0.6151362419278626
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.2631,	 Acc = 0.5586
2967 0.407
14640 0.6
6803 0.547
700 0.456
20 0.0
0 0.0
0 0.0
0 0.0
0.5788476289311014
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3467,	 Acc1 = 0.3501,	 Acc2 = 0.3658

 ===== Epoch 243	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576   1.5071553   2.1680536  -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.13708965  0.00621134  0.0031886   0.00774676
 -0.00679664 -0.00744709] 3 0
train:	 Loss = 1.2703,	 Acc = 0.5353
24291 0.294
47777 0.59
24632 0.654
3188 0.63
536 0.552
56 0.411
0 0.0
0 0.0
0.6121356101274462
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.2944,	 Acc = 0.5411
2967 0.403
14640 0.587
6803 0.518
700 0.407
20 0.0
0 0.0
0 0.0
0 0.0
0.5595361638767314
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3773,	 Acc1 = 0.3536,	 Acc2 = 0.3700

 ===== Epoch 244	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576   1.5972317   2.158159   -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00035784  0.03944441  0.0031886   0.00774676
 -0.00679664 -0.00744709] 0 0
train:	 Loss = 1.2683,	 Acc = 0.5354
24292 0.291
47773 0.591
24633 0.656
3190 0.632
536 0.556
56 0.393
0 0.0
0 0.0
0.6132461805008663
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2168,	 Acc = 0.5825
2967 0.41
14640 0.615
6803 0.605
700 0.434
20 0.0
0 0.0
0 0.0
0 0.0
0.6055588142399495
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3090,	 Acc1 = 0.3450,	 Acc2 = 0.3596

 ===== Epoch 245	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 6 1
train:	 Loss = 1.2691,	 Acc = 0.5383
24291 0.293
47773 0.594
24634 0.659
3190 0.643
536 0.565
56 0.429
0 0.0
0 0.0
0.6164538187927391
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.2676,	 Acc = 0.5551
2967 0.409
14640 0.589
6803 0.558
700 0.451
20 0.0
0 0.0
0 0.0
0 0.0
0.5746063258584126
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3562,	 Acc1 = 0.3386,	 Acc2 = 0.3519

 ===== Epoch 246	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576   3.5536833   1.7030125  -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-1.7898616e-03 -1.3066782e-03  7.6247272e-03  8.5936040e-03
  3.9517106e-03  1.5506904e-03 -4.8065083e-03 -4.2258762e-03
  1.4367938e-01  1.9071422e+00  3.1886040e-03  7.7467631e-03
  5.7870860e+00  3.5314364e+00] 6 6
train:	 Loss = 1.2702,	 Acc = 0.5361
24291 0.291
47777 0.592
24629 0.657
3191 0.639
536 0.554
56 0.393
0 0.0
0 0.0
0.6143012770872436
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2435,	 Acc = 0.5396
2967 0.14
14640 0.584
6803 0.628
700 0.473
20 0.0
0 0.0
0 0.0
0 0.0
0.593060506249154
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3401,	 Acc1 = 0.3099,	 Acc2 = 0.3419

 ===== Epoch 247	 =====
[ 1.7748903   1.3717597  -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00954522 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 6 0
train:	 Loss = 1.2705,	 Acc = 0.5352
24290 0.292
47782 0.59
24626 0.656
3190 0.632
536 0.558
56 0.411
0 0.0
0 0.0
0.6126263289145557
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2264,	 Acc = 0.5607
2967 0.407
14640 0.6
6803 0.563
700 0.381
20 0.0
0 0.0
0 0.0
0 0.0
0.5812390019401705
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3230,	 Acc1 = 0.3357,	 Acc2 = 0.3484

 ===== Epoch 248	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495   0.5878993   0.88510114
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936  -0.04715632  0.4006352
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 6 6
train:	 Loss = 1.2702,	 Acc = 0.5371
24284 0.292
47781 0.592
24633 0.658
3190 0.639
536 0.526
56 0.393
0 0.0
0 0.0
0.6150191611108194
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2704,	 Acc = 0.5503
2967 0.412
14640 0.596
6803 0.53
700 0.4
20 0.0
0 0.0
0 0.0
0 0.0
0.5688309344402833
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3606,	 Acc1 = 0.3359,	 Acc2 = 0.3487

 ===== Epoch 249	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563  2.4169233   2.1837528
 -0.38958293 -0.38242146] [-1.7898616e-03 -1.3066782e-03  7.6247272e-03  8.5936040e-03
  3.9517106e-03  1.5506904e-03  7.0943422e+00  2.5276678e+00
  6.1251274e-03  1.2857955e-02  4.4463027e-01  5.0126827e-01
 -6.7966389e-03 -7.4470886e-03] 2 2
train:	 Loss = 1.2701,	 Acc = 0.5368
24283 0.292
47783 0.591
24634 0.66
3188 0.636
536 0.545
56 0.446
0 0.0
0 0.0
0.6148273554076932
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.2182,	 Acc = 0.5735
2967 0.396
14640 0.613
6803 0.579
700 0.471
20 0.0
0 0.0
0 0.0
0 0.0
0.5972566890763886
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3090,	 Acc1 = 0.3452,	 Acc2 = 0.3598

 ===== Epoch 250	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.1958616   3.5974424  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
  0.0927268  -1.0620985   0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 5 5
train:	 Loss = 1.2683,	 Acc = 0.5362
24291 0.293
47779 0.59
24628 0.659
3190 0.639
536 0.541
56 0.429
0 0.0
0 0.0
0.6138681436952841
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2229,	 Acc = 0.5721
2967 0.406
14640 0.597
6803 0.603
700 0.469
20 0.0
0 0.0
0 0.0
0 0.0
0.5943238731218697
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3180,	 Acc1 = 0.3452,	 Acc2 = 0.3598

 ===== Epoch 251	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576   1.0642275   2.499519   -0.42759606 -0.42288172
  2.5920844   1.1302781 ] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588 -0.30522692  0.21890292  0.0031886   0.00774676
 -0.17073481 -0.03948929] 1 1
train:	 Loss = 1.2669,	 Acc = 0.5367
24292 0.294
47775 0.591
24632 0.657
3190 0.638
536 0.569
55 0.473
0 0.0
0 0.0
0.6142699637738227
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.2925,	 Acc = 0.5466
2967 0.407
14640 0.599
6803 0.511
700 0.399
20 0.0
0 0.0
0 0.0
0 0.0
0.565356675540315
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3686,	 Acc1 = 0.3518,	 Acc2 = 0.3678

 ===== Epoch 252	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 1 1
train:	 Loss = 1.2696,	 Acc = 0.5371
24288 0.293
47780 0.592
24631 0.658
3190 0.638
536 0.554
55 0.418
0 0.0
0 0.0
0.6148414531709366
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2419,	 Acc = 0.5632
2967 0.412
14640 0.6
6803 0.567
700 0.42
20 0.0
0 0.0
0 0.0
0 0.0
0.5834498939674232
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 1
Testing:	 Loss = 1.3348,	 Acc1 = 0.3320,	 Acc2 = 0.3439

 ===== Epoch 253	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 3 1
train:	 Loss = 1.2690,	 Acc = 0.5364
24290 0.294
47780 0.592
24627 0.654
3191 0.642
536 0.534
56 0.429
0 0.0
0 0.0
0.6135450846567791
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.2390,	 Acc = 0.5736
2967 0.41
14640 0.609
6803 0.583
700 0.44
20 0.0
0 0.0
0 0.0
0 0.0
0.5954518792582232
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3295,	 Acc1 = 0.3458,	 Acc2 = 0.3606

 ===== Epoch 254	 =====
[-0.36757618 -0.38323686  3.4499779   2.4432647   3.6563854   1.0518388
  2.8505545   3.5664697  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.13848346  0.1087025   0.22708066  0.05989638
  0.07772367  0.13578665  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 3 3
train:	 Loss = 1.2700,	 Acc = 0.5371
24290 0.292
47781 0.591
24628 0.661
3189 0.643
536 0.56
56 0.411
0 0.0
0 0.0
0.6153563459771624
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.2545,	 Acc = 0.5703
2967 0.409
14640 0.606
6803 0.575
700 0.464
20 0.0
0 0.0
0 0.0
0 0.0
0.5918422596218923
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3505,	 Acc1 = 0.3381,	 Acc2 = 0.3514

 ===== Epoch 255	 =====
[-0.36757618 -0.38323686  1.7843887   2.820225   -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.47636232  0.08595049  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 1 1
train:	 Loss = 1.2677,	 Acc = 0.5363
24286 0.295
47780 0.59
24632 0.656
3190 0.637
536 0.55
56 0.446
0 0.0
0 0.0
0.6133422579205712
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2569,	 Acc = 0.5569
2967 0.406
14640 0.587
6803 0.571
700 0.453
20 0.0
0 0.0
0 0.0
0 0.0
0.5771781798492984
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3512,	 Acc1 = 0.3402,	 Acc2 = 0.3539

 ===== Epoch 256	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563  1.4664993   2.7529616
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795 -0.22821271  0.26642013
 -0.00679664 -0.00744709] 4 1
train:	 Loss = 1.2695,	 Acc = 0.5372
24287 0.294
47782 0.593
24630 0.657
3189 0.636
536 0.556
56 0.357
0 0.0
0 0.0
0.6147808853831717
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2138,	 Acc = 0.5878
2967 0.41
14640 0.619
6803 0.611
700 0.48
20 0.0
0 0.0
0 0.0
0 0.0
0.6116049271308036
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.2963,	 Acc1 = 0.3561,	 Acc2 = 0.3730

 ===== Epoch 257	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 0 1
train:	 Loss = 1.2691,	 Acc = 0.5361
24289 0.291
47776 0.592
24634 0.655
3189 0.644
536 0.576
56 0.5
0 0.0
0 0.0
0.6142720268798152
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2289,	 Acc = 0.5807
2967 0.411
14640 0.613
6803 0.6
700 0.467
20 0.0
0 0.0
0 0.0
0 0.0
0.6034381627036051
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3200,	 Acc1 = 0.3402,	 Acc2 = 0.3539

 ===== Epoch 258	 =====
[-0.36757618 -0.38323686  0.5822562   1.4736139  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.03432579  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 0 0
train:	 Loss = 1.2689,	 Acc = 0.5374
24290 0.293
47776 0.594
24632 0.656
3190 0.64
536 0.534
56 0.411
0 0.0
0 0.0
0.6154219713873211
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.2412,	 Acc = 0.5812
2967 0.407
14640 0.608
6803 0.609
700 0.496
20 0.0
0 0.0
0 0.0
0 0.0
0.6044759283490502
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3438,	 Acc1 = 0.3318,	 Acc2 = 0.3437

 ===== Epoch 259	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495   1.3002541   2.3523915
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936  -0.30394113  0.18825689
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 1 1
train:	 Loss = 1.2687,	 Acc = 0.5358
24293 0.292
47780 0.591
24626 0.655
3189 0.644
536 0.552
56 0.411
0 0.0
0 0.0
0.6136348720910392
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2290,	 Acc = 0.5852
2967 0.406
14640 0.612
6803 0.624
700 0.43
20 0.0
0 0.0
0 0.0
0 0.0
0.6091684338762803
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3201,	 Acc1 = 0.3381,	 Acc2 = 0.3514

 ===== Epoch 260	 =====
[ 3.2939622   3.0151668  -0.420647   -0.3362495  -0.4419852  -0.36654198
  1.4347097   2.519035   -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.5670748  -0.17387697  0.00762473  0.0085936   0.00395171  0.00155069
  0.12023599 -0.25313705  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 5 5
train:	 Loss = 1.2679,	 Acc = 0.5374
24287 0.293
47778 0.593
24634 0.657
3189 0.648
536 0.552
56 0.411
0 0.0
0 0.0
0.6152139960363813
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2461,	 Acc = 0.5603
2967 0.41
14640 0.594
6803 0.568
700 0.436
20 0.0
0 0.0
0 0.0
0 0.0
0.5804268375219961
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 1
Testing:	 Loss = 1.3395,	 Acc1 = 0.3452,	 Acc2 = 0.3598

 ===== Epoch 261	 =====
[ 2.5078006   3.3372543  -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
  1.8294348   3.544327  ] [-4.8647833e-01  5.3602051e-02  7.6247272e-03  8.5936040e-03
  3.9517106e-03  1.5506904e-03 -4.8065083e-03 -4.2258762e-03
  6.1251274e-03  1.2857955e-02  3.1886040e-03  7.7467631e-03
 -3.2927980e+00 -5.3584952e+00] 3 1
train:	 Loss = 1.2687,	 Acc = 0.5367
24292 0.292
47771 0.59
24637 0.66
3189 0.647
535 0.548
56 0.464
0 0.0
0 0.0
0.6146899774242663
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2469,	 Acc = 0.5622
2967 0.408
14640 0.595
6803 0.574
700 0.434
20 0.0
0 0.0
0 0.0
0 0.0
0.5828633307765194
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3361,	 Acc1 = 0.3433,	 Acc2 = 0.3576

 ===== Epoch 262	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495   1.7111987   2.8036947
 -0.36261833 -0.3698576   0.8083394   0.9337155  -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-1.7898616e-03 -1.3066782e-03  7.6247272e-03  8.5936040e-03
 -9.2655681e-02 -1.4967667e+00 -4.8065083e-03 -4.2258762e-03
 -1.7343888e+00 -1.7950206e+00  3.1886040e-03  7.7467631e-03
 -6.7966389e-03 -7.4470886e-03] 5 5
train:	 Loss = 1.2692,	 Acc = 0.5372
24288 0.293
47783 0.592
24628 0.659
3189 0.635
536 0.545
56 0.446
0 0.0
0 0.0
0.6151301973960521
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.2956,	 Acc = 0.5380
2967 0.401
14640 0.593
6803 0.494
700 0.403
20 0.0
0 0.0
0 0.0
0 0.0
0.5563326264494879
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3789,	 Acc1 = 0.3474,	 Acc2 = 0.3626

 ===== Epoch 263	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563  0.40820843  1.9883163
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795 -0.06326407  0.84843516
 -0.00679664 -0.00744709] 1 6
train:	 Loss = 1.2685,	 Acc = 0.5361
24287 0.293
47779 0.591
24632 0.655
3190 0.642
536 0.534
56 0.411
0 0.0
0 0.0
0.6136127990760306
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2336,	 Acc = 0.5807
2967 0.403
14640 0.613
6803 0.602
700 0.463
20 0.0
0 0.0
0 0.0
0 0.0
0.604385687858142
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3340,	 Acc1 = 0.3402,	 Acc2 = 0.3539

 ===== Epoch 264	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 5 1
train:	 Loss = 1.2688,	 Acc = 0.5348
24287 0.293
47782 0.589
24629 0.656
3190 0.634
536 0.539
56 0.393
0 0.0
0 0.0
0.6119853529851824
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2145,	 Acc = 0.5759
2967 0.409
14640 0.602
6803 0.609
700 0.421
20 0.0
0 0.0
0 0.0
0 0.0
0.5982493344763795
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3149,	 Acc1 = 0.3287,	 Acc2 = 0.3400

 ===== Epoch 265	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 1
train:	 Loss = 1.2691,	 Acc = 0.5367
24285 0.292
47785 0.593
24630 0.655
3188 0.639
536 0.547
56 0.411
0 0.0
0 0.0
0.6146466303563226
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2455,	 Acc = 0.5772
2967 0.411
14640 0.606
6803 0.6
700 0.476
20 0.0
0 0.0
0 0.0
0 0.0
0.599377340612733
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3435,	 Acc1 = 0.3264,	 Acc2 = 0.3372

 ===== Epoch 266	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 1
train:	 Loss = 1.2678,	 Acc = 0.5368
24286 0.294
47779 0.592
24633 0.656
3190 0.641
536 0.558
56 0.375
0 0.0
0 0.0
0.6142872142163425
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.2984,	 Acc = 0.5381
2967 0.405
14640 0.585
6803 0.506
700 0.45
20 0.0
0 0.0
0 0.0
0 0.0
0.5559265442404007
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3777,	 Acc1 = 0.3549,	 Acc2 = 0.3715

 ===== Epoch 267	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495   2.051673    0.7606038
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-1.7898616e-03 -1.3066782e-03  7.9220420e-01  2.6683052e+00
 -4.0541887e+00 -1.1817000e+00 -4.8065083e-03 -4.2258762e-03
  6.1251274e-03  1.2857955e-02  3.1886040e-03  7.7467631e-03
 -6.7966389e-03 -7.4470886e-03] 5 5
train:	 Loss = 1.2700,	 Acc = 0.5366
24289 0.293
47778 0.593
24632 0.655
3190 0.64
535 0.544
56 0.482
0 0.0
0 0.0
0.6142720268798152
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.2118,	 Acc = 0.5784
2967 0.409
14640 0.603
6803 0.61
700 0.483
20 0.0
0 0.0
0 0.0
0 0.0
0.6010016694490818
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 1
Testing:	 Loss = 1.3073,	 Acc1 = 0.3412,	 Acc2 = 0.3551

 ===== Epoch 268	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495   1.9926927   1.2007909
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936  -0.31890014  0.31194976
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 1
train:	 Loss = 1.2689,	 Acc = 0.5366
24289 0.292
47776 0.594
24632 0.653
3191 0.646
536 0.552
56 0.411
0 0.0
0 0.0
0.6145476499849064
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.2758,	 Acc = 0.5672
2967 0.409
14640 0.598
6803 0.581
700 0.471
20 0.0
0 0.0
0 0.0
0 0.0
0.5884131209673781
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3710,	 Acc1 = 0.3400,	 Acc2 = 0.3536

 ===== Epoch 269	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495   2.7893047   1.734351
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.0531889  -0.3788632
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 5 5
train:	 Loss = 1.2706,	 Acc = 0.5355
24289 0.291
47774 0.591
24635 0.656
3190 0.64
536 0.545
56 0.429
0 0.0
0 0.0
0.6134320326547755
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2424,	 Acc = 0.5739
2967 0.411
14640 0.601
6803 0.601
700 0.46
20 0.0
0 0.0
0 0.0
0 0.0
0.595722600730948
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3311,	 Acc1 = 0.3476,	 Acc2 = 0.3628

 ===== Epoch 270	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576   1.1508613   2.368417   -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.09096663  0.00288804  0.0031886   0.00774676
 -0.00679664 -0.00744709] 2 0
train:	 Loss = 1.2679,	 Acc = 0.5367
24290 0.293
47779 0.591
24631 0.657
3189 0.645
535 0.563
56 0.411
0 0.0
0 0.0
0.6145294658091613
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2241,	 Acc = 0.5790
2967 0.41
14640 0.617
6803 0.587
700 0.444
20 0.0
0 0.0
0 0.0
0 0.0
0.6016333528854397
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 1
Testing:	 Loss = 1.3148,	 Acc1 = 0.3454,	 Acc2 = 0.3601

 ===== Epoch 271	 =====
[-0.36757618 -0.38323686  2.4597712   3.0791013  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-1.7898616e-03 -1.3066782e-03 -4.4090476e+00 -3.4133108e+00
  2.2895246e+00  1.4345207e+00 -4.8065083e-03 -4.2258762e-03
  6.1251274e-03  1.2857955e-02  3.1886040e-03  7.7467631e-03
 -6.7966389e-03 -7.4470886e-03] 2 6
train:	 Loss = 1.2703,	 Acc = 0.5371
24293 0.294
47775 0.592
24631 0.657
3189 0.638
536 0.552
56 0.429
0 0.0
0 0.0
0.6146324176040532
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2470,	 Acc = 0.5757
2967 0.407
14640 0.602
6803 0.605
700 0.469
20 0.0
0 0.0
0 0.0
0 0.0
0.5982493344763795
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3582,	 Acc1 = 0.3047,	 Acc2 = 0.3111

 ===== Epoch 272	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563  3.3500001   2.0811486
 -0.38958293 -0.38242146] [-1.7898616e-03 -1.3066782e-03  7.6247272e-03  8.5936040e-03
  3.9517106e-03  1.5506904e-03  5.9781303e+00  1.9715067e+00
  6.1251274e-03  1.2857955e-02 -1.5463971e-01  4.1782733e-02
 -6.7966389e-03 -7.4470886e-03] 4 2
train:	 Loss = 1.2698,	 Acc = 0.5353
24289 0.292
47778 0.591
24633 0.654
3189 0.634
535 0.551
56 0.446
0 0.0
0 0.0
0.6129201611738919
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.2673,	 Acc = 0.5504
2967 0.408
14640 0.591
6803 0.543
700 0.399
20 0.0
0 0.0
0 0.0
0 0.0
0.5695077381220954
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3485,	 Acc1 = 0.3567,	 Acc2 = 0.3738

 ===== Epoch 273	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 0 1
train:	 Loss = 1.2696,	 Acc = 0.5362
24289 0.293
47776 0.591
24634 0.657
3189 0.632
536 0.547
56 0.446
0 0.0
0 0.0
0.6138782795868278
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2543,	 Acc = 0.5592
2967 0.388
14640 0.599
6803 0.565
700 0.411
20 0.0
0 0.0
0 0.0
0 0.0
0.5821414068492533
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 1
Testing:	 Loss = 1.3590,	 Acc1 = 0.3188,	 Acc2 = 0.3280

 ===== Epoch 274	 =====
[ 2.6978486   2.0083263  -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576   1.1915976   3.2515006  -0.42759606 -0.42288172
  2.7726426   2.2145102 ] [-3.0186617e+00 -2.4669702e+00  7.6247272e-03  8.5936040e-03
  3.9517106e-03  1.5506904e-03 -4.8065083e-03 -4.2258762e-03
  4.7150576e-01 -3.7585737e-03  3.1886040e-03  7.7467631e-03
  7.0619389e-02  1.8480614e-01] 2 2
train:	 Loss = 1.2675,	 Acc = 0.5372
24288 0.295
47777 0.593
24632 0.655
3191 0.642
536 0.563
56 0.464
0 0.0
0 0.0
0.6144214615707686
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2281,	 Acc = 0.5708
2967 0.399
14640 0.6
6803 0.598
700 0.44
20 0.0
0 0.0
0 0.0
0 0.0
0.5938275504218743
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3329,	 Acc1 = 0.3229,	 Acc2 = 0.3330

 ===== Epoch 275	 =====
[-0.36757618 -0.38323686  1.706397    0.8945484  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-1.7898616e-03 -1.3066782e-03 -3.2538660e+00 -1.2245660e+00
  3.9517106e-03  1.5506904e-03 -4.8065083e-03 -4.2258762e-03
  6.1251274e-03  1.2857955e-02  3.1886040e-03  7.7467631e-03
 -6.7966389e-03 -7.4470886e-03] 1 5
train:	 Loss = 1.2683,	 Acc = 0.5375
24289 0.292
47778 0.593
24630 0.656
3191 0.655
536 0.539
56 0.429
0 0.0
0 0.0
0.6155582680369073
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.2363,	 Acc = 0.5683
2967 0.409
14640 0.603
6803 0.578
700 0.447
20 0.0
0 0.0
0 0.0
0 0.0
0.5896313675946397
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3178,	 Acc1 = 0.3615,	 Acc2 = 0.3795

 ===== Epoch 276	 =====
[-0.36757618 -0.38323686  2.2449877   2.100367   -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 0 0
train:	 Loss = 1.2688,	 Acc = 0.5371
24295 0.293
47780 0.592
24624 0.658
3189 0.64
536 0.56
56 0.411
0 0.0
0 0.0
0.6148454420161449
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2155,	 Acc = 0.5908
2967 0.399
14640 0.606
6803 0.656
700 0.467
20 0.0
0 0.0
0 0.0
0 0.0
0.6164779136398502
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 1
Testing:	 Loss = 1.3295,	 Acc1 = 0.3039,	 Acc2 = 0.3101

 ===== Epoch 277	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 0 1
train:	 Loss = 1.2685,	 Acc = 0.5381
24286 0.295
47784 0.593
24627 0.658
3191 0.642
536 0.55
56 0.518
0 0.0
0 0.0
0.6155602803370344
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2064,	 Acc = 0.5904
2967 0.409
14640 0.621
6803 0.619
700 0.467
20 0.0
0 0.0
0 0.0
0 0.0
0.6146731038216848
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3004,	 Acc1 = 0.3431,	 Acc2 = 0.3574

 ===== Epoch 278	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.788501    2.0938666  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [ 8.1283731e+00  1.8577461e+00  7.6247272e-03  8.5936040e-03
  3.9517106e-03  1.5506904e-03  7.2377360e-01 -1.5171391e+00
  6.1251274e-03  1.2857955e-02  3.1852996e+00  4.7829933e+00
 -6.7966389e-03 -7.4470886e-03] 5 5
train:	 Loss = 1.2681,	 Acc = 0.5354
24286 0.291
47777 0.591
24636 0.654
3190 0.646
535 0.546
56 0.464
0 0.0
0 0.0
0.6132635115625902
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.1913,	 Acc = 0.5976
2967 0.408
14640 0.622
6803 0.646
700 0.45
20 0.0
0 0.0
0 0.0
0 0.0
0.6229301087397916
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3035,	 Acc1 = 0.3171,	 Acc2 = 0.3260

 ===== Epoch 279	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563  1.9658496   1.7391347
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.35147548 -0.01948201
 -0.00679664 -0.00744709] 2 2
train:	 Loss = 1.2690,	 Acc = 0.5372
24290 0.293
47776 0.592
24631 0.658
3191 0.647
536 0.547
56 0.482
0 0.0
0 0.0
0.6151725948287177
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2458,	 Acc = 0.5707
2967 0.402
14640 0.614
6803 0.568
700 0.429
20 0.0
0 0.0
0 0.0
0 0.0
0.5932861074764246
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3394,	 Acc1 = 0.3355,	 Acc2 = 0.3482

 ===== Epoch 280	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563  0.73670894  1.5925573
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795 -0.41096032  0.2221734
 -0.00679664 -0.00744709] 1 1
train:	 Loss = 1.2673,	 Acc = 0.5380
24288 0.294
47783 0.594
24628 0.657
3189 0.646
536 0.562
56 0.464
0 0.0
0 0.0
0.6159176816463671
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.2500,	 Acc = 0.5752
2967 0.408
14640 0.602
6803 0.601
700 0.486
20 0.0
0 0.0
0 0.0
0 0.0
0.5975274105491134
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3402,	 Acc1 = 0.3501,	 Acc2 = 0.3658

 ===== Epoch 281	 =====
[-0.36757618 -0.38323686  3.3409543   2.2615972  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-1.7898616e-03 -1.3066782e-03 -5.7602048e+00 -2.5942380e+00
  3.5323172e+00  2.6457775e+00 -4.8065083e-03 -4.2258762e-03
  6.1251274e-03  1.2857955e-02  3.1886040e-03  7.7467631e-03
 -6.7966389e-03 -7.4470886e-03] 6 6
train:	 Loss = 1.2688,	 Acc = 0.5356
24289 0.293
47780 0.589
24628 0.656
3191 0.643
536 0.558
56 0.464
0 0.0
0 0.0
0.612815161895762
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2152,	 Acc = 0.5891
2967 0.406
14640 0.621
6803 0.621
700 0.406
20 0.0
0 0.0
0 0.0
0 0.0
0.6135450976853314
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 1
Testing:	 Loss = 1.3157,	 Acc1 = 0.3251,	 Acc2 = 0.3357

 ===== Epoch 282	 =====
[ 5.534717    1.5746495  -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576   5.087864   -5.1167674   0.5006107   1.7391347
  2.9835145   1.6371238 ] [-1.4685471e+00  1.0328137e-01  7.6247272e-03  8.5936040e-03
  3.9517106e-03  1.5506904e-03 -4.8065083e-03 -4.2258762e-03
  6.7077585e-02  1.0544414e+01  2.5535661e-01  1.4048705e-01
  9.2024100e-01  1.4208320e-01] 2 2
train:	 Loss = 1.2701,	 Acc = 0.5359
24287 0.294
47779 0.592
24632 0.652
3190 0.637
536 0.545
56 0.464
0 0.0
0 0.0
0.6129565708135918
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2410,	 Acc = 0.5690
2967 0.399
14640 0.593
6803 0.603
700 0.453
20 0.55
0 0.0
0 0.0
0 0.0
0.5917520191309841
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3355,	 Acc1 = 0.3429,	 Acc2 = 0.3571

 ===== Epoch 283	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 1 1
train:	 Loss = 1.2686,	 Acc = 0.5369
24287 0.294
47780 0.592
24631 0.656
3190 0.637
536 0.56
56 0.446
0 0.0
0 0.0
0.6143346501647133
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2617,	 Acc = 0.5737
2967 0.402
14640 0.604
6803 0.6
700 0.436
20 0.0
0 0.0
0 0.0
0 0.0
0.5966701258854848
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 1
Testing:	 Loss = 1.3815,	 Acc1 = 0.2851,	 Acc2 = 0.2875

 ===== Epoch 284	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495   1.3155732   2.943754
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-1.7898616e-03 -1.3066782e-03  7.6247272e-03  8.5936040e-03
 -2.8562715e+00 -3.4735184e+00 -4.8065083e-03 -4.2258762e-03
  6.1251274e-03  1.2857955e-02  3.1886040e-03  7.7467631e-03
 -6.7966389e-03 -7.4470886e-03] 1 1
train:	 Loss = 1.2685,	 Acc = 0.5368
24283 0.293
47782 0.591
24633 0.657
3190 0.644
536 0.56
56 0.482
0 0.0
0 0.0
0.6143024003569695
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.2536,	 Acc = 0.5661
2967 0.409
14640 0.614
6803 0.547
700 0.436
20 0.0
0 0.0
0 0.0
0 0.0
0.587059513603754
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3371,	 Acc1 = 0.3429,	 Acc2 = 0.3571

 ===== Epoch 285	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 6 1
train:	 Loss = 1.2681,	 Acc = 0.5370
24292 0.293
47781 0.591
24628 0.66
3187 0.631
536 0.56
56 0.464
0 0.0
0 0.0
0.6146637265711136
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2102,	 Acc = 0.5857
2967 0.411
14640 0.608
6803 0.625
700 0.497
20 0.0
0 0.0
0 0.0
0 0.0
0.6090781933853721
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3093,	 Acc1 = 0.3437,	 Acc2 = 0.3581

 ===== Epoch 286	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 5 1
train:	 Loss = 1.2702,	 Acc = 0.5351
24291 0.293
47777 0.59
24632 0.653
3188 0.641
536 0.545
56 0.429
0 0.0
0 0.0
0.6122931131790679
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2262,	 Acc = 0.5727
2967 0.406
14640 0.606
6803 0.592
700 0.426
20 0.0
0 0.0
0 0.0
0 0.0
0.5950006768036818
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3220,	 Acc1 = 0.3456,	 Acc2 = 0.3603

 ===== Epoch 287	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576   0.8083394   3.2143962  -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.01024498  0.10258722  0.0031886   0.00774676
 -0.00679664 -0.00744709] 1 6
train:	 Loss = 1.2705,	 Acc = 0.5366
24292 0.295
47772 0.591
24635 0.654
3189 0.641
536 0.573
56 0.446
0 0.0
0 0.0
0.6134299364729354
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2075,	 Acc = 0.5861
2967 0.411
14640 0.609
6803 0.629
700 0.456
20 0.0
0 0.0
0 0.0
0 0.0
0.6095293958399134
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 1
Testing:	 Loss = 1.3135,	 Acc1 = 0.3351,	 Acc2 = 0.3477

 ===== Epoch 288	 =====
[-0.36757618 -0.38323686  2.1919045   1.1579665  -0.4419852  -0.36654198
  4.822037    2.8118663  -0.40402457 -0.41193563  4.2464924   2.9166398
 -0.38958293 -0.38242146] [-1.7898616e-03 -1.3066782e-03 -3.9983163e+00 -1.4884895e+00
  1.2854424e+00  2.9538424e+00 -2.3571885e-01 -2.1238601e+00
  6.1251274e-03  1.2857955e-02  5.7456976e-01 -1.8472136e+00
 -6.7966389e-03 -7.4470886e-03] 5 5
train:	 Loss = 1.2696,	 Acc = 0.5370
24291 0.292
47773 0.592
24634 0.658
3190 0.642
536 0.539
56 0.446
0 0.0
0 0.0
0.6149444145480318
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.2370,	 Acc = 0.5577
2967 0.406
14640 0.599
6803 0.549
700 0.439
20 0.0
0 0.0
0 0.0
0 0.0
0.5779903442674729
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 1
Testing:	 Loss = 1.3307,	 Acc1 = 0.3379,	 Acc2 = 0.3511

 ===== Epoch 289	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576   1.3849481   1.5100697  -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588 -0.14872845  1.089609    0.0031886   0.00774676
 -0.00679664 -0.00744709] 6 6
train:	 Loss = 1.2689,	 Acc = 0.5345
24288 0.292
47779 0.588
24630 0.656
3191 0.636
536 0.549
56 0.411
0 0.0
0 0.0
0.6116915161696767
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.2342,	 Acc = 0.5747
2967 0.41
14640 0.613
6803 0.581
700 0.419
20 0.0
0 0.0
0 0.0
0 0.0
0.5967603663763931
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3246,	 Acc1 = 0.3514,	 Acc2 = 0.3673

 ===== Epoch 290	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576   1.8795112   2.6553574  -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [ 2.0828729e+00  1.3138882e+00  7.6247272e-03  8.5936040e-03
  3.9517106e-03  1.5506904e-03 -4.8065083e-03 -4.2258762e-03
 -9.2792839e-01  9.0018058e-01  3.1886040e-03  7.7467631e-03
  3.6617298e+00  1.9934106e+00] 1 6
train:	 Loss = 1.2702,	 Acc = 0.5369
24289 0.294
47781 0.591
24627 0.656
3191 0.643
536 0.573
56 0.446
0 0.0
0 0.0
0.6142851517895814
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2420,	 Acc = 0.5685
2967 0.409
14640 0.605
6803 0.577
700 0.417
20 0.0
0 0.0
0 0.0
0 0.0
0.5898118485764563
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3257,	 Acc1 = 0.3615,	 Acc2 = 0.3795

 ===== Epoch 291	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
  1.2838061   1.1955932 ] [ 2.0607929e+00  1.3243470e+00  7.6247272e-03  8.5936040e-03
  3.9517106e-03  1.5506904e-03 -4.8065083e-03 -4.2258762e-03
  6.1251274e-03  1.2857955e-02  3.1886040e-03  7.7467631e-03
  1.1420525e-01 -6.7971259e-02] 3 2
train:	 Loss = 1.2691,	 Acc = 0.5367
24284 0.292
47781 0.593
24633 0.656
3190 0.635
536 0.556
56 0.464
0 0.0
0 0.0
0.6146779358496509
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.2263,	 Acc = 0.5884
2967 0.408
14640 0.624
6803 0.606
700 0.441
20 0.0
0 0.0
0 0.0
0 0.0
0.6125524522853404
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3168,	 Acc1 = 0.3524,	 Acc2 = 0.3685

 ===== Epoch 292	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-1.7898616e-03 -1.3066782e-03  7.6247272e-03  8.5936040e-03
  3.9517106e-03  1.5506904e-03 -4.8065083e-03 -4.2258762e-03
  6.1251274e-03  1.2857955e-02  2.4626162e+00  2.6046913e+00
 -6.7966389e-03 -7.4470886e-03] 6 6
train:	 Loss = 1.2715,	 Acc = 0.5352
24288 0.293
47780 0.589
24629 0.656
3191 0.636
536 0.541
56 0.518
0 0.0
0 0.0
0.6125314993700126
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.2129,	 Acc = 0.5831
2967 0.408
14640 0.612
6803 0.611
700 0.466
20 0.0
0 0.0
0 0.0
0 0.0
0.6065514596399404
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 1
Testing:	 Loss = 1.3045,	 Acc1 = 0.3396,	 Acc2 = 0.3531

 ===== Epoch 293	 =====
[-0.36757618 -0.38323686  1.0849144   1.5508227  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.01616865  0.2019858   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
train:	 Loss = 1.2684,	 Acc = 0.5366
24295 0.293
47774 0.591
24633 0.657
3187 0.64
535 0.548
56 0.393
0 0.0
0 0.0
0.6141891448447857
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.2410,	 Acc = 0.5575
2967 0.411
14640 0.599
6803 0.547
700 0.437
20 0.0
0 0.0
0 0.0
0 0.0
0.5771330596038442
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3237,	 Acc1 = 0.3652,	 Acc2 = 0.3839

 ===== Epoch 294	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [ 2.4682136e+00  2.6656888e+00  7.6247272e-03  8.5936040e-03
  3.9517106e-03  1.5506904e-03 -4.8065083e-03 -4.2258762e-03
  6.1251274e-03  1.2857955e-02  3.1886040e-03  7.7467631e-03
  3.5641468e+00  3.7913342e+00] 3 5
train:	 Loss = 1.2687,	 Acc = 0.5373
24288 0.293
47779 0.592
24633 0.657
3188 0.644
536 0.556
56 0.482
0 0.0
0 0.0
0.6151433221335574
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2098,	 Acc = 0.5925
2967 0.405
14640 0.615
6803 0.645
700 0.43
20 0.0
0 0.0
0 0.0
0 0.0
0.6176059197762036
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 1
Testing:	 Loss = 1.3179,	 Acc1 = 0.3150,	 Acc2 = 0.3236

 ===== Epoch 295	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.0626454   2.3303843  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-1.7898616e-03 -1.3066782e-03  7.6247272e-03  8.5936040e-03
  3.9517106e-03  1.5506904e-03 -3.5560858e+00 -3.7340040e+00
  6.1251274e-03  1.2857955e-02  4.4485097e+00  3.5270662e+00
 -6.7966389e-03 -7.4470886e-03] 5 5
train:	 Loss = 1.2706,	 Acc = 0.5356
24287 0.291
47780 0.591
24632 0.656
3190 0.642
535 0.557
56 0.429
0 0.0
0 0.0
0.6134553042930453
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.2100,	 Acc = 0.5868
2967 0.408
14640 0.615
6803 0.616
700 0.483
20 0.0
0 0.0
0 0.0
0 0.0
0.6107476424671751
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3038,	 Acc1 = 0.3464,	 Acc2 = 0.3613

 ===== Epoch 296	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-1.7898616e-03 -1.3066782e-03  3.1683116e+00  3.0186861e+00
  3.9517106e-03  1.5506904e-03 -4.8065083e-03 -4.2258762e-03
  6.1251274e-03  1.2857955e-02  3.1886040e-03  7.7467631e-03
 -6.7966389e-03 -7.4470886e-03] 5 5
train:	 Loss = 1.2685,	 Acc = 0.5366
24292 0.294
47773 0.591
24633 0.656
3190 0.645
536 0.569
56 0.464
0 0.0
0 0.0
0.613994329815719
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.2198,	 Acc = 0.5793
2967 0.405
14640 0.608
6803 0.605
700 0.483
20 0.0
0 0.0
0 0.0
0 0.0
0.6025808780399765
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 1
Testing:	 Loss = 1.3273,	 Acc1 = 0.3155,	 Acc2 = 0.3241

 ===== Epoch 297	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563  1.5411752   1.6218729
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.22924876 -0.11818632
 -0.00679664 -0.00744709] 3 2
train:	 Loss = 1.2698,	 Acc = 0.5359
24290 0.293
47778 0.592
24631 0.654
3189 0.635
536 0.55
56 0.375
0 0.0
0 0.0
0.6132957080981756
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.2415,	 Acc = 0.5551
2967 0.41
14640 0.598
6803 0.541
700 0.433
20 0.0
0 0.0
0 0.0
0 0.0
0.5745612056129585
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3267,	 Acc1 = 0.3563,	 Acc2 = 0.3733

 ===== Epoch 298	 =====
[-0.36757618 -0.38323686  1.8615639   1.7211361  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-1.7898616e-03 -1.3066782e-03 -3.4917903e+00 -2.0527399e+00
  4.0334225e+00  1.3785088e+00 -4.8065083e-03 -4.2258762e-03
  6.1251274e-03  1.2857955e-02  3.1886040e-03  7.7467631e-03
 -6.7966389e-03 -7.4470886e-03] 6 6
train:	 Loss = 1.2695,	 Acc = 0.5361
24291 0.293
47774 0.591
24632 0.657
3191 0.633
536 0.534
56 0.5
0 0.0
0 0.0
0.6136712648807571
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 3
val:	 Loss = 1.1950,	 Acc = 0.5929
2967 0.411
14640 0.611
6803 0.646
700 0.491
20 0.0
0 0.0
0 0.0
0 0.0
0.6172449578125705
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 1
Testing:	 Loss = 1.2979,	 Acc1 = 0.3303,	 Acc2 = 0.3419

 ===== Epoch 299	 =====
[ 2.75373     3.286532   -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
  2.5942817   3.518201  ] [ 0.0202836  -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 0 0
train:	 Loss = 1.2694,	 Acc = 0.5356
24287 0.292
47775 0.59
24637 0.656
3189 0.643
536 0.552
56 0.482
0 0.0
0 0.0
0.6130746919008307
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.2364,	 Acc = 0.5541
2967 0.403
14640 0.597
6803 0.543
700 0.411
20 0.0
0 0.0
0 0.0
0 0.0
0.574380724631142
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 1
Testing:	 Loss = 1.3237,	 Acc1 = 0.3532,	 Acc2 = 0.3695

 ===== Epoch 300	 =====
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563  1.2526535   0.87921405
 -0.38958293 -0.38242146] [-1.7898616e-03 -1.3066782e-03  7.6247272e-03  8.5936040e-03
  3.9517106e-03  1.5506904e-03 -4.8065083e-03 -4.2258762e-03
  6.1251274e-03  1.2857955e-02 -2.5965804e-01  2.8871899e+00
 -6.7966389e-03 -7.4470886e-03] 6 5
train:	 Loss = 1.2693,	 Acc = 0.5369
24290 0.295
47779 0.592
24629 0.656
3191 0.641
535 0.563
56 0.482
0 0.0
0 0.0
0.6141357133482084
0.6234715516852412
[-0.36757618 -0.38323686 -0.420647   -0.3362495  -0.4419852  -0.36654198
  2.9405038   1.7447217  -0.40402457 -0.41193563  2.48104     3.0461166
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668  0.00762473  0.0085936   0.00395171  0.00155069
 -0.1198452   0.17856826  0.00612513  0.01285795 -0.15938477  0.28684172
 -0.00679664 -0.00744709] 3 4
val:	 Loss = 1.3086,	 Acc = 0.5183
2967 0.269
14640 0.582
6803 0.505
700 0.396
20 0.0
0 0.0
0 0.0
0 0.0
0.551685241167712
0.6234715516852412
[-0.36757618 -0.38323686  1.0824643   2.3410766  -0.4419852  -0.36654198
 -0.36261833 -0.3698576  -0.40402457 -0.41193563 -0.42759606 -0.42288172
 -0.38958293 -0.38242146] [-0.00178986 -0.00130668 -0.02305524  0.21563701  0.00395171  0.00155069
 -0.00480651 -0.00422588  0.00612513  0.01285795  0.0031886   0.00774676
 -0.00679664 -0.00744709] 4 4
Testing:	 Loss = 1.3929,	 Acc1 = 0.3208,	 Acc2 = 0.3705
