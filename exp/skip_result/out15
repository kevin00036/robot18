(0.24300000000005184, array([-1.000000e+00, -1.000000e+00,  1.189783e+03,  1.790000e-01,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 4, array([  0.   ,   0.   , -15.851,   0.091,   0.   ,   0.   ,   0.   ,
         0.   ,   0.   ,   0.   ,   0.   ,   0.   ,   0.   ,   0.   ]), 1)
((0.236, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.364685e+03, -3.300000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), array([0., 0., 0.])), (0.24, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.543573e+03, -3.290000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), array([0., 0., 0.])))
(0.236, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.364685e+03, -3.300000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([0.   , 0.   , 0.   , 0.   , 5.823, 3.129, 0.   , 0.   , 0.   ,
       0.   , 0.   , 0.   , 0.   , 0.   ]))
(0.236, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.364685e+03, -3.300000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([ 0.   ,  0.   ,  0.   ,  0.   , -8.411, -0.011,  0.   ,  0.   ,
        0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ]))
(0.236, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.364685e+03, -3.300000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  6.2433e+01,
       -3.0000e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
        0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]))
(0.236, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.364685e+03, -3.300000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([ 0.00000e+00,  0.00000e+00,  0.00000e+00,  0.00000e+00,
       -1.19043e+02, -4.00000e-03,  0.00000e+00,  0.00000e+00,
        0.00000e+00,  0.00000e+00,  0.00000e+00,  0.00000e+00,
        0.00000e+00,  0.00000e+00]))
(0.236, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.364685e+03, -3.300000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  5.0464e+01,
       -1.3000e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
        0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]))
(0.236, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.364685e+03, -3.300000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([ 0.00000e+00,  0.00000e+00,  0.00000e+00,  0.00000e+00,
       -1.53656e+02, -1.00000e-02,  0.00000e+00,  0.00000e+00,
        0.00000e+00,  0.00000e+00,  0.00000e+00,  0.00000e+00,
        0.00000e+00,  0.00000e+00]))
(0.236, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.364685e+03, -3.300000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([ 0.000000e+00,  0.000000e+00,  2.399328e+03,  1.498000e+00,
       -1.455700e+01, -6.000000e-03,  0.000000e+00,  0.000000e+00,
        0.000000e+00,  0.000000e+00,  0.000000e+00,  0.000000e+00,
        0.000000e+00,  0.000000e+00]))
(0.236, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.364685e+03, -3.300000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  1.9086e+01,
       -9.0000e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
        0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]))
(0.236, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.364685e+03, -3.300000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([ 0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  7.052e+01,
       -2.000e-02,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,
        0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00]))
(0.236, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.364685e+03, -3.300000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  4.1406e+01,
       -1.7000e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
        0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]))
14 1 14

 ===== Epoch 1	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305  1.939941    1.9186336
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009 -0.7459161  -0.2566303
 -0.01013718 -0.01100384] 1 4
train:	 Loss = 1.4718,	 Acc = 0.4463
33273 0.282
65376 0.488
33819 0.518
4287 0.514
706 0.514
75 0.267
0 0.0
0 0.0
0.49877713090933506
0.0
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2693,	 Acc = 0.5294
3965 0.39
20118 0.545
9324 0.562
956 0.467
29 0.69
0 0.0
0 0.0
0 0.0
0.5475728793505767
0.5475728793505767
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 1
Testing:	 Loss = 1.3518,	 Acc1 = 0.2459,	 Acc2 = 0.2403

 ===== Epoch 2	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305  0.50155056  1.3004413
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009 -0.05639087  0.4878532
 -0.01013718 -0.01100384] 4 6
train:	 Loss = 1.3383,	 Acc = 0.5044
33272 0.29
65371 0.562
33825 0.595
4287 0.585
706 0.516
75 0.347
0 0.0
0 0.0
0.5728439346274841
0.5475728793505767
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.3085,	 Acc = 0.5024
3965 0.385
20118 0.513
9324 0.541
956 0.394
29 0.0
0 0.0
0 0.0
0 0.0
0.5175995004436849
0.5475728793505767
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3948,	 Acc1 = 0.2899,	 Acc2 = 0.2932

 ===== Epoch 3	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378  0.65964246  2.0780587  -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-6.0253930e-03 -6.5372344e-03  1.1128664e-02  1.3828198e-02
  3.0917606e+00  3.1737382e+00 -9.0292487e-03 -7.4226381e-03
  2.0336002e-01 -1.3729141e+00  2.3359330e-03  9.9131642e-03
 -1.0137177e-02 -1.1003844e-02] 5 5
train:	 Loss = 1.3096,	 Acc = 0.5150
33274 0.289
65371 0.576
33826 0.608
4284 0.603
706 0.501
75 0.387
0 0.0
0 0.0
0.5871554353455717
0.5475728793505767
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2394,	 Acc = 0.5485
3965 0.387
20118 0.573
9324 0.571
956 0.458
29 0.966
0 0.0
0 0.0
0 0.0
0.5695270647779932
0.5695270647779932
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3263,	 Acc1 = 0.2942,	 Acc2 = 0.2985

 ===== Epoch 4	 =====
[ 0.22054236  1.5501207  -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [ 0.03831197  0.24153647  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 2 0
train:	 Loss = 1.2962,	 Acc = 0.5192
33273 0.286
65373 0.584
33824 0.614
4286 0.607
705 0.482
75 0.453
0 0.0
0 0.0
0.5935758610437067
0.5695270647779932
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2214,	 Acc = 0.5550
3965 0.389
20118 0.585
9324 0.57
956 0.487
29 0.0
0 0.0
0 0.0
0 0.0
0.5765602918460577
0.5765602918460577
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.2991,	 Acc1 = 0.3216,	 Acc2 = 0.3315

 ===== Epoch 5	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [ 4.1298366e-01  2.2881444e+00  1.1128664e-02  1.3828198e-02
  2.5970940e-03  9.9307357e-04 -9.0292487e-03 -7.4226381e-03
  9.6202353e-03  1.8220095e-02  2.3359330e-03  9.9131642e-03
 -1.0137177e-02 -1.1003844e-02] 6 6
train:	 Loss = 1.2917,	 Acc = 0.5191
33274 0.287
65372 0.584
33822 0.613
4287 0.601
706 0.496
75 0.293
0 0.0
0 0.0
0.593245861387658
0.5765602918460577
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2709,	 Acc = 0.5329
3965 0.391
20118 0.559
9324 0.542
956 0.496
29 0.0
0 0.0
0 0.0
0 0.0
0.5513524172609853
0.5765602918460577
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3590,	 Acc1 = 0.3078,	 Acc2 = 0.3149

 ===== Epoch 6	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305  1.2235827   3.1647918
 -0.3911008  -0.38360253] [ 2.4056668e+00  1.6357106e+00  1.1128664e-02  1.3828198e-02
  2.5970940e-03  9.9307357e-04 -9.0292487e-03 -7.4226381e-03
  9.6202353e-03  1.8220095e-02 -2.2880871e+00 -4.4906888e+00
 -1.0137177e-02 -1.1003844e-02] 6 6
train:	 Loss = 1.2889,	 Acc = 0.5203
33275 0.286
65376 0.586
33819 0.614
4286 0.609
705 0.477
75 0.307
0 0.0
0 0.0
0.5951985881585636
0.5765602918460577
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2452,	 Acc = 0.5487
3965 0.388
20118 0.589
9324 0.54
956 0.461
29 0.0
0 0.0
0 0.0
0 0.0
0.5696256614191343
0.5765602918460577
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3242,	 Acc1 = 0.3103,	 Acc2 = 0.3178

 ===== Epoch 7	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
  1.7786148   3.5296905 ] [-6.0253930e-03 -6.5372344e-03  1.1128664e-02  1.3828198e-02
  2.5970940e-03  9.9307357e-04 -9.0292487e-03 -7.4226381e-03
  9.6202353e-03  1.8220095e-02  2.3359330e-03  9.9131642e-03
 -2.8318925e+00 -4.7861462e+00] 1 1
train:	 Loss = 1.2849,	 Acc = 0.5213
33276 0.288
65368 0.586
33824 0.616
4287 0.605
706 0.5
75 0.347
0 0.0
0 0.0
0.5957605985037406
0.5765602918460577
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2279,	 Acc = 0.5480
3965 0.386
20118 0.572
9324 0.576
956 0.463
29 0.0
0 0.0
0 0.0
0 0.0
0.5691326782134288
0.5765602918460577
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3055,	 Acc1 = 0.2787,	 Acc2 = 0.2798

 ===== Epoch 8	 =====
[ 0.21681917  0.8792982  -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.03198517  0.6012433   0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 0 6
train:	 Loss = 1.2850,	 Acc = 0.5201
33271 0.285
65374 0.584
33823 0.616
4287 0.617
706 0.494
75 0.32
0 0.0
0 0.0
0.5949839351652041
0.5765602918460577
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2305,	 Acc = 0.5512
3965 0.394
20118 0.586
9324 0.554
956 0.452
29 0.0
0 0.0
0 0.0
0 0.0
0.5716633253360502
0.5765602918460577
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3157,	 Acc1 = 0.3126,	 Acc2 = 0.3206

 ===== Epoch 9	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305  0.8880965   1.2735634
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.6174032   0.25807434
 -0.01013718 -0.01100384] 2 2
train:	 Loss = 1.2803,	 Acc = 0.5206
33272 0.285
65375 0.585
33822 0.618
4286 0.612
706 0.486
75 0.373
0 0.0
0 0.0
0.5957761068058006
0.5765602918460577
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2288,	 Acc = 0.5390
3965 0.397
20118 0.575
9324 0.531
956 0.46
29 0.0
0 0.0
0 0.0
0 0.0
0.55749827455878
0.5765602918460577
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3175,	 Acc1 = 0.3111,	 Acc2 = 0.3188

 ===== Epoch 10	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513   1.5071117   1.4594793
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282  -0.22048812  0.13448866
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 1 1
train:	 Loss = 1.2744,	 Acc = 0.5214
33276 0.284
65371 0.586
33824 0.621
4284 0.604
706 0.504
75 0.373
0 0.0
0 0.0
0.5970938039516593
0.5765602918460577
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 3
val:	 Loss = 1.2198,	 Acc = 0.5659
3965 0.397
20118 0.596
9324 0.585
956 0.47
29 0.0
0 0.0
0 0.0
0 0.0
0.5878331744831893
0.5878331744831893
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3094,	 Acc1 = 0.2806,	 Acc2 = 0.2821

 ===== Epoch 11	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 6 1
train:	 Loss = 1.2746,	 Acc = 0.5226
33272 0.283
65378 0.586
33819 0.626
4286 0.604
706 0.513
75 0.32
0 0.0
0 0.0
0.5991905163814931
0.5878331744831893
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2633,	 Acc = 0.5364
3965 0.394
20118 0.571
9324 0.532
956 0.448
29 0.0
0 0.0
0 0.0
0 0.0
0.5549676274361587
0.5878331744831893
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3373,	 Acc1 = 0.3053,	 Acc2 = 0.3119

 ===== Epoch 12	 =====
[-0.36717504 -0.38357562  1.7710973   3.0353394  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-6.0253930e-03 -6.5372344e-03 -2.9703727e+00 -3.2274156e+00
  2.5970940e-03  9.9307357e-04 -9.0292487e-03 -7.4226381e-03
  9.6202353e-03  1.8220095e-02  2.3359330e-03  9.9131642e-03
 -1.0137177e-02 -1.1003844e-02] 2 6
train:	 Loss = 1.2749,	 Acc = 0.5230
33269 0.282
65378 0.587
33824 0.626
4284 0.607
706 0.49
75 0.36
0 0.0
0 0.0
0.5998350388905407
0.5878331744831893
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2597,	 Acc = 0.5267
3965 0.405
20118 0.557
9324 0.523
956 0.451
29 0.0
0 0.0
0 0.0
0 0.0
0.5426430472935222
0.5878331744831893
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3333,	 Acc1 = 0.3239,	 Acc2 = 0.3342

 ===== Epoch 13	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
  1.8480783   3.3911436 ] [-6.0253930e-03 -6.5372344e-03  1.1128664e-02  1.3828198e-02
  2.5970940e-03  9.9307357e-04 -9.0292487e-03 -7.4226381e-03
  9.6202353e-03  1.8220095e-02  2.3359330e-03  9.9131642e-03
 -2.9222312e+00 -4.6170864e+00] 1 1
train:	 Loss = 1.2715,	 Acc = 0.5229
33273 0.282
65373 0.586
33824 0.627
4285 0.613
706 0.503
75 0.347
0 0.0
0 0.0
0.5998388690139359
0.5878331744831893
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2626,	 Acc = 0.5377
3965 0.402
20118 0.565
9324 0.54
956 0.521
29 0.0
0 0.0
0 0.0
0 0.0
0.555329148453676
0.5878331744831893
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3431,	 Acc1 = 0.2946,	 Acc2 = 0.2990

 ===== Epoch 14	 =====
[-0.36717504 -0.38357562  2.053659    1.5349269  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.48789456  0.279933    0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 2 2
train:	 Loss = 1.2704,	 Acc = 0.5247
33273 0.282
65373 0.589
33823 0.629
4287 0.62
705 0.516
75 0.333
0 0.0
0 0.0
0.6023037894555116
0.5878331744831893
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2238,	 Acc = 0.5557
3965 0.402
20118 0.578
9324 0.581
956 0.477
29 0.69
0 0.0
0 0.0
0 0.0
0.575771518716929
0.5878331744831893
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3098,	 Acc1 = 0.2802,	 Acc2 = 0.2816

 ===== Epoch 15	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513   3.1516926  -4.7778053
 -0.36121437 -0.36837378  3.8112154   1.0539572  -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-6.0253930e-03 -6.5372344e-03  1.1128664e-02  1.3828198e-02
 -4.6774760e-01  7.1830554e+00 -9.0292487e-03 -7.4226381e-03
  3.6942267e-01  2.7495301e-01  2.3359330e-03  9.9131642e-03
 -1.0137177e-02 -1.1003844e-02] 6 2
train:	 Loss = 1.2717,	 Acc = 0.5236
33274 0.282
65370 0.587
33824 0.627
4287 0.62
706 0.51
75 0.44
0 0.0
0 0.0
0.6007941531909996
0.5878331744831893
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2214,	 Acc = 0.5574
3965 0.379
20118 0.568
9324 0.613
956 0.535
29 0.69
0 0.0
0 0.0
0 0.0
0.5807342163210306
0.5878331744831893
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 1
Testing:	 Loss = 1.3109,	 Acc1 = 0.2765,	 Acc2 = 0.2771

 ===== Epoch 16	 =====
[ 2.7883995   2.8384044  -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
  1.8748972   3.0722246 ] [-0.5540326   0.472245    0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
  0.06590925  0.5759208 ] 4 4
train:	 Loss = 1.2701,	 Acc = 0.5254
33271 0.283
65375 0.589
33823 0.628
4286 0.622
706 0.516
75 0.507
0 0.0
0 0.0
0.602829329113317
0.5878331744831893
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2687,	 Acc = 0.5297
3965 0.389
20118 0.57
9324 0.515
956 0.421
29 0.0
0 0.0
0 0.0
0 0.0
0.5480329970092352
0.5878331744831893
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3369,	 Acc1 = 0.2985,	 Acc2 = 0.3037

 ===== Epoch 17	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513   1.8798326   0.8924535
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282  -0.07927465  1.4939187
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 6 6
train:	 Loss = 1.2694,	 Acc = 0.5254
33270 0.281
65377 0.589
33822 0.632
4286 0.619
706 0.504
75 0.373
0 0.0
0 0.0
0.6033030901732108
0.5878331744831893
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2951,	 Acc = 0.5141
3965 0.397
20118 0.56
9324 0.473
956 0.442
29 0.103
0 0.0
0 0.0
0 0.0
0.5293653662865219
0.5878331744831893
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3684,	 Acc1 = 0.2769,	 Acc2 = 0.2776

 ===== Epoch 18	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378  2.1779187   2.7616155  -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-6.0253930e-03 -6.5372344e-03  1.1128664e-02  1.3828198e-02
  2.5970940e-03  9.9307357e-04 -9.0292487e-03 -7.4226381e-03
 -3.3520932e+00 -3.8238177e+00  2.3359330e-03  9.9131642e-03
 -1.0137177e-02 -1.1003844e-02] 6 2
train:	 Loss = 1.2685,	 Acc = 0.5260
33278 0.283
65373 0.59
33819 0.63
4285 0.626
706 0.52
75 0.44
0 0.0
0 0.0
0.6036083561932897
0.5878331744831893
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2477,	 Acc = 0.5402
3965 0.402
20118 0.565
9324 0.549
956 0.501
29 0.966
0 0.0
0 0.0
0 0.0
0.5581227199526736
0.5878331744831893
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3160,	 Acc1 = 0.3190,	 Acc2 = 0.3283

 ===== Epoch 19	 =====
[-0.36717504 -0.38357562  1.090081    1.5485464  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.00945855  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 0 0
train:	 Loss = 1.2657,	 Acc = 0.5265
33275 0.284
65375 0.59
33820 0.632
4286 0.62
705 0.489
75 0.4
0 0.0
0 0.0
0.6037444490269612
0.5878331744831893
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2245,	 Acc = 0.5686
3965 0.407
20118 0.592
9324 0.594
956 0.502
29 0.655
0 0.0
0 0.0
0 0.0
0.5896079140237289
0.5896079140237289
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3020,	 Acc1 = 0.3122,	 Acc2 = 0.3201

 ===== Epoch 20	 =====
[ 1.7123592   2.0252864  -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
  1.440083    2.0605717 ] [ 2.1492081e+00  6.3349283e-01  1.1128664e-02  1.3828198e-02
  2.5970940e-03  9.9307357e-04  2.1555905e+00  3.8736088e+00
  9.6202353e-03  1.8220095e-02  2.3359330e-03  9.9131642e-03
 -2.3916259e+00 -2.9934742e+00] 4 2
train:	 Loss = 1.2665,	 Acc = 0.5271
33273 0.284
65374 0.59
33824 0.633
4285 0.62
706 0.483
74 0.378
0 0.0
0 0.0
0.6045577050343842
0.5896079140237289
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2344,	 Acc = 0.5501
3965 0.403
20118 0.572
9324 0.57
956 0.506
29 0.552
0 0.0
0 0.0
0 0.0
0.569329871495711
0.5896079140237289
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3095,	 Acc1 = 0.3369,	 Acc2 = 0.3499

 ===== Epoch 21	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
  1.1762927   1.3050997 ] [ 2.1336884e+00  1.4719820e+00  1.1128664e-02  1.3828198e-02
  2.5970940e-03  9.9307357e-04 -9.0292487e-03 -7.4226381e-03
  9.6202353e-03  1.8220095e-02  2.3359330e-03  9.9131642e-03
  2.8718522e-01 -3.4593365e-01] 3 2
train:	 Loss = 1.2672,	 Acc = 0.5271
33273 0.282
65376 0.59
33819 0.635
4287 0.617
706 0.508
75 0.373
0 0.0
0 0.0
0.6053825422249504
0.5896079140237289
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2109,	 Acc = 0.5719
3965 0.402
20118 0.594
9324 0.605
956 0.478
29 0.69
0 0.0
0 0.0
0 0.0
0.5940118973280311
0.5940118973280311
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.2926,	 Acc1 = 0.3058,	 Acc2 = 0.3124

 ===== Epoch 22	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  1.702267    1.4405482  -0.40665367 -0.41433305  2.428986    2.123883
 -0.3911008  -0.38360253] [-6.0253930e-03 -6.5372344e-03  1.1128664e-02  1.3828198e-02
  2.5970940e-03  9.9307357e-04 -2.6682127e+00 -2.1824317e+00
  3.3249559e+00  3.6930833e+00 -3.9592009e+00 -3.1855447e+00
  5.1850486e+00  2.5185139e+00] 5 5
train:	 Loss = 1.2644,	 Acc = 0.5292
33268 0.285
65378 0.591
33823 0.64
4286 0.618
706 0.483
75 0.28
0 0.0
0 0.0
0.6071853301108682
0.5940118973280311
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2566,	 Acc = 0.5510
3965 0.394
20118 0.583
9324 0.556
956 0.488
29 0.034
0 0.0
0 0.0
0 0.0
0.571367535412627
0.5940118973280311
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3444,	 Acc1 = 0.2928,	 Acc2 = 0.2967

 ===== Epoch 23	 =====
[ 2.4740553   2.8384044  -0.4210503  -0.3377513  -0.4399085  -0.3643795
  1.7538962   2.0199685  -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-2.6742496e+00 -3.1521118e+00  4.2575045e+00  2.4305835e+00
  2.5970940e-03  9.9307357e-04 -2.7347467e+00 -2.8791144e+00
  9.6202353e-03  1.8220095e-02  2.3359330e-03  9.9131642e-03
 -1.0137177e-02 -1.1003844e-02] 6 6
train:	 Loss = 1.2635,	 Acc = 0.5301
33268 0.284
65379 0.594
33821 0.64
4287 0.618
706 0.506
75 0.293
0 0.0
0 0.0
0.6087773813634096
0.5940118973280311
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2204,	 Acc = 0.5507
3965 0.4
20118 0.56
9324 0.596
956 0.528
29 0.69
0 0.0
0 0.0
0 0.0
0.5703158379071219
0.5940118973280311
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3048,	 Acc1 = 0.3000,	 Acc2 = 0.3054

 ===== Epoch 24	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 6 1
train:	 Loss = 1.2593,	 Acc = 0.5318
33275 0.284
65373 0.596
33821 0.641
4286 0.625
706 0.496
75 0.293
0 0.0
0 0.0
0.6109283432923145
0.5940118973280311
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2512,	 Acc = 0.5418
3965 0.396
20118 0.577
9324 0.539
956 0.437
29 0.69
0 0.0
0 0.0
0 0.0
0.560686232622342
0.5940118973280311
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3194,	 Acc1 = 0.3414,	 Acc2 = 0.3554

 ===== Epoch 25	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  1.6638317   1.9493076  -0.40665367 -0.41433305  2.482315    2.5832512
 -0.3911008  -0.38360253] [-6.0253930e-03 -6.5372344e-03  1.1128664e-02  1.3828198e-02
  2.5970940e-03  9.9307357e-04 -2.6186814e+00 -2.7941530e+00
  9.6202353e-03  1.8220095e-02  6.9977924e-02 -1.5587107e+00
  4.9231777e+00  4.7800875e+00] 5 5
train:	 Loss = 1.2590,	 Acc = 0.5319
33274 0.284
65373 0.595
33823 0.643
4286 0.622
705 0.491
75 0.293
0 0.0
0 0.0
0.6108841188544244
0.5940118973280311
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2259,	 Acc = 0.5615
3965 0.393
20118 0.591
9324 0.576
956 0.482
29 0.69
0 0.0
0 0.0
0 0.0
0.5833963256318402
0.5940118973280311
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3126,	 Acc1 = 0.2866,	 Acc2 = 0.2893

 ===== Epoch 26	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513   2.9157648   2.9819663
 -0.36121437 -0.36837378  1.934649    2.2853467  -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282  -0.22213574 -1.240516
 -0.00902925 -0.00742264  0.0470233  -1.6147207   0.00233593  0.00991316
 -0.01013718 -0.01100384] 5 5
train:	 Loss = 1.2599,	 Acc = 0.5320
33271 0.284
65375 0.594
33824 0.645
4285 0.623
706 0.501
75 0.28
0 0.0
0 0.0
0.6110391790150098
0.5940118973280311
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2523,	 Acc = 0.5588
3965 0.392
20118 0.593
9324 0.57
956 0.453
29 0.0
0 0.0
0 0.0
0 0.0
0.5805698885857955
0.5940118973280311
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3399,	 Acc1 = 0.2793,	 Acc2 = 0.2806

 ===== Epoch 27	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 2 1
train:	 Loss = 1.2591,	 Acc = 0.5319
33272 0.284
65371 0.595
33826 0.642
4287 0.624
705 0.509
75 0.333
0 0.0
0 0.0
0.6110450395150772
0.5940118973280311
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2942,	 Acc = 0.5361
3965 0.401
20118 0.579
9324 0.514
956 0.425
29 0.0
0 0.0
0 0.0
0 0.0
0.5537187366483715
0.5940118973280311
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3635,	 Acc1 = 0.3295,	 Acc2 = 0.3410

 ===== Epoch 28	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513   0.92074937  2.9239242
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-6.0253930e-03 -6.5372344e-03  1.1128664e-02  1.3828198e-02
 -1.9431223e+00 -3.2763238e+00 -9.0292487e-03 -7.4226381e-03
  9.6202353e-03  1.8220095e-02  2.3359330e-03  9.9131642e-03
 -1.0137177e-02 -1.1003844e-02] 1 6
train:	 Loss = 1.2585,	 Acc = 0.5323
33271 0.283
65372 0.597
33828 0.642
4285 0.625
705 0.505
75 0.333
0 0.0
0 0.0
0.6119982736296936
0.5940118973280311
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2321,	 Acc = 0.5546
3965 0.404
20118 0.573
9324 0.582
956 0.526
29 0.621
0 0.0
0 0.0
0 0.0
0.5742597035527657
0.5940118973280311
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3146,	 Acc1 = 0.3272,	 Acc2 = 0.3382

 ===== Epoch 29	 =====
[ 0.6061513   1.2223325  -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [ 0.00035564 -0.01149871  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 0 0
train:	 Loss = 1.2577,	 Acc = 0.5317
33275 0.283
65375 0.597
33821 0.641
4286 0.615
704 0.489
75 0.347
0 0.0
0 0.0
0.611005073805162
0.5940118973280311
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2663,	 Acc = 0.5450
3965 0.387
20118 0.573
9324 0.553
956 0.529
29 0.69
0 0.0
0 0.0
0 0.0
0.5656489302264436
0.5940118973280311
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3426,	 Acc1 = 0.3249,	 Acc2 = 0.3355

 ===== Epoch 30	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513   2.101928    0.88352394
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282  -0.05454836 -0.11470311
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 5 3
train:	 Loss = 1.2563,	 Acc = 0.5316
33270 0.283
65376 0.596
33823 0.64
4286 0.627
706 0.524
75 0.387
0 0.0
0 0.0
0.6110237277731955
0.5940118973280311
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 3
val:	 Loss = 1.2523,	 Acc = 0.5522
3965 0.392
20118 0.583
9324 0.563
956 0.467
29 0.0
0 0.0
0 0.0
0 0.0
0.5730436783120255
0.5940118973280311
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 1
Testing:	 Loss = 1.3256,	 Acc1 = 0.3320,	 Acc2 = 0.3439

 ===== Epoch 31	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513   1.6293019   2.245279
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282  -0.01223834 -1.4607837
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 5 5
train:	 Loss = 1.2559,	 Acc = 0.5319
33275 0.282
65370 0.597
33823 0.641
4287 0.634
706 0.503
75 0.28
0 0.0
0 0.0
0.6116956484207902
0.5940118973280311
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 3
val:	 Loss = 1.2020,	 Acc = 0.5765
3965 0.398
20118 0.602
9324 0.607
956 0.493
29 0.0
0 0.0
0 0.0
0 0.0
0.5997305025142143
0.5997305025142143
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.2885,	 Acc1 = 0.3074,	 Acc2 = 0.3144

 ===== Epoch 32	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378  1.9524771   1.643741   -0.42853883 -0.42463273
  3.3679507   1.3260124 ] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.7928092   0.3764521   0.00233593  0.00991316
  0.92757034  0.38453233] 2 2
train:	 Loss = 1.2574,	 Acc = 0.5324
33273 0.282
65376 0.597
33819 0.643
4287 0.626
706 0.52
75 0.293
0 0.0
0 0.0
0.6123361115640256
0.5997305025142143
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 3
val:	 Loss = 1.2330,	 Acc = 0.5573
3965 0.399
20118 0.59
9324 0.565
956 0.459
29 0.0
0 0.0
0 0.0
0 0.0
0.577809182633845
0.5997305025142143
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3158,	 Acc1 = 0.3018,	 Acc2 = 0.3077

 ===== Epoch 33	 =====
[ 0.17333552  0.7929045  -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-4.2566508e-02  1.9830137e+00  1.1128664e-02  1.3828198e-02
  2.5970940e-03  9.9307357e-04 -9.0292487e-03 -7.4226381e-03
  9.6202353e-03  1.8220095e-02  2.3359330e-03  9.9131642e-03
 -1.0137177e-02 -1.1003844e-02] 4 6
train:	 Loss = 1.2574,	 Acc = 0.5323
33276 0.283
65371 0.596
33824 0.644
4284 0.629
706 0.516
75 0.267
0 0.0
0 0.0
0.6120276232495684
0.5997305025142143
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 3
val:	 Loss = 1.2118,	 Acc = 0.5699
3965 0.399
20118 0.592
9324 0.596
956 0.544
29 0.69
0 0.0
0 0.0
0 0.0
0.5921056955993033
0.5997305025142143
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.2970,	 Acc1 = 0.3146,	 Acc2 = 0.3231

 ===== Epoch 34	 =====
[ 1.450775    3.44062    -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305  1.6464388   1.124513
 -0.3911008  -0.38360253] [-1.7132781e+00 -3.7400463e+00  1.1128664e-02  1.3828198e-02
  2.5970940e-03  9.9307357e-04 -9.0292487e-03 -7.4226381e-03
  9.6202353e-03  1.8220095e-02  1.2241223e-01  4.2351514e-01
 -1.0137177e-02 -1.1003844e-02] 2 4
train:	 Loss = 1.2558,	 Acc = 0.5329
33269 0.282
65374 0.597
33827 0.644
4285 0.631
706 0.504
75 0.32
0 0.0
0 0.0
0.6130894722203574
0.5997305025142143
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2594,	 Acc = 0.5370
3965 0.359
20118 0.545
9324 0.595
956 0.543
29 0.0
0 0.0
0 0.0
0 0.0
0.5601275183225425
0.5997305025142143
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 1
Testing:	 Loss = 1.3552,	 Acc1 = 0.2503,	 Acc2 = 0.2455

 ===== Epoch 35	 =====
[-0.36717504 -0.38357562  1.7964869   3.0217197  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-6.0253930e-03 -6.5372344e-03 -3.0049045e+00 -3.2143285e+00
  2.5970940e-03  9.9307357e-04 -9.0292487e-03 -7.4226381e-03
  9.6202353e-03  1.8220095e-02  2.3359330e-03  9.9131642e-03
 -1.0137177e-02 -1.1003844e-02] 2 6
train:	 Loss = 1.2553,	 Acc = 0.5322
33273 0.282
65377 0.597
33823 0.642
4283 0.631
705 0.511
75 0.387
0 0.0
0 0.0
0.6120867421808311
0.5997305025142143
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2498,	 Acc = 0.5587
3965 0.402
20118 0.592
9324 0.562
956 0.497
29 0.0
0 0.0
0 0.0
0 0.0
0.5790580734216321
0.5997305025142143
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3245,	 Acc1 = 0.3260,	 Acc2 = 0.3367

 ===== Epoch 36	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378  1.8322815   2.8775983  -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.09115544  0.454069    0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
train:	 Loss = 1.2556,	 Acc = 0.5336
33271 0.284
65375 0.597
33823 0.643
4287 0.642
705 0.521
75 0.36
0 0.0
0 0.0
0.6132067328441951
0.5997305025142143
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 3
val:	 Loss = 1.2131,	 Acc = 0.5621
3965 0.395
20118 0.582
9324 0.598
956 0.506
29 0.0
0 0.0
0 0.0
0 0.0
0.5838893088375456
0.5997305025142143
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3032,	 Acc1 = 0.3025,	 Acc2 = 0.3084

 ===== Epoch 37	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513   1.3484172   1.6068169
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282  -0.04905345  0.7552431
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 6 4
train:	 Loss = 1.2551,	 Acc = 0.5321
33276 0.283
65368 0.595
33825 0.644
4286 0.63
706 0.517
75 0.36
0 0.0
0 0.0
0.6114329560713601
0.5997305025142143
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 3
val:	 Loss = 1.2514,	 Acc = 0.5552
3965 0.392
20118 0.586
9324 0.572
956 0.447
29 0.0
0 0.0
0 0.0
0 0.0
0.5765274262990108
0.5997305025142143
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3211,	 Acc1 = 0.3466,	 Acc2 = 0.3616

 ===== Epoch 38	 =====
[-0.36717504 -0.38357562  1.733422    2.908224   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.2807006   0.12288755  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 0 2
train:	 Loss = 1.2543,	 Acc = 0.5333
33276 0.284
65374 0.596
33821 0.646
4285 0.631
705 0.521
75 0.413
0 0.0
0 0.0
0.6130443122961826
0.5997305025142143
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 3
val:	 Loss = 1.2242,	 Acc = 0.5717
3965 0.395
20118 0.595
9324 0.604
956 0.508
29 0.0
0 0.0
0 0.0
0 0.0
0.5946363427219246
0.5997305025142143
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3044,	 Acc1 = 0.3196,	 Acc2 = 0.3290

 ===== Epoch 39	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305  2.1865435   2.1800823
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.08623275 -0.9582218
 -0.01013718 -0.01100384] 5 5
train:	 Loss = 1.2543,	 Acc = 0.5330
33273 0.283
65376 0.595
33820 0.645
4286 0.642
706 0.524
75 0.4
0 0.0
0 0.0
0.6129211705015202
0.5997305025142143
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2637,	 Acc = 0.5345
3965 0.39
20118 0.559
9324 0.549
956 0.504
29 0.0
0 0.0
0 0.0
0 0.0
0.5534229467249482
0.5997305025142143
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3351,	 Acc1 = 0.3425,	 Acc2 = 0.3566

 ===== Epoch 40	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513   1.1958706   1.8166611
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.7465768   0.17453735
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 2 2
train:	 Loss = 1.2544,	 Acc = 0.5322
33270 0.284
65376 0.593
33822 0.645
4287 0.636
706 0.516
75 0.4
0 0.0
0 0.0
0.6113881802313315
0.5997305025142143
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 3
val:	 Loss = 1.2535,	 Acc = 0.5421
3965 0.385
20118 0.551
9324 0.593
956 0.518
29 0.0
0 0.0
0 0.0
0 0.0
0.5625595688040227
0.5997305025142143
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3405,	 Acc1 = 0.2878,	 Acc2 = 0.2908

 ===== Epoch 41	 =====
[-0.36717504 -0.38357562  1.915655    0.85622287 -0.4399085  -0.3643795
  5.3948693   2.2432575  -0.40665367 -0.41433305  4.6431155   2.2411687
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.28292853  0.0967133   0.00259709  0.00099307
 -0.11917783  0.12171852  0.00962024  0.01822009 -0.04642855  0.16616279
 -0.01013718 -0.01100384] 3 3
train:	 Loss = 1.2566,	 Acc = 0.5315
33274 0.284
65374 0.594
33821 0.643
4286 0.629
706 0.531
75 0.387
0 0.0
0 0.0
0.610692294412154
0.5997305025142143
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2429,	 Acc = 0.5590
3965 0.397
20118 0.578
9324 0.588
956 0.551
29 0.0
0 0.0
0 0.0
0 0.0
0.58007690538009
0.5997305025142143
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 1
Testing:	 Loss = 1.3237,	 Acc1 = 0.3033,	 Acc2 = 0.3094

 ===== Epoch 42	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 0 1
train:	 Loss = 1.2525,	 Acc = 0.5342
33272 0.286
65374 0.597
33823 0.645
4286 0.641
706 0.534
75 0.307
0 0.0
0 0.0
0.6135483004680427
0.5997305025142143
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2517,	 Acc = 0.5411
3965 0.396
20118 0.559
9324 0.565
956 0.552
29 0.0
0 0.0
0 0.0
0 0.0
0.5598974594932132
0.5997305025142143
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3246,	 Acc1 = 0.3076,	 Acc2 = 0.3146

 ===== Epoch 43	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305  1.5624728   2.536826
 -0.3911008  -0.38360253] [-6.0253930e-03 -6.5372344e-03  1.1128664e-02  1.3828198e-02
  2.5970940e-03  9.9307357e-04 -9.0292487e-03 -7.4226381e-03
  9.6202353e-03  1.8220095e-02 -2.7579083e+00 -3.7033131e+00
 -1.0137177e-02 -1.1003844e-02] 1 1
train:	 Loss = 1.2527,	 Acc = 0.5329
33269 0.284
65374 0.595
33826 0.646
4286 0.635
706 0.525
75 0.36
0 0.0
0 0.0
0.6123797558191949
0.5997305025142143
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 3
val:	 Loss = 1.2467,	 Acc = 0.5488
3965 0.389
20118 0.576
9324 0.566
956 0.488
29 0.0
0 0.0
0 0.0
0 0.0
0.5696256614191343
0.5997305025142143
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 1
Testing:	 Loss = 1.3226,	 Acc1 = 0.3192,	 Acc2 = 0.3285

 ===== Epoch 44	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 0 1
train:	 Loss = 1.2510,	 Acc = 0.5330
33268 0.284
65376 0.596
33826 0.644
4286 0.639
705 0.525
75 0.36
0 0.0
0 0.0
0.6126232401120191
0.5997305025142143
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2597,	 Acc = 0.5520
3965 0.391
20118 0.584
9324 0.56
956 0.48
29 0.0
0 0.0
0 0.0
0 0.0
0.5729779472179315
0.5997305025142143
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3431,	 Acc1 = 0.3016,	 Acc2 = 0.3074

 ===== Epoch 45	 =====
[-0.36717504 -0.38357562  1.0827094   2.0910556  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.03285048 -0.08868758  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 5 2
train:	 Loss = 1.2529,	 Acc = 0.5337
33274 0.285
65373 0.596
33824 0.647
4284 0.637
706 0.524
75 0.293
0 0.0
0 0.0
0.6131668297174426
0.5997305025142143
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2325,	 Acc = 0.5622
3965 0.396
20118 0.579
9324 0.6
956 0.543
29 0.0
0 0.0
0 0.0
0 0.0
0.5838893088375456
0.5997305025142143
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3182,	 Acc1 = 0.3138,	 Acc2 = 0.3221

 ===== Epoch 46	 =====
[ 2.7699645   3.2932425  -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
  2.4029174   3.519234  ] [-0.01718591 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
  0.23972869 -0.01100384] 0 0
train:	 Loss = 1.2531,	 Acc = 0.5333
33272 0.285
65373 0.596
33824 0.644
4286 0.632
706 0.545
75 0.373
0 0.0
0 0.0
0.6124549221207704
0.5997305025142143
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2300,	 Acc = 0.5469
3965 0.391
20118 0.566
9324 0.579
956 0.499
29 0.0
0 0.0
0 0.0
0 0.0
0.5672593420317481
0.5997305025142143
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3074,	 Acc1 = 0.3169,	 Acc2 = 0.3258

 ===== Epoch 47	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513   1.8498613   1.1246216
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282  -0.07433008 -0.12137789
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 5 3
train:	 Loss = 1.2523,	 Acc = 0.5327
33269 0.286
65377 0.594
33822 0.646
4287 0.634
706 0.508
75 0.347
0 0.0
0 0.0
0.6114782241744752
0.5997305025142143
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.3095,	 Acc = 0.5193
3965 0.392
20118 0.543
9324 0.532
956 0.438
29 0.0
0 0.0
0 0.0
0 0.0
0.5358070135077398
0.5997305025142143
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3849,	 Acc1 = 0.3167,	 Acc2 = 0.3255

 ===== Epoch 48	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 6 1
train:	 Loss = 1.2511,	 Acc = 0.5338
33274 0.286
65371 0.596
33823 0.645
4287 0.638
706 0.527
75 0.333
0 0.0
0 0.0
0.6128119544992423
0.5997305025142143
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2628,	 Acc = 0.5421
3965 0.397
20118 0.562
9324 0.566
956 0.522
29 0.0
0 0.0
0 0.0
0 0.0
0.5610806191869063
0.5997305025142143
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3421,	 Acc1 = 0.3233,	 Acc2 = 0.3335

 ===== Epoch 49	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305  1.3124654   2.053023
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009 -0.01129783 -0.00540543
 -0.01013718 -0.01100384] 3 0
train:	 Loss = 1.2504,	 Acc = 0.5349
33273 0.284
65381 0.598
33816 0.649
4285 0.632
706 0.508
75 0.347
0 0.0
0 0.0
0.6149448989574442
0.5997305025142143
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2553,	 Acc = 0.5518
3965 0.39
20118 0.588
9324 0.55
956 0.5
29 0.0
0 0.0
0 0.0
0 0.0
0.5728793505767904
0.5997305025142143
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3265,	 Acc1 = 0.3437,	 Acc2 = 0.3581

 ===== Epoch 50	 =====
[-0.36717504 -0.38357562  1.7158129   2.179582   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.25955722  0.46969628  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 1 1
train:	 Loss = 1.2507,	 Acc = 0.5336
33272 0.284
65373 0.596
33823 0.646
4287 0.634
706 0.524
75 0.387
0 0.0
0 0.0
0.613135885828282
0.5997305025142143
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.3049,	 Acc = 0.5350
3965 0.378
20118 0.568
9324 0.534
956 0.51
29 0.345
0 0.0
0 0.0
0 0.0
0.5555592072830052
0.5997305025142143
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3825,	 Acc1 = 0.3157,	 Acc2 = 0.3243

 ===== Epoch 51	 =====
[ 3.4382982   3.0111918  -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-3.5797784e+00 -3.3208017e+00  1.1128664e-02  1.3828198e-02
  2.5970940e-03  9.9307357e-04 -9.0292487e-03 -7.4226381e-03
  9.6202353e-03  1.8220095e-02  2.3359330e-03  9.9131642e-03
 -1.0137177e-02 -1.1003844e-02] 3 1
train:	 Loss = 1.2529,	 Acc = 0.5333
33273 0.286
65368 0.595
33827 0.646
4287 0.633
706 0.504
75 0.32
0 0.0
0 0.0
0.6123457026941485
0.5997305025142143
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2403,	 Acc = 0.5485
3965 0.39
20118 0.557
9324 0.606
956 0.476
29 0.0
0 0.0
0 0.0
0 0.0
0.5690998126663819
0.5997305025142143
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3282,	 Acc1 = 0.2818,	 Acc2 = 0.2835

 ===== Epoch 52	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 1 1
train:	 Loss = 1.2505,	 Acc = 0.5333
33276 0.283
65369 0.596
33824 0.647
4286 0.632
706 0.521
75 0.333
0 0.0
0 0.0
0.613140226357184
0.5997305025142143
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2694,	 Acc = 0.5254
3965 0.394
20118 0.538
9324 0.552
956 0.549
29 0.0
0 0.0
0 0.0
0 0.0
0.5424787195582871
0.5997305025142143
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3476,	 Acc1 = 0.2907,	 Acc2 = 0.2942

 ===== Epoch 53	 =====
[-0.36717504 -0.38357562  1.1199758   2.3203166  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-6.0253930e-03 -6.5372344e-03  1.6039555e-01 -1.1422009e+00
  2.5970940e-03  9.9307357e-04 -9.0292487e-03 -7.4226381e-03
  9.6202353e-03  1.8220095e-02  2.3359330e-03  9.9131642e-03
 -1.0137177e-02 -1.1003844e-02] 5 5
train:	 Loss = 1.2515,	 Acc = 0.5336
33277 0.283
65376 0.596
33816 0.647
4287 0.636
705 0.492
75 0.333
0 0.0
0 0.0
0.6135873162029177
0.5997305025142143
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2174,	 Acc = 0.5573
3965 0.394
20118 0.569
9324 0.61
956 0.499
29 0.0
0 0.0
0 0.0
0 0.0
0.5786308213100207
0.5997305025142143
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3046,	 Acc1 = 0.2895,	 Acc2 = 0.2927

 ===== Epoch 54	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305  1.764823    2.0041542
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009 -0.4900323  -0.42819855
 -0.01013718 -0.01100384] 1 1
train:	 Loss = 1.2509,	 Acc = 0.5357
33272 0.284
65376 0.599
33822 0.648
4285 0.637
706 0.513
75 0.28
0 0.0
0 0.0
0.6160611524591422
0.5997305025142143
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2137,	 Acc = 0.5569
3965 0.392
20118 0.569
9324 0.61
956 0.485
29 0.0
0 0.0
0 0.0
0 0.0
0.5783678969336444
0.5997305025142143
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3072,	 Acc1 = 0.2690,	 Acc2 = 0.2681

 ===== Epoch 55	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513   1.8506297   2.877044
 -0.36121437 -0.36837378  1.2197949   0.9429101  -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-6.0253930e-03 -6.5372344e-03  1.1128664e-02  1.3828198e-02
 -3.2728367e+00 -3.2296002e+00 -9.0292487e-03 -7.4226381e-03
  1.3359386e-02  1.8362476e+00  2.3359330e-03  9.9131642e-03
 -1.0137177e-02 -1.1003844e-02] 6 6
train:	 Loss = 1.2509,	 Acc = 0.5344
33271 0.284
65374 0.597
33826 0.648
4285 0.634
705 0.513
75 0.36
0 0.0
0 0.0
0.6143576463818156
0.5997305025142143
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 3
val:	 Loss = 1.2463,	 Acc = 0.5495
3965 0.383
20118 0.577
9324 0.571
956 0.471
29 0.0
0 0.0
0 0.0
0 0.0
0.5712032076773917
0.5997305025142143
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 0
Testing:	 Loss = 1.3288,	 Acc1 = 0.3101,	 Acc2 = 0.3176

 ===== Epoch 56	 =====
[-0.36717504 -0.38357562  2.694544    1.580325   -0.4399085  -0.3643795
  1.3299658   3.221206   -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.4076895   0.05090838  0.00259709  0.00099307
  0.5801613  -0.21132973  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 2 2
train:	 Loss = 1.2505,	 Acc = 0.5342
33273 0.283
65375 0.597
33822 0.648
4285 0.628
706 0.518
75 0.333
0 0.0
0 0.0
0.6141584262873694
0.5997305025142143
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 3
val:	 Loss = 1.2274,	 Acc = 0.5569
3965 0.382
20118 0.574
9324 0.599
956 0.537
29 0.0
0 0.0
0 0.0
0 0.0
0.5797153843625727
0.5997305025142143
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3132,	 Acc1 = 0.3010,	 Acc2 = 0.3067

 ===== Epoch 57	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513   1.354949    1.7898724
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-6.0253930e-03 -6.5372344e-03  1.1128664e-02  1.3828198e-02
  6.4272881e-01 -6.6448622e+00 -9.0292487e-03 -7.4226381e-03
  9.6202353e-03  1.8220095e-02  2.3359330e-03  9.9131642e-03
 -1.0137177e-02 -1.1003844e-02] 2 2
train:	 Loss = 1.2519,	 Acc = 0.5339
33270 0.283
65375 0.597
33826 0.647
4285 0.645
705 0.505
75 0.36
0 0.0
0 0.0
0.6139201657299599
0.5997305025142143
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2372,	 Acc = 0.5416
3965 0.397
20118 0.552
9324 0.584
956 0.537
29 0.0
0 0.0
0 0.0
0 0.0
0.5604561737930128
0.5997305025142143
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3193,	 Acc1 = 0.3080,	 Acc2 = 0.3151

 ===== Epoch 58	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513   1.3203664   2.627017
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282  -0.16828723  0.1634127
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 3 1
train:	 Loss = 1.2505,	 Acc = 0.5343
33271 0.285
65373 0.596
33827 0.647
4284 0.641
706 0.544
75 0.44
0 0.0
0 0.0
0.6138301443437395
0.5997305025142143
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2261,	 Acc = 0.5689
3965 0.398
20118 0.589
9324 0.605
956 0.514
29 0.0
0 0.0
0 0.0
0 0.0
0.5912511913760805
0.5997305025142143
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 1
Testing:	 Loss = 1.3191,	 Acc1 = 0.2948,	 Acc2 = 0.2992

 ===== Epoch 59	 =====
[ 1.6955385   1.796597   -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
  1.3736966   2.2749271 ] [-1.9431376e+00 -2.1350093e+00  1.1128664e-02  1.3828198e-02
  2.5970940e-03  9.9307357e-04 -9.0292487e-03 -7.4226381e-03
  9.6202353e-03  1.8220095e-02  2.3359330e-03  9.9131642e-03
 -2.3052888e+00 -3.2550383e+00] 6 6
train:	 Loss = 1.2504,	 Acc = 0.5347
33275 0.284
65373 0.598
33821 0.646
4286 0.638
706 0.528
75 0.333
0 0.0
0 0.0
0.6146593644795273
0.5997305025142143
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2158,	 Acc = 0.5486
3965 0.375
20118 0.562
9324 0.6
956 0.501
29 0.0
0 0.0
0 0.0
0 0.0
0.5712689387714859
0.5997305025142143
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3039,	 Acc1 = 0.2806,	 Acc2 = 0.2821

 ===== Epoch 60	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305  1.0333352   1.7475922
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.04218684  0.9198374
 -0.01013718 -0.01100384] 6 4
train:	 Loss = 1.2501,	 Acc = 0.5350
33271 0.285
65373 0.598
33825 0.647
4286 0.633
706 0.518
75 0.387
0 0.0
0 0.0
0.614741284227689
0.5997305025142143
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2443,	 Acc = 0.5469
3965 0.395
20118 0.567
9324 0.574
956 0.515
29 0.0
0 0.0
0 0.0
0 0.0
0.5666348966378545
0.5997305025142143
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3273,	 Acc1 = 0.3280,	 Acc2 = 0.3392

 ===== Epoch 61	 =====
[-0.36717504 -0.38357562  0.5900683   1.45094    -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01560522  0.95391977  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 6 6
train:	 Loss = 1.2526,	 Acc = 0.5350
33274 0.285
65370 0.598
33825 0.648
4287 0.637
705 0.519
75 0.28
0 0.0
0 0.0
0.6149603882526712
0.5997305025142143
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 3
val:	 Loss = 1.2334,	 Acc = 0.5468
3965 0.393
20118 0.553
9324 0.606
956 0.502
29 0.0
0 0.0
0 0.0
0 0.0
0.5668320899201367
0.5997305025142143
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 1
Testing:	 Loss = 1.3172,	 Acc1 = 0.3163,	 Acc2 = 0.3250

 ===== Epoch 62	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 1
train:	 Loss = 1.2510,	 Acc = 0.5349
33275 0.285
65372 0.597
33823 0.649
4285 0.632
706 0.52
75 0.347
0 0.0
0 0.0
0.6147648689346927
0.5997305025142143
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2221,	 Acc = 0.5652
3965 0.396
20118 0.586
9324 0.599
956 0.523
29 0.0
0 0.0
0 0.0
0 0.0
0.5872744601833898
0.5997305025142143
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 1
Testing:	 Loss = 1.3134,	 Acc1 = 0.2783,	 Acc2 = 0.2793

 ===== Epoch 63	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305  2.1086295   0.83618623
  2.6372507   3.3048785 ] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.6839948   0.29177523
  0.5210408   0.10382923] 2 2
train:	 Loss = 1.2507,	 Acc = 0.5344
33277 0.285
65369 0.597
33823 0.648
4287 0.638
705 0.505
75 0.333
0 0.0
0 0.0
0.6141723975867791
0.5997305025142143
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2511,	 Acc = 0.5335
3965 0.395
20118 0.546
9324 0.57
956 0.497
29 0.0
0 0.0
0 0.0
0 0.0
0.5515496105432675
0.5997305025142143
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3300,	 Acc1 = 0.3049,	 Acc2 = 0.3114

 ===== Epoch 64	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513   2.8100963   2.0086462
 -0.36121437 -0.36837378  2.2026484   1.878174   -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-6.0253930e-03 -6.5372344e-03  1.1128664e-02  1.3828198e-02
 -4.6448588e+00 -2.3641038e+00 -9.0292487e-03 -7.4226381e-03
 -3.3842587e+00 -2.7550926e+00  2.3359330e-03  9.9131642e-03
 -1.0137177e-02 -1.1003844e-02] 3 6
train:	 Loss = 1.2514,	 Acc = 0.5345
33268 0.285
65378 0.596
33823 0.649
4286 0.639
706 0.499
75 0.347
0 0.0
0 0.0
0.6142728353857367
0.5997305025142143
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2715,	 Acc = 0.5450
3965 0.397
20118 0.577
9324 0.543
956 0.52
29 0.0
0 0.0
0 0.0
0 0.0
0.5642028461563743
0.5997305025142143
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3524,	 Acc1 = 0.3025,	 Acc2 = 0.3084

 ===== Epoch 65	 =====
[ 0.57666755  0.93011814 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378  3.2361114   3.0774832  -0.42853883 -0.42463273
  1.1864043   1.284187  ] [ 0.77301097  0.76249117  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.7202484   0.26898247  0.00233593  0.00991316
  0.50960654  0.59186983] 2 2
train:	 Loss = 1.2522,	 Acc = 0.5341
33275 0.282
65374 0.597
33819 0.648
4287 0.634
706 0.523
75 0.36
0 0.0
0 0.0
0.6145346773961501
0.5997305025142143
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2911,	 Acc = 0.5300
3965 0.395
20118 0.545
9324 0.559
956 0.51
29 0.0
0 0.0
0 0.0
0 0.0
0.5475728793505767
0.5997305025142143
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3660,	 Acc1 = 0.3107,	 Acc2 = 0.3183

 ===== Epoch 66	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378  1.1341033   3.008387   -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264 -0.16317381  0.3555552   0.00233593  0.00991316
 -0.01013718 -0.01100384] 1 1
train:	 Loss = 1.2511,	 Acc = 0.5339
33271 0.283
65375 0.597
33823 0.646
4286 0.644
706 0.508
75 0.333
0 0.0
0 0.0
0.6140219632666762
0.5997305025142143
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2284,	 Acc = 0.5459
3965 0.398
20118 0.554
9324 0.595
956 0.53
29 0.0
0 0.0
0 0.0
0 0.0
0.5650902159266441
0.5997305025142143
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3078,	 Acc1 = 0.3270,	 Acc2 = 0.3380

 ===== Epoch 67	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
  1.1758537   2.7951312 ] [ 1.8703374e+00  3.5260320e+00  1.1128664e-02  1.3828198e-02
  2.5970940e-03  9.9307357e-04 -9.0292487e-03 -7.4226381e-03
  9.6202353e-03  1.8220095e-02  2.3359330e-03  9.9131642e-03
  2.7460569e-01  8.5981375e-01] 4 6
train:	 Loss = 1.2519,	 Acc = 0.5327
33274 0.282
65373 0.597
33823 0.646
4286 0.622
705 0.519
75 0.333
0 0.0
0 0.0
0.6128599106098099
0.5997305025142143
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2088,	 Acc = 0.5565
3965 0.4
20118 0.567
9324 0.608
956 0.513
29 0.0
0 0.0
0 0.0
0 0.0
0.5769546784106221
0.5997305025142143
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.2893,	 Acc1 = 0.3144,	 Acc2 = 0.3228

 ===== Epoch 68	 =====
[-0.36717504 -0.38357562  3.9926915   2.6903126   4.131141    1.2786561
 -0.36121437 -0.36837378  3.156747    0.8614754  -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-6.0253930e-03 -6.5372344e-03  2.0272519e-01 -6.8416424e+00
 -7.6116437e-01 -1.4581412e-02 -9.0292487e-03 -7.4226381e-03
 -5.3214602e-02  1.8220095e-02  2.3359330e-03  9.9131642e-03
 -1.0137177e-02 -1.1003844e-02] 2 3
train:	 Loss = 1.2502,	 Acc = 0.5338
33275 0.283
65370 0.597
33823 0.646
4287 0.633
706 0.514
75 0.413
0 0.0
0 0.0
0.6137194156971447
0.5997305025142143
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2015,	 Acc = 0.5625
3965 0.402
20118 0.576
9324 0.609
956 0.513
29 0.0
0 0.0
0 0.0
0 0.0
0.5834291911788871
0.5997305025142143
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.2935,	 Acc1 = 0.2773,	 Acc2 = 0.2781

 ===== Epoch 69	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 1 1
train:	 Loss = 1.2500,	 Acc = 0.5340
33273 0.282
65371 0.597
33824 0.647
4287 0.638
706 0.52
75 0.347
0 0.0
0 0.0
0.6145132981019154
0.5997305025142143
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 3
val:	 Loss = 1.2111,	 Acc = 0.5530
3965 0.392
20118 0.569
9324 0.598
956 0.46
29 0.0
0 0.0
0 0.0
0 0.0
0.5739310480822953
0.5997305025142143
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.2914,	 Acc1 = 0.3041,	 Acc2 = 0.3104

 ===== Epoch 70	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513   4.0615926   2.4149404
 -0.36121437 -0.36837378  4.2655478   1.7301111  -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-6.0253930e-03 -6.5372344e-03  1.1128664e-02  1.3828198e-02
  3.8227987e-01 -6.7850327e+00 -9.0292487e-03 -7.4226381e-03
  2.5330679e-02  3.4659940e-01  2.3359330e-03  9.9131642e-03
 -1.0137177e-02 -1.1003844e-02] 2 4
train:	 Loss = 1.2515,	 Acc = 0.5343
33277 0.283
65372 0.598
33821 0.647
4285 0.635
706 0.507
75 0.373
0 0.0
0 0.0
0.6145560575106226
0.5997305025142143
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2372,	 Acc = 0.5418
3965 0.376
20118 0.56
9324 0.578
956 0.519
29 0.0
0 0.0
0 0.0
0 0.0
0.5634140730272456
0.5997305025142143
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3155,	 Acc1 = 0.3229,	 Acc2 = 0.3330

 ===== Epoch 71	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305  2.6528952   1.8453301
 -0.3911008  -0.38360253] [-6.0253930e-03 -6.5372344e-03  1.1128664e-02  1.3828198e-02
  2.5970940e-03  9.9307357e-04  4.3282728e+00  2.1166096e+00
  9.6202353e-03  1.8220095e-02  1.2555848e-01  9.0145516e-01
 -1.0137177e-02 -1.1003844e-02] 6 6
train:	 Loss = 1.2506,	 Acc = 0.5342
33274 0.284
65368 0.598
33826 0.646
4287 0.631
706 0.521
75 0.387
0 0.0
0 0.0
0.6141067694845678
0.5997305025142143
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2093,	 Acc = 0.5668
3965 0.396
20118 0.592
9324 0.592
956 0.508
29 0.0
0 0.0
0 0.0
0 0.0
0.5890491997239294
0.5997305025142143
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.2920,	 Acc1 = 0.3150,	 Acc2 = 0.3236

 ===== Epoch 72	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513   1.1958706   1.8166611
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.4311821   0.09666491
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 2 2
train:	 Loss = 1.2491,	 Acc = 0.5348
33271 0.285
65377 0.598
33820 0.646
4287 0.635
706 0.514
75 0.36
0 0.0
0 0.0
0.6145974200354865
0.5997305025142143
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2518,	 Acc = 0.5628
3965 0.37
20118 0.592
9324 0.588
956 0.508
29 0.0
0 0.0
0 0.0
0 0.0
0.5879646366713774
0.5997305025142143
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 1
Testing:	 Loss = 1.3471,	 Acc1 = 0.2907,	 Acc2 = 0.2942

 ===== Epoch 73	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378  2.6569805   3.3193192  -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264 -0.38159847 -0.26239496  0.00233593  0.00991316
 -0.01013718 -0.01100384] 5 3
train:	 Loss = 1.2506,	 Acc = 0.5342
33274 0.285
65370 0.597
33824 0.647
4287 0.631
706 0.513
75 0.36
0 0.0
0 0.0
0.6138094415990486
0.5997305025142143
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2608,	 Acc = 0.5342
3965 0.39
20118 0.545
9324 0.581
956 0.46
29 0.0
0 0.0
0 0.0
0 0.0
0.5530285601603838
0.5997305025142143
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3534,	 Acc1 = 0.2886,	 Acc2 = 0.2917

 ===== Epoch 74	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
  1.0439584   1.3286265 ] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
  0.2048499  -0.09074904] 3 2
train:	 Loss = 1.2493,	 Acc = 0.5352
33275 0.285
65374 0.598
33820 0.648
4286 0.634
706 0.518
75 0.387
0 0.0
0 0.0
0.6149758778450235
0.5997305025142143
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2362,	 Acc = 0.5413
3965 0.389
20118 0.563
9324 0.568
956 0.476
29 0.0
0 0.0
0 0.0
0 0.0
0.5611792158280474
0.5997305025142143
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3201,	 Acc1 = 0.3101,	 Acc2 = 0.3176

 ===== Epoch 75	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513   3.61548     2.3993137
 -0.36121437 -0.36837378  3.8739018   1.7251755  -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.7322917   0.01211771
 -0.00902925 -0.00742264  0.5556818   0.02717589  0.00233593  0.00991316
 -0.01013718 -0.01100384] 0 0
train:	 Loss = 1.2490,	 Acc = 0.5349
33269 0.285
65373 0.596
33826 0.651
4287 0.643
706 0.493
75 0.32
0 0.0
0 0.0
0.6146239941688166
0.5997305025142143
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 3
val:	 Loss = 1.2349,	 Acc = 0.5631
3965 0.401
20118 0.592
9324 0.579
956 0.499
29 0.0
0 0.0
0 0.0
0 0.0
0.5841850987609689
0.5997305025142143
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3068,	 Acc1 = 0.3466,	 Acc2 = 0.3616

 ===== Epoch 76	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.8449838   1.7062337  -0.40665367 -0.41433305  2.4233117   2.9546554
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.05929872  0.09453091  0.00962024  0.01822009  0.13027866  0.21211857
 -0.01013718 -0.01100384] 3 4
train:	 Loss = 1.2506,	 Acc = 0.5344
33273 0.282
65373 0.597
33824 0.649
4285 0.631
706 0.528
75 0.293
0 0.0
0 0.0
0.6148681699164612
0.5997305025142143
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2892,	 Acc = 0.5287
3965 0.395
20118 0.554
9324 0.54
956 0.452
29 0.0
0 0.0
0 0.0
0 0.0
0.5461596608275545
0.5997305025142143
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3607,	 Acc1 = 0.3254,	 Acc2 = 0.3360

 ===== Epoch 77	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513   0.3343858   1.2741915
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.05974256  0.21681096
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 6 0
train:	 Loss = 1.2506,	 Acc = 0.5346
33276 0.283
65371 0.599
33821 0.646
4287 0.633
706 0.531
75 0.32
0 0.0
0 0.0
0.6149242278918089
0.5997305025142143
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 3
val:	 Loss = 1.2869,	 Acc = 0.5380
3965 0.391
20118 0.571
9324 0.53
956 0.542
29 0.0
0 0.0
0 0.0
0 0.0
0.5571367535412627
0.5997305025142143
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3658,	 Acc1 = 0.2998,	 Acc2 = 0.3052

 ===== Epoch 78	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 1 1
train:	 Loss = 1.2505,	 Acc = 0.5348
33273 0.284
65371 0.599
33826 0.647
4285 0.629
706 0.533
75 0.333
0 0.0
0 0.0
0.6149065344369528
0.5997305025142143
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2716,	 Acc = 0.5454
3965 0.357
20118 0.575
9324 0.57
956 0.47
29 0.0
0 0.0
0 0.0
0 0.0
0.5698885857955106
0.5997305025142143
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3643,	 Acc1 = 0.2756,	 Acc2 = 0.2761

 ===== Epoch 79	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-6.0253930e-03 -6.5372344e-03  1.1128664e-02  1.3828198e-02
  2.5970940e-03  9.9307357e-04 -9.0292487e-03 -7.4226381e-03
  4.9960532e+00  2.1108918e+00  2.3359330e-03  9.9131642e-03
 -1.0137177e-02 -1.1003844e-02] 2 2
train:	 Loss = 1.2497,	 Acc = 0.5341
33272 0.284
65379 0.597
33819 0.646
4285 0.634
706 0.508
75 0.373
0 0.0
0 0.0
0.613749712268856
0.5997305025142143
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 3
val:	 Loss = 1.1985,	 Acc = 0.5729
3965 0.397
20118 0.591
9324 0.616
956 0.52
29 0.0
0 0.0
0 0.0
0 0.0
0.5958195024156178
0.5997305025142143
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.2808,	 Acc1 = 0.3113,	 Acc2 = 0.3191

 ===== Epoch 80	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 1 1
train:	 Loss = 1.2512,	 Acc = 0.5342
33279 0.284
65369 0.598
33820 0.647
4287 0.625
706 0.528
75 0.32
0 0.0
0 0.0
0.6141649961153688
0.5997305025142143
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2151,	 Acc = 0.5664
3965 0.344
20118 0.598
9324 0.601
956 0.514
29 0.0
0 0.0
0 0.0
0 0.0
0.5953922503040063
0.5997305025142143
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3001,	 Acc1 = 0.2899,	 Acc2 = 0.2932

 ===== Epoch 81	 =====
[ 2.7305489   2.635125   -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
  2.2978413   2.8657115 ] [-0.13379464  0.10013445  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
  0.02188172  0.11020885] 4 3
train:	 Loss = 1.2503,	 Acc = 0.5343
33274 0.284
65376 0.597
33818 0.647
4287 0.629
706 0.524
75 0.387
0 0.0
0 0.0
0.6139629011528649
0.5997305025142143
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 3
val:	 Loss = 1.2143,	 Acc = 0.5634
3965 0.392
20118 0.583
9324 0.602
956 0.515
29 0.0
0 0.0
0 0.0
0 0.0
0.5858283761133204
0.5997305025142143
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.2959,	 Acc1 = 0.2950,	 Acc2 = 0.2995

 ===== Epoch 82	 =====
[-0.36717504 -0.38357562  1.4447169   2.4837503  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.30968475  0.49368933  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 1 1
train:	 Loss = 1.2513,	 Acc = 0.5348
33270 0.285
65373 0.598
33826 0.648
4286 0.628
706 0.513
75 0.36
0 0.0
0 0.0
0.6143517541672261
0.5997305025142143
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2264,	 Acc = 0.5627
3965 0.395
20118 0.591
9324 0.582
956 0.487
29 0.0
0 0.0
0 0.0
0 0.0
0.5845466197784862
0.5997305025142143
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 1
Testing:	 Loss = 1.3143,	 Acc1 = 0.2971,	 Acc2 = 0.3019

 ===== Epoch 83	 =====
[-0.36717504 -0.38357562  3.2375538  -4.4758024   3.279647    1.0732768
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-6.0253930e-03 -6.5372344e-03  3.0910802e-01  6.7733269e+00
  1.7432526e-02  9.8927803e-03  3.8958049e+00  5.1412315e+00
  9.6202353e-03  1.8220095e-02  2.3359330e-03  9.9131642e-03
 -1.0137177e-02 -1.1003844e-02] 2 3
train:	 Loss = 1.2489,	 Acc = 0.5370
33275 0.285
65373 0.602
33822 0.649
4285 0.632
706 0.514
75 0.44
0 0.0
0 0.0
0.6174888021407813
0.5997305025142143
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2132,	 Acc = 0.5776
3965 0.391
20118 0.604
9324 0.61
956 0.505
29 0.0
0 0.0
0 0.0
0 0.0
0.6019653597134125
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.2910,	 Acc1 = 0.3150,	 Acc2 = 0.3236

 ===== Epoch 84	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 1 1
train:	 Loss = 1.2497,	 Acc = 0.5347
33274 0.287
65370 0.598
33825 0.645
4286 0.631
706 0.507
75 0.387
0 0.0
0 0.0
0.6137902591548215
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.1998,	 Acc = 0.5627
3965 0.387
20118 0.577
9324 0.615
956 0.498
29 0.0
0 0.0
0 0.0
0 0.0
0.5856311828310382
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.2896,	 Acc1 = 0.2810,	 Acc2 = 0.2826

 ===== Epoch 85	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 1 1
train:	 Loss = 1.2493,	 Acc = 0.5342
33271 0.288
65374 0.595
33825 0.649
4285 0.63
706 0.503
75 0.413
0 0.0
0 0.0
0.6128230949983215
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 3
val:	 Loss = 1.2717,	 Acc = 0.5521
3965 0.386
20118 0.577
9324 0.576
956 0.493
29 0.0
0 0.0
0 0.0
0 0.0
0.5738324514411542
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3495,	 Acc1 = 0.3171,	 Acc2 = 0.3260

 ===== Epoch 86	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 1 1
train:	 Loss = 1.2484,	 Acc = 0.5356
33275 0.288
65370 0.598
33823 0.647
4287 0.631
706 0.524
75 0.413
0 0.0
0 0.0
0.6147456863064809
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2349,	 Acc = 0.5557
3965 0.379
20118 0.574
9324 0.598
956 0.504
29 0.0
0 0.0
0 0.0
0 0.0
0.5787294179511618
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 1
Testing:	 Loss = 1.3290,	 Acc1 = 0.2952,	 Acc2 = 0.2997

 ===== Epoch 87	 =====
[-0.36717504 -0.38357562  3.0442657   2.3180468  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-6.0253930e-03 -6.5372344e-03 -4.7019868e+00 -2.5381603e+00
  4.6088424e+00  1.9700530e+00 -9.0292487e-03 -7.4226381e-03
  2.7085791e+00  1.7526599e+00  2.3359330e-03  9.9131642e-03
 -1.0137177e-02 -1.1003844e-02] 6 6
train:	 Loss = 1.2477,	 Acc = 0.5346
33275 0.287
65371 0.597
33822 0.647
4287 0.627
706 0.507
75 0.4
0 0.0
0 0.0
0.6136235025560852
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 3
val:	 Loss = 1.2441,	 Acc = 0.5386
3965 0.387
20118 0.56
9324 0.568
956 0.452
29 0.0
0 0.0
0 0.0
0 0.0
0.5583856443290498
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3242,	 Acc1 = 0.3200,	 Acc2 = 0.3295

 ===== Epoch 88	 =====
[ 0.21376367  1.5399566  -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00874627 -0.00901797  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 0 0
train:	 Loss = 1.2490,	 Acc = 0.5344
33269 0.288
65374 0.595
33827 0.648
4287 0.629
704 0.493
75 0.373
0 0.0
0 0.0
0.6130894722203574
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2283,	 Acc = 0.5428
3965 0.383
20118 0.558
9324 0.587
956 0.469
29 0.0
0 0.0
0 0.0
0 0.0
0.5636112663095277
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3094,	 Acc1 = 0.3132,	 Acc2 = 0.3213

 ===== Epoch 89	 =====
[ 1.6975442   2.670699   -0.4210503  -0.3377513  -0.4399085  -0.3643795
  0.9364411   2.7265787  -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [ 0.42975715  0.15471065  0.01112866  0.0138282   0.00259709  0.00099307
  0.512887    0.16249995  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 2 2
train:	 Loss = 1.2474,	 Acc = 0.5357
33272 0.288
65372 0.597
33825 0.649
4287 0.627
705 0.512
75 0.44
0 0.0
0 0.0
0.614641678815315
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 3
val:	 Loss = 1.2325,	 Acc = 0.5529
3965 0.38
20118 0.571
9324 0.6
956 0.452
29 0.0
0 0.0
0 0.0
0 0.0
0.5755085943405528
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3221,	 Acc1 = 0.2826,	 Acc2 = 0.2845

 ===== Epoch 90	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513   1.822579    2.3903842
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [ 1.7485904e+00  1.1866971e+00  1.1128664e-02  1.3828198e-02
 -3.2327247e+00 -2.7445662e+00 -9.0292487e-03 -7.4226381e-03
  9.6202353e-03  1.8220095e-02  2.3359330e-03  9.9131642e-03
  3.1809833e+00  1.6955435e+00] 6 6
train:	 Loss = 1.2491,	 Acc = 0.5347
33270 0.287
65378 0.596
33820 0.648
4287 0.632
706 0.521
75 0.453
0 0.0
0 0.0
0.6137091669384075
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2609,	 Acc = 0.5336
3965 0.395
20118 0.558
9324 0.552
956 0.423
29 0.0
0 0.0
0 0.0
0 0.0
0.5516482071844085
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3375,	 Acc1 = 0.3161,	 Acc2 = 0.3248

 ===== Epoch 91	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
  1.9659055   1.0410765 ] [ 2.8889406e+00  1.8713807e+00  1.1128664e-02  1.3828198e-02
  2.5970940e-03  9.9307357e-04 -9.0292487e-03 -7.4226381e-03
  9.6202353e-03  1.8220095e-02  2.3359330e-03  9.9131642e-03
 -7.2790724e-03  9.3317938e-01] 6 6
train:	 Loss = 1.2491,	 Acc = 0.5346
33274 0.286
65370 0.597
33824 0.649
4287 0.619
706 0.528
75 0.4
0 0.0
0 0.0
0.6138190328211621
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2391,	 Acc = 0.5444
3965 0.392
20118 0.553
9324 0.602
956 0.439
29 0.0
0 0.0
0 0.0
0 0.0
0.5642028461563743
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3197,	 Acc1 = 0.3035,	 Acc2 = 0.3096

 ===== Epoch 92	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 0 1
train:	 Loss = 1.2473,	 Acc = 0.5349
33273 0.288
65373 0.596
33823 0.649
4286 0.627
706 0.514
75 0.387
0 0.0
0 0.0
0.6137939633427006
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2554,	 Acc = 0.5461
3965 0.398
20118 0.58
9324 0.548
956 0.444
29 0.0
0 0.0
0 0.0
0 0.0
0.5652874092089263
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3276,	 Acc1 = 0.3398,	 Acc2 = 0.3534

 ===== Epoch 93	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305  2.46643     1.3395364
  3.4053206   3.5453749 ] [-6.0253930e-03 -6.5372344e-03  1.1128664e-02  1.3828198e-02
  2.5970940e-03  9.9307357e-04 -9.0292487e-03 -7.4226381e-03
  9.6202353e-03  1.8220095e-02  3.2848284e-01  3.1322128e-01
 -4.9474535e+00 -4.8052855e+00] 2 2
train:	 Loss = 1.2490,	 Acc = 0.5361
33269 0.288
65377 0.597
33822 0.65
4287 0.633
706 0.524
75 0.493
0 0.0
0 0.0
0.6151898491373109
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2908,	 Acc = 0.5433
3965 0.381
20118 0.574
9324 0.554
956 0.487
29 0.0
0 0.0
0 0.0
0 0.0
0.5644657705327505
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3598,	 Acc1 = 0.3388,	 Acc2 = 0.3521

 ===== Epoch 94	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  5.517058    1.8673408   5.0016823   3.2699652  -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-6.0253930e-03 -6.5372344e-03  3.1914666e+00  1.2461988e+00
  2.5970940e-03  9.9307357e-04 -3.2912949e-01  4.7175905e-01
 -7.0249186e+00 -4.4387827e+00  6.8090138e+00 -6.2829638e+00
 -1.0137177e-02 -1.1003844e-02] 4 2
train:	 Loss = 1.2469,	 Acc = 0.5353
33273 0.288
65372 0.596
33823 0.65
4287 0.62
706 0.53
75 0.4
0 0.0
0 0.0
0.6143310666295809
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 3
val:	 Loss = 1.2432,	 Acc = 0.5582
3965 0.381
20118 0.587
9324 0.582
956 0.477
29 0.0
0 0.0
0 0.0
0 0.0
0.5813257961678772
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3162,	 Acc1 = 0.3202,	 Acc2 = 0.3298

 ===== Epoch 95	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 5 1
train:	 Loss = 1.2462,	 Acc = 0.5352
33278 0.287
65374 0.596
33818 0.65
4285 0.626
706 0.521
75 0.48
0 0.0
0 0.0
0.6143893034587274
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2818,	 Acc = 0.5274
3965 0.38
20118 0.543
9324 0.56
956 0.496
29 0.0
0 0.0
0 0.0
0 0.0
0.5465869129391658
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3596,	 Acc1 = 0.2952,	 Acc2 = 0.2997

 ===== Epoch 96	 =====
[ 2.7156332   2.586846   -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
  3.596129    2.7454631 ] [-2.9011176e+00 -2.9065187e+00  1.1128664e-02  1.3828198e-02
  2.5970940e-03  9.9307357e-04 -9.0292487e-03 -7.4226381e-03
  9.6202353e-03  1.8220095e-02  2.2544804e+00  2.5589266e+00
 -5.1956029e+00 -3.8292041e+00] 6 6
train:	 Loss = 1.2464,	 Acc = 0.5361
33272 0.29
65377 0.597
33819 0.65
4287 0.628
706 0.503
75 0.427
0 0.0
0 0.0
0.6146992250441188
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2541,	 Acc = 0.5572
3965 0.395
20118 0.578
9324 0.588
956 0.5
29 0.0
0 0.0
0 0.0
0 0.0
0.5782693002925033
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 1
Testing:	 Loss = 1.3465,	 Acc1 = 0.2820,	 Acc2 = 0.2838

 ===== Epoch 97	 =====
[-0.36717504 -0.38357562  2.06963     1.5825951  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.20996694  0.10980043  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 2 3
train:	 Loss = 1.2489,	 Acc = 0.5342
33275 0.286
65368 0.596
33825 0.646
4287 0.634
706 0.523
75 0.413
0 0.0
0 0.0
0.6133453544470128
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2702,	 Acc = 0.5475
3965 0.382
20118 0.572
9324 0.569
956 0.515
29 0.0
0 0.0
0 0.0
0 0.0
0.5689683504781937
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 1
Testing:	 Loss = 1.3441,	 Acc1 = 0.3146,	 Acc2 = 0.3231

 ===== Epoch 98	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  0.9329991   2.5400336  -0.40665367 -0.41433305  2.1245143   1.9259639
 -0.3911008  -0.38360253] [-6.0253930e-03 -6.5372344e-03  1.1128664e-02  1.3828198e-02
  2.5970940e-03  9.9307357e-04 -1.6768664e+00 -3.5044296e+00
  9.6202353e-03  1.8220095e-02 -2.1579450e-01  5.1849037e-01
 -1.0137177e-02 -1.1003844e-02] 1 1
train:	 Loss = 1.2494,	 Acc = 0.5344
33274 0.286
65373 0.597
33821 0.648
4287 0.619
706 0.513
75 0.387
0 0.0
0 0.0
0.613579252268324
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2343,	 Acc = 0.5610
3965 0.388
20118 0.584
9324 0.592
956 0.5
29 0.0
0 0.0
0 0.0
0 0.0
0.5835606533670753
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3203,	 Acc1 = 0.2905,	 Acc2 = 0.2940

 ===== Epoch 99	 =====
[ 3.0872505   3.2373402  -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
  2.3585129   3.4643383 ] [-3.2501066e+00 -3.5415874e+00  1.1128664e-02  1.3828198e-02
  2.5970940e-03  9.9307357e-04 -9.0292487e-03 -7.4226381e-03
  9.6202353e-03  1.8220095e-02  2.6178570e+00  1.7439774e+00
 -3.5860608e+00 -4.7064013e+00] 4 6
train:	 Loss = 1.2496,	 Acc = 0.5330
33275 0.288
65371 0.595
33822 0.644
4287 0.616
706 0.524
75 0.413
0 0.0
0 0.0
0.6111009869462215
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 3
val:	 Loss = 1.2331,	 Acc = 0.5399
3965 0.385
20118 0.555
9324 0.578
956 0.509
29 0.0
0 0.0
0 0.0
0 0.0
0.5601275183225425
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3091,	 Acc1 = 0.3150,	 Acc2 = 0.3236

 ===== Epoch 100	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  1.6449012   1.1211605  -0.40665367 -0.41433305  1.8571092   3.0499496
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
  0.11516699 -0.01421954  0.00962024  0.01822009  0.21941926  0.0160406
 -0.01013718 -0.01100384] 0 0
train:	 Loss = 1.2487,	 Acc = 0.5349
33275 0.286
65374 0.596
33819 0.65
4287 0.626
706 0.533
75 0.427
0 0.0
0 0.0
0.6142277553447598
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2642,	 Acc = 0.5466
3965 0.39
20118 0.583
9324 0.549
956 0.429
29 0.0
0 0.0
0 0.0
0 0.0
0.5669964176553719
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3385,	 Acc1 = 0.3289,	 Acc2 = 0.3402

 ===== Epoch 101	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [ 2.2107103e+00  3.1564021e+00  1.1128664e-02  1.3828198e-02
  2.5970940e-03  9.9307357e-04 -9.0292487e-03 -7.4226381e-03
  9.6202353e-03  1.8220095e-02  2.3359330e-03  9.9131642e-03
  2.4868491e+00  4.2537694e+00] 5 5
train:	 Loss = 1.2480,	 Acc = 0.5361
33268 0.288
65373 0.598
33828 0.649
4286 0.627
706 0.513
75 0.44
0 0.0
0 0.0
0.6153182184371044
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 3
val:	 Loss = 1.2471,	 Acc = 0.5492
3965 0.395
20118 0.58
9324 0.555
956 0.47
29 0.966
0 0.0
0 0.0
0 0.0
0.5693627370427581
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3238,	 Acc1 = 0.3231,	 Acc2 = 0.3333

 ===== Epoch 102	 =====
[ 2.128058    0.9174132  -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305  1.2288783   3.1232533
 -0.3911008  -0.38360253] [-2.3493204e+00 -1.2766745e+00  1.1128664e-02  1.3828198e-02
  2.5970940e-03  9.9307357e-04 -9.0292487e-03 -7.4226381e-03
  9.6202353e-03  1.8220095e-02  2.2833121e-01 -1.1757457e+00
 -1.0137177e-02 -1.1003844e-02] 5 5
train:	 Loss = 1.2483,	 Acc = 0.5347
33272 0.288
65373 0.597
33824 0.645
4286 0.631
706 0.52
75 0.467
0 0.0
0 0.0
0.6134523900867029
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 3
val:	 Loss = 1.2759,	 Acc = 0.5361
3965 0.389
20118 0.554
9324 0.567
956 0.475
29 0.0
0 0.0
0 0.0
0 0.0
0.555296282906629
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3556,	 Acc1 = 0.2835,	 Acc2 = 0.2855

 ===== Epoch 103	 =====
[-0.36717504 -0.38357562  1.9115598   2.081976   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.02284697  0.04218363  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 3 0
train:	 Loss = 1.2486,	 Acc = 0.5366
33274 0.289
65372 0.598
33822 0.652
4287 0.625
706 0.513
75 0.453
0 0.0
0 0.0
0.6156413650227311
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2467,	 Acc = 0.5383
3965 0.361
20118 0.55
9324 0.596
956 0.486
29 0.0
0 0.0
0 0.0
0 0.0
0.5614092746573767
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3280,	 Acc1 = 0.3070,	 Acc2 = 0.3139

 ===== Epoch 104	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 6 1
train:	 Loss = 1.2487,	 Acc = 0.5339
33274 0.287
65375 0.595
33819 0.646
4287 0.628
706 0.508
75 0.453
0 0.0
0 0.0
0.6125146266137231
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2551,	 Acc = 0.5557
3965 0.391
20118 0.586
9324 0.569
956 0.483
29 0.0
0 0.0
0 0.0
0 0.0
0.5771518716929044
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3258,	 Acc1 = 0.3119,	 Acc2 = 0.3198

 ===== Epoch 105	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378  1.364146    0.9330392  -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.12032868  0.0898665   0.00233593  0.00991316
 -0.01013718 -0.01100384] 2 3
train:	 Loss = 1.2493,	 Acc = 0.5343
33270 0.286
65373 0.596
33826 0.648
4286 0.625
706 0.528
75 0.467
0 0.0
0 0.0
0.6135173498551781
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2226,	 Acc = 0.5449
3965 0.394
20118 0.548
9324 0.61
956 0.477
29 0.0
0 0.0
0 0.0
0 0.0
0.5644657705327505
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 1
Testing:	 Loss = 1.3117,	 Acc1 = 0.2734,	 Acc2 = 0.2734

 ===== Epoch 106	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378  1.3836986   2.89734    -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [ 2.4035151e+00  2.0202248e+00  1.1128664e-02  1.3828198e-02
  2.5970940e-03  9.9307357e-04 -9.0292487e-03 -7.4226381e-03
 -2.3190632e+00 -3.9880075e+00  2.3359330e-03  9.9131642e-03
  2.4679809e+00  2.7960272e+00] 6 6
train:	 Loss = 1.2496,	 Acc = 0.5337
33274 0.286
65371 0.595
33824 0.647
4286 0.625
706 0.537
75 0.413
0 0.0
0 0.0
0.6125913563906313
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 3
val:	 Loss = 1.2763,	 Acc = 0.5371
3965 0.387
20118 0.561
9324 0.556
956 0.483
29 0.0
0 0.0
0 0.0
0 0.0
0.5567095014296513
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3545,	 Acc1 = 0.3181,	 Acc2 = 0.3273

 ===== Epoch 107	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 1
train:	 Loss = 1.2468,	 Acc = 0.5358
33279 0.288
65374 0.597
33819 0.649
4284 0.637
705 0.532
75 0.44
0 0.0
0 0.0
0.614903555636552
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2520,	 Acc = 0.5400
3965 0.354
20118 0.561
9324 0.578
956 0.507
29 0.0
0 0.0
0 0.0
0 0.0
0.5642028461563743
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3376,	 Acc1 = 0.2928,	 Acc2 = 0.2967

 ===== Epoch 108	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  4.3542633   1.7062337  -0.40665367 -0.41433305  2.84995     2.7103105
 -0.3911008  -0.38360253] [-6.0253930e-03 -6.5372344e-03  1.1128664e-02  1.3828198e-02
  2.5970940e-03  9.9307357e-04 -3.2684259e-02  8.7734014e-02
  1.9336200e+00  1.6720579e+00  1.9639511e-02 -9.5672693e+00
 -1.0137177e-02 -1.1003844e-02] 1 3
train:	 Loss = 1.2487,	 Acc = 0.5340
33273 0.286
65372 0.596
33824 0.648
4286 0.622
706 0.524
75 0.467
0 0.0
0 0.0
0.6132568600558204
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 3
val:	 Loss = 1.2371,	 Acc = 0.5552
3965 0.398
20118 0.583
9324 0.571
956 0.478
29 0.0
0 0.0
0 0.0
0 0.0
0.5756729220757879
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3146,	 Acc1 = 0.3093,	 Acc2 = 0.3166

 ===== Epoch 109	 =====
[-0.36717504 -0.38357562  1.6900147   1.3783028  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-6.0253930e-03 -6.5372344e-03 -2.8600936e+00 -1.6351492e+00
  2.5970940e-03  9.9307357e-04  2.5015624e+00  3.9653673e+00
  9.6202353e-03  1.8220095e-02  2.3359330e-03  9.9131642e-03
 -1.0137177e-02 -1.1003844e-02] 5 5
train:	 Loss = 1.2491,	 Acc = 0.5345
33271 0.287
65373 0.595
33824 0.65
4287 0.621
706 0.51
75 0.427
0 0.0
0 0.0
0.6134752793363065
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2356,	 Acc = 0.5548
3965 0.394
20118 0.58
9324 0.578
956 0.47
29 0.0
0 0.0
0 0.0
0 0.0
0.575738653169882
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3235,	 Acc1 = 0.2791,	 Acc2 = 0.2803

 ===== Epoch 110	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 1
train:	 Loss = 1.2489,	 Acc = 0.5352
33272 0.285
65378 0.597
33819 0.649
4286 0.632
706 0.518
75 0.453
0 0.0
0 0.0
0.6150540934550756
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 3
val:	 Loss = 1.2278,	 Acc = 0.5536
3965 0.396
20118 0.568
9324 0.598
956 0.484
29 0.0
0 0.0
0 0.0
0 0.0
0.5741282413645775
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3141,	 Acc1 = 0.3074,	 Acc2 = 0.3144

 ===== Epoch 111	 =====
[ 0.90890104  3.2297173  -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
  0.85402954  3.519234  ] [-1.2043995e+00 -3.5341454e+00  1.1128664e-02  1.3828198e-02
  2.5970940e-03  9.9307357e-04 -9.0292487e-03 -7.4226381e-03
  9.6202353e-03  1.8220095e-02  2.7395074e+00  2.9817197e+00
 -1.6294522e+00 -4.7733870e+00] 6 6
train:	 Loss = 1.2473,	 Acc = 0.5355
33273 0.29
65372 0.596
33823 0.648
4287 0.631
706 0.504
75 0.48
0 0.0
0 0.0
0.6136980520414721
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2722,	 Acc = 0.5283
3965 0.371
20118 0.553
9324 0.55
956 0.467
29 0.0
0 0.0
0 0.0
0 0.0
0.548821770138364
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3497,	 Acc1 = 0.3037,	 Acc2 = 0.3099

 ===== Epoch 112	 =====
[-0.36717504 -0.38357562  2.1781504   2.649454   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-6.0253930e-03 -6.5372344e-03 -3.5239985e+00 -2.8566139e+00
  3.0609889e+00  2.7198532e+00 -9.0292487e-03 -7.4226381e-03
  9.6202353e-03  1.8220095e-02  2.3359330e-03  9.9131642e-03
 -1.0137177e-02 -1.1003844e-02] 6 6
train:	 Loss = 1.2486,	 Acc = 0.5354
33271 0.287
65375 0.596
33822 0.65
4287 0.633
706 0.518
75 0.507
0 0.0
0 0.0
0.6144823286817245
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2352,	 Acc = 0.5602
3965 0.389
20118 0.584
9324 0.591
956 0.496
29 0.0
0 0.0
0 0.0
0 0.0
0.5825089558615704
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 1
Testing:	 Loss = 1.3184,	 Acc1 = 0.2835,	 Acc2 = 0.2855

 ===== Epoch 113	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513   1.5705123   1.1804311
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.35370797  0.19901153
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 2 2
train:	 Loss = 1.2491,	 Acc = 0.5352
33273 0.287
65372 0.598
33823 0.648
4287 0.627
706 0.51
75 0.427
0 0.0
0 0.0
0.6144269779308096
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2641,	 Acc = 0.5340
3965 0.377
20118 0.558
9324 0.562
956 0.427
29 0.0
0 0.0
0 0.0
0 0.0
0.5544746442304532
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3331,	 Acc1 = 0.3165,	 Acc2 = 0.3253

 ===== Epoch 114	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [ 1.9091737e+00  3.2630737e+00  4.8217134e+00  1.1087840e+00
  2.5970940e-03  9.9307357e-04 -9.0292487e-03 -7.4226381e-03
  9.6202353e-03  1.8220095e-02  2.3359330e-03  9.9131642e-03
 -1.0137177e-02 -1.1003844e-02] 6 5
train:	 Loss = 1.2451,	 Acc = 0.5364
33272 0.288
65374 0.598
33822 0.652
4287 0.62
706 0.523
75 0.467
0 0.0
0 0.0
0.6156007826287118
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2999,	 Acc = 0.5281
3965 0.385
20118 0.563
9324 0.524
956 0.446
29 0.0
0 0.0
0 0.0
0 0.0
0.5466855095803069
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3856,	 Acc1 = 0.2893,	 Acc2 = 0.2925

 ===== Epoch 115	 =====
[-0.36717504 -0.38357562  2.1965783   0.801745   -0.4399085  -0.3643795
  0.7069793   1.9210432  -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-6.0253930e-03 -6.5372344e-03  1.6429535e-01  1.0107568e-01
  2.5970940e-03  9.9307357e-04 -1.3855975e+00 -2.7601686e+00
  9.6202353e-03  1.8220095e-02  2.3359330e-03  9.9131642e-03
 -1.0137177e-02 -1.1003844e-02] 1 4
train:	 Loss = 1.2478,	 Acc = 0.5348
33274 0.288
65370 0.597
33824 0.646
4287 0.633
706 0.517
75 0.427
0 0.0
0 0.0
0.6137518942663674
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2436,	 Acc = 0.5435
3965 0.389
20118 0.566
9324 0.567
956 0.486
29 0.0
0 0.0
0 0.0
0 0.0
0.5636769974036218
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3237,	 Acc1 = 0.2998,	 Acc2 = 0.3052

 ===== Epoch 116	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-6.0253930e-03 -6.5372344e-03  3.6392691e+00 -3.6309352e+00
  3.7752995e+00  1.1846539e+00 -9.0292487e-03 -7.4226381e-03
  9.6202353e-03  1.8220095e-02  2.3359330e-03  9.9131642e-03
 -1.0137177e-02 -1.1003844e-02] 2 2
train:	 Loss = 1.2490,	 Acc = 0.5352
33273 0.287
65373 0.597
33824 0.648
4286 0.629
705 0.522
75 0.48
0 0.0
0 0.0
0.6142639287187209
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2353,	 Acc = 0.5627
3965 0.396
20118 0.592
9324 0.581
956 0.476
29 0.0
0 0.0
0 0.0
0 0.0
0.5845137542314391
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3154,	 Acc1 = 0.3218,	 Acc2 = 0.3318

 ===== Epoch 117	 =====
[-0.36717504 -0.38357562  2.2514527   2.0365777  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.0222685   0.07272024  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 0 3
train:	 Loss = 1.2498,	 Acc = 0.5337
33271 0.288
65374 0.595
33824 0.647
4286 0.622
706 0.507
75 0.507
0 0.0
0 0.0
0.6122572291756582
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 3
val:	 Loss = 1.2533,	 Acc = 0.5457
3965 0.395
20118 0.579
9324 0.544
956 0.502
29 0.0
0 0.0
0 0.0
0 0.0
0.5653202747559734
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3355,	 Acc1 = 0.3227,	 Acc2 = 0.3328

 ===== Epoch 118	 =====
[ 4.1277494   1.7940559  -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
  2.6631906   1.9690785 ] [-1.4797106e+00 -8.5920818e-02  1.1128664e-02  1.3828198e-02
  2.5970940e-03  9.9307357e-04 -9.0292487e-03 -7.4226381e-03
  3.2509017e+00  4.5289583e+00  2.3359330e-03  9.9131642e-03
 -1.8967457e-01 -1.2902674e-01] 3 5
train:	 Loss = 1.2496,	 Acc = 0.5339
33272 0.284
65374 0.597
33824 0.647
4285 0.621
706 0.513
75 0.48
0 0.0
0 0.0
0.6137593033069899
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 3
val:	 Loss = 1.2384,	 Acc = 0.5399
3965 0.389
20118 0.543
9324 0.606
956 0.469
29 0.0
0 0.0
0 0.0
0 0.0
0.5595359384756959
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3238,	 Acc1 = 0.2868,	 Acc2 = 0.2895

 ===== Epoch 119	 =====
[-0.36717504 -0.38357562  1.611798    2.2272503  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-6.0253930e-03 -6.5372344e-03 -2.7537127e+00 -2.4509130e+00
  4.6681857e+00  2.7843761e+00 -9.0292487e-03 -7.4226381e-03
  5.8555403e+00  2.0511866e+00  2.3359330e-03  9.9131642e-03
 -1.0137177e-02 -1.1003844e-02] 6 5
train:	 Loss = 1.2480,	 Acc = 0.5341
33273 0.285
65372 0.597
33825 0.647
4285 0.626
706 0.513
75 0.44
0 0.0
0 0.0
0.6135062294390148
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2716,	 Acc = 0.5310
3965 0.38
20118 0.535
9324 0.592
956 0.496
29 0.0
0 0.0
0 0.0
0 0.0
0.5506622407729976
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3579,	 Acc1 = 0.2744,	 Acc2 = 0.2746

 ===== Epoch 120	 =====
[ 2.3492436   1.9185648  -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378  1.0708427   3.0478706  -0.42853883 -0.42463273
  2.5286574   2.2801552 ] [-2.5570376e+00 -2.2540848e+00  1.1128664e-02  1.3828198e-02
  2.5970940e-03  9.9307357e-04 -9.0292487e-03 -7.4226381e-03
 -1.9121364e+00 -4.1701088e+00  2.3359330e-03  9.9131642e-03
  1.3566503e-01  5.1212466e-01] 4 4
train:	 Loss = 1.2493,	 Acc = 0.5361
33269 0.288
65378 0.598
33823 0.65
4285 0.624
706 0.518
75 0.413
0 0.0
0 0.0
0.6152857567590897
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 3
val:	 Loss = 1.2386,	 Acc = 0.5460
3965 0.388
20118 0.561
9324 0.589
956 0.484
29 0.034
0 0.0
0 0.0
0 0.0
0.5666348966378545
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 1
Testing:	 Loss = 1.3219,	 Acc1 = 0.2880,	 Acc2 = 0.2910

 ===== Epoch 121	 =====
[-0.36717504 -0.38357562  1.5913216   1.2352984  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.4450277   0.13379349  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 1 1
train:	 Loss = 1.2488,	 Acc = 0.5348
33273 0.287
65372 0.596
33823 0.649
4287 0.629
706 0.528
75 0.453
0 0.0
0 0.0
0.6138706923836835
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2643,	 Acc = 0.5480
3965 0.384
20118 0.59
9324 0.537
956 0.47
29 0.034
0 0.0
0 0.0
0 0.0
0.569329871495711
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3393,	 Acc1 = 0.3231,	 Acc2 = 0.3333

 ===== Epoch 122	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
  1.2268524   0.96788204] [-6.0253930e-03 -6.5372344e-03  1.1128664e-02  1.3828198e-02
  2.5970940e-03  9.9307357e-04 -9.0292487e-03 -7.4226381e-03
  9.6202353e-03  1.8220095e-02  2.3359330e-03  9.9131642e-03
 -2.1143155e+00 -1.6601346e+00] 1 5
train:	 Loss = 1.2510,	 Acc = 0.5333
33270 0.284
65375 0.596
33824 0.646
4286 0.623
706 0.524
75 0.453
0 0.0
0 0.0
0.6127308998139375
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2588,	 Acc = 0.5491
3965 0.392
20118 0.574
9324 0.574
956 0.448
29 0.0
0 0.0
0 0.0
0 0.0
0.5695927958720873
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3411,	 Acc1 = 0.3064,	 Acc2 = 0.3131

 ===== Epoch 123	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 2 1
train:	 Loss = 1.2489,	 Acc = 0.5342
33269 0.285
65375 0.596
33824 0.648
4287 0.627
706 0.537
75 0.467
0 0.0
0 0.0
0.6136553271888517
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2131,	 Acc = 0.5561
3965 0.366
20118 0.572
9324 0.609
956 0.503
29 0.0
0 0.0
0 0.0
0 0.0
0.5807670818680777
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 1
Testing:	 Loss = 1.2998,	 Acc1 = 0.3146,	 Acc2 = 0.3231

 ===== Epoch 124	 =====
[ 3.0135965   1.5094647  -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378  2.20955     3.3242548  -0.42853883 -0.42463273
  2.5752609   1.6606159 ] [ 5.2182007e-01  1.6215289e-01  1.1128664e-02  1.3828198e-02
  2.5970940e-03  9.9307357e-04 -9.0292487e-03 -7.4226381e-03
 -3.3932352e+00 -4.5044584e+00  2.3359330e-03  9.9131642e-03
  4.4956872e-01  2.2185214e-01] 3 2
train:	 Loss = 1.2494,	 Acc = 0.5343
33269 0.283
65374 0.597
33827 0.649
4285 0.627
706 0.525
75 0.467
0 0.0
0 0.0
0.6143938158765477
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 3
val:	 Loss = 1.2377,	 Acc = 0.5294
3965 0.251
20118 0.563
9324 0.585
956 0.455
29 0.0
0 0.0
0 0.0
0 0.0
0.5656489302264436
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3219,	 Acc1 = 0.2581,	 Acc2 = 0.2873

 ===== Epoch 125	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513   2.051591    1.9037241
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-6.0253930e-03 -6.5372344e-03  1.1128664e-02  1.3828198e-02
 -3.2104087e-01  5.6167459e-01 -9.0292487e-03 -7.4226381e-03
  5.3012514e+00  1.6571316e+00  2.3359330e-03  9.9131642e-03
 -1.0137177e-02 -1.1003844e-02] 6 1
train:	 Loss = 1.2512,	 Acc = 0.5337
33275 0.285
65374 0.597
33820 0.647
4286 0.617
706 0.516
75 0.44
0 0.0
0 0.0
0.6131439368507879
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2882,	 Acc = 0.5454
3965 0.389
20118 0.564
9324 0.575
956 0.527
29 0.0
0 0.0
0 0.0
0 0.0
0.5657803924146317
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3620,	 Acc1 = 0.3053,	 Acc2 = 0.3119

 ===== Epoch 126	 =====
[-0.36717504 -0.38357562  1.8804368   0.9447495  -0.4399085  -0.3643795
  3.777742    3.2353382  -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01948268 -0.00144011  0.00259709  0.00099307
 -0.41709933 -0.00062573  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 3 3
train:	 Loss = 1.2480,	 Acc = 0.5354
33272 0.286
65376 0.598
33820 0.648
4287 0.625
706 0.523
75 0.427
0 0.0
0 0.0
0.6148814547686642
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2590,	 Acc = 0.5437
3965 0.379
20118 0.573
9324 0.56
956 0.472
29 0.0
0 0.0
0 0.0
0 0.0
0.5652216781148323
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3455,	 Acc1 = 0.2987,	 Acc2 = 0.3039

 ===== Epoch 127	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378  1.3566703   1.7523205  -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-6.0253930e-03 -6.5372344e-03  1.1128664e-02  1.3828198e-02
  2.5970940e-03  9.9307357e-04 -9.0292487e-03 -7.4226381e-03
  1.5997246e-01  1.0093286e+00  2.3359330e-03  9.9131642e-03
 -1.0137177e-02 -1.1003844e-02] 4 4
train:	 Loss = 1.2492,	 Acc = 0.5358
33278 0.285
65369 0.598
33821 0.651
4287 0.629
706 0.527
75 0.387
0 0.0
0 0.0
0.6157321260718602
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2239,	 Acc = 0.5429
3965 0.391
20118 0.545
9324 0.609
956 0.504
29 0.0
0 0.0
0 0.0
0 0.0
0.5625924343510698
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3119,	 Acc1 = 0.3060,	 Acc2 = 0.3126

 ===== Epoch 128	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-6.0253930e-03 -6.5372344e-03  1.1128664e-02  1.3828198e-02
  2.5970940e-03  9.9307357e-04 -9.0292487e-03 -7.4226381e-03
  3.9405839e+00  2.1407447e+00  2.3359330e-03  9.9131642e-03
  5.1604619e+00  1.9890057e+00] 6 5
train:	 Loss = 1.2490,	 Acc = 0.5346
33271 0.286
65376 0.596
33823 0.648
4285 0.633
706 0.521
75 0.413
0 0.0
0 0.0
0.6137821896130053
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2689,	 Acc = 0.5216
3965 0.389
20118 0.525
9324 0.573
956 0.513
29 0.0
0 0.0
0 0.0
0 0.0
0.5389621060242548
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3459,	 Acc1 = 0.2928,	 Acc2 = 0.2967

 ===== Epoch 129	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [ 2.7778959e+00  1.9433221e+00  1.1128664e-02  1.3828198e-02
  2.5970940e-03  9.9307357e-04 -9.0292487e-03 -7.4226381e-03
  9.6202353e-03  1.8220095e-02  2.3359330e-03  9.9131642e-03
 -1.0137177e-02 -1.1003844e-02] 4 6
train:	 Loss = 1.2500,	 Acc = 0.5345
33273 0.286
65375 0.596
33821 0.648
4287 0.635
705 0.518
75 0.4
0 0.0
0 0.0
0.6138802835138064
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2578,	 Acc = 0.5491
3965 0.39
20118 0.577
9324 0.566
956 0.463
29 0.0
0 0.0
0 0.0
0 0.0
0.5697899891543695
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 1
Testing:	 Loss = 1.3267,	 Acc1 = 0.3326,	 Acc2 = 0.3447

 ===== Epoch 130	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305  1.3930278   1.969946
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.01387057  0.03442291
 -0.01013718 -0.01100384] 3 0
train:	 Loss = 1.2480,	 Acc = 0.5354
33271 0.288
65374 0.596
33825 0.649
4285 0.626
706 0.53
75 0.44
0 0.0
0 0.0
0.6142041912434661
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2478,	 Acc = 0.5498
3965 0.395
20118 0.575
9324 0.57
956 0.474
29 0.0
0 0.0
0 0.0
0 0.0
0.5700529135307457
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3306,	 Acc1 = 0.3152,	 Acc2 = 0.3238

 ===== Epoch 131	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 5 1
train:	 Loss = 1.2493,	 Acc = 0.5348
33268 0.286
65376 0.598
33826 0.646
4286 0.626
705 0.508
75 0.427
0 0.0
0 0.0
0.6141002033222082
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 3
val:	 Loss = 1.2245,	 Acc = 0.5609
3965 0.398
20118 0.587
9324 0.585
956 0.469
29 0.0
0 0.0
0 0.0
0 0.0
0.5822131659381471
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 1
Testing:	 Loss = 1.3106,	 Acc1 = 0.3171,	 Acc2 = 0.3260

 ===== Epoch 132	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513   2.5826216   1.1335511
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282  -0.25565392 -0.03460575
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 0 3
train:	 Loss = 1.2470,	 Acc = 0.5353
33276 0.288
65366 0.597
33826 0.649
4287 0.624
706 0.514
75 0.493
0 0.0
0 0.0
0.6141665068098984
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2069,	 Acc = 0.5686
3965 0.388
20118 0.589
9324 0.616
956 0.445
29 0.0
0 0.0
0 0.0
0 0.0
0.5920728300522562
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3009,	 Acc1 = 0.2678,	 Acc2 = 0.2667

 ===== Epoch 133	 =====
[ 1.3492931   0.9199542  -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
  1.3090672   1.1717811 ] [-1.6179755e+00 -1.2791553e+00  1.1128664e-02  1.3828198e-02
  2.5970940e-03  9.9307357e-04 -9.0292487e-03 -7.4226381e-03
  9.6202353e-03  1.8220095e-02  2.3359330e-03  9.9131642e-03
  9.3926460e-02 -5.8850963e-02] 3 5
train:	 Loss = 1.2493,	 Acc = 0.5351
33263 0.286
65381 0.597
33825 0.648
4286 0.636
706 0.525
75 0.48
0 0.0
0 0.0
0.6145790377182971
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2643,	 Acc = 0.5488
3965 0.399
20118 0.568
9324 0.579
956 0.498
29 0.0
0 0.0
0 0.0
0 0.0
0.568245308443159
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3370,	 Acc1 = 0.2963,	 Acc2 = 0.3009

 ===== Epoch 134	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305  2.202051    1.7818005
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009 -0.06530606  0.08650611
 -0.01013718 -0.01100384] 3 3
train:	 Loss = 1.2478,	 Acc = 0.5354
33275 0.29
65374 0.597
33819 0.647
4287 0.627
706 0.523
75 0.44
0 0.0
0 0.0
0.6137098243830387
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2378,	 Acc = 0.5535
3965 0.387
20118 0.57
9324 0.599
956 0.472
29 0.0
0 0.0
0 0.0
0 0.0
0.5751799388700825
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3271,	 Acc1 = 0.2812,	 Acc2 = 0.2828

 ===== Epoch 135	 =====
[ 2.760514    2.617338   -0.4210503  -0.3377513  -0.4399085  -0.3643795
  1.1888484   3.6593041  -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-2.9432654e+00 -2.9362876e+00  1.6581348e+00  1.1305959e+00
  2.5970940e-03  9.9307357e-04 -2.0065763e+00 -4.8502159e+00
  9.6202353e-03  1.8220095e-02  2.3359330e-03  9.9131642e-03
 -1.0137177e-02 -1.1003844e-02] 4 6
train:	 Loss = 1.2487,	 Acc = 0.5346
33275 0.286
65371 0.597
33823 0.647
4286 0.623
706 0.538
75 0.44
0 0.0
0 0.0
0.6139208332933695
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2869,	 Acc = 0.5245
3965 0.388
20118 0.543
9324 0.547
956 0.512
29 0.0
0 0.0
0 0.0
0 0.0
0.542380122917146
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 1
Testing:	 Loss = 1.3569,	 Acc1 = 0.3177,	 Acc2 = 0.3268

 ===== Epoch 136	 =====
[-0.36717504 -0.38357562  2.022536    0.8607627  -0.4399085  -0.3643795
  5.393149    2.2743483  -0.40665367 -0.41433305  4.6446285  -5.379945
 -0.3911008  -0.38360253] [-6.0253930e-03 -6.5372344e-03 -6.1276812e-02  1.7959841e-01
  2.5970940e-03  9.9307357e-04  4.0499788e-02  2.7125040e-01
  9.6202353e-03  1.8220095e-02  1.7542008e-02  9.9057226e+00
 -1.0137177e-02 -1.1003844e-02] 3 3
train:	 Loss = 1.2474,	 Acc = 0.5359
33271 0.289
65373 0.597
33824 0.649
4287 0.632
706 0.525
75 0.453
0 0.0
0 0.0
0.6147316932815422
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2779,	 Acc = 0.5509
3965 0.383
20118 0.576
9324 0.581
956 0.44
29 0.0
0 0.0
0 0.0
0 0.0
0.5728136194826963
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3596,	 Acc1 = 0.2876,	 Acc2 = 0.2905

 ===== Epoch 137	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378  3.0428772   1.5376292  -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.31631148  0.06299909  0.00233593  0.00991316
 -0.01013718 -0.01100384] 2 3
train:	 Loss = 1.2469,	 Acc = 0.5352
33271 0.287
65377 0.597
33820 0.647
4287 0.636
706 0.535
75 0.467
0 0.0
0 0.0
0.6145398743586055
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2389,	 Acc = 0.5583
3965 0.386
20118 0.586
9324 0.582
956 0.468
29 0.0
0 0.0
0 0.0
0 0.0
0.5807670818680777
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3176,	 Acc1 = 0.3229,	 Acc2 = 0.3330

 ===== Epoch 138	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305  0.95996     2.1214397
 -0.3911008  -0.38360253] [-6.0253930e-03 -6.5372344e-03  1.1128664e-02  1.3828198e-02
  2.5970940e-03  9.9307357e-04 -9.0292487e-03 -7.4226381e-03
  9.6202353e-03  1.8220095e-02  1.2660722e-01 -1.4851813e+00
 -1.0137177e-02 -1.1003844e-02] 5 5
train:	 Loss = 1.2469,	 Acc = 0.5361
33267 0.288
65377 0.598
33825 0.649
4286 0.631
706 0.507
75 0.453
0 0.0
0 0.0
0.6152643642885229
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2598,	 Acc = 0.5471
3965 0.39
20118 0.574
9324 0.564
956 0.477
29 0.448
0 0.0
0 0.0
0 0.0
0.5676208630492654
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3392,	 Acc1 = 0.3093,	 Acc2 = 0.3166

 ===== Epoch 139	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378  3.339055    1.436453   -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.39709976  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 0 3
train:	 Loss = 1.2478,	 Acc = 0.5359
33274 0.288
65369 0.598
33827 0.648
4285 0.631
706 0.52
75 0.44
0 0.0
0 0.0
0.6149220233642171
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 3
val:	 Loss = 1.2520,	 Acc = 0.5355
3965 0.385
20118 0.55
9324 0.578
956 0.46
29 0.0
0 0.0
0 0.0
0 0.0
0.5550990896243468
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3320,	 Acc1 = 0.2998,	 Acc2 = 0.3052

 ===== Epoch 140	 =====
[ 5.7413635  10.133597   -0.4210503  -0.3377513   0.71171784  2.151519
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-5.7426071e+00 -1.0274306e+01  1.1128664e-02  1.3828198e-02
 -3.7433904e-01  2.5018486e-01 -9.0292487e-03 -7.4226381e-03
  9.6202353e-03  1.8220095e-02  2.3359330e-03  9.9131642e-03
 -1.0137177e-02 -1.1003844e-02] 1 1
train:	 Loss = 1.2485,	 Acc = 0.5347
33277 0.287
65371 0.596
33822 0.648
4285 0.63
706 0.517
75 0.48
0 0.0
0 0.0
0.6135681332067255
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.3008,	 Acc = 0.5238
3965 0.388
20118 0.543
9324 0.553
956 0.415
29 0.0
0 0.0
0 0.0
0 0.0
0.5414598875998291
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3764,	 Acc1 = 0.3002,	 Acc2 = 0.3057

 ===== Epoch 141	 =====
[-0.36717504 -0.38357562  2.2596433   2.0933254  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.00110281  0.01819057  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 0 0
train:	 Loss = 1.2488,	 Acc = 0.5349
33271 0.287
65377 0.596
33820 0.65
4287 0.617
706 0.528
75 0.48
0 0.0
0 0.0
0.6141082817819978
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2265,	 Acc = 0.5504
3965 0.396
20118 0.558
9324 0.608
956 0.478
29 0.0
0 0.0
0 0.0
0 0.0
0.5705458967364512
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3116,	 Acc1 = 0.2810,	 Acc2 = 0.2826

 ===== Epoch 142	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
  1.5420822   1.4096632 ] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.300598   -0.01738346] 1 1
train:	 Loss = 1.2471,	 Acc = 0.5356
33275 0.288
65371 0.598
33823 0.648
4286 0.627
706 0.518
75 0.44
0 0.0
0 0.0
0.6145250860820441
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2313,	 Acc = 0.5570
3965 0.385
20118 0.589
9324 0.571
956 0.473
29 0.0
0 0.0
0 0.0
0 0.0
0.5793867288921024
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 1
Testing:	 Loss = 1.3098,	 Acc1 = 0.3212,	 Acc2 = 0.3310

 ===== Epoch 143	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
  1.4431615   1.6370891 ] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.09590329  0.6875641 ] 4 4
train:	 Loss = 1.2472,	 Acc = 0.5342
33275 0.288
65377 0.596
33818 0.647
4285 0.626
706 0.516
75 0.4
0 0.0
0 0.0
0.6129233366263511
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2756,	 Acc = 0.5393
3965 0.392
20118 0.562
9324 0.556
956 0.514
29 0.0
0 0.0
0 0.0
0 0.0
0.5583856443290498
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 1
Testing:	 Loss = 1.3615,	 Acc1 = 0.2849,	 Acc2 = 0.2873

 ===== Epoch 144	 =====
[ 2.128058    0.9174132  -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305  1.2288783   3.1232533
 -0.3911008  -0.38360253] [-2.3493204e+00 -1.2766745e+00  1.1128664e-02  1.3828198e-02
  2.5970940e-03  9.9307357e-04 -9.0292487e-03 -7.4226381e-03
  9.6202353e-03  1.8220095e-02  9.2523634e-02 -1.0103049e+00
 -1.0137177e-02 -1.1003844e-02] 5 5
train:	 Loss = 1.2494,	 Acc = 0.5345
33271 0.288
65375 0.597
33822 0.645
4287 0.627
706 0.517
75 0.467
0 0.0
0 0.0
0.6130820505442862
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2453,	 Acc = 0.5540
3965 0.382
20118 0.58
9324 0.583
956 0.448
29 0.0
0 0.0
0 0.0
0 0.0
0.5763959641108226
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 1
Testing:	 Loss = 1.3258,	 Acc1 = 0.2826,	 Acc2 = 0.2845

 ===== Epoch 145	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513   2.2125912   1.5420778
 -0.36121437 -0.36837378  0.9046367   1.8757061  -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.05479627  0.33918187
 -0.00902925 -0.00742264  0.0806849   0.8391684   0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
train:	 Loss = 1.2492,	 Acc = 0.5353
33271 0.285
65376 0.598
33824 0.649
4284 0.632
706 0.51
75 0.387
0 0.0
0 0.0
0.6151249220735625
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 3
val:	 Loss = 1.2610,	 Acc = 0.5545
3965 0.394
20118 0.577
9324 0.578
956 0.524
29 0.0
0 0.0
0 0.0
0 0.0
0.5754428632464588
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 1
Testing:	 Loss = 1.3413,	 Acc1 = 0.3029,	 Acc2 = 0.3089

 ===== Epoch 146	 =====
[ 2.8044894   3.2932425  -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
  2.5950453   3.519234  ] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 0 0
train:	 Loss = 1.2496,	 Acc = 0.5341
33266 0.284
65379 0.595
33824 0.65
4287 0.629
705 0.538
75 0.4
0 0.0
0 0.0
0.613704804833605
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2657,	 Acc = 0.5359
3965 0.374
20118 0.549
9324 0.578
956 0.526
29 0.586
0 0.0
0 0.0
0 0.0
0.5569395602589805
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3452,	 Acc1 = 0.2775,	 Acc2 = 0.2783

 ===== Epoch 147	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  1.670143    2.8679008  -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-6.0253930e-03 -6.5372344e-03  1.1128664e-02  1.3828198e-02
  2.5970940e-03  9.9307357e-04  9.9640742e-02 -2.1688378e+00
  9.6202353e-03  1.8220095e-02  3.9790792e+00  2.7243671e+00
 -1.0137177e-02 -1.1003844e-02] 5 5
train:	 Loss = 1.2475,	 Acc = 0.5354
33272 0.286
65373 0.597
33824 0.65
4286 0.628
706 0.521
75 0.427
0 0.0
0 0.0
0.6150732755313435
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2367,	 Acc = 0.5622
3965 0.394
20118 0.586
9324 0.591
956 0.494
29 0.0
0 0.0
0 0.0
0 0.0
0.5841850987609689
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3208,	 Acc1 = 0.2919,	 Acc2 = 0.2957

 ===== Epoch 148	 =====
[ 2.3589866   1.8956958  -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
  1.4994365   2.3507357 ] [-2.5661874e+00 -2.2317581e+00  1.1128664e-02  1.3828198e-02
  2.5970940e-03  9.9307357e-04 -9.0292487e-03 -7.4226381e-03
  9.6202353e-03  1.8220095e-02  2.3359330e-03  9.9131642e-03
 -2.4688158e+00 -3.3475430e+00] 6 6
train:	 Loss = 1.2505,	 Acc = 0.5336
33276 0.286
65374 0.595
33820 0.648
4285 0.625
706 0.508
75 0.44
0 0.0
0 0.0
0.6125839248033762
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 3
val:	 Loss = 1.2594,	 Acc = 0.5406
3965 0.398
20118 0.552
9324 0.583
956 0.469
29 0.966
0 0.0
0 0.0
0 0.0
0.5592072830052256
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3454,	 Acc1 = 0.2806,	 Acc2 = 0.2821

 ===== Epoch 149	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 3 1
train:	 Loss = 1.2490,	 Acc = 0.5349
33275 0.284
65374 0.597
33820 0.649
4287 0.628
705 0.546
75 0.387
0 0.0
0 0.0
0.6148511907616463
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2540,	 Acc = 0.5275
3965 0.389
20118 0.539
9324 0.57
956 0.469
29 0.0
0 0.0
0 0.0
0 0.0
0.5455352154336609
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3384,	 Acc1 = 0.3159,	 Acc2 = 0.3246

 ===== Epoch 150	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 1
train:	 Loss = 1.2492,	 Acc = 0.5360
33276 0.286
65369 0.599
33825 0.649
4285 0.63
706 0.506
75 0.373
0 0.0
0 0.0
0.6157299060042202
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2785,	 Acc = 0.5196
3965 0.383
20118 0.537
9324 0.55
956 0.451
29 0.0
0 0.0
0 0.0
0 0.0
0.5374174253130444
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3576,	 Acc1 = 0.2884,	 Acc2 = 0.2915

 ===== Epoch 151	 =====
[ 2.847102    3.2932425  -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
  2.5976834   3.519234  ] [-0.04604284 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01356796 -0.01100384] 0 0
train:	 Loss = 1.2488,	 Acc = 0.5358
33276 0.288
65368 0.598
33824 0.647
4287 0.632
706 0.52
75 0.44
0 0.0
0 0.0
0.6147995396125072
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2354,	 Acc = 0.5586
3965 0.399
20118 0.583
9324 0.585
956 0.459
29 0.0
0 0.0
0 0.0
0 0.0
0.5794853255332435
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3166,	 Acc1 = 0.3251,	 Acc2 = 0.3357

 ===== Epoch 152	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 1 1
train:	 Loss = 1.2499,	 Acc = 0.5341
33276 0.284
65374 0.596
33819 0.648
4286 0.627
706 0.524
75 0.453
0 0.0
0 0.0
0.6137636677536927
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2783,	 Acc = 0.5276
3965 0.381
20118 0.539
9324 0.577
956 0.43
29 0.0
0 0.0
0 0.0
0 0.0
0.546718375127354
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3551,	 Acc1 = 0.2948,	 Acc2 = 0.2992

 ===== Epoch 153	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513   2.3082688   1.5822606
 -0.36121437 -0.36837378  0.87300724  1.8312873  -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282  -0.06278989  0.09888984
 -0.00902925 -0.00742264  0.13154845  0.37943736  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 2
train:	 Loss = 1.2488,	 Acc = 0.5348
33275 0.285
65372 0.597
33821 0.647
4287 0.632
706 0.527
75 0.493
0 0.0
0 0.0
0.6144387642550906
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 3
val:	 Loss = 1.2333,	 Acc = 0.5462
3965 0.383
20118 0.561
9324 0.595
956 0.456
29 0.0
0 0.0
0 0.0
0 0.0
0.5674236697669832
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3196,	 Acc1 = 0.2992,	 Acc2 = 0.3044

 ===== Epoch 154	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305  1.4618641   3.2576427
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00128718  0.00991316
 -0.01013718 -0.01100384] 0 0
train:	 Loss = 1.2491,	 Acc = 0.5347
33273 0.286
65374 0.597
33824 0.648
4284 0.632
706 0.525
75 0.48
0 0.0
0 0.0
0.6140145593355265
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2534,	 Acc = 0.5396
3965 0.393
20118 0.559
9324 0.569
956 0.475
29 0.0
0 0.0
0 0.0
0 0.0
0.5587142997995201
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3382,	 Acc1 = 0.2977,	 Acc2 = 0.3027

 ===== Epoch 155	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513   3.5858934   2.372525
 -0.36121437 -0.36837378  4.255771    1.712837   -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-6.0253930e-03 -6.5372344e-03  1.1128664e-02  1.3828198e-02
 -5.7542367e+00 -2.7267668e+00 -9.0292487e-03 -7.4226381e-03
 -6.0547237e+00 -2.5550797e+00  2.3359330e-03  9.9131642e-03
 -1.0137177e-02 -1.1003844e-02] 2 6
train:	 Loss = 1.2479,	 Acc = 0.5347
33274 0.286
65374 0.598
33821 0.646
4286 0.626
706 0.527
75 0.387
0 0.0
0 0.0
0.614241046594157
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2418,	 Acc = 0.5591
3965 0.394
20118 0.59
9324 0.578
956 0.435
29 0.0
0 0.0
0 0.0
0 0.0
0.5806356196798895
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3258,	 Acc1 = 0.3037,	 Acc2 = 0.3099

 ===== Epoch 156	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 0 1
train:	 Loss = 1.2496,	 Acc = 0.5338
33274 0.285
65372 0.596
33824 0.647
4285 0.625
706 0.521
75 0.493
0 0.0
0 0.0
0.6132915156049183
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2709,	 Acc = 0.5076
3965 0.243
20118 0.531
9324 0.577
956 0.464
29 0.0
0 0.0
0 0.0
0 0.0
0.5421500640878167
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3481,	 Acc1 = 0.2672,	 Acc2 = 0.2982

 ===== Epoch 157	 =====
[ 2.506789    3.3923411  -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-2.7049901e+00 -3.6929121e+00  1.1128664e-02  1.3828198e-02
  2.5970940e-03  9.9307357e-04 -9.0292487e-03 -7.4226381e-03
  9.6202353e-03  1.8220095e-02  3.6450658e+00  1.7746147e+00
 -1.0137177e-02 -1.1003844e-02] 2 2
train:	 Loss = 1.2491,	 Acc = 0.5344
33275 0.286
65371 0.596
33824 0.649
4285 0.626
706 0.523
75 0.44
0 0.0
0 0.0
0.6136906417548268
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 3
val:	 Loss = 1.2306,	 Acc = 0.5455
3965 0.393
20118 0.564
9324 0.578
956 0.478
29 0.0
0 0.0
0 0.0
0 0.0
0.5653202747559734
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3096,	 Acc1 = 0.3020,	 Acc2 = 0.3079

 ===== Epoch 158	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378  1.3457419   2.9911132  -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.3567068  -0.12208743  0.00233593  0.00991316
 -0.01013718 -0.01100384] 2 2
train:	 Loss = 1.2488,	 Acc = 0.5354
33269 0.287
65376 0.597
33823 0.649
4287 0.62
706 0.51
75 0.48
0 0.0
0 0.0
0.6145184957848601
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 3
val:	 Loss = 1.3146,	 Acc = 0.5356
3965 0.386
20118 0.562
9324 0.55
956 0.476
29 0.0
0 0.0
0 0.0
0 0.0
0.5550004929832058
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3955,	 Acc1 = 0.3064,	 Acc2 = 0.3131

 ===== Epoch 159	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305  2.6725626   1.6694019
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.55185866 -0.11876299
 -0.01013718 -0.01100384] 0 2
train:	 Loss = 1.2485,	 Acc = 0.5343
33276 0.288
65374 0.596
33820 0.647
4287 0.625
704 0.503
75 0.48
0 0.0
0 0.0
0.612929215422981
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2378,	 Acc = 0.5568
3965 0.382
20118 0.573
9324 0.603
956 0.497
29 0.0
0 0.0
0 0.0
0 0.0
0.5795510566273375
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 1
Testing:	 Loss = 1.3260,	 Acc1 = 0.2773,	 Acc2 = 0.2781

 ===== Epoch 160	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 3 1
train:	 Loss = 1.2456,	 Acc = 0.5367
33276 0.291
65370 0.599
33824 0.648
4285 0.627
706 0.514
75 0.533
0 0.0
0 0.0
0.6151352388260118
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 3
val:	 Loss = 1.2492,	 Acc = 0.5389
3965 0.386
20118 0.553
9324 0.583
956 0.458
29 0.0
0 0.0
0 0.0
0 0.0
0.5588457619877083
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3335,	 Acc1 = 0.2956,	 Acc2 = 0.3002

 ===== Epoch 161	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513   1.616238    1.8166611
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-6.0253930e-03 -6.5372344e-03  1.1128664e-02  1.3828198e-02
 -2.9376602e+00 -2.1727600e+00 -9.0292487e-03 -7.4226381e-03
  9.6202353e-03  1.8220095e-02  2.3359330e-03  9.9131642e-03
 -1.0137177e-02 -1.1003844e-02] 5 5
train:	 Loss = 1.2476,	 Acc = 0.5351
33275 0.286
65372 0.597
33821 0.65
4287 0.62
706 0.534
75 0.4
0 0.0
0 0.0
0.6145730426525738
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2456,	 Acc = 0.5407
3965 0.384
20118 0.553
9324 0.588
956 0.474
29 0.0
0 0.0
0 0.0
0 0.0
0.5610806191869063
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 1
Testing:	 Loss = 1.3232,	 Acc1 = 0.3126,	 Acc2 = 0.3206

 ===== Epoch 162	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513   1.924405    1.926048
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282  -0.04740412 -0.11470311
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 3 3
train:	 Loss = 1.2488,	 Acc = 0.5351
33273 0.288
65372 0.596
33824 0.648
4286 0.629
706 0.508
75 0.453
0 0.0
0 0.0
0.6139186480342979
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 3
val:	 Loss = 1.2577,	 Acc = 0.5474
3965 0.391
20118 0.574
9324 0.56
956 0.519
29 0.0
0 0.0
0 0.0
0 0.0
0.5677523252374536
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3335,	 Acc1 = 0.3041,	 Acc2 = 0.3104

 ===== Epoch 163	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 5 1
train:	 Loss = 1.2514,	 Acc = 0.5334
33272 0.284
65372 0.596
33825 0.646
4286 0.625
706 0.524
75 0.4
0 0.0
0 0.0
0.612857745722397
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2726,	 Acc = 0.5447
3965 0.389
20118 0.574
9324 0.557
956 0.474
29 0.0
0 0.0
0 0.0
0 0.0
0.5649587537384559
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3473,	 Acc1 = 0.3249,	 Acc2 = 0.3355

 ===== Epoch 164	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513   4.0615926   2.4149404
 -0.36121437 -0.36837378  4.2655478   1.7301111  -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.3240354   0.2212608
 -0.00902925 -0.00742264  0.00737721  0.37943736  0.00233593  0.00991316
 -0.01013718 -0.01100384] 2 3
train:	 Loss = 1.2475,	 Acc = 0.5359
33271 0.29
65374 0.597
33823 0.649
4287 0.624
706 0.521
75 0.387
0 0.0
0 0.0
0.6142329640819066
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2469,	 Acc = 0.5590
3965 0.394
20118 0.581
9324 0.589
956 0.507
29 0.0
0 0.0
0 0.0
0 0.0
0.5804712919446544
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3228,	 Acc1 = 0.3119,	 Acc2 = 0.3198

 ===== Epoch 165	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   1.3389546   1.9878523
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 6 6
train:	 Loss = 1.2493,	 Acc = 0.5345
33275 0.286
65370 0.596
33823 0.648
4287 0.621
706 0.527
75 0.387
0 0.0
0 0.0
0.6136426851842971
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 3
val:	 Loss = 1.2784,	 Acc = 0.5259
3965 0.389
20118 0.544
9324 0.548
956 0.505
29 0.0
0 0.0
0 0.0
0 0.0
0.5436947447990271
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3539,	 Acc1 = 0.3148,	 Acc2 = 0.3233

 ===== Epoch 166	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305  0.673265    2.8960125
  3.397847    2.5101953 ] [-6.0253930e-03 -6.5372344e-03  1.1128664e-02  1.3828198e-02
  2.5970940e-03  9.9307357e-04 -9.0292487e-03 -7.4226381e-03
  9.6202353e-03  1.8220095e-02 -1.5251526e+00 -4.1536794e+00
 -4.9377337e+00 -3.5421212e+00] 4 4
train:	 Loss = 1.2481,	 Acc = 0.5342
33272 0.288
65372 0.596
33825 0.646
4286 0.621
706 0.52
75 0.413
0 0.0
0 0.0
0.6126563339215837
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2629,	 Acc = 0.5608
3965 0.386
20118 0.596
9324 0.578
956 0.395
29 0.0
0 0.0
0 0.0
0 0.0
0.5834949222729813
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3338,	 Acc1 = 0.3270,	 Acc2 = 0.3380

 ===== Epoch 167	 =====
[-0.36717504 -0.38357562  1.7964869   3.0217197  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.01164701  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 0 0
train:	 Loss = 1.2481,	 Acc = 0.5347
33272 0.285
65374 0.597
33824 0.648
4285 0.631
706 0.527
75 0.493
0 0.0
0 0.0
0.6142868104043582
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2729,	 Acc = 0.5324
3965 0.376
20118 0.552
9324 0.562
956 0.493
29 0.0
0 0.0
0 0.0
0 0.0
0.5527656357840076
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3579,	 Acc1 = 0.2639,	 Acc2 = 0.2619

 ===== Epoch 168	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513   2.9157648   2.9797337
 -0.36121437 -0.36837378  1.935225    2.2853467  -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 0 0
train:	 Loss = 1.2469,	 Acc = 0.5362
33271 0.286
65371 0.599
33826 0.649
4287 0.627
706 0.535
75 0.48
0 0.0
0 0.0
0.6158730158730159
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2872,	 Acc = 0.5197
3965 0.379
20118 0.535
9324 0.557
956 0.435
29 0.0
0 0.0
0 0.0
0 0.0
0.538107601801032
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3758,	 Acc1 = 0.2694,	 Acc2 = 0.2686

 ===== Epoch 169	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378  2.6201742   3.3094485   3.652166    1.1465042
  4.1769066   2.9885738 ] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.05599772  0.02120536 -0.07631713  0.12939817
  0.66398484  0.03684327] 2 0
train:	 Loss = 1.2487,	 Acc = 0.5351
33272 0.288
65369 0.596
33827 0.648
4287 0.636
706 0.51
75 0.467
0 0.0
0 0.0
0.6139223509552674
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2352,	 Acc = 0.5542
3965 0.39
20118 0.586
9324 0.564
956 0.481
29 0.0
0 0.0
0 0.0
0 0.0
0.5755743254346468
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3150,	 Acc1 = 0.3171,	 Acc2 = 0.3260

 ===== Epoch 170	 =====
[-0.36717504 -0.38357562  1.5778078   1.7074403  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.16151126  0.1316123   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 6 2
train:	 Loss = 1.2484,	 Acc = 0.5360
33276 0.289
65368 0.597
33824 0.649
4287 0.627
706 0.528
75 0.507
0 0.0
0 0.0
0.6148091310186073
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 3
val:	 Loss = 1.2667,	 Acc = 0.5434
3965 0.386
20118 0.563
9324 0.577
956 0.467
29 0.0
0 0.0
0 0.0
0 0.0
0.5638741906859039
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3431,	 Acc1 = 0.3058,	 Acc2 = 0.3124

 ===== Epoch 171	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305  1.6014299   2.6296768
 -0.3911008  -0.38360253] [-6.0253930e-03 -6.5372344e-03  1.1128664e-02  1.3828198e-02
  2.5970940e-03  9.9307357e-04  2.9015014e+00  2.5074315e+00
  9.6202353e-03  1.8220095e-02 -2.8119166e+00 -3.8197343e+00
 -1.0137177e-02 -1.1003844e-02] 6 6
train:	 Loss = 1.2470,	 Acc = 0.5355
33275 0.291
65375 0.597
33822 0.647
4283 0.621
706 0.533
75 0.413
0 0.0
0 0.0
0.6135467720432376
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2893,	 Acc = 0.5463
3965 0.389
20118 0.567
9324 0.574
956 0.5
29 0.034
0 0.0
0 0.0
0 0.0
0.5667992243730897
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3605,	 Acc1 = 0.3198,	 Acc2 = 0.3293

 ===== Epoch 172	 =====
[ 2.0731916   2.5487313  -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.03826926 -0.33399448  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 3 5
train:	 Loss = 1.2476,	 Acc = 0.5352
33276 0.288
65372 0.596
33821 0.65
4286 0.629
706 0.508
75 0.44
0 0.0
0 0.0
0.6140514099366967
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2945,	 Acc = 0.5213
3965 0.385
20118 0.535
9324 0.555
956 0.482
29 0.0
0 0.0
0 0.0
0 0.0
0.5389949715713018
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3630,	 Acc1 = 0.3297,	 Acc2 = 0.3412

 ===== Epoch 173	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-6.0253930e-03 -6.5372344e-03  1.1128664e-02  1.3828198e-02
  2.5970940e-03  9.9307357e-04 -9.0292487e-03 -7.4226381e-03
  3.5359001e+00  3.7886119e+00  5.7671232e+00  1.7868695e+00
  5.1684670e+00  3.7051222e+00] 6 2
train:	 Loss = 1.2495,	 Acc = 0.5350
33276 0.286
65371 0.597
33823 0.647
4285 0.628
706 0.537
75 0.453
0 0.0
0 0.0
0.6143679263380012
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2454,	 Acc = 0.5447
3965 0.385
20118 0.571
9324 0.569
956 0.434
29 0.0
0 0.0
0 0.0
0 0.0
0.5654188713971144
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 1
Testing:	 Loss = 1.3264,	 Acc1 = 0.2928,	 Acc2 = 0.2967

 ===== Epoch 174	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 6 1
train:	 Loss = 1.2471,	 Acc = 0.5350
33270 0.288
65374 0.597
33824 0.646
4287 0.633
706 0.508
75 0.44
0 0.0
0 0.0
0.6137475303550535
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 3
val:	 Loss = 1.2525,	 Acc = 0.5511
3965 0.377
20118 0.572
9324 0.59
956 0.447
29 0.552
0 0.0
0 0.0
0 0.0
0.5737338548000132
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 1
Testing:	 Loss = 1.3269,	 Acc1 = 0.3192,	 Acc2 = 0.3285

 ===== Epoch 175	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305  1.5261629   1.102522
 -0.3911008  -0.38360253] [ 2.5230951e+00  2.4593153e+00  1.1128664e-02  1.3828198e-02
  2.5970940e-03  9.9307357e-04 -9.0292487e-03 -7.4226381e-03
  9.6202353e-03  1.8220095e-02 -2.7075698e+00 -1.9049107e+00
  3.1123693e+00  3.3223453e+00] 5 5
train:	 Loss = 1.2482,	 Acc = 0.5345
33275 0.288
65369 0.595
33825 0.65
4286 0.625
706 0.517
75 0.413
0 0.0
0 0.0
0.6132590326200593
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 3
val:	 Loss = 1.2270,	 Acc = 0.5524
3965 0.381
20118 0.57
9324 0.596
956 0.479
29 0.0
0 0.0
0 0.0
0 0.0
0.5747198212114241
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3046,	 Acc1 = 0.3134,	 Acc2 = 0.3216

 ===== Epoch 176	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 5 1
train:	 Loss = 1.2495,	 Acc = 0.5343
33271 0.288
65371 0.596
33826 0.647
4287 0.63
706 0.52
75 0.413
0 0.0
0 0.0
0.6130149139212583
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2773,	 Acc = 0.5423
3965 0.386
20118 0.573
9324 0.554
956 0.445
29 0.0
0 0.0
0 0.0
0 0.0
0.5625924343510698
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3748,	 Acc1 = 0.2785,	 Acc2 = 0.2796

 ===== Epoch 177	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  4.0720267   1.4631598  -0.40665367 -0.41433305  3.8450608   2.0505798
 -0.3911008  -0.38360253] [-6.0253930e-03 -6.5372344e-03  1.1128664e-02  1.3828198e-02
  2.5970940e-03  9.9307357e-04 -5.7220922e+00 -2.2096193e+00
  2.8200369e+00  3.6184516e+00  3.6466396e-01 -1.0164324e+00
  4.7830915e+00  4.0591912e+00] 5 5
train:	 Loss = 1.2480,	 Acc = 0.5353
33271 0.289
65373 0.597
33824 0.646
4287 0.627
706 0.534
75 0.453
0 0.0
0 0.0
0.6139452356975016
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2492,	 Acc = 0.5428
3965 0.398
20118 0.556
9324 0.583
956 0.479
29 0.0
0 0.0
0 0.0
0 0.0
0.561737930127847
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3327,	 Acc1 = 0.3070,	 Acc2 = 0.3139

 ===== Epoch 178	 =====
[-0.36717504 -0.38357562  2.6204226   0.7858556  -0.4399085  -0.3643795
  1.4647756   2.5456865  -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [ 3.8103693e+00  2.7520421e+00 -4.1255250e+00 -1.0658593e+00
  2.5970940e-03  9.9307357e-04 -1.1248263e-02 -1.0473489e+00
  9.6202353e-03  1.8220095e-02  2.3359330e-03  9.9131642e-03
 -1.0137177e-02 -1.1003844e-02] 5 5
train:	 Loss = 1.2479,	 Acc = 0.5350
33276 0.287
65370 0.597
33822 0.647
4287 0.635
706 0.52
75 0.467
0 0.0
0 0.0
0.6140801841549971
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2232,	 Acc = 0.5607
3965 0.387
20118 0.582
9324 0.598
956 0.482
29 0.0
0 0.0
0 0.0
0 0.0
0.583330594537746
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 1
Testing:	 Loss = 1.3114,	 Acc1 = 0.2998,	 Acc2 = 0.3052

 ===== Epoch 179	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513   1.9393907   1.2942828
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282  -0.05619599  0.73966867
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
train:	 Loss = 1.2495,	 Acc = 0.5345
33274 0.287
65376 0.596
33820 0.648
4286 0.626
705 0.536
75 0.413
0 0.0
0 0.0
0.613358654159713
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2286,	 Acc = 0.5534
3965 0.392
20118 0.572
9324 0.592
956 0.462
29 0.0
0 0.0
0 0.0
0 0.0
0.5743911657409537
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3185,	 Acc1 = 0.2985,	 Acc2 = 0.3037

 ===== Epoch 180	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
  1.2184994   1.404435  ] [-6.0253930e-03 -6.5372344e-03  1.1128664e-02  1.3828198e-02
  2.5970940e-03  9.9307357e-04 -9.0292487e-03 -7.4226381e-03
  9.6202353e-03  1.8220095e-02  2.3359330e-03  9.9131642e-03
  1.5199279e+00  1.7848581e+00] 2 2
train:	 Loss = 1.2463,	 Acc = 0.5357
33272 0.29
65372 0.596
33824 0.65
4287 0.625
706 0.514
75 0.467
0 0.0
0 0.0
0.6141717179467505
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 3
val:	 Loss = 1.2782,	 Acc = 0.5288
3965 0.357
20118 0.541
9324 0.585
956 0.458
29 0.0
0 0.0
0 0.0
0 0.0
0.5511552239787031
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3626,	 Acc1 = 0.3000,	 Acc2 = 0.3054

 ===== Epoch 181	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 6 1
train:	 Loss = 1.2463,	 Acc = 0.5362
33272 0.289
65374 0.598
33823 0.65
4287 0.628
705 0.511
75 0.453
0 0.0
0 0.0
0.6150732755313435
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2600,	 Acc = 0.5358
3965 0.387
20118 0.547
9324 0.579
956 0.504
29 0.0
0 0.0
0 0.0
0 0.0
0.5552305518125349
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3392,	 Acc1 = 0.2781,	 Acc2 = 0.2791

 ===== Epoch 182	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513   2.0769513   1.6001197
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.2960139   0.1211391
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 2 2
train:	 Loss = 1.2491,	 Acc = 0.5355
33273 0.286
65370 0.598
33825 0.648
4287 0.635
706 0.521
75 0.453
0 0.0
0 0.0
0.6149736723478127
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2631,	 Acc = 0.5457
3965 0.394
20118 0.583
9324 0.538
956 0.48
29 0.0
0 0.0
0 0.0
0 0.0
0.5653531403030203
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 1
Testing:	 Loss = 1.3436,	 Acc1 = 0.3188,	 Acc2 = 0.3280

 ===== Epoch 183	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 3 1
train:	 Loss = 1.2500,	 Acc = 0.5347
33275 0.286
65371 0.597
33823 0.647
4286 0.629
706 0.52
75 0.52
0 0.0
0 0.0
0.6139400159215814
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 3
val:	 Loss = 1.3378,	 Acc = 0.5180
3965 0.386
20118 0.556
9324 0.502
956 0.436
29 0.0
0 0.0
0 0.0
0 0.0
0.5351825681138462
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.4010,	 Acc1 = 0.3229,	 Acc2 = 0.3330

 ===== Epoch 184	 =====
[ 2.7467687   3.2932425  -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
  2.5950453   3.519234  ] [ 0.00539953 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.260003   -0.01100384] 0 0
train:	 Loss = 1.2479,	 Acc = 0.5353
33274 0.29
65371 0.597
33823 0.647
4287 0.625
706 0.504
75 0.453
0 0.0
0 0.0
0.6134737488250753
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2351,	 Acc = 0.5355
3965 0.383
20118 0.535
9324 0.61
956 0.462
29 0.0
0 0.0
0 0.0
0 0.0
0.5553948795477701
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3207,	 Acc1 = 0.2948,	 Acc2 = 0.2992

 ===== Epoch 185	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.0797305   1.5112092  -0.40665367 -0.41433305  2.3488023   2.8618045
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.01272684  0.01976497  0.00962024  0.01822009  0.01859076 -0.00540543
 -0.01013718 -0.01100384] 2 0
train:	 Loss = 1.2481,	 Acc = 0.5340
33276 0.285
65377 0.596
33816 0.646
4286 0.629
706 0.514
75 0.4
0 0.0
0 0.0
0.6133320544791866
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2698,	 Acc = 0.5285
3965 0.375
20118 0.554
9324 0.552
956 0.417
29 0.0
0 0.0
0 0.0
0 0.0
0.5485588457619877
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3503,	 Acc1 = 0.2874,	 Acc2 = 0.2903

 ===== Epoch 186	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378  1.2479753   2.855389   -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.08442174  0.23912981  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
train:	 Loss = 1.2501,	 Acc = 0.5338
33272 0.285
65375 0.597
33824 0.646
4285 0.624
705 0.528
75 0.493
0 0.0
0 0.0
0.6133468886672293
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2241,	 Acc = 0.5523
3965 0.397
20118 0.564
9324 0.6
956 0.497
29 0.0
0 0.0
0 0.0
0 0.0
0.572517829559273
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3074,	 Acc1 = 0.3018,	 Acc2 = 0.3077

 ===== Epoch 187	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305  1.4085351   1.5838813
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.14443438  0.02216804
 -0.01013718 -0.01100384] 3 0
train:	 Loss = 1.2492,	 Acc = 0.5346
33275 0.287
65370 0.597
33823 0.646
4287 0.627
706 0.513
75 0.4
0 0.0
0 0.0
0.6135563633573435
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2709,	 Acc = 0.5301
3965 0.384
20118 0.553
9324 0.553
956 0.456
29 0.0
0 0.0
0 0.0
0 0.0
0.5491504256088343
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3494,	 Acc1 = 0.3091,	 Acc2 = 0.3164

 ===== Epoch 188	 =====
[-0.36717504 -0.38357562  3.2490206   2.8038082   2.630653    1.0621148
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-6.0253930e-03 -6.5372344e-03 -4.9804702e+00 -3.0049343e+00
  2.1798772e-01  5.4832500e-01 -9.0292487e-03 -7.4226381e-03
  2.7085791e+00  1.7526599e+00  2.3359330e-03  9.9131642e-03
 -1.0137177e-02 -1.1003844e-02] 6 6
train:	 Loss = 1.2486,	 Acc = 0.5349
33273 0.287
65375 0.596
33821 0.648
4286 0.63
706 0.541
75 0.48
0 0.0
0 0.0
0.6138994657740522
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2305,	 Acc = 0.5624
3965 0.394
20118 0.59
9324 0.589
956 0.436
29 0.0
0 0.0
0 0.0
0 0.0
0.584349426496204
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3167,	 Acc1 = 0.3124,	 Acc2 = 0.3203

 ===== Epoch 189	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  3.6354773   1.9549603  -0.40665367 -0.41433305  3.52773     2.6370072
 -0.3911008  -0.38360253] [-6.0253930e-03 -6.5372344e-03  1.1128664e-02  1.3828198e-02
  2.5970940e-03  9.9307357e-04 -5.1595163e+00 -2.8009501e+00
  9.6202353e-03  1.8220095e-02 -5.4824476e+00 -3.8289256e+00
 -1.0137177e-02 -1.1003844e-02] 2 6
train:	 Loss = 1.2494,	 Acc = 0.5348
33275 0.287
65376 0.597
33820 0.647
4284 0.624
706 0.53
75 0.413
0 0.0
0 0.0
0.6139400159215814
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2486,	 Acc = 0.5501
3965 0.387
20118 0.582
9324 0.565
956 0.428
29 0.0
0 0.0
0 0.0
0 0.0
0.571367535412627
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 1
Testing:	 Loss = 1.3356,	 Acc1 = 0.2930,	 Acc2 = 0.2970

 ===== Epoch 190	 =====
[-0.36717504 -0.38357562  1.0933572   0.8380636  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-6.0253930e-03 -6.5372344e-03 -1.2922806e-01  1.3618016e+00
  2.5970940e-03  9.9307357e-04 -9.0292487e-03 -7.4226381e-03
  9.6202353e-03  1.8220095e-02  2.3359330e-03  9.9131642e-03
 -1.0137177e-02 -1.1003844e-02] 6 6
train:	 Loss = 1.2486,	 Acc = 0.5351
33273 0.286
65369 0.597
33826 0.648
4287 0.628
706 0.531
75 0.52
0 0.0
0 0.0
0.6144845247115468
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2846,	 Acc = 0.5233
3965 0.385
20118 0.547
9324 0.541
956 0.446
29 0.0
0 0.0
0 0.0
0 0.0
0.541427022052782
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3620,	 Acc1 = 0.3103,	 Acc2 = 0.3178

 ===== Epoch 191	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378  1.4837675   2.9960487  -0.42853883 -0.42463273
  2.237609    0.9338989 ] [-6.0253930e-03 -6.5372344e-03  1.1128664e-02  1.3828198e-02
  2.5970940e-03  9.9307357e-04 -9.0292487e-03 -7.4226381e-03
 -2.4492214e+00 -4.1074181e+00  2.3359330e-03  9.9131642e-03
 -7.1227461e-01  1.6124579e-01] 1 1
train:	 Loss = 1.2486,	 Acc = 0.5350
33272 0.289
65376 0.596
33820 0.649
4287 0.628
706 0.521
75 0.427
0 0.0
0 0.0
0.6136346198112483
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.3136,	 Acc = 0.5318
3965 0.392
20118 0.548
9324 0.571
956 0.412
29 0.0
0 0.0
0 0.0
0 0.0
0.5500377953791041
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3857,	 Acc1 = 0.3076,	 Acc2 = 0.3146

 ===== Epoch 192	 =====
[ 2.01934     3.5066857  -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [ 0.16136847 -0.01397945  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 3 0
train:	 Loss = 1.2478,	 Acc = 0.5354
33273 0.287
65374 0.598
33823 0.648
4286 0.629
705 0.512
75 0.453
0 0.0
0 0.0
0.6145996182730211
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 3
val:	 Loss = 1.2236,	 Acc = 0.5477
3965 0.394
20118 0.556
9324 0.603
956 0.484
29 0.0
0 0.0
0 0.0
0 0.0
0.5677194596904065
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 1
Testing:	 Loss = 1.3146,	 Acc1 = 0.2668,	 Acc2 = 0.2654

 ===== Epoch 193	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 6 1
train:	 Loss = 1.2478,	 Acc = 0.5357
33277 0.291
65370 0.597
33823 0.647
4285 0.633
706 0.528
75 0.44
0 0.0
0 0.0
0.6139230186362808
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2553,	 Acc = 0.5495
3965 0.379
20118 0.57
9324 0.585
956 0.491
29 0.0
0 0.0
0 0.0
0 0.0
0.5716633253360502
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3385,	 Acc1 = 0.3037,	 Acc2 = 0.3099

 ===== Epoch 194	 =====
[-0.36717504 -0.38357562  2.966049    2.2590292  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.09857205  0.14251824  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 3
train:	 Loss = 1.2476,	 Acc = 0.5360
33275 0.291
65369 0.597
33825 0.647
4286 0.629
706 0.52
75 0.467
0 0.0
0 0.0
0.6141222508895944
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2607,	 Acc = 0.5231
3965 0.363
20118 0.528
9324 0.583
956 0.507
29 0.0
0 0.0
0 0.0
0 0.0
0.5439248036283564
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 1
Testing:	 Loss = 1.3376,	 Acc1 = 0.3018,	 Acc2 = 0.3077

 ===== Epoch 195	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513   1.3587917   1.5978873
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-6.0253930e-03 -6.5372344e-03  1.1128664e-02  1.3828198e-02
 -2.5695159e+00 -1.9547174e+00 -9.0292487e-03 -7.4226381e-03
  2.8634219e+00  2.2332878e+00  2.3359330e-03  9.9131642e-03
 -1.0137177e-02 -1.1003844e-02] 6 5
train:	 Loss = 1.2482,	 Acc = 0.5346
33277 0.287
65374 0.595
33818 0.648
4286 0.636
706 0.527
75 0.493
0 0.0
0 0.0
0.6135201757162452
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2383,	 Acc = 0.5518
3965 0.387
20118 0.577
9324 0.576
956 0.475
29 0.0
0 0.0
0 0.0
0 0.0
0.5732080060472606
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3168,	 Acc1 = 0.3113,	 Acc2 = 0.3191

 ===== Epoch 196	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513   2.2517838   2.3903842
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-6.0253930e-03 -6.5372344e-03  1.1128664e-02  1.3828198e-02
  8.7215453e-02 -6.9941759e+00 -9.0292487e-03 -7.4226381e-03
  9.6202353e-03  1.8220095e-02  2.3359330e-03  9.9131642e-03
 -1.0137177e-02 -1.1003844e-02] 1 4
train:	 Loss = 1.2501,	 Acc = 0.5336
33271 0.285
65377 0.596
33822 0.647
4285 0.627
706 0.514
75 0.4
0 0.0
0 0.0
0.6131204143288735
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 3
val:	 Loss = 1.2850,	 Acc = 0.5458
3965 0.392
20118 0.581
9324 0.545
956 0.459
29 0.0
0 0.0
0 0.0
0 0.0
0.5657803924146317
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3627,	 Acc1 = 0.3216,	 Acc2 = 0.3315

 ===== Epoch 197	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
  1.5592285   0.9338989 ] [-6.0253930e-03 -6.5372344e-03  1.1128664e-02  1.3828198e-02
  2.5970940e-03  9.9307357e-04 -9.0292487e-03 -7.4226381e-03
  9.6202353e-03  1.8220095e-02  2.3359330e-03  9.9131642e-03
 -2.5465767e+00 -1.6186670e+00] 1 5
train:	 Loss = 1.2483,	 Acc = 0.5342
33277 0.285
65371 0.596
33821 0.648
4286 0.632
706 0.531
75 0.453
0 0.0
0 0.0
0.6138942441419926
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2586,	 Acc = 0.5333
3965 0.389
20118 0.535
9324 0.595
956 0.503
29 0.0
0 0.0
0 0.0
0 0.0
0.5520425937489729
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3396,	 Acc1 = 0.2705,	 Acc2 = 0.2699

 ===== Epoch 198	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378  1.638471    3.2823036  -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [ 8.8920552e-01  1.7498245e+00  1.1128664e-02  1.3828198e-02
  2.5970940e-03  9.9307357e-04 -9.0292487e-03 -7.4226381e-03
 -2.6504419e+00 -4.4537091e+00  2.3359330e-03  9.9131642e-03
  1.1900669e+00  2.6301572e+00] 6 6
train:	 Loss = 1.2485,	 Acc = 0.5342
33274 0.286
65374 0.596
33823 0.647
4285 0.626
705 0.526
75 0.44
0 0.0
0 0.0
0.6134066102702806
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 3
val:	 Loss = 1.2595,	 Acc = 0.5425
3965 0.395
20118 0.566
9324 0.563
956 0.485
29 0.0
0 0.0
0 0.0
0 0.0
0.561737930127847
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3347,	 Acc1 = 0.3202,	 Acc2 = 0.3298

 ===== Epoch 199	 =====
[ 2.974682    3.2779963  -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
  2.3840125   3.514006  ] [-0.12583724  0.00834719  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
  0.2643151  -0.00462423] 0 0
train:	 Loss = 1.2499,	 Acc = 0.5335
33276 0.287
65372 0.595
33820 0.646
4287 0.625
706 0.535
75 0.453
0 0.0
0 0.0
0.6122194513715711
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 3
val:	 Loss = 1.2167,	 Acc = 0.5668
3965 0.382
20118 0.589
9324 0.608
956 0.478
29 0.0
0 0.0
0 0.0
0 0.0
0.5908239392644691
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3027,	 Acc1 = 0.2895,	 Acc2 = 0.2927

 ===== Epoch 200	 =====
[ 1.6150296   1.1562666  -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
  1.2985165   1.4227339 ] [-1.8675309e+00 -1.5098637e+00  1.1128664e-02  1.3828198e-02
  2.5970940e-03  9.9307357e-04 -9.0292487e-03 -7.4226381e-03
  9.6202353e-03  1.8220095e-02  2.3359330e-03  9.9131642e-03
 -2.2075157e+00 -2.2151611e+00] 5 5
train:	 Loss = 1.2495,	 Acc = 0.5338
33277 0.286
65372 0.597
33820 0.645
4286 0.624
706 0.517
75 0.44
0 0.0
0 0.0
0.6127912218609425
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 3
val:	 Loss = 1.2182,	 Acc = 0.5547
3965 0.395
20118 0.57
9324 0.597
956 0.488
29 0.0
0 0.0
0 0.0
0 0.0
0.5754428632464588
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3008,	 Acc1 = 0.3002,	 Acc2 = 0.3057

 ===== Epoch 201	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 2 1
train:	 Loss = 1.2477,	 Acc = 0.5348
33270 0.286
65371 0.596
33827 0.649
4287 0.632
706 0.541
75 0.44
0 0.0
0 0.0
0.6142750273339344
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 3
val:	 Loss = 1.2370,	 Acc = 0.5624
3965 0.383
20118 0.587
9324 0.593
956 0.501
29 0.0
0 0.0
0 0.0
0 0.0
0.5858612416603675
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3193,	 Acc1 = 0.3008,	 Acc2 = 0.3064

 ===== Epoch 202	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.0906296   3.614081   -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-6.0253930e-03 -6.5372344e-03  2.0858855e+00  2.6050785e+00
  2.5970940e-03  9.9307357e-04 -3.1686907e+00 -4.7958412e+00
  9.6202353e-03  1.8220095e-02  2.3359330e-03  9.9131642e-03
 -1.0137177e-02 -1.1003844e-02] 6 6
train:	 Loss = 1.2483,	 Acc = 0.5349
33275 0.289
65371 0.596
33823 0.648
4287 0.624
705 0.53
75 0.387
0 0.0
0 0.0
0.6135371807291317
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2479,	 Acc = 0.5390
3965 0.39
20118 0.575
9324 0.537
956 0.439
29 0.0
0 0.0
0 0.0
0 0.0
0.5584513754231439
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3327,	 Acc1 = 0.2851,	 Acc2 = 0.2875

 ===== Epoch 203	 =====
[ 1.9508052   2.8892243  -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
  2.0151458   3.1192782 ] [-2.1828609e+00 -3.2017264e+00  1.1128664e-02  1.3828198e-02
  2.5970940e-03  9.9307357e-04 -9.0292487e-03 -7.4226381e-03
  9.6202353e-03  1.8220095e-02  2.3359330e-03  9.9131642e-03
 -3.3661860e-01  3.6858332e-01] 4 1
train:	 Loss = 1.2483,	 Acc = 0.5352
33276 0.289
65366 0.596
33827 0.649
4287 0.625
705 0.522
75 0.44
0 0.0
0 0.0
0.6139171302512948
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2472,	 Acc = 0.5560
3965 0.387
20118 0.581
9324 0.583
956 0.479
29 0.0
0 0.0
0 0.0
0 0.0
0.5780392414631742
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 1
Testing:	 Loss = 1.3390,	 Acc1 = 0.2781,	 Acc2 = 0.2791

 ===== Epoch 204	 =====
[-0.36717504 -0.38357562  2.3407261   1.423701   -0.4399085  -0.3643795
  1.7424228   3.7893205  -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.5302242   0.24721521  0.00259709  0.00099307
  0.42343628  0.05035104  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 2 2
train:	 Loss = 1.2488,	 Acc = 0.5355
33272 0.288
65376 0.597
33822 0.648
4285 0.632
706 0.521
75 0.507
0 0.0
0 0.0
0.6145265863577074
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2400,	 Acc = 0.5441
3965 0.382
20118 0.563
9324 0.583
956 0.453
29 0.0
0 0.0
0 0.0
0 0.0
0.5652874092089263
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 1
Testing:	 Loss = 1.3211,	 Acc1 = 0.3194,	 Acc2 = 0.3288

 ===== Epoch 205	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 6 1
train:	 Loss = 1.2493,	 Acc = 0.5335
33276 0.286
65373 0.595
33820 0.647
4287 0.629
705 0.523
75 0.453
0 0.0
0 0.0
0.6124784193362747
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 3
val:	 Loss = 1.2639,	 Acc = 0.5381
3965 0.394
20118 0.564
9324 0.55
956 0.487
29 0.0
0 0.0
0 0.0
0 0.0
0.5568738291648865
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3350,	 Acc1 = 0.3245,	 Acc2 = 0.3350

 ===== Epoch 206	 =====
[ 2.375287    2.7037318  -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
  1.8089497   2.938906  ] [-0.12275248 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
  0.16482583 -0.01100384] 0 0
train:	 Loss = 1.2477,	 Acc = 0.5347
33272 0.289
65373 0.596
33824 0.647
4286 0.63
706 0.523
75 0.453
0 0.0
0 0.0
0.6130687485613443
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2320,	 Acc = 0.5497
3965 0.393
20118 0.562
9324 0.596
956 0.494
29 0.0
0 0.0
0 0.0
0 0.0
0.5700200479836987
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3146,	 Acc1 = 0.2870,	 Acc2 = 0.2898

 ===== Epoch 207	 =====
[ 0.6035892   1.2324964  -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [ 0.00081395 -0.01397945  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 0 0
train:	 Loss = 1.2480,	 Acc = 0.5356
33270 0.286
65376 0.599
33823 0.647
4286 0.625
706 0.525
75 0.387
0 0.0
0 0.0
0.615080659083498
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2944,	 Acc = 0.5205
3965 0.39
20118 0.541
9324 0.54
956 0.452
29 0.0
0 0.0
0 0.0
0 0.0
0.5374174253130444
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3884,	 Acc1 = 0.2558,	 Acc2 = 0.2522

 ===== Epoch 208	 =====
[-0.36717504 -0.38357562  3.0450847   1.2557275  -0.4399085  -0.3643795
  2.839247    3.1335862  -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-6.0253930e-03 -6.5372344e-03 -9.0796508e-02  7.9251194e-01
  2.5970940e-03  9.9307357e-04 -4.1334248e+00 -4.2181044e+00
  9.6202353e-03  1.8220095e-02  2.3359330e-03  9.9131642e-03
 -1.0137177e-02 -1.1003844e-02] 6 4
train:	 Loss = 1.2482,	 Acc = 0.5341
33273 0.285
65374 0.596
33823 0.647
4285 0.636
706 0.525
75 0.413
0 0.0
0 0.0
0.6136117318703663
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2639,	 Acc = 0.5262
3965 0.393
20118 0.537
9324 0.566
956 0.483
29 0.0
0 0.0
0 0.0
0 0.0
0.543497551516745
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3477,	 Acc1 = 0.2853,	 Acc2 = 0.2878

 ===== Epoch 209	 =====
[ 0.8476622   3.079799   -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00198877 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 0 0
train:	 Loss = 1.2483,	 Acc = 0.5344
33271 0.288
65375 0.594
33824 0.649
4285 0.63
706 0.53
75 0.453
0 0.0
0 0.0
0.6130436867596989
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2672,	 Acc = 0.5385
3965 0.395
20118 0.555
9324 0.569
956 0.497
29 0.0
0 0.0
0 0.0
0 0.0
0.5571367535412627
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3390,	 Acc1 = 0.3192,	 Acc2 = 0.3285

 ===== Epoch 210	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513   2.2233496   2.07785
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-6.0253930e-03 -6.5372344e-03  1.1128664e-02  1.3828198e-02
  7.6306152e-01 -1.6365148e-01 -9.0292487e-03 -7.4226381e-03
  5.8547931e+00  1.4660745e+00  2.3359330e-03  9.9131642e-03
 -1.0137177e-02 -1.1003844e-02] 2 2
train:	 Loss = 1.2497,	 Acc = 0.5344
33270 0.286
65376 0.595
33822 0.649
4287 0.626
706 0.527
75 0.48
0 0.0
0 0.0
0.6135940766884699
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2364,	 Acc = 0.5599
3965 0.399
20118 0.591
9324 0.575
956 0.445
29 0.0
0 0.0
0 0.0
0 0.0
0.5808656785092188
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3199,	 Acc1 = 0.3103,	 Acc2 = 0.3178

 ===== Epoch 211	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 3 1
train:	 Loss = 1.2493,	 Acc = 0.5341
33274 0.285
65374 0.594
33821 0.652
4286 0.628
706 0.518
75 0.453
0 0.0
0 0.0
0.6136559820452322
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2350,	 Acc = 0.5537
3965 0.398
20118 0.58
9324 0.573
956 0.477
29 0.0
0 0.0
0 0.0
0 0.0
0.5740296447234364
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3146,	 Acc1 = 0.3159,	 Acc2 = 0.3246

 ===== Epoch 212	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 0 1
train:	 Loss = 1.2499,	 Acc = 0.5343
33272 0.286
65372 0.596
33824 0.648
4287 0.631
706 0.513
75 0.427
0 0.0
0 0.0
0.6135962556587125
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 3
val:	 Loss = 1.2480,	 Acc = 0.5498
3965 0.382
20118 0.576
9324 0.575
956 0.456
29 0.0
0 0.0
0 0.0
0 0.0
0.5716961908830972
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3208,	 Acc1 = 0.3330,	 Acc2 = 0.3452

 ===== Epoch 213	 =====
[-0.36717504 -0.38357562  2.1589031   2.887795   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-6.0253930e-03 -6.5372344e-03 -3.4978206e+00 -3.0856385e+00
  2.5970940e-03  9.9307357e-04  2.7898743e+00  4.1250944e+00
  9.6202353e-03  1.8220095e-02  2.3359330e-03  9.9131642e-03
 -1.0137177e-02 -1.1003844e-02] 5 6
train:	 Loss = 1.2499,	 Acc = 0.5344
33274 0.286
65374 0.597
33822 0.646
4285 0.628
706 0.537
75 0.4
0 0.0
0 0.0
0.6135121137135294
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2578,	 Acc = 0.5437
3965 0.388
20118 0.573
9324 0.555
956 0.49
29 0.0
0 0.0
0 0.0
0 0.0
0.563972787327045
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3313,	 Acc1 = 0.3177,	 Acc2 = 0.3268

 ===== Epoch 214	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.6281428   1.0561523  -0.40665367 -0.41433305  2.7107632   1.7891309
 -0.3911008  -0.38360253] [-6.0253930e-03 -6.5372344e-03  1.1128664e-02  1.3828198e-02
  2.5970940e-03  9.9307357e-04  1.1434755e+00  4.0718848e-01
  9.6202353e-03  1.8220095e-02  9.0055293e-01  2.3050086e-01
 -1.0137177e-02 -1.1003844e-02] 2 2
train:	 Loss = 1.2483,	 Acc = 0.5353
33273 0.288
65377 0.596
33819 0.649
4286 0.626
706 0.544
75 0.48
0 0.0
0 0.0
0.6142927021090895
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2307,	 Acc = 0.5585
3965 0.397
20118 0.582
9324 0.586
956 0.493
29 0.0
0 0.0
0 0.0
0 0.0
0.5795181910802906
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3136,	 Acc1 = 0.3039,	 Acc2 = 0.3101

 ===== Epoch 215	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 2 1
train:	 Loss = 1.2473,	 Acc = 0.5348
33275 0.287
65371 0.596
33824 0.648
4285 0.632
706 0.531
75 0.453
0 0.0
0 0.0
0.613834511466416
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2534,	 Acc = 0.5501
3965 0.386
20118 0.573
9324 0.578
956 0.495
29 0.069
0 0.0
0 0.0
0 0.0
0.5715318631478621
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3447,	 Acc1 = 0.2736,	 Acc2 = 0.2736

 ===== Epoch 216	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 0 1
train:	 Loss = 1.2469,	 Acc = 0.5348
33272 0.287
65371 0.596
33825 0.649
4287 0.626
706 0.545
75 0.48
0 0.0
0 0.0
0.613730530192588
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2252,	 Acc = 0.5596
3965 0.387
20118 0.582
9324 0.594
956 0.495
29 0.0
0 0.0
0 0.0
0 0.0
0.5820817037499589
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 1
Testing:	 Loss = 1.3171,	 Acc1 = 0.2713,	 Acc2 = 0.2709

 ===== Epoch 217	 =====
[ 1.7537179   0.8945442  -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378  1.2301472   3.304513   -0.42853883 -0.42463273
  2.088128    1.0855161 ] [-2.4246109e-01  6.0442664e-02  1.1128664e-02  1.3828198e-02
  2.5970940e-03  9.9307357e-04 -9.0292487e-03 -7.4226381e-03
 -2.1193414e+00 -4.4805765e+00  2.3359330e-03  9.9131642e-03
 -3.1089038e-01  7.5120978e-02] 1 1
train:	 Loss = 1.2500,	 Acc = 0.5326
33275 0.287
65372 0.593
33823 0.647
4285 0.626
706 0.533
75 0.467
0 0.0
0 0.0
0.6111297608885393
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2485,	 Acc = 0.5394
3965 0.4
20118 0.543
9324 0.597
956 0.507
29 0.0
0 0.0
0 0.0
0 0.0
0.5575968711999211
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3270,	 Acc1 = 0.3006,	 Acc2 = 0.3062

 ===== Epoch 218	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [ 2.5150006e+00  1.7374208e+00  1.1128664e-02  1.3828198e-02
  2.5970940e-03  9.9307357e-04 -9.0292487e-03 -7.4226381e-03
  9.6202353e-03  1.8220095e-02  2.3359330e-03  9.9131642e-03
  3.4502885e+00  2.4068706e+00] 5 6
train:	 Loss = 1.2477,	 Acc = 0.5352
33275 0.287
65376 0.597
33818 0.649
4286 0.625
706 0.525
75 0.44
0 0.0
0 0.0
0.6143140771717133
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2573,	 Acc = 0.5535
3965 0.395
20118 0.583
9324 0.564
956 0.499
29 0.0
0 0.0
0 0.0
0 0.0
0.5740953758175305
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3397,	 Acc1 = 0.3169,	 Acc2 = 0.3258

 ===== Epoch 219	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [ 5.3857720e-01  1.7671896e+00  1.1128664e-02  1.3828198e-02
  2.5970940e-03  9.9307357e-04 -9.0292487e-03 -7.4226381e-03
  9.6202353e-03  1.8220095e-02  2.3359330e-03  9.9131642e-03
 -1.0137177e-02 -1.1003844e-02] 5 6
train:	 Loss = 1.2495,	 Acc = 0.5336
33270 0.286
65378 0.595
33821 0.648
4286 0.625
706 0.525
75 0.427
0 0.0
0 0.0
0.6125870370015154
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 3
val:	 Loss = 1.2295,	 Acc = 0.5511
3965 0.383
20118 0.564
9324 0.6
956 0.509
29 0.0
0 0.0
0 0.0
0 0.0
0.5729779472179315
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 1
Testing:	 Loss = 1.3155,	 Acc1 = 0.3315,	 Acc2 = 0.3434

 ===== Epoch 220	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378  1.3457419   2.0262368  -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264 -0.05770066 -0.44449624  0.00233593  0.00991316
 -0.01013718 -0.01100384] 5 5
train:	 Loss = 1.2494,	 Acc = 0.5342
33273 0.29
65376 0.594
33821 0.647
4285 0.621
706 0.528
75 0.48
0 0.0
0 0.0
0.6119428752289883
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2314,	 Acc = 0.5472
3965 0.393
20118 0.562
9324 0.591
956 0.471
29 0.0
0 0.0
0 0.0
0 0.0
0.567226476484701
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3171,	 Acc1 = 0.3058,	 Acc2 = 0.3124

 ===== Epoch 221	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305  1.5549079   3.0059679
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.18848029  0.0160406
 -0.01013718 -0.01100384] 4 0
train:	 Loss = 1.2494,	 Acc = 0.5349
33275 0.288
65369 0.597
33824 0.647
4287 0.627
706 0.54
75 0.44
0 0.0
0 0.0
0.6137385983253566
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2617,	 Acc = 0.5462
3965 0.386
20118 0.571
9324 0.569
956 0.485
29 0.0
0 0.0
0 0.0
0 0.0
0.5670621487494659
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3410,	 Acc1 = 0.3175,	 Acc2 = 0.3265

 ===== Epoch 222	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 1
train:	 Loss = 1.2496,	 Acc = 0.5337
33275 0.286
65372 0.595
33824 0.646
4284 0.631
706 0.54
75 0.453
0 0.0
0 0.0
0.6128370147993977
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2923,	 Acc = 0.5371
3965 0.394
20118 0.575
9324 0.53
956 0.414
29 0.0
0 0.0
0 0.0
0 0.0
0.5557892661123345
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3707,	 Acc1 = 0.2973,	 Acc2 = 0.3022

 ===== Epoch 223	 =====
[-0.36717504 -0.38357562  3.0487697   2.1773121  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-6.0253930e-03 -6.5372344e-03  1.7710529e-01 -8.3029115e-01
  2.5970940e-03  9.9307357e-04  3.9608603e+00  4.1930633e+00
  9.6202353e-03  1.8220095e-02  2.3359330e-03  9.9131642e-03
 -1.0137177e-02 -1.1003844e-02] 5 5
train:	 Loss = 1.2482,	 Acc = 0.5348
33272 0.287
65375 0.596
33821 0.65
4287 0.62
706 0.524
75 0.44
0 0.0
0 0.0
0.6139319419934014
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2645,	 Acc = 0.5333
3965 0.387
20118 0.548
9324 0.573
956 0.455
29 0.0
0 0.0
0 0.0
0 0.0
0.5523055181253492
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3428,	 Acc1 = 0.3025,	 Acc2 = 0.3084

 ===== Epoch 224	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305  2.0965257   0.8093083
 -0.3911008  -0.38360253] [-6.0253930e-03 -6.5372344e-03  1.1128664e-02  1.3828198e-02
  2.5970940e-03  9.9307357e-04 -9.0292487e-03 -7.4226381e-03
  9.6202353e-03  1.8220095e-02  3.6938414e-01  3.2547614e-01
  3.8408096e+00  4.7800875e+00] 2 2
train:	 Loss = 1.2502,	 Acc = 0.5340
33276 0.287
65370 0.596
33822 0.646
4287 0.627
706 0.527
75 0.427
0 0.0
0 0.0
0.6127949357375791
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2563,	 Acc = 0.5361
3965 0.387
20118 0.547
9324 0.582
956 0.48
29 0.207
0 0.0
0 0.0
0 0.0
0.5555920728300523
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3337,	 Acc1 = 0.3055,	 Acc2 = 0.3121

 ===== Epoch 225	 =====
[ 3.5977945   3.051848   -0.4210503  -0.3377513  -0.4399085  -0.3643795
  1.1859791   2.7689753  -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-3.7295630e+00 -3.3604937e+00  1.1128664e-02  1.3828198e-02
  2.5970940e-03  9.9307357e-04  1.6913177e-01  5.8730638e-01
  9.6202353e-03  1.8220095e-02  2.3359330e-03  9.9131642e-03
 -1.0137177e-02 -1.1003844e-02] 4 4
train:	 Loss = 1.2486,	 Acc = 0.5346
33275 0.284
65375 0.597
33820 0.649
4286 0.628
705 0.53
75 0.48
0 0.0
0 0.0
0.6145634513384679
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2460,	 Acc = 0.5344
3965 0.387
20118 0.538
9324 0.599
956 0.46
29 0.0
0 0.0
0 0.0
0 0.0
0.5535215433660893
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3292,	 Acc1 = 0.3053,	 Acc2 = 0.3119

 ===== Epoch 226	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305  1.9202735   1.2295812
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009 -0.1025359   0.29177523
 -0.01013718 -0.01100384] 4 4
train:	 Loss = 1.2481,	 Acc = 0.5355
33272 0.289
65375 0.598
33821 0.648
4287 0.627
706 0.513
75 0.453
0 0.0
0 0.0
0.614334765595028
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2248,	 Acc = 0.5576
3965 0.399
20118 0.556
9324 0.639
956 0.464
29 0.0
0 0.0
0 0.0
0 0.0
0.5782035691984093
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3201,	 Acc1 = 0.2614,	 Acc2 = 0.2589

 ===== Epoch 227	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 5 1
train:	 Loss = 1.2479,	 Acc = 0.5353
33276 0.288
65370 0.597
33823 0.649
4286 0.628
706 0.531
75 0.44
0 0.0
0 0.0
0.6143871091502014
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2910,	 Acc = 0.5423
3965 0.39
20118 0.563
9324 0.57
956 0.478
29 0.0
0 0.0
0 0.0
0 0.0
0.5620665855983172
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3663,	 Acc1 = 0.2977,	 Acc2 = 0.3027

 ===== Epoch 228	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 1 1
train:	 Loss = 1.2492,	 Acc = 0.5348
33277 0.287
65372 0.597
33819 0.648
4287 0.621
706 0.52
75 0.453
0 0.0
0 0.0
0.6139997506210495
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2591,	 Acc = 0.5420
3965 0.387
20118 0.566
9324 0.566
956 0.469
29 0.0
0 0.0
0 0.0
0 0.0
0.5621651822394583
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 0
Testing:	 Loss = 1.3438,	 Acc1 = 0.2796,	 Acc2 = 0.2808

 ===== Epoch 229	 =====
[-0.36717504 -0.38357562  3.3829305   2.2181706  -0.4399085  -0.3643795
  3.004459    3.452974   -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-6.0253930e-03 -6.5372344e-03  7.6582050e-01  2.5157759e-01
  6.0792217e+00  1.4249460e+00 -1.4283513e-01  2.0328136e-01
  9.6202353e-03  1.8220095e-02  2.3359330e-03  9.9131642e-03
 -1.0137177e-02 -1.1003844e-02] 3 2
train:	 Loss = 1.2484,	 Acc = 0.5356
33270 0.288
65374 0.597
33825 0.648
4286 0.635
706 0.518
75 0.44
0 0.0
0 0.0
0.61457234381294
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2596,	 Acc = 0.5436
3965 0.392
20118 0.552
9324 0.596
956 0.507
29 0.0
0 0.0
0 0.0
0 0.0
0.5633483419331515
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3344,	 Acc1 = 0.2843,	 Acc2 = 0.2865

 ===== Epoch 230	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-6.0253930e-03 -6.5372344e-03  1.1128664e-02  1.3828198e-02
  2.5970940e-03  9.9307357e-04 -9.0292487e-03 -7.4226381e-03
  3.5359001e+00  3.7886119e+00  5.7671232e+00  1.7868695e+00
  5.1684670e+00  3.7051222e+00] 6 2
train:	 Loss = 1.2485,	 Acc = 0.5342
33274 0.287
65378 0.595
33816 0.651
4287 0.622
706 0.514
75 0.427
0 0.0
0 0.0
0.6131860121616697
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2662,	 Acc = 0.5288
3965 0.398
20118 0.545
9324 0.561
956 0.428
29 0.0
0 0.0
0 0.0
0 0.0
0.5457981398100371
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3425,	 Acc1 = 0.3367,	 Acc2 = 0.3497

 ===== Epoch 231	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  3.8500233   2.017142   -0.40665367 -0.41433305  3.8825047   2.4513052
 -0.3911008  -0.38360253] [-6.0253930e-03 -6.5372344e-03  1.1128664e-02  1.3828198e-02
  2.5970940e-03  9.9307357e-04 -1.5318519e-01 -2.4531426e-01
  9.6202353e-03  1.8220095e-02 -5.6816018e-01 -9.7909212e+00
 -1.0137177e-02 -1.1003844e-02] 3 2
train:	 Loss = 1.2498,	 Acc = 0.5342
33273 0.286
65370 0.598
33827 0.644
4286 0.631
705 0.512
75 0.413
0 0.0
0 0.0
0.6133144068365576
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2536,	 Acc = 0.5448
3965 0.386
20118 0.576
9324 0.552
956 0.487
29 0.0
0 0.0
0 0.0
0 0.0
0.5655174680382555
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3317,	 Acc1 = 0.3150,	 Acc2 = 0.3236

 ===== Epoch 232	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378  2.3619528   1.809078   -0.42853883 -0.42463273
  3.9421341   1.752109  ] [-6.0253930e-03 -6.5372344e-03  1.1128664e-02  1.3828198e-02
  2.5970940e-03  9.9307357e-04  5.3861532e+00  3.1633327e+00
 -3.5914636e+00 -2.6715052e+00  6.1084776e+00  3.8027964e+00
 -5.6455894e+00 -2.6170769e+00] 6 6
train:	 Loss = 1.2488,	 Acc = 0.5338
33275 0.288
65366 0.595
33828 0.647
4286 0.625
706 0.528
75 0.36
0 0.0
0 0.0
0.6120984836132398
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2693,	 Acc = 0.5358
3965 0.331
20118 0.559
9324 0.581
956 0.463
29 0.276
0 0.0
0 0.0
0 0.0
0.5624281066158346
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3486,	 Acc1 = 0.2893,	 Acc2 = 0.2925

 ===== Epoch 233	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 1 1
train:	 Loss = 1.2500,	 Acc = 0.5343
33271 0.286
65377 0.596
33822 0.648
4285 0.634
706 0.5
75 0.32
0 0.0
0 0.0
0.6135999616362154
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2485,	 Acc = 0.5409
3965 0.388
20118 0.561
9324 0.574
956 0.451
29 0.0
0 0.0
0 0.0
0 0.0
0.5608176948105301
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3279,	 Acc1 = 0.3099,	 Acc2 = 0.3173

 ===== Epoch 234	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513   1.8844426   2.0577586
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.55206686  0.10778954
 -0.00902925 -0.00742264  2.03984     1.6660872   0.00233593  0.00991316
 -0.01013718 -0.01100384] 2 2
train:	 Loss = 1.2470,	 Acc = 0.5341
33273 0.287
65375 0.595
33820 0.648
4287 0.624
706 0.523
75 0.413
0 0.0
0 0.0
0.6130362640629945
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 3
val:	 Loss = 1.3355,	 Acc = 0.5121
3965 0.386
20118 0.547
9324 0.502
956 0.423
29 0.0
0 0.0
0 0.0
0 0.0
0.5285437276103461
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.4168,	 Acc1 = 0.3029,	 Acc2 = 0.3089

 ===== Epoch 235	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
  1.4071099   2.3925612 ] [ 1.8848672e+00  2.6354475e+00  1.1128664e-02  1.3828198e-02
  2.5970940e-03  9.9307357e-04 -9.0292487e-03 -7.4226381e-03
  9.6202353e-03  1.8220095e-02  2.3359330e-03  9.9131642e-03
 -1.6427834e-02  1.8038462e-01] 4 6
train:	 Loss = 1.2476,	 Acc = 0.5351
33273 0.288
65377 0.595
33822 0.65
4283 0.636
706 0.521
75 0.347
0 0.0
0 0.0
0.6139474214246665
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2201,	 Acc = 0.5479
3965 0.397
20118 0.551
9324 0.612
956 0.488
29 0.0
0 0.0
0 0.0
0 0.0
0.5675222664081243
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 1
Testing:	 Loss = 1.3024,	 Acc1 = 0.2932,	 Acc2 = 0.2972

 ===== Epoch 236	 =====
[-0.36717504 -0.38357562  2.849338    1.2012497  -0.4399085  -0.3643795
  2.1049707   2.7067935  -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-6.0253930e-03 -6.5372344e-03 -4.4368691e+00 -1.4650166e+00
  2.5970940e-03  9.9307357e-04 -3.1871719e+00 -3.7049384e+00
  9.6202353e-03  1.8220095e-02  1.6273550e+00  4.0601487e+00
 -1.0137177e-02 -1.1003844e-02] 5 5
train:	 Loss = 1.2494,	 Acc = 0.5338
33275 0.287
65370 0.596
33823 0.645
4287 0.628
706 0.52
75 0.333
0 0.0
0 0.0
0.6125013188056896
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2275,	 Acc = 0.5434
3965 0.395
20118 0.545
9324 0.609
956 0.492
29 0.207
0 0.0
0 0.0
0 0.0
0.5627567620863049
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3090,	 Acc1 = 0.3066,	 Acc2 = 0.3134

 ===== Epoch 237	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 5 1
train:	 Loss = 1.2505,	 Acc = 0.5328
33272 0.286
65370 0.595
33828 0.645
4285 0.624
706 0.517
75 0.347
0 0.0
0 0.0
0.6115821376505793
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2581,	 Acc = 0.5458
3965 0.392
20118 0.568
9324 0.571
956 0.492
29 0.0
0 0.0
0 0.0
0 0.0
0.5658461235087258
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3306,	 Acc1 = 0.3045,	 Acc2 = 0.3109

 ===== Epoch 238	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
  2.3378491   2.243558  ] [-6.0253930e-03 -6.5372344e-03  1.1128664e-02  1.3828198e-02
  2.5970940e-03  9.9307357e-04 -9.0292487e-03 -7.4226381e-03
  2.5043664e+00  4.4185033e+00  2.3359330e-03  9.9131642e-03
  6.9142938e-01  1.9314387e-01] 2 2
train:	 Loss = 1.2492,	 Acc = 0.5341
33277 0.285
65369 0.596
33824 0.649
4286 0.635
706 0.521
74 0.324
0 0.0
0 0.0
0.6136736396857825
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 3
val:	 Loss = 1.2171,	 Acc = 0.5564
3965 0.397
20118 0.565
9324 0.611
956 0.527
29 0.0
0 0.0
0 0.0
0 0.0
0.5771847372399513
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3042,	 Acc1 = 0.2837,	 Acc2 = 0.2858

 ===== Epoch 239	 =====
[-0.36717504 -0.38357562  1.7584027   2.3543656  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.3898881   0.23849046  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 1 1
train:	 Loss = 1.2487,	 Acc = 0.5346
33275 0.286
65371 0.596
33823 0.648
4286 0.628
706 0.528
75 0.307
0 0.0
0 0.0
0.613834511466416
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2747,	 Acc = 0.5227
3965 0.39
20118 0.536
9324 0.562
956 0.417
29 0.0
0 0.0
0 0.0
0 0.0
0.5399480724356657
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3593,	 Acc1 = 0.3136,	 Acc2 = 0.3218

 ===== Epoch 240	 =====
[-0.36717504 -0.38357562  1.8816649   0.81990427 -0.4399085  -0.3643795
  1.2657176   3.8938985  -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-6.0253930e-03 -6.5372344e-03 -3.2639450e-01  9.8881876e-01
  2.5970940e-03  9.9307357e-04 -2.1056366e+00 -5.1322880e+00
  9.6202353e-03  1.8220095e-02  2.3359330e-03  9.9131642e-03
 -1.0137177e-02 -1.1003844e-02] 4 4
train:	 Loss = 1.2499,	 Acc = 0.5344
33274 0.286
65369 0.597
33826 0.646
4286 0.625
706 0.508
75 0.307
0 0.0
0 0.0
0.6136272083788916
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 3
val:	 Loss = 1.2423,	 Acc = 0.5417
3965 0.395
20118 0.552
9324 0.587
956 0.504
29 0.0
0 0.0
0 0.0
0 0.0
0.560719098169389
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3146,	 Acc1 = 0.3357,	 Acc2 = 0.3484

 ===== Epoch 241	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 1 1
train:	 Loss = 1.2481,	 Acc = 0.5355
33272 0.287
65371 0.597
33826 0.649
4286 0.624
706 0.517
75 0.36
0 0.0
0 0.0
0.6146896340059848
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2188,	 Acc = 0.5557
3965 0.394
20118 0.57
9324 0.602
956 0.483
29 0.0
0 0.0
0 0.0
0 0.0
0.576790350675387
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3068,	 Acc1 = 0.2785,	 Acc2 = 0.2796

 ===== Epoch 242	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378  1.071417    1.8707708  -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-6.0253930e-03 -6.5372344e-03  1.1128664e-02  1.3828198e-02
  2.5970940e-03  9.9307357e-04 -9.0292487e-03 -7.4226381e-03
  2.0036775e-01  1.2123268e+00  2.3359330e-03  9.9131642e-03
 -1.0137177e-02 -1.1003844e-02] 4 4
train:	 Loss = 1.2479,	 Acc = 0.5354
33276 0.288
65370 0.597
33823 0.649
4286 0.622
706 0.535
75 0.32
0 0.0
0 0.0
0.6144542489929024
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 3
val:	 Loss = 1.2597,	 Acc = 0.5315
3965 0.399
20118 0.534
9324 0.588
956 0.487
29 0.0
0 0.0
0 0.0
0 0.0
0.5486903079501758
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3398,	 Acc1 = 0.2989,	 Acc2 = 0.3042

 ===== Epoch 243	 =====
[-0.36717504 -0.38357562  1.6408725   2.2272503  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 0 0
train:	 Loss = 1.2512,	 Acc = 0.5337
33271 0.286
65374 0.595
33823 0.648
4287 0.625
706 0.52
75 0.32
0 0.0
0 0.0
0.6126600489138253
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 3
val:	 Loss = 1.2655,	 Acc = 0.5452
3965 0.369
20118 0.579
9324 0.556
956 0.473
29 0.0
0 0.0
0 0.0
0 0.0
0.568245308443159
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3537,	 Acc1 = 0.2820,	 Acc2 = 0.2838

 ===== Epoch 244	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 5 1
train:	 Loss = 1.2484,	 Acc = 0.5350
33275 0.288
65374 0.596
33819 0.649
4287 0.623
706 0.504
75 0.4
0 0.0
0 0.0
0.6137577809535685
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2437,	 Acc = 0.5406
3965 0.385
20118 0.551
9324 0.588
956 0.508
29 0.0
0 0.0
0 0.0
0 0.0
0.5608505603575772
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 1
Testing:	 Loss = 1.3191,	 Acc1 = 0.3144,	 Acc2 = 0.3228

 ===== Epoch 245	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513   2.77513     2.0465968
 -0.36121437 -0.36837378  2.0623221   1.2563097  -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282  -0.02103021 -0.3082717
 -0.00902925 -0.00742264 -0.11903937 -0.36986455  0.00233593  0.00991316
 -0.01013718 -0.01100384] 5 5
train:	 Loss = 1.2493,	 Acc = 0.5338
33269 0.287
65374 0.595
33826 0.647
4286 0.628
706 0.516
75 0.36
0 0.0
0 0.0
0.6125140264896851
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2616,	 Acc = 0.5504
3965 0.397
20118 0.58
9324 0.56
956 0.476
29 0.172
0 0.0
0 0.0
0 0.0
0.5703815690012161
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3396,	 Acc1 = 0.3078,	 Acc2 = 0.3149

 ===== Epoch 246	 =====
[-0.36717504 -0.38357562  1.8411235   1.376033   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-6.0253930e-03 -6.5372344e-03  1.1763052e+00  5.9402394e-01
  2.5970940e-03  9.9307357e-04 -9.0292487e-03 -7.4226381e-03
  9.6202353e-03  1.8220095e-02  2.3359330e-03  9.9131642e-03
 -1.0137177e-02 -1.1003844e-02] 2 2
train:	 Loss = 1.2479,	 Acc = 0.5353
33272 0.287
65376 0.596
33821 0.65
4286 0.633
706 0.508
75 0.373
0 0.0
0 0.0
0.6144978132433054
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2555,	 Acc = 0.5441
3965 0.387
20118 0.556
9324 0.589
956 0.519
29 0.0
0 0.0
0 0.0
0 0.0
0.5646300982679857
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3331,	 Acc1 = 0.3111,	 Acc2 = 0.3188

 ===== Epoch 247	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378  1.3365417   2.9639683  -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264 -0.07939558  0.02120536  0.00233593  0.00991316
 -0.01013718 -0.01100384] 0 0
train:	 Loss = 1.2497,	 Acc = 0.5343
33268 0.286
65377 0.597
33823 0.647
4287 0.625
706 0.516
75 0.307
0 0.0
0 0.0
0.6137261671845629
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2852,	 Acc = 0.5201
3965 0.382
20118 0.542
9324 0.539
956 0.468
29 0.0
0 0.0
0 0.0
0 0.0
0.5380090051598909
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3605,	 Acc1 = 0.2866,	 Acc2 = 0.2893

 ===== Epoch 248	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305  1.3975668   2.1361003
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.21836889  0.88920027
 -0.01013718 -0.01100384] 4 4
train:	 Loss = 1.2463,	 Acc = 0.5365
33271 0.289
65376 0.598
33821 0.651
4287 0.627
706 0.499
75 0.347
0 0.0
0 0.0
0.6152879681580588
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 3
val:	 Loss = 1.2792,	 Acc = 0.5220
3965 0.398
20118 0.536
9324 0.55
956 0.474
29 0.0
0 0.0
0 0.0
0 0.0
0.5382061984421731
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3482,	 Acc1 = 0.3171,	 Acc2 = 0.3260

 ===== Epoch 249	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [ 9.6366644e-01  1.2983304e+00  1.1128664e-02  1.3828198e-02
  2.5970940e-03  9.9307357e-04 -9.0292487e-03 -7.4226381e-03
  9.6202353e-03  1.8220095e-02  2.3359330e-03  9.9131642e-03
 -1.0137177e-02 -1.1003844e-02] 2 6
train:	 Loss = 1.2500,	 Acc = 0.5330
33272 0.287
65371 0.594
33826 0.647
4287 0.622
706 0.513
74 0.324
0 0.0
0 0.0
0.6116205018031152
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2445,	 Acc = 0.5499
3965 0.39
20118 0.566
9324 0.593
956 0.461
29 0.0
0 0.0
0 0.0
0 0.0
0.5706773589246393
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3312,	 Acc1 = 0.3093,	 Acc2 = 0.3166

 ===== Epoch 250	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305  1.357852    0.7995345
 -0.3911008  -0.38360253] [-6.0253930e-03 -6.5372344e-03  1.1128664e-02  1.3828198e-02
  2.5970940e-03  9.9307357e-04 -9.0292487e-03 -7.4226381e-03
  9.6202353e-03  1.8220095e-02 -2.4742315e+00 -1.5250095e+00
 -1.0137177e-02 -1.1003844e-02] 2 5
train:	 Loss = 1.2487,	 Acc = 0.5351
33268 0.287
65378 0.597
33823 0.649
4287 0.628
705 0.505
75 0.32
0 0.0
0 0.0
0.6141481566731883
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 3
val:	 Loss = 1.2546,	 Acc = 0.5473
3965 0.383
20118 0.568
9324 0.584
956 0.453
29 0.0
0 0.0
0 0.0
0 0.0
0.5687054261018175
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3305,	 Acc1 = 0.3161,	 Acc2 = 0.3248

 ===== Epoch 251	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 2 1
train:	 Loss = 1.2466,	 Acc = 0.5357
33273 0.29
65376 0.597
33821 0.649
4285 0.625
706 0.51
75 0.32
0 0.0
0 0.0
0.6139378302945436
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2410,	 Acc = 0.5473
3965 0.395
20118 0.572
9324 0.571
956 0.453
29 0.0
0 0.0
0 0.0
0 0.0
0.567160745390607
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3262,	 Acc1 = 0.3192,	 Acc2 = 0.3285

 ===== Epoch 252	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 3 1
train:	 Loss = 1.2469,	 Acc = 0.5350
33272 0.289
65374 0.596
33823 0.648
4286 0.63
706 0.508
75 0.333
0 0.0
0 0.0
0.6135866646205785
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2421,	 Acc = 0.5477
3965 0.383
20118 0.557
9324 0.602
956 0.523
29 0.0
0 0.0
0 0.0
0 0.0
0.5690998126663819
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 1
Testing:	 Loss = 1.3262,	 Acc1 = 0.3016,	 Acc2 = 0.3074

 ===== Epoch 253	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305  0.9996736   1.425057
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.07417293  0.08650611
 -0.01013718 -0.01100384] 1 2
train:	 Loss = 1.2488,	 Acc = 0.5351
33272 0.287
65378 0.595
33819 0.653
4286 0.63
706 0.506
75 0.387
0 0.0
0 0.0
0.6142676283280902
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2560,	 Acc = 0.5542
3965 0.393
20118 0.579
9324 0.575
956 0.506
29 0.0
0 0.0
0 0.0
0 0.0
0.5751470733230355
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3336,	 Acc1 = 0.2880,	 Acc2 = 0.2910

 ===== Epoch 254	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305  1.7625535   1.2735634
 -0.3911008  -0.38360253] [-6.0253930e-03 -6.5372344e-03  1.1128664e-02  1.3828198e-02
  2.5970940e-03  9.9307357e-04 -9.0292487e-03 -7.4226381e-03
  9.6202353e-03  1.8220095e-02  5.7392918e-02  1.1557438e+00
 -1.0137177e-02 -1.1003844e-02] 6 4
train:	 Loss = 1.2463,	 Acc = 0.5359
33271 0.289
65377 0.597
33823 0.649
4284 0.628
706 0.507
75 0.333
0 0.0
0 0.0
0.6144823286817245
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 3
val:	 Loss = 1.2774,	 Acc = 0.5238
3965 0.362
20118 0.536
9324 0.566
956 0.505
29 0.931
0 0.0
0 0.0
0 0.0
0.5448450389456733
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 1
Testing:	 Loss = 1.3566,	 Acc1 = 0.3055,	 Acc2 = 0.3121

 ===== Epoch 255	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513   3.1908865   2.3568985
 -0.36121437 -0.36837378  3.058979    1.505549   -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282  -0.04136056  0.4148294
 -0.00902925 -0.00742264  0.24674524  0.6809493   0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
train:	 Loss = 1.2489,	 Acc = 0.5346
33271 0.288
65376 0.595
33824 0.649
4285 0.633
705 0.499
75 0.347
0 0.0
0 0.0
0.6133410060902508
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 3
val:	 Loss = 1.2372,	 Acc = 0.5624
3965 0.393
20118 0.59
9324 0.588
956 0.455
29 0.0
0 0.0
0 0.0
0 0.0
0.5844808886843922
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 0
Testing:	 Loss = 1.3140,	 Acc1 = 0.3173,	 Acc2 = 0.3263

 ===== Epoch 256	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513   2.9157648   2.9797337
 -0.36121437 -0.36837378  1.935225    2.2853467  -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 0 0
train:	 Loss = 1.2490,	 Acc = 0.5345
33273 0.286
65372 0.596
33824 0.648
4287 0.636
705 0.515
75 0.293
0 0.0
0 0.0
0.6137076431715949
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2391,	 Acc = 0.5493
3965 0.388
20118 0.566
9324 0.589
956 0.501
29 0.0
0 0.0
0 0.0
0 0.0
0.5703815690012161
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3134,	 Acc1 = 0.3101,	 Acc2 = 0.3176

 ===== Epoch 257	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378  2.1089067   2.4852316  -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [ 2.9278247e+00  1.9135531e+00  1.1128664e-02  1.3828198e-02
  2.5970940e-03  9.9307357e-04 -9.0292487e-03 -7.4226381e-03
 -3.2623303e+00 -3.4894679e+00  2.3359330e-03  9.9131642e-03
  3.5549231e+00  2.5823102e+00] 6 6
train:	 Loss = 1.2513,	 Acc = 0.5334
33270 0.285
65374 0.595
33824 0.648
4287 0.627
706 0.53
75 0.333
0 0.0
0 0.0
0.6124815376057392
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2379,	 Acc = 0.5518
3965 0.385
20118 0.573
9324 0.59
956 0.454
29 0.0
0 0.0
0 0.0
0 0.0
0.5735037959706839
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3262,	 Acc1 = 0.3049,	 Acc2 = 0.3114

 ===== Epoch 258	 =====
[ 4.3745174   2.7088137  -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305  2.5541785   0.7873173
  3.285736    2.912765  ] [-4.4589901e+00 -3.0255942e+00  1.1128664e-02  1.3828198e-02
  2.5970940e-03  9.9307357e-04 -9.0292487e-03 -7.4226381e-03
  3.4311762e+00  3.2602196e+00 -4.1327620e+00 -1.5096910e+00
 -2.9773989e-01 -1.9567868e+00] 5 5
train:	 Loss = 1.2488,	 Acc = 0.5341
33273 0.287
65373 0.595
33823 0.648
4286 0.624
706 0.537
75 0.4
0 0.0
0 0.0
0.6130842197136088
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 3
val:	 Loss = 1.2747,	 Acc = 0.5326
3965 0.393
20118 0.547
9324 0.574
956 0.423
29 0.0
0 0.0
0 0.0
0 0.0
0.5507279718670918
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3592,	 Acc1 = 0.2855,	 Acc2 = 0.2880

 ===== Epoch 259	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-6.0253930e-03 -6.5372344e-03  1.1128664e-02  1.3828198e-02
  2.5970940e-03  9.9307357e-04 -9.0292487e-03 -7.4226381e-03
  9.6202353e-03  1.8220095e-02  2.9073012e+00  2.7488770e+00
 -1.0137177e-02 -1.1003844e-02] 6 6
train:	 Loss = 1.2496,	 Acc = 0.5343
33273 0.285
65374 0.595
33822 0.649
4286 0.636
706 0.544
75 0.373
0 0.0
0 0.0
0.6137555988222092
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 3
val:	 Loss = 1.2359,	 Acc = 0.5411
3965 0.382
20118 0.547
9324 0.603
956 0.5
29 0.0
0 0.0
0 0.0
0 0.0
0.561869392316035
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3231,	 Acc1 = 0.2765,	 Acc2 = 0.2771

 ===== Epoch 260	 =====
[ 0.83057195  1.0342989  -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
  1.3275332   1.3155562 ] [-1.1308399e+00 -1.3907884e+00  1.1128664e-02  1.3828198e-02
  2.5970940e-03  9.9307357e-04 -9.0292487e-03 -7.4226381e-03
  9.6202353e-03  1.8220095e-02  2.3359330e-03  9.9131642e-03
 -3.3547676e-01 -3.6507252e-01] 5 1
train:	 Loss = 1.2499,	 Acc = 0.5347
33272 0.288
65372 0.596
33824 0.647
4287 0.633
706 0.504
75 0.32
0 0.0
0 0.0
0.6134907542392388
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2347,	 Acc = 0.5586
3965 0.398
20118 0.585
9324 0.579
956 0.492
29 0.0
0 0.0
0 0.0
0 0.0
0.5795181910802906
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3153,	 Acc1 = 0.3175,	 Acc2 = 0.3265

 ===== Epoch 261	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.9017751   1.4377217  -0.40665367 -0.41433305  2.4887457   2.6761024
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.15244703  0.6416816   0.00962024  0.01822009  0.01963951  0.6042745
 -0.01013718 -0.01100384] 6 4
train:	 Loss = 1.2491,	 Acc = 0.5339
33273 0.288
65373 0.595
33823 0.647
4287 0.629
705 0.511
75 0.347
0 0.0
0 0.0
0.6124799785158686
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2257,	 Acc = 0.5615
3965 0.391
20118 0.587
9324 0.588
956 0.496
29 0.0
0 0.0
0 0.0
0 0.0
0.5837249811023104
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3080,	 Acc1 = 0.3029,	 Acc2 = 0.3089

 ===== Epoch 262	 =====
[-0.36717504 -0.38357562  2.6511352   1.5735154  -0.4399085  -0.3643795
  1.325951    3.2183793  -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.092446    0.1316123   0.00259709  0.00099307
  0.62895226 -0.10597773  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 2 2
train:	 Loss = 1.2477,	 Acc = 0.5365
33273 0.29
65376 0.598
33820 0.649
4286 0.635
706 0.52
75 0.373
0 0.0
0 0.0
0.6152038594707614
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2281,	 Acc = 0.5462
3965 0.395
20118 0.558
9324 0.593
956 0.483
29 0.0
0 0.0
0 0.0
0 0.0
0.5659118546028199
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3161,	 Acc1 = 0.2567,	 Acc2 = 0.2532

 ===== Epoch 263	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513   0.7140235   2.4752147
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-6.0253930e-03 -6.5372344e-03  1.1128664e-02  1.3828198e-02
 -1.6475074e+00 -2.8291132e+00 -9.0292487e-03 -7.4226381e-03
  9.6202353e-03  1.8220095e-02  2.3359330e-03  9.9131642e-03
 -1.0137177e-02 -1.1003844e-02] 4 6
train:	 Loss = 1.2488,	 Acc = 0.5344
33272 0.289
65371 0.595
33827 0.648
4285 0.621
706 0.518
75 0.387
0 0.0
0 0.0
0.6126755159978516
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 3
val:	 Loss = 1.2587,	 Acc = 0.5591
3965 0.396
20118 0.579
9324 0.592
956 0.52
29 0.0
0 0.0
0 0.0
0 0.0
0.5803069642094193
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3372,	 Acc1 = 0.3008,	 Acc2 = 0.3064

 ===== Epoch 264	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305  2.786408    1.3297627
  3.036893    3.4904792 ] [-6.0253930e-03 -6.5372344e-03  1.1128664e-02  1.3828198e-02
  2.5970940e-03  9.9307357e-04  4.1198020e+00  4.1284928e+00
  9.6202353e-03  1.8220095e-02 -4.4547138e+00 -2.1898365e+00
 -4.4683061e+00 -4.7382994e+00] 6 6
train:	 Loss = 1.2484,	 Acc = 0.5354
33273 0.287
65375 0.597
33823 0.649
4284 0.632
706 0.518
75 0.36
0 0.0
0 0.0
0.614743485224864
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2896,	 Acc = 0.5254
3965 0.401
20118 0.544
9324 0.551
956 0.426
29 0.0
0 0.0
0 0.0
0 0.0
0.5415256186939231
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3641,	 Acc1 = 0.3045,	 Acc2 = 0.3109

 ===== Epoch 265	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 1 1
train:	 Loss = 1.2493,	 Acc = 0.5350
33274 0.288
65374 0.596
33822 0.649
4285 0.63
706 0.534
75 0.333
0 0.0
0 0.0
0.6139341274865243
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2270,	 Acc = 0.5604
3965 0.382
20118 0.586
9324 0.592
956 0.47
29 0.0
0 0.0
0 0.0
0 0.0
0.5835935189141224
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3131,	 Acc1 = 0.2771,	 Acc2 = 0.2778

 ===== Epoch 266	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [ 2.7603037e+00  2.5560639e+00  1.1128664e-02  1.3828198e-02
  2.5970940e-03  9.9307357e-04  2.0764875e+00  3.2245047e+00
  9.6202353e-03  1.8220095e-02  2.3359330e-03  9.9131642e-03
 -1.0137177e-02 -1.1003844e-02] 1 5
train:	 Loss = 1.2472,	 Acc = 0.5356
33276 0.288
65369 0.597
33825 0.649
4285 0.634
706 0.508
75 0.333
0 0.0
0 0.0
0.6146077114905045
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 3
val:	 Loss = 1.2716,	 Acc = 0.5238
3965 0.379
20118 0.528
9324 0.584
956 0.471
29 0.0
0 0.0
0 0.0
0 0.0
0.5427416439346633
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3485,	 Acc1 = 0.3126,	 Acc2 = 0.3206

 ===== Epoch 267	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.1107073   2.613521   -0.40665367 -0.41433305  3.109413    2.5856948
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.02233648  0.15910149  0.00962024  0.01822009 -0.30913007  0.3224124
 -0.01013718 -0.01100384] 3 4
train:	 Loss = 1.2495,	 Acc = 0.5346
33271 0.288
65374 0.595
33823 0.65
4287 0.628
706 0.501
75 0.347
0 0.0
0 0.0
0.6131971418980482
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 3
val:	 Loss = 1.2473,	 Acc = 0.5388
3965 0.392
20118 0.551
9324 0.585
956 0.457
29 0.0
0 0.0
0 0.0
0 0.0
0.5578926611233443
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3262,	 Acc1 = 0.3194,	 Acc2 = 0.3288

 ===== Epoch 268	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513   1.9347795   2.1046388
 -0.36121437 -0.36837378  1.0001045   0.8367983  -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.5245923   0.06774087
 -0.00902925 -0.00742264  0.53772837  0.33465832  0.00233593  0.00991316
 -0.01013718 -0.01100384] 2 2
train:	 Loss = 1.2491,	 Acc = 0.5351
33272 0.287
65373 0.596
33823 0.65
4287 0.625
706 0.507
75 0.333
0 0.0
0 0.0
0.6142772193662241
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2370,	 Acc = 0.5583
3965 0.389
20118 0.584
9324 0.584
956 0.482
29 0.0
0 0.0
0 0.0
0 0.0
0.5804055608505604
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3155,	 Acc1 = 0.3078,	 Acc2 = 0.3149

 ===== Epoch 269	 =====
[ 0.21376367  1.5399566  -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00770627 -0.0040565   0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 0 0
train:	 Loss = 1.2498,	 Acc = 0.5342
33274 0.287
65377 0.595
33818 0.648
4286 0.628
706 0.52
75 0.373
0 0.0
0 0.0
0.613138056051102
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2290,	 Acc = 0.5502
3965 0.399
20118 0.557
9324 0.606
956 0.5
29 0.0
0 0.0
0 0.0
0 0.0
0.5697899891543695
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3157,	 Acc1 = 0.2938,	 Acc2 = 0.2980

 ===== Epoch 270	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305  1.2803167   2.1409874
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009 -0.23205094  0.7911613
 -0.01013718 -0.01100384] 4 1
train:	 Loss = 1.2485,	 Acc = 0.5339
33272 0.287
65370 0.595
33826 0.648
4287 0.63
706 0.511
75 0.347
0 0.0
0 0.0
0.6126946980741196
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2549,	 Acc = 0.5488
3965 0.395
20118 0.582
9324 0.557
956 0.424
29 0.0
0 0.0
0 0.0
0 0.0
0.5688368882900056
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 1
Testing:	 Loss = 1.3315,	 Acc1 = 0.3070,	 Acc2 = 0.3139

 ===== Epoch 271	 =====
[-0.36717504 -0.38357562  1.915655    0.85622287 -0.4399085  -0.3643795
  5.3948693   2.2432575  -0.40665367 -0.41433305  4.6431155   2.2411687
 -0.3911008  -0.38360253] [-6.02539303e-03 -6.53723441e-03  1.21963985e-01  1.03256866e-01
  2.59709405e-03  9.93073569e-04 -7.11250827e-02  1.82890669e-01
  9.62023530e-03  1.82200950e-02  1.21831143e+00  2.21309721e-01
 -1.01371771e-02 -1.10038444e-02] 3 3
train:	 Loss = 1.2490,	 Acc = 0.5354
33270 0.287
65377 0.597
33822 0.649
4286 0.634
706 0.523
75 0.333
0 0.0
0 0.0
0.6147929334586538
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 3
val:	 Loss = 1.2250,	 Acc = 0.5574
3965 0.383
20118 0.58
9324 0.592
956 0.487
29 0.0
0 0.0
0 0.0
0 0.0
0.5801426364741841
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3115,	 Acc1 = 0.2886,	 Acc2 = 0.2917

 ===== Epoch 272	 =====
[ 0.97420084  3.0569298  -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
  0.74103934  3.4486537 ] [-8.3572157e-03 -8.0781531e-01  1.1128664e-02  1.3828198e-02
  2.5970940e-03  9.9307357e-04 -9.0292487e-03 -7.4226381e-03
  9.6202353e-03  1.8220095e-02  2.3359330e-03  9.9131642e-03
  8.7312618e-03 -1.0381219e+00] 5 5
train:	 Loss = 1.2477,	 Acc = 0.5355
33271 0.29
65377 0.597
33821 0.648
4286 0.625
706 0.516
75 0.387
0 0.0
0 0.0
0.61385891718218
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2479,	 Acc = 0.5411
3965 0.39
20118 0.561
9324 0.57
956 0.47
29 0.0
0 0.0
0 0.0
0 0.0
0.560784829263483
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3240,	 Acc1 = 0.3099,	 Acc2 = 0.3173

 ===== Epoch 273	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
  1.9364483   3.5558314 ] [-6.0253930e-03 -6.5372344e-03  1.1128664e-02  1.3828198e-02
  2.5970940e-03  9.9307357e-04 -9.0292487e-03 -7.4226381e-03
  9.6202353e-03  1.8220095e-02  3.9088159e+00  1.8236343e+00
  1.1265488e+00  1.7553881e-03] 2 2
train:	 Loss = 1.2473,	 Acc = 0.5358
33268 0.288
65373 0.597
33827 0.65
4287 0.626
706 0.527
75 0.347
0 0.0
0 0.0
0.6148099129167146
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2305,	 Acc = 0.5513
3965 0.385
20118 0.575
9324 0.58
956 0.486
29 0.0
0 0.0
0 0.0
0 0.0
0.5729450816708844
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 1
Testing:	 Loss = 1.3114,	 Acc1 = 0.3045,	 Acc2 = 0.3109

 ===== Epoch 274	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 0 1
train:	 Loss = 1.2499,	 Acc = 0.5342
33275 0.285
65371 0.596
33823 0.65
4286 0.631
706 0.507
75 0.373
0 0.0
0 0.0
0.6138536940946279
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2089,	 Acc = 0.5636
3965 0.396
20118 0.584
9324 0.601
956 0.486
29 0.0
0 0.0
0 0.0
0 0.0
0.585401124001709
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 1
Testing:	 Loss = 1.2992,	 Acc1 = 0.2746,	 Acc2 = 0.2749

 ===== Epoch 275	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  3.0446146   1.0250614  -0.40665367 -0.41433305  1.4860704   3.0792713
 -0.3911008  -0.38360253] [-6.0253930e-03 -6.5372344e-03  4.3393793e+00  1.8307569e+00
  2.5970940e-03  9.9307357e-04 -4.8585221e-01  2.9900117e+00
  9.6202353e-03  1.8220095e-02 -2.6519876e+00 -4.3834586e+00
 -1.0137177e-02 -1.1003844e-02] 6 6
train:	 Loss = 1.2491,	 Acc = 0.5346
33272 0.287
65377 0.597
33821 0.649
4286 0.621
705 0.511
75 0.333
0 0.0
0 0.0
0.6136921660400522
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2328,	 Acc = 0.5541
3965 0.398
20118 0.567
9324 0.602
956 0.48
29 0.0
0 0.0
0 0.0
0 0.0
0.5744568968350479
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 0
Testing:	 Loss = 1.3195,	 Acc1 = 0.2971,	 Acc2 = 0.3019

 ===== Epoch 276	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 0 1
train:	 Loss = 1.2487,	 Acc = 0.5342
33278 0.286
65369 0.595
33822 0.649
4286 0.626
706 0.513
75 0.413
0 0.0
0 0.0
0.6134013696790654
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2262,	 Acc = 0.5625
3965 0.395
20118 0.576
9324 0.613
956 0.491
29 0.0
0 0.0
0 0.0
0 0.0
0.584316560949157
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 1
Testing:	 Loss = 1.3184,	 Acc1 = 0.2804,	 Acc2 = 0.2818

 ===== Epoch 277	 =====
[-0.36717504 -0.38357562  3.2097073   2.6585338   2.8150914   0.8924535
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-6.0253930e-03 -6.5372344e-03 -4.9270010e+00 -2.8653388e+00
 -1.0070228e-01  1.0200094e+00 -9.0292487e-03 -7.4226381e-03
  2.6921241e+00  2.1407447e+00  2.3359330e-03  9.9131642e-03
 -1.0137177e-02 -1.1003844e-02] 6 6
train:	 Loss = 1.2492,	 Acc = 0.5344
33273 0.287
65374 0.596
33821 0.647
4287 0.625
706 0.517
75 0.36
0 0.0
0 0.0
0.6132472689256975
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2576,	 Acc = 0.5362
3965 0.385
20118 0.548
9324 0.583
956 0.481
29 0.0
0 0.0
0 0.0
0 0.0
0.5558221316593814
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3370,	 Acc1 = 0.3062,	 Acc2 = 0.3129

 ===== Epoch 278	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.3441842   1.6610106  -0.40665367 -0.41433305  2.4917705   2.778727
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
  0.3635572   0.10472627  0.00962024  0.01822009  0.13027866  0.12327074
 -0.01013718 -0.01100384] 3 2
train:	 Loss = 1.2504,	 Acc = 0.5333
33267 0.287
65376 0.594
33827 0.648
4285 0.627
706 0.499
75 0.36
0 0.0
0 0.0
0.6120419300079601
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2210,	 Acc = 0.5618
3965 0.392
20118 0.578
9324 0.609
956 0.492
29 0.0
0 0.0
0 0.0
0 0.0
0.5839550399316397
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3121,	 Acc1 = 0.2855,	 Acc2 = 0.2880

 ===== Epoch 279	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 1
train:	 Loss = 1.2465,	 Acc = 0.5362
33273 0.289
65372 0.598
33825 0.65
4285 0.625
706 0.516
75 0.36
0 0.0
0 0.0
0.6152901796418672
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2371,	 Acc = 0.5496
3965 0.391
20118 0.556
9324 0.608
956 0.508
29 0.0
0 0.0
0 0.0
0 0.0
0.570282972360075
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3164,	 Acc1 = 0.3020,	 Acc2 = 0.3079

 ===== Epoch 280	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 3 1
train:	 Loss = 1.2482,	 Acc = 0.5344
33276 0.287
65370 0.595
33822 0.65
4287 0.624
706 0.537
75 0.333
0 0.0
0 0.0
0.6133800115096874
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2515,	 Acc = 0.5406
3965 0.394
20118 0.557
9324 0.577
956 0.465
29 0.0
0 0.0
0 0.0
0 0.0
0.559634535116837
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 1
Testing:	 Loss = 1.3427,	 Acc1 = 0.2525,	 Acc2 = 0.2483

 ===== Epoch 281	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305  2.2145321   0.93881106
  2.9766617   3.4042141 ] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.42706215  0.3561133
  0.1516736   0.13572732] 2 2
train:	 Loss = 1.2489,	 Acc = 0.5356
33271 0.289
65377 0.597
33824 0.649
4284 0.63
705 0.502
75 0.347
0 0.0
0 0.0
0.6142809188126409
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2848,	 Acc = 0.5324
3965 0.385
20118 0.561
9324 0.543
956 0.454
29 0.0
0 0.0
0 0.0
0 0.0
0.5516153416373616
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 0
Testing:	 Loss = 1.3813,	 Acc1 = 0.2680,	 Acc2 = 0.2669

 ===== Epoch 282	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 3 1
train:	 Loss = 1.2483,	 Acc = 0.5348
33272 0.286
65374 0.595
33823 0.651
4286 0.627
706 0.545
75 0.36
0 0.0
0 0.0
0.6141813089848845
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2391,	 Acc = 0.5572
3965 0.398
20118 0.588
9324 0.568
956 0.475
29 0.0
0 0.0
0 0.0
0 0.0
0.5779406448220331
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3199,	 Acc1 = 0.3053,	 Acc2 = 0.3119

 ===== Epoch 283	 =====
[-0.36717504 -0.38357562  2.8096156  -4.6914444  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-6.0253930e-03 -6.5372344e-03  1.1913450e+00  7.2880869e+00
  3.7478254e+00  1.5473169e+00 -9.0292487e-03 -7.4226381e-03
  1.6703140e+00  1.8780413e+00  2.3359330e-03  9.9131642e-03
 -1.0137177e-02 -1.1003844e-02] 2 6
train:	 Loss = 1.2491,	 Acc = 0.5351
33273 0.288
65371 0.596
33824 0.649
4287 0.626
706 0.516
75 0.36
0 0.0
0 0.0
0.6139186480342979
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 3
val:	 Loss = 1.2107,	 Acc = 0.5595
3965 0.395
20118 0.562
9324 0.634
956 0.477
29 0.0
0 0.0
0 0.0
0 0.0
0.5808985440562658
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3036,	 Acc1 = 0.2600,	 Acc2 = 0.2572

 ===== Epoch 284	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 3 1
train:	 Loss = 1.2481,	 Acc = 0.5351
33271 0.288
65373 0.597
33826 0.65
4285 0.617
706 0.499
75 0.36
0 0.0
0 0.0
0.6140027813743826
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2694,	 Acc = 0.5338
3965 0.334
20118 0.561
9324 0.57
956 0.46
29 0.0
0 0.0
0 0.0
0 0.0
0.5597988628520721
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3559,	 Acc1 = 0.2812,	 Acc2 = 0.2828

 ===== Epoch 285	 =====
[ 0.69136226  0.88183933 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
  0.5238524   1.1717811 ] [-1.3390292e-01  1.5513656e+00  1.1128664e-02  1.3828198e-02
  2.5970940e-03  9.9307357e-04 -9.0292487e-03 -7.4226381e-03
  9.6202353e-03  1.8220095e-02  2.3359330e-03  9.9131642e-03
 -9.9018204e-04  2.0209038e+00] 6 6
train:	 Loss = 1.2477,	 Acc = 0.5348
33274 0.288
65375 0.597
33821 0.648
4285 0.624
706 0.521
75 0.307
0 0.0
0 0.0
0.6136463908231187
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2573,	 Acc = 0.5384
3965 0.389
20118 0.556
9324 0.571
956 0.49
29 0.0
0 0.0
0 0.0
0 0.0
0.5578269300292503
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3296,	 Acc1 = 0.3266,	 Acc2 = 0.3375

 ===== Epoch 286	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 1 1
train:	 Loss = 1.2491,	 Acc = 0.5352
33272 0.288
65377 0.596
33822 0.649
4285 0.639
705 0.502
75 0.32
0 0.0
0 0.0
0.6141237627560807
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2754,	 Acc = 0.5398
3965 0.394
20118 0.565
9324 0.556
956 0.471
29 0.0
0 0.0
0 0.0
0 0.0
0.5588128964406612
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3737,	 Acc1 = 0.2525,	 Acc2 = 0.2483

 ===== Epoch 287	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513   1.374161    0.9728194
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.37953323  0.07664058
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 3 2
train:	 Loss = 1.2481,	 Acc = 0.5354
33273 0.289
65378 0.596
33820 0.649
4285 0.631
705 0.53
75 0.387
0 0.0
0 0.0
0.6140433327258951
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2818,	 Acc = 0.5250
3965 0.4
20118 0.549
9324 0.54
956 0.41
29 0.0
0 0.0
0 0.0
0 0.0
0.541295559864594
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3541,	 Acc1 = 0.3037,	 Acc2 = 0.3099

 ===== Epoch 288	 =====
[ 3.6152523   1.5323337  -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-3.7459576e+00 -1.8770128e+00  1.1128664e-02  1.3828198e-02
  2.5970940e-03  9.9307357e-04  2.2620440e+00  3.8294289e+00
  9.6202353e-03  1.8220095e-02  2.3359330e-03  9.9131642e-03
 -1.0137177e-02 -1.1003844e-02] 6 5
train:	 Loss = 1.2494,	 Acc = 0.5339
33279 0.285
65371 0.595
33820 0.649
4285 0.628
706 0.527
75 0.32
0 0.0
0 0.0
0.61317705285976
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 3
val:	 Loss = 1.2796,	 Acc = 0.5302
3965 0.391
20118 0.561
9324 0.532
956 0.458
29 0.0
0 0.0
0 0.0
0 0.0
0.5483616524797055
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3491,	 Acc1 = 0.3287,	 Acc2 = 0.3400

 ===== Epoch 289	 =====
[ 2.108981    0.8488064  -0.4210503  -0.3377513   4.5545826  -4.2398005
 -0.36121437 -0.36837378  4.8791842   2.2310572  -0.42853883 -0.42463273
  3.0782204   1.1221133 ] [ 3.2185030e+00  7.0543420e-01  1.1128664e-02  1.3828198e-02
 -7.1394477e+00  3.8634653e+00 -9.0292487e-03 -7.4226381e-03
  2.9462120e-01 -8.8599625e+00  1.2928165e+00  2.7213035e+00
 -1.3192390e-01  6.1738831e-01] 2 2
train:	 Loss = 1.2496,	 Acc = 0.5338
33274 0.285
65375 0.596
33822 0.648
4284 0.631
706 0.506
75 0.32
0 0.0
0 0.0
0.6132627419385778
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2362,	 Acc = 0.5462
3965 0.39
20118 0.567
9324 0.578
956 0.465
29 0.0
0 0.0
0 0.0
0 0.0
0.5665362999967134
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3115,	 Acc1 = 0.3328,	 Acc2 = 0.3449

 ===== Epoch 290	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
  2.2213418   3.2107713 ] [-6.0253930e-03 -6.5372344e-03  1.1128664e-02  1.3828198e-02
  2.5970940e-03  9.9307357e-04 -9.0292487e-03 -7.4226381e-03
  9.6202353e-03  1.8220095e-02  4.1080689e+00  2.2035353e+00
  1.2334706e+00  4.4513869e-01] 2 2
train:	 Loss = 1.2470,	 Acc = 0.5364
33271 0.289
65372 0.598
33826 0.649
4286 0.634
706 0.503
75 0.4
0 0.0
0 0.0
0.615393468565674
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 3
val:	 Loss = 1.2431,	 Acc = 0.5427
3965 0.389
20118 0.562
9324 0.577
956 0.463
29 0.0
0 0.0
0 0.0
0 0.0
0.5627567620863049
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3290,	 Acc1 = 0.2697,	 Acc2 = 0.2689

 ===== Epoch 291	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  1.9076345   3.0346608  -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-6.0253930e-03 -6.5372344e-03  1.1128664e-02  1.3828198e-02
  2.5970940e-03  9.9307357e-04 -2.9328673e+00 -4.0991583e+00
  9.6202353e-03  1.8220095e-02  3.9303148e+00  2.8162789e+00
 -1.0137177e-02 -1.1003844e-02] 5 6
train:	 Loss = 1.2504,	 Acc = 0.5337
33272 0.287
65377 0.596
33820 0.645
4286 0.632
706 0.524
75 0.333
0 0.0
0 0.0
0.612560423540244
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 3
val:	 Loss = 1.2322,	 Acc = 0.5534
3965 0.396
20118 0.567
9324 0.599
956 0.495
29 0.0
0 0.0
0 0.0
0 0.0
0.5738653169882013
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3101,	 Acc1 = 0.2946,	 Acc2 = 0.2990

 ===== Epoch 292	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513   1.7745478   1.4550146
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.40590715  0.2991332
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 2 2
train:	 Loss = 1.2481,	 Acc = 0.5347
33273 0.288
65374 0.597
33823 0.646
4286 0.629
705 0.525
75 0.307
0 0.0
0 0.0
0.6135829584799977
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2933,	 Acc = 0.5313
3965 0.387
20118 0.55
9324 0.555
956 0.516
29 0.0
0 0.0
0 0.0
0 0.0
0.5500706609261511
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3717,	 Acc1 = 0.2942,	 Acc2 = 0.2985

 ===== Epoch 293	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 1 1
train:	 Loss = 1.2472,	 Acc = 0.5349
33274 0.286
65370 0.596
33824 0.65
4287 0.632
706 0.521
75 0.36
0 0.0
0 0.0
0.6142985939268382
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 3
val:	 Loss = 1.2453,	 Acc = 0.5425
3965 0.391
20118 0.561
9324 0.577
956 0.46
29 0.0
0 0.0
0 0.0
0 0.0
0.5622966444276465
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3359,	 Acc1 = 0.2771,	 Acc2 = 0.2778

 ===== Epoch 294	 =====
[ 3.2839048   2.6224198  -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
  2.9221454   2.8186579 ] [-3.4347863e+00 -2.9412489e+00  1.1128664e-02  1.3828198e-02
  2.5970940e-03  9.9307357e-04 -9.0292487e-03 -7.4226381e-03
  9.6202353e-03  1.8220095e-02  3.5763752e+00  2.7795143e+00
 -4.3190751e+00 -3.9185185e+00] 6 6
train:	 Loss = 1.2504,	 Acc = 0.5341
33274 0.285
65371 0.595
33823 0.65
4287 0.625
706 0.531
75 0.347
0 0.0
0 0.0
0.6134833400471889
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2443,	 Acc = 0.5470
3965 0.397
20118 0.57
9324 0.572
956 0.464
29 0.0
0 0.0
0 0.0
0 0.0
0.5665691655437605
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 0
Testing:	 Loss = 1.3331,	 Acc1 = 0.2996,	 Acc2 = 0.3049

 ===== Epoch 295	 =====
[-0.36717504 -0.38357562  2.358335    1.1830904  -0.4399085  -0.3643795
  2.0636678   3.574511   -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-6.0253930e-03 -6.5372344e-03 -3.7690642e+00 -1.4475672e+00
  2.5970940e-03  9.9307357e-04 -7.8155470e-01  3.9699313e-01
  9.6202353e-03  1.8220095e-02  2.3359330e-03  9.9131642e-03
 -1.0137177e-02 -1.1003844e-02] 1 1
train:	 Loss = 1.2478,	 Acc = 0.5350
33271 0.286
65372 0.597
33826 0.65
4286 0.626
706 0.501
75 0.36
0 0.0
0 0.0
0.6142521459742003
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2284,	 Acc = 0.5481
3965 0.396
20118 0.552
9324 0.612
956 0.491
29 0.0
0 0.0
0 0.0
0 0.0
0.5679495185197357
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3128,	 Acc1 = 0.2878,	 Acc2 = 0.2908

 ===== Epoch 296	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 6 1
train:	 Loss = 1.2493,	 Acc = 0.5341
33272 0.288
65375 0.595
33822 0.648
4286 0.621
706 0.518
75 0.347
0 0.0
0 0.0
0.6126851070359856
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 3
val:	 Loss = 1.2566,	 Acc = 0.5394
3965 0.38
20118 0.559
9324 0.572
956 0.485
29 0.0
0 0.0
0 0.0
0 0.0
0.5601603838695896
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3298,	 Acc1 = 0.3241,	 Acc2 = 0.3345

 ===== Epoch 297	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513   0.8838611   0.7696723
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282  -0.35840464  0.01211771
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 1 0
train:	 Loss = 1.2471,	 Acc = 0.5352
33277 0.288
65368 0.597
33823 0.649
4287 0.626
706 0.518
75 0.333
0 0.0
0 0.0
0.61426831256774
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 3
val:	 Loss = 1.2684,	 Acc = 0.5498
3965 0.371
20118 0.579
9324 0.575
956 0.455
29 0.0
0 0.0
0 0.0
0 0.0
0.5731422749531666
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 1
Testing:	 Loss = 1.3577,	 Acc1 = 0.2789,	 Acc2 = 0.2801

 ===== Epoch 298	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513   2.9157648   2.9797337
 -0.36121437 -0.36837378  1.935225    2.2853467  -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 0 0
train:	 Loss = 1.2489,	 Acc = 0.5351
33271 0.287
65376 0.597
33821 0.649
4287 0.627
706 0.508
75 0.36
0 0.0
0 0.0
0.6141466455665852
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 0
val:	 Loss = 1.2232,	 Acc = 0.5439
3965 0.252
20118 0.581
9324 0.598
956 0.464
29 0.0
0 0.0
0 0.0
0 0.0
0.5819173760147238
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3072,	 Acc1 = 0.2874,	 Acc2 = 0.3226

 ===== Epoch 299	 =====
[ 1.8185805   2.172664   -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
  2.1263773   2.384719  ] [ 0.12162803  0.0356353   0.01112866  0.0138282   0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.32232454  0.08150058] 1 3
train:	 Loss = 1.2492,	 Acc = 0.5344
33273 0.285
65373 0.596
33824 0.649
4286 0.626
705 0.523
75 0.32
0 0.0
0 0.0
0.613841918993315
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 3
val:	 Loss = 1.3299,	 Acc = 0.4997
3965 0.396
20118 0.512
9324 0.515
956 0.541
29 0.0
0 0.0
0 0.0
0 0.0
0.5132941137805239
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 1
Testing:	 Loss = 1.3952,	 Acc1 = 0.2992,	 Acc2 = 0.3044

 ===== Epoch 300	 =====
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305  1.3359152   1.102522
 -0.3911008  -0.38360253] [-6.0253930e-03 -6.5372344e-03  1.1128664e-02  1.3828198e-02
  2.5970940e-03  9.9307357e-04 -9.0292487e-03 -7.4226381e-03
  9.6202353e-03  1.8220095e-02 -2.4438195e+00 -1.9049107e+00
  3.3124914e+00  2.8821521e+00] 5 5
train:	 Loss = 1.2499,	 Acc = 0.5346
33273 0.285
65375 0.598
33821 0.648
4287 0.617
705 0.509
75 0.307
0 0.0
0 0.0
0.6142735198488438
0.6019653597134125
[-0.36717504 -0.38357562 -0.4210503  -0.3377513  -0.4399085  -0.3643795
  2.888007    1.8871257  -0.40665367 -0.41433305  2.3847337   3.2454255
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723  0.01112866  0.0138282   0.00259709  0.00099307
 -0.0681702  -0.0244149   0.00962024  0.01822009 -0.11354697 -0.02685145
 -0.01013718 -0.01100384] 3 3
val:	 Loss = 1.2387,	 Acc = 0.5497
3965 0.39
20118 0.565
9324 0.595
956 0.469
29 0.0
0 0.0
0 0.0
0 0.0
0.5706116278305452
0.6019653597134125
[-0.36717504 -0.38357562  1.0863959   2.338476   -0.4399085  -0.3643795
 -0.36121437 -0.36837378 -0.40665367 -0.41433305 -0.42853883 -0.42463273
 -0.3911008  -0.38360253] [-0.00602539 -0.00653723 -0.01616307  0.21231622  0.00259709  0.00099307
 -0.00902925 -0.00742264  0.00962024  0.01822009  0.00233593  0.00991316
 -0.01013718 -0.01100384] 4 4
Testing:	 Loss = 1.3187,	 Acc1 = 0.2996,	 Acc2 = 0.3049
