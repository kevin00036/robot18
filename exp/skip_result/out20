(0.24300000000005184, array([-1.000000e+00, -1.000000e+00,  1.189783e+03,  1.790000e-01,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 4, array([  0.   ,   0.   , -15.851,   0.091,   0.   ,   0.   ,   0.   ,
         0.   ,   0.   ,   0.   ,   0.   ,   0.   ,   0.   ,   0.   ]), 1)
((0.236, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.364685e+03, -3.300000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), array([0., 0., 0.])), (0.24, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.543573e+03, -3.290000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), array([0., 0., 0.])))
(0.236, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.364685e+03, -3.300000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([0.   , 0.   , 0.   , 0.   , 5.823, 3.129, 0.   , 0.   , 0.   ,
       0.   , 0.   , 0.   , 0.   , 0.   ]))
(0.236, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.364685e+03, -3.300000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([ 0.   ,  0.   ,  0.   ,  0.   , -8.411, -0.011,  0.   ,  0.   ,
        0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ]))
(0.236, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.364685e+03, -3.300000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  6.2433e+01,
       -3.0000e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
        0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]))
(0.236, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.364685e+03, -3.300000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([ 0.00000e+00,  0.00000e+00,  0.00000e+00,  0.00000e+00,
       -1.19043e+02, -4.00000e-03,  0.00000e+00,  0.00000e+00,
        0.00000e+00,  0.00000e+00,  0.00000e+00,  0.00000e+00,
        0.00000e+00,  0.00000e+00]))
(0.236, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.364685e+03, -3.300000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  5.0464e+01,
       -1.3000e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
        0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]))
(0.236, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.364685e+03, -3.300000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([ 0.00000e+00,  0.00000e+00,  0.00000e+00,  0.00000e+00,
       -1.53656e+02, -1.00000e-02,  0.00000e+00,  0.00000e+00,
        0.00000e+00,  0.00000e+00,  0.00000e+00,  0.00000e+00,
        0.00000e+00,  0.00000e+00]))
(0.236, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.364685e+03, -3.300000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([ 0.000000e+00,  0.000000e+00,  2.399328e+03,  1.498000e+00,
       -1.455700e+01, -6.000000e-03,  0.000000e+00,  0.000000e+00,
        0.000000e+00,  0.000000e+00,  0.000000e+00,  0.000000e+00,
        0.000000e+00,  0.000000e+00]))
(0.236, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.364685e+03, -3.300000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  1.9086e+01,
       -9.0000e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
        0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]))
(0.236, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.364685e+03, -3.300000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([ 0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  7.052e+01,
       -2.000e-02,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,
        0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00]))
(0.236, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.364685e+03, -3.300000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  4.1406e+01,
       -1.7000e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
        0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]))
14 1 14

 ===== Epoch 1	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493  2.438313    1.7233281
 -0.39281774 -0.38534638] [ 3.5584700e+00  3.6320157e+00  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -1.1342351e-02 -8.0557177e-03
  1.4306194e-02  2.5413027e-02  2.3729024e-02  4.7489680e-02
 -1.2275568e-02 -1.3029240e-02] 3 1
train:	 Loss = 1.4141,	 Acc = 0.4674
40155 0.275
78850 0.52
41022 0.545
5025 0.54
814 0.527
86 0.314
0 0.0
0 0.0
0.5287804955603075
0.0
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.3380,	 Acc = 0.4882
4718 0.39
24253 0.518
11362 0.482
1141 0.341
29 0.0
0 0.0
0 0.0
0 0.0
0.5008019573195596
0.5008019573195596
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 1
Testing:	 Loss = 1.4114,	 Acc1 = 0.2474,	 Acc2 = 0.2420

 ===== Epoch 2	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705  1.9010074   1.145934
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.08367579  0.05714503
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 0
train:	 Loss = 1.3226,	 Acc = 0.5034
40153 0.284
78857 0.563
41015 0.589
5027 0.614
814 0.569
86 0.372
0 0.0
0 0.0
0.5734226822152799
0.5008019573195596
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.3456,	 Acc = 0.5151
4718 0.399
24253 0.537
11362 0.525
1141 0.451
29 0.0
0 0.0
0 0.0
0 0.0
0.5299986407503058
0.5299986407503058
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 1
Testing:	 Loss = 1.4067,	 Acc1 = 0.2833,	 Acc2 = 0.2853

 ===== Epoch 3	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  2.1004484   2.4640691  -0.4089302  -0.41695493  2.9975426   2.3854861
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01755038  0.0013912   0.01430619  0.02541303  0.06866521  0.03305517
 -0.01227557 -0.01302924] 3 0
train:	 Loss = 1.3003,	 Acc = 0.5108
40153 0.284
78855 0.573
41020 0.598
5024 0.628
814 0.559
86 0.465
0 0.0
0 0.0
0.583176336854824
0.5299986407503058
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.3040,	 Acc = 0.5225
4718 0.407
24253 0.556
11362 0.507
1141 0.456
29 0.0
0 0.0
0 0.0
0 0.0
0.537311404104934
0.537311404104934
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3628,	 Acc1 = 0.2901,	 Acc2 = 0.2935

 ===== Epoch 4	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 3 1
train:	 Loss = 1.2836,	 Acc = 0.5158
40156 0.283
78852 0.578
41022 0.61
5024 0.628
812 0.564
86 0.395
0 0.0
0 0.0
0.5899869630194918
0.537311404104934
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2738,	 Acc = 0.5212
4718 0.397
24253 0.545
11362 0.534
1141 0.429
29 0.0
0 0.0
0 0.0
0 0.0
0.5371482941416338
0.537311404104934
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3436,	 Acc1 = 0.2798,	 Acc2 = 0.2811

 ===== Epoch 5	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705  0.39713323  1.6313332
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137 -1.1043979  -1.9230353
 -0.01134235 -0.00805572  0.01430619  0.02541303  2.9264324   2.2704031
 -0.01227557 -0.01302924] 6 6
train:	 Loss = 1.2671,	 Acc = 0.5208
40158 0.286
78848 0.583
41022 0.619
5026 0.619
812 0.537
86 0.5
0 0.0
0 0.0
0.5956722896163569
0.537311404104934
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2201,	 Acc = 0.5504
4718 0.401
24253 0.56
11362 0.601
1141 0.472
29 0.0
0 0.0
0 0.0
0 0.0
0.5695528068506185
0.5695528068506185
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 1
Testing:	 Loss = 1.3031,	 Acc1 = 0.2839,	 Acc2 = 0.2860

 ===== Epoch 6	 =====
[-0.36759388 -0.3845501   1.2996719   1.3736957  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02 -2.1685576e+00 -1.5798309e+00
  4.3038260e-03  3.1008099e-03 -1.1342351e-02 -8.0557177e-03
  1.4306194e-02  2.5413027e-02 -2.1558569e-03  7.0730671e-03
 -1.2275568e-02 -1.3029240e-02] 6 5
train:	 Loss = 1.2583,	 Acc = 0.5236
40159 0.287
78853 0.589
41017 0.62
5025 0.61
813 0.509
85 0.529
0 0.0
0 0.0
0.5990794400324342
0.5695528068506185
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2471,	 Acc = 0.5189
4718 0.381
24253 0.559
11362 0.506
1141 0.387
29 0.0
0 0.0
0 0.0
0 0.0
0.5366045942639663
0.5695528068506185
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3259,	 Acc1 = 0.2913,	 Acc2 = 0.2950

 ===== Epoch 7	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705  2.1118813   1.8348876
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137 -0.138056    0.56299895
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
train:	 Loss = 1.2531,	 Acc = 0.5279
40158 0.289
78853 0.593
41017 0.628
5025 0.609
813 0.53
86 0.581
0 0.0
0 0.0
0.6042498052371337
0.5695528068506185
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.1977,	 Acc = 0.5648
4718 0.41
24253 0.586
11362 0.6
1141 0.422
29 0.0
0 0.0
0 0.0
0 0.0
0.5846948484436592
0.5846948484436592
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2814,	 Acc1 = 0.2769,	 Acc2 = 0.2776

 ===== Epoch 8	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705  3.0860643   2.139101
 -0.36039132 -0.36746153  3.1615288   1.213319   -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
 -4.6761651e+00 -2.4137571e+00 -1.1342351e-02 -8.0557177e-03
 -4.3561330e+00 -1.8335437e+00 -2.1558569e-03  7.0730671e-03
 -1.2275568e-02 -1.3029240e-02] 2 2
train:	 Loss = 1.2514,	 Acc = 0.5264
40152 0.287
78856 0.592
41019 0.625
5026 0.61
813 0.522
86 0.558
0 0.0
0 0.0
0.6027821939586645
0.5846948484436592
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.1881,	 Acc = 0.5566
4718 0.4
24253 0.581
11362 0.584
1141 0.425
29 0.0
0 0.0
0 0.0
0 0.0
0.5767024602419465
0.5846948484436592
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2661,	 Acc1 = 0.3103,	 Acc2 = 0.3178

 ===== Epoch 9	 =====
[-0.36759388 -0.3845501   2.1817348   1.207758   -0.4375299  -0.36171135
  1.9255426   3.7524157  -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02 -3.2881112e+00 -1.4248924e+00
  4.3038260e-03  3.1008099e-03 -5.4787666e-01  1.7773357e-01
  1.4306194e-02  2.5413027e-02 -2.1558569e-03  7.0730671e-03
 -1.2275568e-02 -1.3029240e-02] 1 1
train:	 Loss = 1.2508,	 Acc = 0.5261
40150 0.285
78856 0.592
41021 0.624
5025 0.617
814 0.522
86 0.512
0 0.0
0 0.0
0.6029713359088091
0.5846948484436592
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2396,	 Acc = 0.5331
4718 0.386
24253 0.56
11362 0.547
1141 0.447
29 0.0
0 0.0
0 0.0
0 0.0
0.5520728557836074
0.5846948484436592
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3140,	 Acc1 = 0.2779,	 Acc2 = 0.2788

 ===== Epoch 10	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705  2.6508238   2.9577925
 -0.36039132 -0.36746153  1.933752    2.2214823  -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  4.3669643e+00  2.0070984e+00
 -4.0980253e+00 -3.2049644e+00  2.9851804e+00  4.1989694e+00
 -2.8532662e+00 -2.9831245e+00 -2.1558569e-03  7.0730671e-03
 -1.2275568e-02 -1.3029240e-02] 5 6
train:	 Loss = 1.2499,	 Acc = 0.5280
40155 0.288
78852 0.594
41019 0.627
5026 0.611
814 0.517
86 0.535
0 0.0
0 0.0
0.6047521006065327
0.5846948484436592
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2030,	 Acc = 0.5663
4718 0.408
24253 0.579
11362 0.612
1141 0.514
29 0.0
0 0.0
0 0.0
0 0.0
0.5866249830093788
0.5866249830093788
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2898,	 Acc1 = 0.2608,	 Acc2 = 0.2582

 ===== Epoch 11	 =====
[-0.36759388 -0.3845501   1.1723719   1.3759688  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.07429576  0.17330727  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 0 4
train:	 Loss = 1.2483,	 Acc = 0.5291
40160 0.289
78850 0.594
41016 0.627
5026 0.62
814 0.528
86 0.547
0 0.0
0 0.0
0.6056982955990842
0.5866249830093788
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.1643,	 Acc = 0.5662
4718 0.395
24253 0.58
11362 0.619
1141 0.461
29 0.0
0 0.0
0 0.0
0 0.0
0.5881201576729645
0.5881201576729645
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2461,	 Acc1 = 0.2796,	 Acc2 = 0.2808

 ===== Epoch 12	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153  2.9523873   2.7772014  -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [ 1.4000168   1.5910102   0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.47835448  0.30579868 -0.00215586  0.00707307
  1.9952902   1.9563954 ] 2 2
train:	 Loss = 1.2491,	 Acc = 0.5277
40161 0.287
78845 0.594
41022 0.627
5024 0.617
814 0.515
86 0.535
0 0.0
0 0.0
0.6047491473952826
0.5881201576729645
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2567,	 Acc = 0.5297
4718 0.392
24253 0.557
11362 0.534
1141 0.486
29 0.0
0 0.0
0 0.0
0 0.0
0.5473154818540166
0.5881201576729645
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3330,	 Acc1 = 0.2837,	 Acc2 = 0.2858

 ===== Epoch 13	 =====
[-0.36759388 -0.3845501   2.0741467   1.1509298  -0.4375299  -0.36171135
  1.3038533   3.817541   -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.29941088  0.3685721   0.00430383  0.00310081
  0.36312863  0.06751958  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 2 2
train:	 Loss = 1.2482,	 Acc = 0.5282
40152 0.288
78851 0.595
41022 0.625
5027 0.612
814 0.537
86 0.535
0 0.0
0 0.0
0.6049602543720191
0.5881201576729645
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2531,	 Acc = 0.5237
4718 0.398
24253 0.566
11362 0.497
1141 0.424
29 0.0
0 0.0
0 0.0
0 0.0
0.5397852385483213
0.5881201576729645
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3289,	 Acc1 = 0.2930,	 Acc2 = 0.2970

 ===== Epoch 14	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  1.1422238   1.6712406  -0.4089302  -0.41695493  2.0148945   3.0280247
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235  0.17773357  0.01430619  0.02541303  0.16683722  0.21492991
 -0.01227557 -0.01302924] 6 4
train:	 Loss = 1.2476,	 Acc = 0.5290
40157 0.286
78853 0.596
41016 0.627
5026 0.614
814 0.514
86 0.547
0 0.0
0 0.0
0.6064867443062125
0.5881201576729645
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2284,	 Acc = 0.5444
4718 0.39
24253 0.579
11362 0.541
1141 0.495
29 0.0
0 0.0
0 0.0
0 0.0
0.5642517330433601
0.5881201576729645
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3142,	 Acc1 = 0.3099,	 Acc2 = 0.3173

 ===== Epoch 15	 =====
[ 0.9718296   2.2406044  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
  0.7536224   2.6069002 ] [-0.05130228 -0.22107512  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.08902038 -0.2776237 ] 5 5
train:	 Loss = 1.2479,	 Acc = 0.5289
40158 0.289
78854 0.595
41015 0.625
5025 0.617
814 0.518
86 0.512
0 0.0
0 0.0
0.6053547863968075
0.5881201576729645
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2072,	 Acc = 0.5642
4718 0.407
24253 0.582
11362 0.599
1141 0.5
29 0.0
0 0.0
0 0.0
0 0.0
0.5843142585292919
0.5881201576729645
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2923,	 Acc1 = 0.2682,	 Acc2 = 0.2671

 ===== Epoch 16	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 2 1
train:	 Loss = 1.2450,	 Acc = 0.5301
40155 0.287
78852 0.597
41023 0.628
5022 0.617
814 0.531
86 0.593
0 0.0
0 0.0
0.6076218033816386
0.5881201576729645
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.1779,	 Acc = 0.5630
4718 0.401
24253 0.583
11362 0.593
1141 0.516
29 0.0
0 0.0
0 0.0
0 0.0
0.5837705586516243
0.5881201576729645
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2565,	 Acc1 = 0.3132,	 Acc2 = 0.3213

 ===== Epoch 17	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153  1.4849303   2.9812932  -0.42787513 -0.42500633
  2.235414    0.93291616] [ 1.3582646e+00  1.2657022e+00  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -1.1342351e-02 -8.0557177e-03
 -2.3038836e+00 -3.8495162e+00 -2.1558569e-03  7.0730671e-03
 -6.3416606e-01  2.7561927e-01] 1 1
train:	 Loss = 1.2457,	 Acc = 0.5300
40152 0.288
78856 0.596
41018 0.628
5027 0.62
813 0.524
86 0.593
0 0.0
0 0.0
0.607066772655008
0.5881201576729645
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.1923,	 Acc = 0.5619
4718 0.383
24253 0.572
11362 0.617
1141 0.556
29 0.0
0 0.0
0 0.0
0 0.0
0.5849123283947261
0.5881201576729645
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2774,	 Acc1 = 0.2595,	 Acc2 = 0.2567

 ===== Epoch 18	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 1 1
train:	 Loss = 1.2459,	 Acc = 0.5303
40158 0.288
78852 0.596
41017 0.629
5025 0.628
814 0.526
86 0.488
0 0.0
0 0.0
0.6075409002019174
0.5881201576729645
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2416,	 Acc = 0.5439
4718 0.403
24253 0.574
11362 0.546
1141 0.482
29 0.0
0 0.0
0 0.0
0 0.0
0.5620225635449232
0.5881201576729645
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3104,	 Acc1 = 0.3146,	 Acc2 = 0.3231

 ===== Epoch 19	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493  1.5656081   2.8489966
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303  0.10187588  0.28998932
 -0.01227557 -0.01302924] 4 4
train:	 Loss = 1.2446,	 Acc = 0.5300
40159 0.287
78851 0.595
41017 0.63
5025 0.632
814 0.518
86 0.535
0 0.0
0 0.0
0.6076570238407543
0.5881201576729645
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2014,	 Acc = 0.5535
4718 0.377
24253 0.571
11362 0.589
1141 0.561
29 0.0
0 0.0
0 0.0
0 0.0
0.5761043903765122
0.5881201576729645
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2797,	 Acc1 = 0.2911,	 Acc2 = 0.2947

 ===== Epoch 20	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493  1.8220177   2.3830335
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.33379155  0.25534654
 -0.01227557 -0.01302924] 1 1
train:	 Loss = 1.2427,	 Acc = 0.5315
40159 0.287
78854 0.597
41016 0.63
5023 0.642
814 0.533
86 0.419
0 0.0
0 0.0
0.6093741305160064
0.5881201576729645
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.1959,	 Acc = 0.5559
4718 0.397
24253 0.575
11362 0.592
1141 0.458
29 0.0
0 0.0
0 0.0
0 0.0
0.5762675003398124
0.5881201576729645
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2751,	 Acc1 = 0.3074,	 Acc2 = 0.3144

 ===== Epoch 21	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153  1.5967028   2.113289   -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.0213589   0.06466702 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 0 3
train:	 Loss = 1.2413,	 Acc = 0.5324
40159 0.286
78851 0.599
41019 0.632
5023 0.644
814 0.536
86 0.477
0 0.0
0 0.0
0.610916346696557
0.5881201576729645
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2402,	 Acc = 0.5373
4718 0.366
24253 0.567
11362 0.548
1141 0.514
29 0.0
0 0.0
0 0.0
0 0.0
0.5593584341443523
0.5881201576729645
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3340,	 Acc1 = 0.2674,	 Acc2 = 0.2662

 ===== Epoch 22	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705  1.9222095   2.1569958
 -0.36039132 -0.36746153  1.2199004   0.8075947  -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.5092166   0.01174789
 -0.01134235 -0.00805572  0.1934383   0.28617167 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 2 2
train:	 Loss = 1.2426,	 Acc = 0.5313
40165 0.287
78850 0.598
41014 0.628
5023 0.641
814 0.509
86 0.57
0 0.0
0 0.0
0.6091090494248214
0.5881201576729645
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.1864,	 Acc = 0.5737
4718 0.388
24253 0.59
11362 0.62
1141 0.557
29 0.0
0 0.0
0 0.0
0 0.0
0.5975261655566128
0.5975261655566128
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2681,	 Acc1 = 0.2936,	 Acc2 = 0.2977

 ===== Epoch 23	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 3 1
train:	 Loss = 1.2432,	 Acc = 0.5306
40154 0.286
78858 0.596
41015 0.631
5026 0.638
814 0.517
85 0.494
0 0.0
0 0.0
0.6084993402120861
0.5975261655566128
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2141,	 Acc = 0.5335
4718 0.398
24253 0.565
11362 0.529
1141 0.477
29 0.0
0 0.0
0 0.0
0 0.0
0.5508223460649722
0.5975261655566128
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2985,	 Acc1 = 0.2556,	 Acc2 = 0.2520

 ===== Epoch 24	 =====
[-0.36759388 -0.3845501   0.69027543  2.5466259  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.16858776 -1.1383626   0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 5 5
train:	 Loss = 1.2429,	 Acc = 0.5307
40158 0.288
78852 0.596
41015 0.63
5027 0.642
814 0.521
86 0.488
0 0.0
0 0.0
0.6081212140483648
0.5975261655566128
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2154,	 Acc = 0.5609
4718 0.379
24253 0.588
11362 0.586
1141 0.504
29 0.0
0 0.0
0 0.0
0 0.0
0.5842327035476417
0.5975261655566128
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 1
Testing:	 Loss = 1.3059,	 Acc1 = 0.2624,	 Acc2 = 0.2602

 ===== Epoch 25	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705  2.7518268   1.6290963
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  3.6221581e+00  2.3509343e+00
 -4.2321901e+00 -1.9208735e+00  1.5548790e+00  4.2525020e+00
  1.4306194e-02  2.5413027e-02 -2.1558569e-03  7.0730671e-03
 -1.2275568e-02 -1.3029240e-02] 5 5
train:	 Loss = 1.2422,	 Acc = 0.5309
40161 0.288
78854 0.596
41012 0.63
5025 0.639
814 0.51
86 0.523
0 0.0
0 0.0
0.6084934534267158
0.5975261655566128
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2395,	 Acc = 0.5308
4718 0.397
24253 0.545
11362 0.562
1141 0.495
29 0.0
0 0.0
0 0.0
0 0.0
0.5479679217072176
0.5975261655566128
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 1
Testing:	 Loss = 1.3183,	 Acc1 = 0.2614,	 Acc2 = 0.2589

 ===== Epoch 26	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 3 1
train:	 Loss = 1.2424,	 Acc = 0.5320
40150 0.288
78855 0.598
41022 0.632
5025 0.643
814 0.531
86 0.535
0 0.0
0 0.0
0.6100697922131604
0.5975261655566128
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2093,	 Acc = 0.5535
4718 0.401
24253 0.574
11362 0.58
1141 0.487
29 0.0
0 0.0
0 0.0
0 0.0
0.5730324860676906
0.5975261655566128
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2873,	 Acc1 = 0.2932,	 Acc2 = 0.2972

 ===== Epoch 27	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493  2.1750555   2.1304326
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303  0.23570302 -0.01602214
 -0.01227557 -0.01302924] 2 3
train:	 Loss = 1.2424,	 Acc = 0.5315
40162 0.287
78850 0.598
41014 0.631
5026 0.637
814 0.525
86 0.523
0 0.0
0 0.0
0.6094443119484856
0.5975261655566128
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2593,	 Acc = 0.5386
4718 0.41
24253 0.567
11362 0.539
1141 0.491
29 0.0
0 0.0
0 0.0
0 0.0
0.5551175750985456
0.5975261655566128
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3440,	 Acc1 = 0.2560,	 Acc2 = 0.2525

 ===== Epoch 28	 =====
[ 2.9755876   3.2845554  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
  2.381791    3.5145135 ] [-0.20210844  0.00302586  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
  0.0136589  -0.00701573] 0 0
train:	 Loss = 1.2414,	 Acc = 0.5308
40153 0.285
78856 0.597
41018 0.631
5025 0.641
814 0.542
86 0.43
0 0.0
0 0.0
0.6092735236369129
0.5975261655566128
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.1736,	 Acc = 0.5629
4718 0.4
24253 0.579
11362 0.601
1141 0.537
29 0.0
0 0.0
0 0.0
0 0.0
0.5837977436455076
0.5975261655566128
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2651,	 Acc1 = 0.2680,	 Acc2 = 0.2669

 ===== Epoch 29	 =====
[ 2.72068     2.0547302  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
  2.0068371   2.2851815 ] [-0.724453    0.11387151  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.0556742   0.13730852] 1 1
train:	 Loss = 1.2409,	 Acc = 0.5316
40157 0.287
78848 0.596
41022 0.635
5026 0.64
813 0.517
86 0.442
0 0.0
0 0.0
0.6095631781867323
0.5975261655566128
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.1917,	 Acc = 0.5720
4718 0.386
24253 0.591
11362 0.611
1141 0.559
29 0.0
0 0.0
0 0.0
0 0.0
0.59581351094196
0.5975261655566128
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2672,	 Acc1 = 0.2800,	 Acc2 = 0.2813

 ===== Epoch 30	 =====
[ 2.0291445   2.8873448  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-2.2033050e+00 -3.1078811e+00  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -1.1342351e-02 -8.0557177e-03
  1.4306194e-02  2.5413027e-02  2.9181283e+00  3.3529911e+00
 -1.2275568e-02 -1.3029240e-02] 5 5
train:	 Loss = 1.2411,	 Acc = 0.5318
40161 0.288
78852 0.597
41016 0.631
5023 0.642
814 0.531
86 0.512
0 0.0
0 0.0
0.6096143603278454
0.5975261655566128
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2613,	 Acc = 0.5161
4718 0.396
24253 0.546
11362 0.509
1141 0.456
29 0.0
0 0.0
0 0.0
0 0.0
0.5314938154138915
0.5975261655566128
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3404,	 Acc1 = 0.2981,	 Acc2 = 0.3032

 ===== Epoch 31	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 5 1
train:	 Loss = 1.2413,	 Acc = 0.5308
40152 0.287
78857 0.595
41020 0.632
5023 0.644
814 0.515
86 0.419
0 0.0
0 0.0
0.6086724960254372
0.5975261655566128
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2385,	 Acc = 0.5446
4718 0.383
24253 0.575
11362 0.556
1141 0.461
29 0.0
0 0.0
0 0.0
0 0.0
0.5652847628109283
0.5975261655566128
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3145,	 Acc1 = 0.2798,	 Acc2 = 0.2811

 ===== Epoch 32	 =====
[-0.36759388 -0.3845501   2.709413    2.1783803  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.18161772  0.40253118  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 6 4
train:	 Loss = 1.2400,	 Acc = 0.5309
40149 0.285
78858 0.596
41022 0.632
5023 0.645
814 0.532
86 0.477
0 0.0
0 0.0
0.6093256917561585
0.5975261655566128
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2410,	 Acc = 0.5350
4718 0.407
24253 0.575
11362 0.512
1141 0.457
29 0.0
0 0.0
0 0.0
0 0.0
0.5514204159304064
0.5975261655566128
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3186,	 Acc1 = 0.2670,	 Acc2 = 0.2657

 ===== Epoch 33	 =====
[ 2.72068     2.0547302  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
  2.0068371   2.2851815 ] [-0.82207996  0.35484028  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.7087937   0.47406512] 1 1
train:	 Loss = 1.2398,	 Acc = 0.5313
40156 0.288
78853 0.595
41017 0.634
5027 0.642
813 0.518
86 0.523
0 0.0
0 0.0
0.6090575217018029
0.5975261655566128
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.1878,	 Acc = 0.5696
4718 0.397
24253 0.578
11362 0.625
1141 0.562
29 0.0
0 0.0
0 0.0
0 0.0
0.5917901318472203
0.5975261655566128
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2721,	 Acc1 = 0.2624,	 Acc2 = 0.2602

 ===== Epoch 34	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705  2.823531    2.103311
 -0.36039132 -0.36746153  2.8112304   1.021522   -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  9.7930634e-01 -6.7221622e+00 -1.1342351e-02 -8.0557177e-03
  1.0869756e+00  3.9832592e-01 -2.1558569e-03  7.0730671e-03
 -1.2275568e-02 -1.3029240e-02] 2 2
train:	 Loss = 1.2394,	 Acc = 0.5307
40160 0.285
78851 0.595
41015 0.634
5026 0.649
814 0.528
86 0.465
0 0.0
0 0.0
0.6092756296107861
0.5975261655566128
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.1867,	 Acc = 0.5637
4718 0.397
24253 0.586
11362 0.593
1141 0.498
29 0.0
0 0.0
0 0.0
0 0.0
0.5851026233519098
0.5975261655566128
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2704,	 Acc1 = 0.2839,	 Acc2 = 0.2860

 ===== Epoch 35	 =====
[ 1.9883437   1.1890149  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153  1.9567974   3.291119   -0.42787513 -0.42500633
  2.3048663   1.3540279 ] [ 0.2509766   0.1909815   0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.2921718   0.03943231 -0.00215586  0.00707307
  0.16820547  0.23352468] 2 2
train:	 Loss = 1.2385,	 Acc = 0.5302
40153 0.286
78851 0.593
41021 0.636
5027 0.641
814 0.511
86 0.419
0 0.0
0 0.0
0.6083514177378199
0.5975261655566128
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2234,	 Acc = 0.5418
4718 0.383
24253 0.565
11362 0.563
1141 0.52
29 0.0
0 0.0
0 0.0
0 0.0
0.5622128585021068
0.5975261655566128
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2950,	 Acc1 = 0.3047,	 Acc2 = 0.3111

 ===== Epoch 36	 =====
[-0.36759388 -0.3845501   1.8404897   0.8917942  -0.4375299  -0.36171135
  1.4914572   3.9279706  -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  5.4333526e-01  6.7844880e-01
  4.3038260e-03  3.1008099e-03 -2.2237513e+00 -4.7850447e+00
  1.4306194e-02  2.5413027e-02 -2.1558569e-03  7.0730671e-03
 -1.2275568e-02 -1.3029240e-02] 2 2
train:	 Loss = 1.2380,	 Acc = 0.5306
40152 0.285
78856 0.595
41019 0.635
5025 0.643
814 0.516
86 0.488
0 0.0
0 0.0
0.6090381558028617
0.5975261655566128
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.1890,	 Acc = 0.5531
4718 0.38
24253 0.571
11362 0.595
1141 0.479
29 0.0
0 0.0
0 0.0
0 0.0
0.575370395541661
0.5975261655566128
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2783,	 Acc1 = 0.2837,	 Acc2 = 0.2858

 ===== Epoch 37	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705  1.7352372   2.8459494
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  5.1928587e-02 -6.7589126e+00 -1.1342351e-02 -8.0557177e-03
  1.4306194e-02  2.5413027e-02 -2.1558569e-03  7.0730671e-03
 -1.2275568e-02 -1.3029240e-02] 2 2
train:	 Loss = 1.2394,	 Acc = 0.5295
40157 0.285
78845 0.594
41024 0.631
5026 0.641
814 0.516
86 0.442
0 0.0
0 0.0
0.6074486267339719
0.5975261655566128
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.1972,	 Acc = 0.5623
4718 0.418
24253 0.576
11362 0.599
1141 0.519
29 0.0
0 0.0
0 0.0
0 0.0
0.5808889492999864
0.5975261655566128
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2879,	 Acc1 = 0.2494,	 Acc2 = 0.2445

 ===== Epoch 38	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153  2.5433187   2.1231248  -0.42787513 -0.42500633
  3.5844567   1.9189974 ] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.28300437  0.03662845 -0.00215586  0.00707307
  0.5693904   0.11024773] 3 3
train:	 Loss = 1.2391,	 Acc = 0.5299
40149 0.284
78855 0.594
41022 0.634
5026 0.646
814 0.506
86 0.453
0 0.0
0 0.0
0.6083638704959341
0.5975261655566128
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2075,	 Acc = 0.5483
4718 0.394
24253 0.576
11362 0.559
1141 0.513
29 0.0
0 0.0
0 0.0
0 0.0
0.568003262199266
0.5975261655566128
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2819,	 Acc1 = 0.2893,	 Acc2 = 0.2925

 ===== Epoch 39	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705  3.1774297   2.3560767
 -0.36039132 -0.36746153  3.042267    1.4936376  -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137 -0.18670338  0.27764547
 -0.01134235 -0.00805572  0.28088528  0.44879532 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 2
train:	 Loss = 1.2389,	 Acc = 0.5289
40156 0.283
78848 0.593
41021 0.634
5027 0.638
814 0.514
86 0.419
0 0.0
0 0.0
0.607435848516646
0.5975261655566128
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2153,	 Acc = 0.5401
4718 0.395
24253 0.561
11362 0.56
1141 0.508
29 0.0
0 0.0
0 0.0
0 0.0
0.5586788092972679
0.5975261655566128
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2977,	 Acc1 = 0.2765,	 Acc2 = 0.2771

 ===== Epoch 40	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 3 1
train:	 Loss = 1.2381,	 Acc = 0.5310
40157 0.283
78853 0.596
41019 0.636
5026 0.648
811 0.51
86 0.453
0 0.0
0 0.0
0.6101593863031122
0.5975261655566128
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2001,	 Acc = 0.5559
4718 0.415
24253 0.575
11362 0.583
1141 0.478
29 0.0
0 0.0
0 0.0
0 0.0
0.573929590865842
0.5975261655566128
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2815,	 Acc1 = 0.2612,	 Acc2 = 0.2587

 ===== Epoch 41	 =====
[-0.36759388 -0.3845501   1.5419524   2.151103   -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.11646637 -0.19599785  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 5 3
train:	 Loss = 1.2389,	 Acc = 0.5289
40155 0.284
78851 0.593
41024 0.633
5023 0.635
814 0.531
85 0.424
0 0.0
0 0.0
0.6070335540593178
0.5975261655566128
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.1719,	 Acc = 0.5631
4718 0.4
24253 0.575
11362 0.614
1141 0.504
29 0.0
0 0.0
0 0.0
0 0.0
0.5840152235965747
0.5975261655566128
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2576,	 Acc1 = 0.2798,	 Acc2 = 0.2811

 ===== Epoch 42	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153  3.2473764   1.7272363  -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -1.1342351e-02 -8.0557177e-03
 -4.4612150e+00 -2.4195495e+00 -2.1558569e-03  7.0730671e-03
  4.7723660e+00  1.6647403e+00] 2 2
train:	 Loss = 1.2381,	 Acc = 0.5302
40152 0.284
78853 0.595
41022 0.633
5025 0.648
814 0.516
86 0.5
0 0.0
0 0.0
0.6087440381558029
0.5975261655566128
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.1936,	 Acc = 0.5498
4718 0.402
24253 0.584
11362 0.546
1141 0.479
29 0.0
0 0.0
0 0.0
0 0.0
0.5687644420280006
0.5975261655566128
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2687,	 Acc1 = 0.2940,	 Acc2 = 0.2982

 ===== Epoch 43	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493  0.8538252   1.5075138
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303  0.3499927   0.1485312
 -0.01227557 -0.01302924] 2 2
train:	 Loss = 1.2382,	 Acc = 0.5295
40155 0.284
78850 0.594
41021 0.633
5026 0.642
814 0.527
86 0.419
0 0.0
0 0.0
0.6078682321518001
0.5975261655566128
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2595,	 Acc = 0.5337
4718 0.395
24253 0.572
11362 0.525
1141 0.396
29 0.0
0 0.0
0 0.0
0 0.0
0.5514476009242898
0.5975261655566128
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3387,	 Acc1 = 0.2730,	 Acc2 = 0.2729

 ===== Epoch 44	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 6 1
train:	 Loss = 1.2383,	 Acc = 0.5301
40150 0.285
78849 0.594
41027 0.633
5027 0.652
813 0.535
86 0.384
0 0.0
0 0.0
0.6084481963720768
0.5975261655566128
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.1817,	 Acc = 0.5783
4718 0.408
24253 0.601
11362 0.609
1141 0.52
29 0.0
0 0.0
0 0.0
0 0.0
0.6001087399755335
0.6001087399755335
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2686,	 Acc1 = 0.2686,	 Acc2 = 0.2676

 ===== Epoch 45	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153  1.2181726   1.2870871  -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572 -0.13379404  0.4824416  -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
train:	 Loss = 1.2388,	 Acc = 0.5291
40160 0.282
78847 0.593
41020 0.634
5025 0.649
814 0.507
86 0.419
0 0.0
0 0.0
0.6079559908420249
0.6001087399755335
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.1930,	 Acc = 0.5608
4718 0.39
24253 0.583
11362 0.59
1141 0.521
29 0.0
0 0.0
0 0.0
0 0.0
0.5826287889085225
0.6001087399755335
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2732,	 Acc1 = 0.2965,	 Acc2 = 0.3012

 ===== Epoch 46	 =====
[-0.36759388 -0.3845501   3.2683      0.9986308  -0.4375299  -0.36171135
  3.887591    2.6736026  -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02 -4.6672282e+00 -1.2296276e+00
  2.9339666e+00  1.7476482e+00 -5.0864196e+00 -3.3900504e+00
  1.4306194e-02  2.5413027e-02 -2.1558569e-03  7.0730671e-03
 -1.2275568e-02 -1.3029240e-02] 6 6
train:	 Loss = 1.2376,	 Acc = 0.5293
40157 0.284
78847 0.593
41022 0.634
5026 0.642
814 0.511
86 0.395
0 0.0
0 0.0
0.6077109583051791
0.6001087399755335
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2302,	 Acc = 0.5289
4718 0.392
24253 0.558
11362 0.533
1141 0.45
29 0.0
0 0.0
0 0.0
0 0.0
0.5465271170313987
0.6001087399755335
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 6
Testing:	 Loss = 1.3025,	 Acc1 = 0.3146,	 Acc2 = 0.3231

 ===== Epoch 47	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.1834266e+00  2.2108529e+00
  4.3038260e-03  3.1008099e-03 -1.1342351e-02 -8.0557177e-03
  1.4306194e-02  2.5413027e-02 -2.1558569e-03  7.0730671e-03
 -1.2275568e-02 -1.3029240e-02] 6 6
train:	 Loss = 1.2387,	 Acc = 0.5295
40157 0.284
78850 0.593
41019 0.634
5027 0.642
813 0.526
86 0.488
0 0.0
0 0.0
0.6078778965777654
0.6001087399755335
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.1853,	 Acc = 0.5525
4718 0.402
24253 0.556
11362 0.613
1141 0.499
29 0.0
0 0.0
0 0.0
0 0.0
0.5717819763490554
0.6001087399755335
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2716,	 Acc1 = 0.2548,	 Acc2 = 0.2510

 ===== Epoch 48	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 0 1
train:	 Loss = 1.2377,	 Acc = 0.5309
40159 0.281
78846 0.597
41022 0.634
5026 0.652
813 0.533
86 0.442
0 0.0
0 0.0
0.6106381118186227
0.6001087399755335
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.1954,	 Acc = 0.5566
4718 0.401
24253 0.563
11362 0.612
1141 0.53
29 0.0
0 0.0
0 0.0
0 0.0
0.5766480902541797
0.6001087399755335
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 1
Testing:	 Loss = 1.2716,	 Acc1 = 0.2678,	 Acc2 = 0.2667

 ===== Epoch 49	 =====
[ 2.8182411   1.9426965  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  1.7015734   1.3314569  -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-2.92548    -2.213887    0.01587301  0.02049137  0.00430383  0.00310081
 -2.4747777  -1.8974384   0.01430619  0.02541303  2.3569393   2.7640631
 -0.01227557 -0.01302924] 5 5
train:	 Loss = 1.2386,	 Acc = 0.5291
40154 0.283
78861 0.593
41013 0.633
5024 0.642
814 0.526
86 0.477
0 0.0
0 0.0
0.6077203135184979
0.6001087399755335
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2032,	 Acc = 0.5559
4718 0.406
24253 0.566
11362 0.604
1141 0.486
29 0.0
0 0.0
0 0.0
0 0.0
0.5751257305967106
0.6001087399755335
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 1
Testing:	 Loss = 1.2964,	 Acc1 = 0.2525,	 Acc2 = 0.2483

 ===== Epoch 50	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [ 1.7556589   1.6753492   0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 0 6
train:	 Loss = 1.2364,	 Acc = 0.5298
40161 0.281
78850 0.595
41018 0.634
5023 0.644
814 0.531
86 0.384
0 0.0
0 0.0
0.6090737811131162
0.6001087399755335
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2343,	 Acc = 0.5498
4718 0.404
24253 0.579
11362 0.558
1141 0.471
29 0.0
0 0.0
0 0.0
0 0.0
0.5685197770830501
0.6001087399755335
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3197,	 Acc1 = 0.2373,	 Acc2 = 0.2299

 ===== Epoch 51	 =====
[-0.36759388 -0.3845501   1.7719125   2.230662   -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.07221083  0.5808164   0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
train:	 Loss = 1.2378,	 Acc = 0.5299
40158 0.283
78851 0.595
41017 0.634
5027 0.647
813 0.508
86 0.43
0 0.0
0 0.0
0.6088128209612541
0.6001087399755335
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2344,	 Acc = 0.5436
4718 0.396
24253 0.571
11362 0.557
1141 0.457
29 0.0
0 0.0
0 0.0
0 0.0
0.562511893434824
0.6001087399755335
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3132,	 Acc1 = 0.2641,	 Acc2 = 0.2622

 ===== Epoch 52	 =====
[-0.36759388 -0.3845501   3.782017   -4.2204537   2.3667026   1.2286978
 -0.36039132 -0.36746153  1.0695248   1.21086    -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02 -5.3192606e+00  3.6435022e+00
 -1.1057674e-02  7.2190852e+00 -1.1342351e-02 -8.0557177e-03
  5.7356268e-01  6.9553465e-01 -2.1558569e-03  7.0730671e-03
 -1.2275568e-02 -1.3029240e-02] 2 2
train:	 Loss = 1.2368,	 Acc = 0.5306
40152 0.283
78855 0.595
41018 0.635
5027 0.643
814 0.531
86 0.384
0 0.0
0 0.0
0.6096025437201907
0.6001087399755335
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.1974,	 Acc = 0.5556
4718 0.404
24253 0.553
11362 0.637
1141 0.45
29 0.0
0 0.0
0 0.0
0 0.0
0.5751257305967106
0.6001087399755335
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2830,	 Acc1 = 0.2540,	 Acc2 = 0.2500

 ===== Epoch 53	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705  1.828917    0.85737866
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137 -0.02846916  0.00958612
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 1 0
train:	 Loss = 1.2369,	 Acc = 0.5297
40154 0.282
78853 0.594
41021 0.635
5024 0.646
814 0.52
86 0.43
0 0.0
0 0.0
0.6086265282436922
0.6001087399755335
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2675,	 Acc = 0.5355
4718 0.41
24253 0.573
11362 0.518
1141 0.449
29 0.0
0 0.0
0 0.0
0 0.0
0.5515291559059399
0.6001087399755335
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3366,	 Acc1 = 0.2835,	 Acc2 = 0.2855

 ===== Epoch 54	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 1 1
train:	 Loss = 1.2371,	 Acc = 0.5297
40156 0.282
78852 0.595
41018 0.632
5026 0.653
814 0.538
86 0.442
0 0.0
0 0.0
0.6088269897294032
0.6001087399755335
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2040,	 Acc = 0.5445
4718 0.4
24253 0.561
11362 0.577
1141 0.487
29 0.0
0 0.0
0 0.0
0 0.0
0.5630827783063749
0.6001087399755335
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2898,	 Acc1 = 0.2517,	 Acc2 = 0.2473

 ===== Epoch 55	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493  2.165925    1.5810869
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
  2.4003696   2.816571    0.01430619  0.02541303  0.6098305   0.6883816
 -0.01227557 -0.01302924] 2 2
train:	 Loss = 1.2374,	 Acc = 0.5290
40159 0.281
78848 0.593
41023 0.634
5023 0.647
813 0.52
86 0.453
0 0.0
0 0.0
0.6080704013736853
0.6001087399755335
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2285,	 Acc = 0.5444
4718 0.38
24253 0.563
11362 0.582
1141 0.461
29 0.0
0 0.0
0 0.0
0 0.0
0.5654206877803453
0.6001087399755335
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 1
Testing:	 Loss = 1.3203,	 Acc1 = 0.2571,	 Acc2 = 0.2537

 ===== Epoch 56	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705  1.8019314   2.790028
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  8.1953651e-01  1.3713048e-01 -1.1342351e-02 -8.0557177e-03
  1.8268378e+00  2.8320732e+00 -2.1558569e-03  7.0730671e-03
 -1.2275568e-02 -1.3029240e-02] 2 2
train:	 Loss = 1.2367,	 Acc = 0.5304
40154 0.283
78859 0.595
41014 0.634
5025 0.652
814 0.516
86 0.523
0 0.0
0 0.0
0.6093181131655511
0.6001087399755335
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.1803,	 Acc = 0.5559
4718 0.404
24253 0.574
11362 0.587
1141 0.515
29 0.0
0 0.0
0 0.0
0 0.0
0.575370395541661
0.6001087399755335
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2648,	 Acc1 = 0.2802,	 Acc2 = 0.2816

 ===== Epoch 57	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493  2.165925    1.5810869
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
  1.9810736   2.4827802   0.01430619  0.02541303  0.28600976  0.46031648
 -0.01227557 -0.01302924] 2 2
train:	 Loss = 1.2364,	 Acc = 0.5307
40156 0.283
78848 0.594
41023 0.638
5025 0.643
814 0.516
86 0.477
0 0.0
0 0.0
0.6098922064294572
0.6001087399755335
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2030,	 Acc = 0.5603
4718 0.402
24253 0.581
11362 0.587
1141 0.534
29 0.0
0 0.0
0 0.0
0 0.0
0.5805627293733859
0.6001087399755335
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2850,	 Acc1 = 0.2909,	 Acc2 = 0.2945

 ===== Epoch 58	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  2.4011912   1.5126748  -0.4089302  -0.41695493  2.7107      2.2849362
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -3.3106132e+00 -2.0989723e+00
  1.4306194e-02  2.5413027e-02  1.7380103e-02 -8.3012813e-01
 -1.2275568e-02 -1.3029240e-02] 0 5
train:	 Loss = 1.2372,	 Acc = 0.5304
40156 0.283
78848 0.594
41023 0.636
5025 0.649
814 0.504
86 0.453
0 0.0
0 0.0
0.6095106362682439
0.6001087399755335
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2183,	 Acc = 0.5548
4718 0.376
24253 0.569
11362 0.606
1141 0.494
29 0.0
0 0.0
0 0.0
0 0.0
0.5777898599972815
0.6001087399755335
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2924,	 Acc1 = 0.3012,	 Acc2 = 0.3069

 ===== Epoch 59	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153  3.3441684   1.4272463  -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.3789168   0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 0 3
train:	 Loss = 1.2369,	 Acc = 0.5305
40158 0.283
78855 0.595
41015 0.636
5025 0.652
814 0.489
85 0.4
0 0.0
0 0.0
0.6095203268836351
0.6001087399755335
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2637,	 Acc = 0.5360
4718 0.371
24253 0.568
11362 0.548
1141 0.434
29 0.0
0 0.0
0 0.0
0 0.0
0.5571292646459155
0.6001087399755335
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3467,	 Acc1 = 0.2443,	 Acc2 = 0.2383

 ===== Epoch 60	 =====
[-0.36759388 -0.3845501   2.706949    2.2761245  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.3593499  -0.16416119  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 2 3
train:	 Loss = 1.2363,	 Acc = 0.5301
40157 0.282
78851 0.594
41020 0.636
5025 0.655
813 0.529
86 0.442
0 0.0
0 0.0
0.6092293016415596
0.6001087399755335
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.1851,	 Acc = 0.5679
4718 0.388
24253 0.582
11362 0.619
1141 0.516
29 0.0
0 0.0
0 0.0
0 0.0
0.5909745820307191
0.6001087399755335
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2696,	 Acc1 = 0.2666,	 Acc2 = 0.2652

 ===== Epoch 61	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705  1.6650751   3.0025299
 -0.36039132 -0.36746153  1.4757113   1.0043095  -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  1.6253646e-01 -1.5058140e+00 -1.1342351e-02 -8.0557177e-03
 -2.2925992e+00 -1.5952159e+00 -2.1558569e-03  7.0730671e-03
 -1.2275568e-02 -1.3029240e-02] 5 5
train:	 Loss = 1.2367,	 Acc = 0.5310
40159 0.282
78849 0.596
41020 0.636
5024 0.65
814 0.512
86 0.488
0 0.0
0 0.0
0.6103757760765702
0.6001087399755335
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2320,	 Acc = 0.5183
4718 0.262
24253 0.544
11362 0.572
1141 0.508
29 0.0
0 0.0
0 0.0
0 0.0
0.5512301209732228
0.6001087399755335
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3160,	 Acc1 = 0.2465,	 Acc2 = 0.2734

 ===== Epoch 62	 =====
[ 2.5081773   2.2100496  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
  2.207722    2.4395018 ] [-0.36921832  0.09218431  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.21181111  0.12828825] 3 1
train:	 Loss = 1.2358,	 Acc = 0.5308
40155 0.282
78851 0.596
41020 0.636
5026 0.644
814 0.52
86 0.453
0 0.0
0 0.0
0.6102053308107507
0.6001087399755335
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2090,	 Acc = 0.5484
4718 0.412
24253 0.567
11362 0.577
1141 0.443
29 0.0
0 0.0
0 0.0
0 0.0
0.5658828326763626
0.6001087399755335
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2840,	 Acc1 = 0.2664,	 Acc2 = 0.2649

 ===== Epoch 63	 =====
[ 0.21435696  2.2660666  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-5.4242641e-01 -2.5199175e+00  1.5873009e-02  2.0491373e-02
  4.7907724e+00  3.0620036e+00 -1.1342351e-02 -8.0557177e-03
  3.5821829e+00  3.5919180e+00 -2.1558569e-03  7.0730671e-03
 -1.2275568e-02 -1.3029240e-02] 5 5
train:	 Loss = 1.2358,	 Acc = 0.5314
40157 0.283
78847 0.596
41022 0.636
5026 0.652
814 0.529
86 0.477
0 0.0
0 0.0
0.6106125044715609
0.6001087399755335
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.1864,	 Acc = 0.5571
4718 0.396
24253 0.563
11362 0.617
1141 0.51
29 0.0
0 0.0
0 0.0
0 0.0
0.577681120021748
0.6001087399755335
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2745,	 Acc1 = 0.2971,	 Acc2 = 0.3019

 ===== Epoch 64	 =====
[ 1.0148758   1.6244187  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
  0.85252523  1.9373066 ] [ 0.15554802  1.3596802   0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.04932433  1.7188617 ] 6 4
train:	 Loss = 1.2367,	 Acc = 0.5309
40147 0.283
78855 0.594
41024 0.637
5026 0.652
814 0.521
86 0.442
0 0.0
0 0.0
0.6098883192241962
0.6001087399755335
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.1979,	 Acc = 0.5370
4718 0.382
24253 0.554
11362 0.578
1141 0.434
29 0.0
0 0.0
0 0.0
0 0.0
0.556884599700965
0.6001087399755335
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 1
Testing:	 Loss = 1.2767,	 Acc1 = 0.2983,	 Acc2 = 0.3034

 ===== Epoch 65	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 0 1
train:	 Loss = 1.2386,	 Acc = 0.5296
40153 0.282
78855 0.595
41020 0.633
5025 0.646
813 0.514
86 0.477
0 0.0
0 0.0
0.6086375885340901
0.6001087399755335
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2020,	 Acc = 0.5558
4718 0.399
24253 0.563
11362 0.607
1141 0.557
29 0.0
0 0.0
0 0.0
0 0.0
0.5759684654070952
0.6001087399755335
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2876,	 Acc1 = 0.2503,	 Acc2 = 0.2455

 ===== Epoch 66	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705  0.10530255  2.868318
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
 -7.1675265e-01 -3.1184936e+00 -1.1342351e-02 -8.0557177e-03
  1.4306194e-02  2.5413027e-02 -2.1558569e-03  7.0730671e-03
 -1.2275568e-02 -1.3029240e-02] 5 1
train:	 Loss = 1.2351,	 Acc = 0.5315
40156 0.282
78851 0.597
41019 0.636
5026 0.655
814 0.517
86 0.419
0 0.0
0 0.0
0.6111561575884766
0.6001087399755335
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2150,	 Acc = 0.5537
4718 0.396
24253 0.569
11362 0.591
1141 0.532
29 0.0
0 0.0
0 0.0
0 0.0
0.573929590865842
0.6001087399755335
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2893,	 Acc1 = 0.2870,	 Acc2 = 0.2898

 ===== Epoch 67	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493  1.8341907   2.5350847
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.1833576  -0.12861125
 -0.01227557 -0.01302924] 3 3
train:	 Loss = 1.2360,	 Acc = 0.5306
40160 0.283
78849 0.594
41018 0.638
5026 0.644
813 0.502
86 0.488
0 0.0
0 0.0
0.6095618163317222
0.6001087399755335
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.1582,	 Acc = 0.5745
4718 0.392
24253 0.59
11362 0.622
1141 0.536
29 0.0
0 0.0
0 0.0
0 0.0
0.59790675547098
0.6001087399755335
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2447,	 Acc1 = 0.2717,	 Acc2 = 0.2714

 ===== Epoch 68	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 5 1
train:	 Loss = 1.2347,	 Acc = 0.5314
40161 0.283
78853 0.595
41016 0.638
5022 0.652
814 0.514
86 0.465
0 0.0
0 0.0
0.610703468451638
0.6001087399755335
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2393,	 Acc = 0.5457
4718 0.389
24253 0.575
11362 0.56
1141 0.438
29 0.0
0 0.0
0 0.0
0 0.0
0.5658828326763626
0.6001087399755335
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3091,	 Acc1 = 0.3165,	 Acc2 = 0.3253

 ===== Epoch 69	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -1.1342351e-02 -8.0557177e-03
  1.4306194e-02  2.5413027e-02 -2.1558569e-03  7.0730671e-03
  2.2509272e+00  2.0947061e+00] 4 6
train:	 Loss = 1.2358,	 Acc = 0.5311
40160 0.283
78851 0.595
41019 0.638
5023 0.653
813 0.52
86 0.407
0 0.0
0 0.0
0.6104044772322564
0.6001087399755335
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2123,	 Acc = 0.5454
4718 0.404
24253 0.56
11362 0.584
1141 0.45
29 0.0
0 0.0
0 0.0
0 0.0
0.5634905532146255
0.6001087399755335
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2942,	 Acc1 = 0.2672,	 Acc2 = 0.2659

 ===== Epoch 70	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 6 1
train:	 Loss = 1.2363,	 Acc = 0.5307
40159 0.282
78856 0.595
41016 0.636
5021 0.651
814 0.514
86 0.43
0 0.0
0 0.0
0.6101770368780457
0.6001087399755335
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.1902,	 Acc = 0.5612
4718 0.405
24253 0.576
11362 0.599
1141 0.524
29 0.0
0 0.0
0 0.0
0 0.0
0.5811879842327036
0.6001087399755335
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2604,	 Acc1 = 0.3167,	 Acc2 = 0.3255

 ===== Epoch 71	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 0 1
train:	 Loss = 1.2354,	 Acc = 0.5316
40156 0.284
78855 0.595
41015 0.638
5026 0.647
814 0.522
86 0.477
0 0.0
0 0.0
0.6108063849406976
0.6001087399755335
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2219,	 Acc = 0.5487
4718 0.395
24253 0.564
11362 0.595
1141 0.42
29 0.0
0 0.0
0 0.0
0 0.0
0.5684110371075166
0.6001087399755335
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3031,	 Acc1 = 0.2602,	 Acc2 = 0.2575

 ===== Epoch 72	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493  1.0524093   0.8134742
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -1.1342351e-02 -8.0557177e-03
  1.4306194e-02  2.5413027e-02 -1.9026277e+00 -1.4508117e+00
  2.2080574e+00  4.1002116e+00] 5 5
train:	 Loss = 1.2354,	 Acc = 0.5309
40156 0.281
78852 0.595
41018 0.637
5026 0.658
814 0.526
86 0.419
0 0.0
0 0.0
0.6105758529682979
0.6001087399755335
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.1908,	 Acc = 0.5501
4718 0.381
24253 0.578
11362 0.567
1141 0.491
29 0.0
0 0.0
0 0.0
0 0.0
0.5717547913551719
0.6001087399755335
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 1
Testing:	 Loss = 1.2813,	 Acc1 = 0.2630,	 Acc2 = 0.2609

 ===== Epoch 73	 =====
[-0.36759388 -0.3845501   1.6688408   3.0330737   2.2117283   0.79027283
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.08941063  0.03534847  0.27877823 -0.00122273
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 3 0
train:	 Loss = 1.2362,	 Acc = 0.5304
40157 0.281
78853 0.594
41019 0.638
5023 0.653
814 0.502
86 0.465
0 0.0
0 0.0
0.6098334591994913
0.6001087399755335
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2220,	 Acc = 0.5514
4718 0.396
24253 0.575
11362 0.567
1141 0.553
29 0.0
0 0.0
0 0.0
0 0.0
0.5713198314530379
0.6001087399755335
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3133,	 Acc1 = 0.2292,	 Acc2 = 0.2202

 ===== Epoch 74	 =====
[-0.36759388 -0.3845501   2.0310295   0.8758823  -0.4375299  -0.36171135
  5.419594    2.2573674  -0.4089302  -0.41695493  4.51926     2.2653167
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.0190486   0.08840957  0.00430383  0.00310081
 -0.05478792  0.15883973  0.01430619  0.02541303  1.32829     0.18894781
 -0.01227557 -0.01302924] 3 3
train:	 Loss = 1.2361,	 Acc = 0.5298
40154 0.283
78857 0.594
41016 0.634
5025 0.652
814 0.494
86 0.477
0 0.0
0 0.0
0.6086344774956677
0.6001087399755335
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2362,	 Acc = 0.5332
4718 0.407
24253 0.553
11362 0.556
1141 0.422
29 0.0
0 0.0
0 0.0
0 0.0
0.5494630963708033
0.6001087399755335
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3127,	 Acc1 = 0.3117,	 Acc2 = 0.3196

 ===== Epoch 75	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 1 1
train:	 Loss = 1.2358,	 Acc = 0.5315
40157 0.283
78861 0.595
41011 0.639
5023 0.653
814 0.525
86 0.453
0 0.0
0 0.0
0.6107953416272507
0.6001087399755335
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2109,	 Acc = 0.5470
4718 0.403
24253 0.574
11362 0.563
1141 0.434
29 0.0
0 0.0
0 0.0
0 0.0
0.5655294277558788
0.6001087399755335
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2958,	 Acc1 = 0.2709,	 Acc2 = 0.2704

 ===== Epoch 76	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705  3.6400404   2.4142349
 -0.36039132 -0.36746153  3.905917    1.7124827  -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.6648895   0.00310081
 -0.01134235 -0.00805572  0.4924577   0.0338246  -0.00215586  0.00707307
 -0.01227557 -0.01302924] 0 0
train:	 Loss = 1.2351,	 Acc = 0.5319
40155 0.283
78858 0.596
41014 0.639
5025 0.651
814 0.514
86 0.36
0 0.0
0 0.0
0.6112625897278949
0.6001087399755335
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2345,	 Acc = 0.5333
4718 0.411
24253 0.56
11362 0.532
1141 0.504
29 0.0
0 0.0
0 0.0
0 0.0
0.5490009514747859
0.6001087399755335
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3031,	 Acc1 = 0.3152,	 Acc2 = 0.3238

 ===== Epoch 77	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705  3.1122787   1.5105426
 -0.36039132 -0.36746153  2.3364816   1.21086    -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137 -0.36593142  0.56948423
 -0.01134235 -0.00805572 -0.31292617  0.7768465  -0.00215586  0.00707307
 -0.01227557 -0.01302924] 6 4
train:	 Loss = 1.2362,	 Acc = 0.5310
40156 0.283
78853 0.596
41019 0.636
5024 0.652
814 0.507
86 0.407
0 0.0
0 0.0
0.6103373716175395
0.6001087399755335
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2223,	 Acc = 0.5410
4718 0.398
24253 0.569
11362 0.554
1141 0.415
29 0.0
0 0.0
0 0.0
0 0.0
0.559412804132119
0.6001087399755335
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3021,	 Acc1 = 0.2860,	 Acc2 = 0.2885

 ===== Epoch 78	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 1 1
train:	 Loss = 1.2355,	 Acc = 0.5313
40153 0.284
78856 0.595
41018 0.637
5026 0.657
813 0.517
86 0.477
0 0.0
0 0.0
0.6103705116892821
0.6001087399755335
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2245,	 Acc = 0.5456
4718 0.402
24253 0.558
11362 0.586
1141 0.477
29 0.0
0 0.0
0 0.0
0 0.0
0.5640886230800598
0.6001087399755335
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3076,	 Acc1 = 0.2777,	 Acc2 = 0.2786

 ===== Epoch 79	 =====
[-0.36759388 -0.3845501   2.0749683   2.6511896  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  2.3061010e-01 -1.7262794e+00
  4.3038260e-03  3.1008099e-03  2.5376072e+00  4.0572658e+00
  1.4306194e-02  2.5413027e-02 -2.1558569e-03  7.0730671e-03
 -1.2275568e-02 -1.3029240e-02] 5 5
train:	 Loss = 1.2352,	 Acc = 0.5312
40156 0.283
78848 0.595
41022 0.638
5026 0.651
814 0.511
86 0.453
0 0.0
0 0.0
0.6106473973735254
0.6001087399755335
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2156,	 Acc = 0.5590
4718 0.406
24253 0.579
11362 0.59
1141 0.474
29 0.0
0 0.0
0 0.0
0 0.0
0.5786869647954329
0.6001087399755335
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2892,	 Acc1 = 0.2888,	 Acc2 = 0.2920

 ===== Epoch 80	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  2.2568815   1.150239   -0.4089302  -0.41695493  2.9983034   1.7184234
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -3.1382060e+00 -1.6959043e+00
  3.2486041e+00  2.4759834e+00 -4.4008756e+00 -2.5160782e+00
  5.2317715e+00  1.5955849e+00] 5 6
train:	 Loss = 1.2347,	 Acc = 0.5316
40154 0.285
78850 0.595
41023 0.636
5025 0.655
814 0.504
86 0.477
0 0.0
0 0.0
0.6102640741506224
0.6001087399755335
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2141,	 Acc = 0.5540
4718 0.4
24253 0.556
11362 0.614
1141 0.564
29 0.0
0 0.0
0 0.0
0 0.0
0.573712110914775
0.6001087399755335
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 1
Testing:	 Loss = 1.3064,	 Acc1 = 0.2505,	 Acc2 = 0.2458

 ===== Epoch 81	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  1.061988    2.6537817  -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [ 2.8537395e+00  2.8344092e+00  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03  1.5279104e-01  1.1192759e+00
  1.4306194e-02  2.5413027e-02 -2.1558569e-03  7.0730671e-03
 -1.2275568e-02 -1.3029240e-02] 4 4
train:	 Loss = 1.2348,	 Acc = 0.5315
40158 0.284
78849 0.595
41020 0.637
5026 0.653
813 0.512
86 0.43
0 0.0
0 0.0
0.610617358538563
0.6001087399755335
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2028,	 Acc = 0.5579
4718 0.412
24253 0.573
11362 0.598
1141 0.446
29 0.0
0 0.0
0 0.0
0 0.0
0.5766752752480631
0.6001087399755335
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 6
Testing:	 Loss = 1.2867,	 Acc1 = 0.2569,	 Acc2 = 0.2535

 ===== Epoch 82	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  1.257925    1.8557367
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 5 6
train:	 Loss = 1.2335,	 Acc = 0.5319
40159 0.283
78847 0.596
41021 0.638
5025 0.654
814 0.51
86 0.453
0 0.0
0 0.0
0.611385371205075
0.6001087399755335
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.1731,	 Acc = 0.5696
4718 0.399
24253 0.587
11362 0.614
1141 0.472
29 0.0
0 0.0
0 0.0
0 0.0
0.5914639119206199
0.6001087399755335
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2575,	 Acc1 = 0.2744,	 Acc2 = 0.2746

 ===== Epoch 83	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [ 0.4887158   0.812681    0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 6 1
train:	 Loss = 1.2356,	 Acc = 0.5299
40153 0.283
78849 0.594
41025 0.634
5025 0.657
814 0.51
86 0.43
0 0.0
0 0.0
0.6086773344780165
0.6001087399755335
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2125,	 Acc = 0.5561
4718 0.407
24253 0.572
11362 0.597
1141 0.443
29 0.0
0 0.0
0 0.0
0 0.0
0.5752344705722441
0.6001087399755335
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2954,	 Acc1 = 0.2816,	 Acc2 = 0.2833

 ===== Epoch 84	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705  0.8512637   0.80816776
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137 -0.01925132  0.01174789
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 2 0
train:	 Loss = 1.2362,	 Acc = 0.5298
40151 0.282
78858 0.593
41018 0.637
5025 0.646
814 0.518
86 0.407
0 0.0
0 0.0
0.6088981804596147
0.6001087399755335
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.1948,	 Acc = 0.5274
4718 0.151
24253 0.565
11362 0.607
1141 0.498
29 0.0
0 0.0
0 0.0
0 0.0
0.5756694304743781
0.6001087399755335
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2810,	 Acc1 = 0.2408,	 Acc2 = 0.2587

 ===== Epoch 85	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 3
train:	 Loss = 1.2343,	 Acc = 0.5305
40156 0.282
78851 0.594
41018 0.636
5027 0.655
814 0.521
86 0.419
0 0.0
0 0.0
0.6097411682406436
0.6001087399755335
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2050,	 Acc = 0.5585
4718 0.415
24253 0.572
11362 0.602
1141 0.452
29 0.0
0 0.0
0 0.0
0 0.0
0.5769199401930134
0.6001087399755335
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2996,	 Acc1 = 0.2383,	 Acc2 = 0.2311

 ===== Epoch 86	 =====
[ 4.250734    1.7033516  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  1.8851357   2.229052   -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-4.2364855e+00 -1.9873763e+00  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -2.6940801e+00 -2.8956621e+00
  1.4306194e-02  2.5413027e-02  3.6571021e+00  2.9921286e+00
 -1.2275568e-02 -1.3029240e-02] 5 6
train:	 Loss = 1.2348,	 Acc = 0.5311
40154 0.283
78851 0.595
41022 0.636
5026 0.652
813 0.509
86 0.453
0 0.0
0 0.0
0.6102799726545732
0.6001087399755335
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2677,	 Acc = 0.5100
4718 0.381
24253 0.536
11362 0.51
1141 0.509
29 0.0
0 0.0
0 0.0
0 0.0
0.5265733315210004
0.6001087399755335
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3363,	 Acc1 = 0.3029,	 Acc2 = 0.3089

 ===== Epoch 87	 =====
[-0.36759388 -0.3845501   3.0067194   1.7874036  -0.4375299  -0.36171135
  2.1477826   3.4154637  -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01378809 -0.14718165  0.00430383  0.00310081
 -0.0899618  -0.2505265   0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 5 5
train:	 Loss = 1.2355,	 Acc = 0.5300
40158 0.283
78856 0.593
41014 0.635
5024 0.653
814 0.531
86 0.43
0 0.0
0 0.0
0.6086697298758288
0.6001087399755335
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.1996,	 Acc = 0.5470
4718 0.412
24253 0.552
11362 0.598
1141 0.499
29 0.0
0 0.0
0 0.0
0 0.0
0.5643604730188936
0.6001087399755335
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 1
Testing:	 Loss = 1.2847,	 Acc1 = 0.2560,	 Acc2 = 0.2525

 ===== Epoch 88	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705  2.6226811   2.403051
 -0.36039132 -0.36746153  1.5575248   1.5797002  -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137 -0.25020096  0.13064517
 -0.01134235 -0.00805572  0.09188375  0.21887912 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 3
train:	 Loss = 1.2353,	 Acc = 0.5313
40151 0.284
78855 0.594
41019 0.638
5027 0.653
814 0.521
86 0.453
0 0.0
0 0.0
0.6102018266945414
0.6001087399755335
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2535,	 Acc = 0.5337
4718 0.404
24253 0.552
11362 0.555
1141 0.477
29 0.0
0 0.0
0 0.0
0 0.0
0.5503330161750714
0.6001087399755335
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 1
Testing:	 Loss = 1.3376,	 Acc1 = 0.2818,	 Acc2 = 0.2835

 ===== Epoch 89	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 3 1
train:	 Loss = 1.2354,	 Acc = 0.5295
40156 0.281
78849 0.594
41022 0.636
5026 0.65
814 0.505
85 0.494
0 0.0
0 0.0
0.6089303316480651
0.6001087399755335
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2231,	 Acc = 0.5569
4718 0.404
24253 0.579
11362 0.578
1141 0.528
29 0.0
0 0.0
0 0.0
0 0.0
0.5764849802908795
0.6001087399755335
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2938,	 Acc1 = 0.3020,	 Acc2 = 0.3079

 ===== Epoch 90	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 1
train:	 Loss = 1.2359,	 Acc = 0.5303
40157 0.282
78851 0.594
41018 0.637
5026 0.647
814 0.52
86 0.488
0 0.0
0 0.0
0.6094518860050081
0.6001087399755335
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.1935,	 Acc = 0.5524
4718 0.409
24253 0.579
11362 0.564
1141 0.473
29 0.0
0 0.0
0 0.0
0 0.0
0.5706945765937202
0.6001087399755335
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2761,	 Acc1 = 0.2676,	 Acc2 = 0.2664

 ===== Epoch 91	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  1.061988    2.6537817  -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
  0.10244671  0.6406323   0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
train:	 Loss = 1.2361,	 Acc = 0.5303
40159 0.282
78851 0.594
41016 0.636
5026 0.653
814 0.514
86 0.442
0 0.0
0 0.0
0.6094615757633572
0.6001087399755335
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2070,	 Acc = 0.5508
4718 0.404
24253 0.57
11362 0.579
1141 0.49
29 0.0
0 0.0
0 0.0
0 0.0
0.5696887318200353
0.6001087399755335
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2938,	 Acc1 = 0.2536,	 Acc2 = 0.2495

 ===== Epoch 92	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705  1.7001561   1.5642272
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137 -0.42738056  0.02904204
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 1 1
train:	 Loss = 1.2351,	 Acc = 0.5309
40159 0.283
78852 0.595
41018 0.637
5024 0.651
813 0.509
86 0.407
0 0.0
0 0.0
0.6099226507039343
0.6001087399755335
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.1730,	 Acc = 0.5728
4718 0.409
24253 0.573
11362 0.642
1141 0.558
29 0.0
0 0.0
0 0.0
0 0.0
0.5937202664129401
0.6001087399755335
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2547,	 Acc1 = 0.2678,	 Acc2 = 0.2667

 ===== Epoch 93	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153  1.2798207   3.1829257  -0.42787513 -0.42500633
  2.0793674   0.9067602 ] [ 1.7788085   1.2102796   0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.17510128  0.11794028 -0.00215586  0.00707307
 -0.07420022  0.19443686] 6 4
train:	 Loss = 1.2361,	 Acc = 0.5308
40152 0.283
78857 0.594
41019 0.638
5024 0.651
814 0.501
86 0.395
0 0.0
0 0.0
0.6098648648648649
0.6001087399755335
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.1890,	 Acc = 0.5608
4718 0.392
24253 0.574
11362 0.613
1141 0.469
29 0.0
0 0.0
0 0.0
0 0.0
0.5823569389696888
0.6001087399755335
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2723,	 Acc1 = 0.2690,	 Acc2 = 0.2681

 ===== Epoch 94	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 1 1
train:	 Loss = 1.2343,	 Acc = 0.5323
40164 0.282
78846 0.596
41019 0.641
5024 0.65
813 0.53
86 0.465
0 0.0
0 0.0
0.6120138645975769
0.6001087399755335
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2145,	 Acc = 0.5564
4718 0.398
24253 0.575
11362 0.591
1141 0.481
29 0.0
0 0.0
0 0.0
0 0.0
0.5767296452358298
0.6001087399755335
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2935,	 Acc1 = 0.2948,	 Acc2 = 0.2992

 ===== Epoch 95	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
  2.0442016   2.622594  ] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
  0.17561391 -0.0491103 ] 3 3
train:	 Loss = 1.2356,	 Acc = 0.5308
40158 0.284
78854 0.594
41016 0.636
5024 0.657
814 0.516
86 0.465
0 0.0
0 0.0
0.6095918724263478
0.6001087399755335
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.1576,	 Acc = 0.5855
4718 0.386
24253 0.601
11362 0.64
1141 0.551
29 0.0
0 0.0
0 0.0
0 0.0
0.6111730324860677
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2452,	 Acc1 = 0.2560,	 Acc2 = 0.2525

 ===== Epoch 96	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  2.7579281   1.2210271  -0.4089302  -0.41695493  1.7101715   2.9495466
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03  2.4382229e-01  9.6812528e-01
  1.4306194e-02  2.5413027e-02 -2.7470994e+00 -3.9653022e+00
 -1.2275568e-02 -1.3029240e-02] 6 4
train:	 Loss = 1.2351,	 Acc = 0.5304
40159 0.282
78851 0.595
41015 0.636
5027 0.651
814 0.514
86 0.477
0 0.0
0 0.0
0.6097795584809965
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.1624,	 Acc = 0.5734
4718 0.411
24253 0.589
11362 0.613
1141 0.518
29 0.0
0 0.0
0 0.0
0 0.0
0.5942095963028409
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2493,	 Acc1 = 0.2701,	 Acc2 = 0.2694

 ===== Epoch 97	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705  1.5609878   1.2533034
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.16099934  0.00526258
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 0 0
train:	 Loss = 1.2359,	 Acc = 0.5310
40152 0.282
78857 0.595
41020 0.637
5023 0.653
814 0.525
86 0.43
0 0.0
0 0.0
0.6103736089030206
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2659,	 Acc = 0.5088
4718 0.4
24253 0.543
11362 0.493
1141 0.405
29 0.0
0 0.0
0 0.0
0 0.0
0.5228489873589779
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3429,	 Acc1 = 0.2682,	 Acc2 = 0.2671

 ===== Epoch 98	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 2 1
train:	 Loss = 1.2348,	 Acc = 0.5321
40152 0.283
78858 0.596
41016 0.637
5026 0.657
814 0.511
86 0.442
0 0.0
0 0.0
0.6116136724960254
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.1844,	 Acc = 0.5568
4718 0.412
24253 0.571
11362 0.591
1141 0.535
29 0.0
0 0.0
0 0.0
0 0.0
0.5753160255538943
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2652,	 Acc1 = 0.2868,	 Acc2 = 0.2895

 ===== Epoch 99	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
  1.0991244   1.3435655 ] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
  0.07822919 -0.05813057] 3 3
train:	 Loss = 1.2337,	 Acc = 0.5312
40156 0.283
78848 0.596
41023 0.637
5025 0.652
814 0.506
86 0.5
0 0.0
0 0.0
0.6105997011033737
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2313,	 Acc = 0.5386
4718 0.38
24253 0.568
11362 0.551
1141 0.459
29 0.0
0 0.0
0 0.0
0 0.0
0.5590322142177518
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3106,	 Acc1 = 0.3018,	 Acc2 = 0.3077

 ===== Epoch 100	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  2.573788    2.5575097  -0.4089302  -0.41695493  2.8803713   3.2659106
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  2.4843621e+00  1.5104467e+00
  4.3038260e-03  3.1008099e-03 -3.5168149e+00 -3.2609427e+00
  1.4306194e-02  2.5413027e-02 -4.2494674e+00 -4.3377123e+00
 -1.2275568e-02 -1.3029240e-02] 6 6
train:	 Loss = 1.2354,	 Acc = 0.5309
40159 0.282
78848 0.596
41018 0.635
5027 0.654
814 0.494
86 0.442
0 0.0
0 0.0
0.6102247342856916
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2350,	 Acc = 0.5380
4718 0.392
24253 0.559
11362 0.561
1141 0.476
29 0.0
0 0.0
0 0.0
0 0.0
0.5567486747315482
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 1
Testing:	 Loss = 1.3177,	 Acc1 = 0.2847,	 Acc2 = 0.2870

 ===== Epoch 101	 =====
[ 5.6517806   3.4118664  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  4.2766533   1.5240011  -0.4089302  -0.41695493  3.4981873   2.1451473
 -0.39281774 -0.38534638] [-5.5187106e+00 -3.6042771e+00  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -5.5512333e+00 -2.1115682e+00
  2.7147374e+00  3.5947220e+00  7.1190852e-01 -8.7920547e-01
  4.5162005e+00  4.0280495e+00] 5 5
train:	 Loss = 1.2345,	 Acc = 0.5317
40157 0.282
78853 0.595
41016 0.639
5027 0.659
813 0.512
86 0.477
0 0.0
0 0.0
0.6113120553281132
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.1782,	 Acc = 0.5627
4718 0.404
24253 0.569
11362 0.621
1141 0.525
29 0.0
0 0.0
0 0.0
0 0.0
0.5829821938290064
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2611,	 Acc1 = 0.2946,	 Acc2 = 0.2990

 ===== Epoch 102	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493  1.2091458   2.2236252
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03  2.1072767e+00  3.5471320e+00
  1.4306194e-02  2.5413027e-02 -2.1038547e+00 -3.1107795e+00
 -1.2275568e-02 -1.3029240e-02] 6 5
train:	 Loss = 1.2349,	 Acc = 0.5314
40156 0.283
78847 0.596
41024 0.636
5025 0.654
814 0.52
86 0.419
0 0.0
0 0.0
0.6107507392921874
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2381,	 Acc = 0.5554
4718 0.397
24253 0.576
11362 0.581
1141 0.529
29 0.0
0 0.0
0 0.0
0 0.0
0.5756422454804948
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3127,	 Acc1 = 0.2649,	 Acc2 = 0.2632

 ===== Epoch 103	 =====
[-0.36759388 -0.3845501   2.9373207   3.098994    2.85283     1.356199
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02 -4.2471342e+00 -3.1907654e+00
 -4.2635638e-01  2.8629252e-01 -1.1342351e-02 -8.0557177e-03
  2.7641041e+00  1.4329488e+00 -2.1558569e-03  7.0730671e-03
 -1.2275568e-02 -1.3029240e-02] 1 4
train:	 Loss = 1.2360,	 Acc = 0.5305
40155 0.281
78854 0.595
41018 0.636
5025 0.654
814 0.515
86 0.442
0 0.0
0 0.0
0.6099986486164217
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2537,	 Acc = 0.5176
4718 0.39
24253 0.527
11362 0.554
1141 0.489
29 0.0
0 0.0
0 0.0
0 0.0
0.5339676498572787
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3229,	 Acc1 = 0.2765,	 Acc2 = 0.2771

 ===== Epoch 104	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  1.2400022e+00  2.5669587e+00 -1.1342351e-02 -8.0557177e-03
  1.4306194e-02  2.5413027e-02 -2.1558569e-03  7.0730671e-03
 -1.2275568e-02 -1.3029240e-02] 6 5
train:	 Loss = 1.2356,	 Acc = 0.5317
40157 0.282
78848 0.597
41021 0.637
5026 0.654
814 0.514
86 0.419
0 0.0
0 0.0
0.6113677014189753
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2211,	 Acc = 0.5607
4718 0.394
24253 0.581
11362 0.594
1141 0.499
29 0.0
0 0.0
0 0.0
0 0.0
0.5820579040369716
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2915,	 Acc1 = 0.2987,	 Acc2 = 0.3039

 ===== Epoch 105	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [ 3.1235960e+00  3.2681530e+00  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -1.1342351e-02 -8.0557177e-03
  1.4306194e-02  2.5413027e-02 -2.1558569e-03  7.0730671e-03
 -1.2275568e-02 -1.3029240e-02] 5 6
train:	 Loss = 1.2352,	 Acc = 0.5304
40157 0.281
78852 0.595
41018 0.636
5025 0.648
814 0.504
86 0.477
0 0.0
0 0.0
0.6100003974720776
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2410,	 Acc = 0.5261
4718 0.403
24253 0.554
11362 0.532
1141 0.397
29 0.0
0 0.0
0 0.0
0 0.0
0.5418241130895746
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3188,	 Acc1 = 0.2936,	 Acc2 = 0.2977

 ===== Epoch 106	 =====
[ 1.7435303   2.2406044  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [ 0.27248     0.11146181  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 2 2
train:	 Loss = 1.2357,	 Acc = 0.5305
40154 0.282
78851 0.595
41024 0.635
5024 0.649
813 0.507
86 0.465
0 0.0
0 0.0
0.6096996772603698
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.1724,	 Acc = 0.5795
4718 0.407
24253 0.595
11362 0.622
1141 0.56
29 0.0
0 0.0
0 0.0
0 0.0
0.601658284626886
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2513,	 Acc1 = 0.2783,	 Acc2 = 0.2793

 ===== Epoch 107	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493  2.411302    1.850855
 -0.39281774 -0.38534638] [ 3.1331241e+00  3.3524921e+00  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03  4.0403199e+00  1.5632808e+00
  1.4306194e-02  2.5413027e-02  7.5342333e-01 -1.3438506e-01
 -1.2275568e-02 -1.3029240e-02] 2 2
train:	 Loss = 1.2346,	 Acc = 0.5306
40154 0.283
78854 0.595
41019 0.636
5027 0.653
813 0.508
85 0.412
0 0.0
0 0.0
0.6095963369846897
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.1848,	 Acc = 0.5440
4718 0.249
24253 0.557
11362 0.642
1141 0.524
29 0.0
0 0.0
0 0.0
0 0.0
0.5818404240859045
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2740,	 Acc1 = 0.2284,	 Acc2 = 0.2515

 ===== Epoch 108	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 1 0
train:	 Loss = 1.2361,	 Acc = 0.5308
40162 0.281
78849 0.596
41014 0.637
5027 0.652
814 0.49
86 0.465
0 0.0
0 0.0
0.6105493282454885
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2285,	 Acc = 0.5345
4718 0.411
24253 0.563
11362 0.533
1141 0.463
29 0.0
0 0.0
0 0.0
0 0.0
0.550305831181188
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3038,	 Acc1 = 0.2882,	 Acc2 = 0.2913

 ===== Epoch 109	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
  1.8969452   1.2624818 ] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.10754476  0.11626124] 1 4
train:	 Loss = 1.2352,	 Acc = 0.5313
40154 0.282
78848 0.595
41024 0.637
5027 0.662
813 0.497
86 0.442
0 0.0
0 0.0
0.6109397605685305
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.1852,	 Acc = 0.5579
4718 0.405
24253 0.578
11362 0.584
1141 0.513
29 0.0
0 0.0
0 0.0
0 0.0
0.5774908250645644
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2690,	 Acc1 = 0.2952,	 Acc2 = 0.2997

 ===== Epoch 110	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705  1.8362415   2.403051
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  2.9607456e+00  2.8093820e+00
 -3.0159974e+00 -2.6688457e+00 -1.1342351e-02 -8.0557177e-03
  1.4306194e-02  2.5413027e-02 -2.1558569e-03  7.0730671e-03
 -1.2275568e-02 -1.3029240e-02] 5 6
train:	 Loss = 1.2348,	 Acc = 0.5306
40153 0.283
78848 0.593
41025 0.638
5026 0.654
814 0.511
86 0.442
0 0.0
0 0.0
0.6095914911883242
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.1935,	 Acc = 0.5577
4718 0.406
24253 0.58
11362 0.579
1141 0.514
29 0.0
0 0.0
0 0.0
0 0.0
0.5771374201440804
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2826,	 Acc1 = 0.2511,	 Acc2 = 0.2465

 ===== Epoch 111	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 1 1
train:	 Loss = 1.2350,	 Acc = 0.5306
40156 0.281
78850 0.594
41020 0.637
5026 0.653
814 0.525
86 0.453
0 0.0
0 0.0
0.6101863334287259
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2390,	 Acc = 0.5449
4718 0.396
24253 0.561
11362 0.573
1141 0.542
29 0.0
0 0.0
0 0.0
0 0.0
0.563952698110643
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3234,	 Acc1 = 0.2420,	 Acc2 = 0.2356

 ===== Epoch 112	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 0 1
train:	 Loss = 1.2357,	 Acc = 0.5319
40154 0.282
78847 0.596
41027 0.638
5025 0.66
813 0.519
86 0.43
0 0.0
0 0.0
0.6117903305298972
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2017,	 Acc = 0.5648
4718 0.396
24253 0.578
11362 0.612
1141 0.527
29 0.0
0 0.0
0 0.0
0 0.0
0.5864075030583118
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2781,	 Acc1 = 0.2725,	 Acc2 = 0.2724

 ===== Epoch 113	 =====
[ 3.2801175   2.6785545  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
  2.0125525   3.0044715 ] [-0.4733941   0.16447493  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.0101584   0.17338958] 6 3
train:	 Loss = 1.2342,	 Acc = 0.5316
40158 0.283
78849 0.596
41022 0.638
5024 0.65
813 0.524
86 0.419
0 0.0
0 0.0
0.6110148337758557
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2236,	 Acc = 0.5473
4718 0.403
24253 0.571
11362 0.568
1141 0.454
29 0.0
0 0.0
0 0.0
0 0.0
0.5658556476824793
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 1
Testing:	 Loss = 1.3014,	 Acc1 = 0.2917,	 Acc2 = 0.2955

 ===== Epoch 114	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 5 1
train:	 Loss = 1.2358,	 Acc = 0.5305
40151 0.282
78857 0.595
41020 0.636
5024 0.655
814 0.504
86 0.523
0 0.0
0 0.0
0.6097566791996883
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.1944,	 Acc = 0.5538
4718 0.395
24253 0.578
11362 0.574
1141 0.51
29 0.0
0 0.0
0 0.0
0 0.0
0.5741198858230256
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2689,	 Acc1 = 0.3097,	 Acc2 = 0.3171

 ===== Epoch 115	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
  0.89472437  1.163089  ] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -1.5625532  -1.7930284 ] 5 1
train:	 Loss = 1.2345,	 Acc = 0.5317
40155 0.282
78857 0.596
41017 0.638
5023 0.654
814 0.49
86 0.407
0 0.0
0 0.0
0.6113738801402259
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.1941,	 Acc = 0.5588
4718 0.394
24253 0.566
11362 0.619
1141 0.504
29 0.0
0 0.0
0 0.0
0 0.0
0.5798559195324181
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 1
Testing:	 Loss = 1.2753,	 Acc1 = 0.2618,	 Acc2 = 0.2594

 ===== Epoch 116	 =====
[ 1.2886375   1.466553   -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
  1.5026509   1.6548219 ] [ 0.12098274  0.9235267   0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
  0.00571871  1.1836593 ] 4 4
train:	 Loss = 1.2341,	 Acc = 0.5319
40153 0.281
78856 0.596
41019 0.639
5024 0.66
814 0.522
86 0.43
0 0.0
0 0.0
0.6119444510687684
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.1980,	 Acc = 0.5712
4718 0.403
24253 0.59
11362 0.609
1141 0.511
29 0.0
0 0.0
0 0.0
0 0.0
0.5927687916270219
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 1
Testing:	 Loss = 1.2801,	 Acc1 = 0.2666,	 Acc2 = 0.2652

 ===== Epoch 117	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
  1.942221    2.7115242 ] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -1.1342351e-02 -8.0557177e-03
  1.4306194e-02  2.5413027e-02  3.6082597e+00  1.8518026e+00
  3.1692901e-01  9.1906488e-01] 4 4
train:	 Loss = 1.2342,	 Acc = 0.5308
40155 0.282
78845 0.595
41025 0.636
5027 0.658
814 0.51
86 0.419
0 0.0
0 0.0
0.6102609760169161
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2128,	 Acc = 0.5593
4718 0.396
24253 0.571
11362 0.606
1141 0.55
29 0.0
0 0.0
0 0.0
0 0.0
0.5803180644284355
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 3
Testing:	 Loss = 1.3002,	 Acc1 = 0.2606,	 Acc2 = 0.2580

 ===== Epoch 118	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 1 1
train:	 Loss = 1.2347,	 Acc = 0.5303
40159 0.283
78855 0.594
41017 0.635
5022 0.651
813 0.507
86 0.419
0 0.0
0 0.0
0.6092628365648327
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.1981,	 Acc = 0.5599
4718 0.389
24253 0.571
11362 0.618
1141 0.474
29 0.0
0 0.0
0 0.0
0 0.0
0.5817316841103711
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 6
Testing:	 Loss = 1.2768,	 Acc1 = 0.2870,	 Acc2 = 0.2898

 ===== Epoch 119	 =====
[ 2.268667    2.2966213  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
  1.535618    2.7612207 ] [-0.06880157 -0.44999546  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.05832148 -0.5452249 ] 6 5
train:	 Loss = 1.2357,	 Acc = 0.5311
40154 0.282
78860 0.595
41011 0.636
5027 0.658
814 0.498
86 0.465
0 0.0
0 0.0
0.6105740949776626
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2885,	 Acc = 0.5230
4718 0.253
24253 0.567
11362 0.551
1141 0.448
29 0.0
0 0.0
0 0.0
0 0.0
0.557672964523583
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3659,	 Acc1 = 0.2542,	 Acc2 = 0.2826

 ===== Epoch 120	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705  2.788065    2.0541
 -0.36039132 -0.36746153  2.0645375   1.247744   -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  4.3878136e+00  2.1577919e+00
 -4.2803259e+00 -2.3316097e+00  3.0962110e+00  4.4288445e+00
 -3.0133550e+00 -1.8727976e+00 -2.1558569e-03  7.0730671e-03
 -1.2275568e-02 -1.3029240e-02] 5 5
train:	 Loss = 1.2348,	 Acc = 0.5311
40158 0.282
78847 0.595
41020 0.637
5027 0.662
814 0.52
86 0.442
0 0.0
0 0.0
0.6105776110148338
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2152,	 Acc = 0.5495
4718 0.41
24253 0.557
11362 0.594
1141 0.536
29 0.0
0 0.0
0 0.0
0 0.0
0.5673780073399484
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2994,	 Acc1 = 0.2707,	 Acc2 = 0.2701

 ===== Epoch 121	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 5 1
train:	 Loss = 1.2348,	 Acc = 0.5318
40149 0.282
78858 0.596
41019 0.638
5026 0.662
814 0.51
86 0.465
0 0.0
0 0.0
0.6115434449098988
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2039,	 Acc = 0.5554
4718 0.406
24253 0.573
11362 0.585
1141 0.523
29 0.0
0 0.0
0 0.0
0 0.0
0.5745548457251597
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2819,	 Acc1 = 0.2769,	 Acc2 = 0.2776

 ===== Epoch 122	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -1.1342351e-02 -8.0557177e-03
  1.4306194e-02  2.5413027e-02  1.3659426e+00  2.8651047e+00
  4.3394232e+00  2.5908208e+00] 2 5
train:	 Loss = 1.2352,	 Acc = 0.5310
40156 0.283
78850 0.594
41020 0.637
5027 0.661
813 0.519
86 0.419
0 0.0
0 0.0
0.6102896753473879
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.1915,	 Acc = 0.5724
4718 0.388
24253 0.581
11362 0.641
1141 0.482
29 0.0
0 0.0
0 0.0
0 0.0
0.5961125458746772
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2765,	 Acc1 = 0.2736,	 Acc2 = 0.2736

 ===== Epoch 123	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 0 1
train:	 Loss = 1.2361,	 Acc = 0.5293
40156 0.282
78851 0.594
41022 0.633
5023 0.651
814 0.511
86 0.407
0 0.0
0 0.0
0.6081035962987694
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.1924,	 Acc = 0.5568
4718 0.407
24253 0.566
11362 0.6
1141 0.551
29 0.0
0 0.0
0 0.0
0 0.0
0.5759956504009787
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2725,	 Acc1 = 0.2808,	 Acc2 = 0.2823

 ===== Epoch 124	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705  1.5794919   1.1861975
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.6536232   0.27764547
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 2 2
train:	 Loss = 1.2346,	 Acc = 0.5306
40157 0.281
78855 0.594
41013 0.638
5027 0.66
814 0.527
86 0.407
0 0.0
0 0.0
0.610167335744664
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2340,	 Acc = 0.5408
4718 0.415
24253 0.554
11362 0.57
1141 0.505
29 0.0
0 0.0
0 0.0
0 0.0
0.5569117846948485
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3097,	 Acc1 = 0.2777,	 Acc2 = 0.2786

 ===== Epoch 125	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 6 1
train:	 Loss = 1.2346,	 Acc = 0.5315
40155 0.28
78855 0.596
41017 0.638
5026 0.663
814 0.502
85 0.388
0 0.0
0 0.0
0.6115964609648878
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2142,	 Acc = 0.5447
4718 0.389
24253 0.552
11362 0.6
1141 0.496
29 0.0
0 0.0
0 0.0
0 0.0
0.5646051379638439
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2951,	 Acc1 = 0.2814,	 Acc2 = 0.2831

 ===== Epoch 126	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
  1.5571573   0.93291616] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.65375066  0.02305182] 1 1
train:	 Loss = 1.2344,	 Acc = 0.5308
40155 0.282
78855 0.595
41016 0.636
5026 0.661
814 0.51
86 0.453
0 0.0
0 0.0
0.6101735335500846
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2449,	 Acc = 0.5429
4718 0.402
24253 0.549
11362 0.593
1141 0.518
29 0.0
0 0.0
0 0.0
0 0.0
0.5609351637895882
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3155,	 Acc1 = 0.3018,	 Acc2 = 0.3077

 ===== Epoch 127	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705  2.2603023   1.8885723
 -0.36039132 -0.36746153  0.9594815   2.541144   -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  1.4154103e-01  5.6948423e-01 -1.1342351e-02 -8.0557177e-03
 -1.6607054e+00 -3.3476260e+00 -2.1558569e-03  7.0730671e-03
 -1.2275568e-02 -1.3029240e-02] 4 4
train:	 Loss = 1.2340,	 Acc = 0.5312
40155 0.282
78854 0.595
41017 0.637
5026 0.662
814 0.531
86 0.465
0 0.0
0 0.0
0.6108094787634045
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2451,	 Acc = 0.5291
4718 0.404
24253 0.565
11362 0.514
1141 0.454
29 0.0
0 0.0
0 0.0
0 0.0
0.5451406823433464
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 1
Testing:	 Loss = 1.3238,	 Acc1 = 0.2694,	 Acc2 = 0.2686

 ===== Epoch 128	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 1 1
train:	 Loss = 1.2344,	 Acc = 0.5307
40151 0.28
78860 0.595
41018 0.636
5023 0.661
814 0.512
86 0.465
0 0.0
0 0.0
0.6105118401284568
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2584,	 Acc = 0.5312
4718 0.411
24253 0.555
11362 0.54
1141 0.456
29 0.0
0 0.0
0 0.0
0 0.0
0.5466902269946989
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3405,	 Acc1 = 0.2573,	 Acc2 = 0.2540

 ===== Epoch 129	 =====
[-0.36759388 -0.3845501   3.4075089  -4.2045417   2.3801954   1.1705395
 -0.36039132 -0.36746153  1.0309237   0.9969326  -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.19621055  0.19453172 -0.03973543  0.2127924
 -0.01134235 -0.00805572 -0.02236568  0.5889882  -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
train:	 Loss = 1.2346,	 Acc = 0.5308
40162 0.281
78845 0.594
41019 0.637
5026 0.664
814 0.516
86 0.442
0 0.0
0 0.0
0.6105095794578266
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2455,	 Acc = 0.5454
4718 0.408
24253 0.572
11362 0.556
1141 0.456
29 0.0
0 0.0
0 0.0
0 0.0
0.5629740383308414
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3256,	 Acc1 = 0.2870,	 Acc2 = 0.2898

 ===== Epoch 130	 =====
[-0.36759388 -0.3845501   1.7033352   1.773765   -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.12897429  0.746367    0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
train:	 Loss = 1.2350,	 Acc = 0.5320
40154 0.281
78854 0.597
41018 0.638
5026 0.663
814 0.509
86 0.512
0 0.0
0 0.0
0.6121718946247158
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2035,	 Acc = 0.5660
4718 0.392
24253 0.586
11362 0.609
1141 0.455
29 0.0
0 0.0
0 0.0
0 0.0
0.5883104526301481
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2819,	 Acc1 = 0.2760,	 Acc2 = 0.2766

 ===== Epoch 131	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  2.1541314   1.7278713  -0.4089302  -0.41695493  3.0013473   2.1255279
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.1427723e+00  1.5868546e+00
  4.3038260e-03  3.1008099e-03 -3.0154502e+00 -2.3382943e+00
  1.4306194e-02  2.5413027e-02 -4.4047832e+00 -2.9953034e+00
 -1.2275568e-02 -1.3029240e-02] 6 6
train:	 Loss = 1.2352,	 Acc = 0.5308
40157 0.282
78852 0.595
41018 0.637
5025 0.656
814 0.511
86 0.488
0 0.0
0 0.0
0.6103422234588021
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2920,	 Acc = 0.5373
4718 0.404
24253 0.557
11362 0.557
1141 0.479
29 0.0
0 0.0
0 0.0
0 0.0
0.5544107652575778
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3625,	 Acc1 = 0.2633,	 Acc2 = 0.2612

 ===== Epoch 132	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705  1.2321478   2.6066053
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137 -0.02693365  0.18901294
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 3 1
train:	 Loss = 1.2356,	 Acc = 0.5310
40157 0.282
78853 0.595
41019 0.636
5023 0.662
814 0.526
86 0.419
0 0.0
0 0.0
0.6104853134067332
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2644,	 Acc = 0.5251
4718 0.4
24253 0.553
11362 0.527
1141 0.438
29 0.0
0 0.0
0 0.0
0 0.0
0.5410901182547234
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3307,	 Acc1 = 0.3070,	 Acc2 = 0.3139

 ===== Epoch 133	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303  2.942061    2.224213
 -0.01227557 -0.01302924] 2 5
train:	 Loss = 1.2353,	 Acc = 0.5316
40158 0.281
78848 0.595
41021 0.64
5025 0.659
814 0.533
86 0.453
0 0.0
0 0.0
0.6115712991080655
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2655,	 Acc = 0.5241
4718 0.362
24253 0.548
11362 0.548
1141 0.457
29 0.0
0 0.0
0 0.0
0 0.0
0.5448960173983961
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 1
Testing:	 Loss = 1.3486,	 Acc1 = 0.2895,	 Acc2 = 0.2927

 ===== Epoch 134	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153  2.0132606   3.2222686  -0.42787513 -0.42500633
  3.554566    2.6696749 ] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -1.1342351e-02 -8.0557177e-03
 -2.9505889e+00 -4.1242943e+00 -2.1558569e-03  7.0730671e-03
 -8.4428740e-01 -1.2427918e-01] 2 1
train:	 Loss = 1.2353,	 Acc = 0.5314
40154 0.281
78852 0.596
41022 0.637
5024 0.663
814 0.516
86 0.384
0 0.0
0 0.0
0.6111623396238414
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2082,	 Acc = 0.5458
4718 0.401
24253 0.562
11362 0.576
1141 0.528
29 0.0
0 0.0
0 0.0
0 0.0
0.5644692129944271
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2942,	 Acc1 = 0.2624,	 Acc2 = 0.2602

 ===== Epoch 135	 =====
[ 0.6806999   1.1406367  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.07888429  0.06808743  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 0 0
train:	 Loss = 1.2343,	 Acc = 0.5312
40164 0.281
78840 0.596
41021 0.636
5027 0.66
814 0.529
86 0.419
0 0.0
0 0.0
0.611179126784749
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2392,	 Acc = 0.5392
4718 0.399
24253 0.557
11362 0.563
1141 0.517
29 0.0
0 0.0
0 0.0
0 0.0
0.5571292646459155
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3155,	 Acc1 = 0.3035,	 Acc2 = 0.3096

 ===== Epoch 136	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 0 1
train:	 Loss = 1.2346,	 Acc = 0.5319
40153 0.281
78859 0.596
41014 0.637
5026 0.661
814 0.538
86 0.5
0 0.0
0 0.0
0.6118649591809155
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2086,	 Acc = 0.5590
4718 0.409
24253 0.572
11362 0.604
1141 0.472
29 0.0
0 0.0
0 0.0
0 0.0
0.5781976349055321
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 6
Testing:	 Loss = 1.2926,	 Acc1 = 0.2864,	 Acc2 = 0.2890

 ===== Epoch 137	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  1.9925039   2.735896   -0.4089302  -0.41695493  2.6620042   2.978976
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.2423233e+00  1.3703653e+00
  4.3038260e-03  3.1008099e-03 -2.8223531e+00 -3.4593277e+00
  1.4306194e-02  2.5413027e-02 -3.9691155e+00 -3.9999449e+00
 -1.2275568e-02 -1.3029240e-02] 6 6
train:	 Loss = 1.2344,	 Acc = 0.5323
40159 0.283
78851 0.597
41018 0.636
5024 0.654
814 0.523
86 0.5
0 0.0
0 0.0
0.6118146478738881
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2123,	 Acc = 0.5484
4718 0.38
24253 0.567
11362 0.59
1141 0.439
29 0.0
0 0.0
0 0.0
0 0.0
0.5700149517466359
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2894,	 Acc1 = 0.2857,	 Acc2 = 0.2883

 ===== Epoch 138	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705  2.4148912   1.1101441
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137 -0.01464318  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 0 0
train:	 Loss = 1.2343,	 Acc = 0.5318
40153 0.281
78855 0.597
41019 0.637
5025 0.661
814 0.517
86 0.442
0 0.0
0 0.0
0.6118013656706333
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.1842,	 Acc = 0.5617
4718 0.411
24253 0.565
11362 0.623
1141 0.521
29 0.0
0 0.0
0 0.0
0 0.0
0.5809705042816365
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2672,	 Acc1 = 0.2845,	 Acc2 = 0.2868

 ===== Epoch 139	 =====
[-0.36759388 -0.3845501   2.7139308   2.1965654  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.27235514  0.01412404  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 0 0
train:	 Loss = 1.2344,	 Acc = 0.5309
40159 0.283
78851 0.595
41018 0.636
5024 0.66
814 0.51
86 0.453
0 0.0
0 0.0
0.6102088351498096
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2400,	 Acc = 0.5393
4718 0.408
24253 0.56
11362 0.564
1141 0.408
29 0.0
0 0.0
0 0.0
0 0.0
0.5560418648905804
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 1
Testing:	 Loss = 1.3294,	 Acc1 = 0.2849,	 Acc2 = 0.2873

 ===== Epoch 140	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
  2.1413682   1.4933552 ] 1 6
train:	 Loss = 1.2340,	 Acc = 0.5319
40156 0.282
78856 0.595
41015 0.64
5025 0.658
814 0.523
86 0.477
0 0.0
0 0.0
0.6115297783713314
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2646,	 Acc = 0.5130
4718 0.4
24253 0.547
11362 0.495
1141 0.447
29 0.0
0 0.0
0 0.0
0 0.0
0.5274976213130352
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3381,	 Acc1 = 0.2851,	 Acc2 = 0.2875

 ===== Epoch 141	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493  1.525663    2.0225255
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00313273  0.00707307
 -0.01227557 -0.01302924] 0 0
train:	 Loss = 1.2345,	 Acc = 0.5310
40153 0.281
78856 0.595
41018 0.637
5025 0.662
814 0.501
86 0.384
0 0.0
0 0.0
0.6107838695061168
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2435,	 Acc = 0.5438
4718 0.404
24253 0.556
11362 0.585
1141 0.473
29 0.0
0 0.0
0 0.0
0 0.0
0.5617507136060894
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 6
Testing:	 Loss = 1.3180,	 Acc1 = 0.2697,	 Acc2 = 0.2689

 ===== Epoch 142	 =====
[ 2.4734561   1.4487294  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
  2.2151942   1.6077411 ] [-2.6099353e+00 -1.7464076e+00  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -1.1342351e-02 -8.0557177e-03
  1.4306194e-02  2.5413027e-02 -2.1558569e-03  7.0730671e-03
 -3.1524780e+00 -2.3041768e+00] 2 5
train:	 Loss = 1.2350,	 Acc = 0.5304
40154 0.279
78849 0.594
41023 0.639
5026 0.658
814 0.517
86 0.465
0 0.0
0 0.0
0.6105581964737118
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2301,	 Acc = 0.5439
4718 0.406
24253 0.564
11362 0.563
1141 0.507
29 0.0
0 0.0
0 0.0
0 0.0
0.5616147886366726
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3124,	 Acc1 = 0.2810,	 Acc2 = 0.2826

 ===== Epoch 143	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493  1.5576195   1.6252308
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00166817  0.00995997
 -0.01227557 -0.01302924] 3 0
train:	 Loss = 1.2343,	 Acc = 0.5317
40152 0.282
78853 0.596
41022 0.638
5025 0.662
814 0.532
86 0.453
0 0.0
0 0.0
0.6114546899841018
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.1950,	 Acc = 0.5649
4718 0.403
24253 0.583
11362 0.605
1141 0.458
29 0.0
0 0.0
0 0.0
0 0.0
0.5856735082234606
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2749,	 Acc1 = 0.2855,	 Acc2 = 0.2880

 ===== Epoch 144	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493  1.8155495   2.1721241
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03  3.5341287e+00  2.5646534e+00
  1.4306194e-02  2.5413027e-02 -2.8823898e+00 -3.0501547e+00
 -1.2275568e-02 -1.3029240e-02] 6 6
train:	 Loss = 1.2335,	 Acc = 0.5317
40155 0.282
78854 0.595
41018 0.637
5025 0.662
814 0.531
86 0.512
0 0.0
0 0.0
0.611278488358228
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2210,	 Acc = 0.5457
4718 0.396
24253 0.569
11362 0.564
1141 0.498
29 0.0
0 0.0
0 0.0
0 0.0
0.5649313578904445
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3020,	 Acc1 = 0.2886,	 Acc2 = 0.2917

 ===== Epoch 145	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705  2.733708   -4.0525346
 -0.36039132 -0.36746153  1.7511126   2.5091777  -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
 -4.2081223e+00  3.5700192e+00 -1.1342351e-02 -8.0557177e-03
  8.5535228e-02  6.7590767e-01 -2.1558569e-03  7.0730671e-03
 -1.2275568e-02 -1.3029240e-02] 4 4
train:	 Loss = 1.2334,	 Acc = 0.5318
40157 0.281
78848 0.596
41022 0.637
5025 0.662
814 0.527
86 0.465
0 0.0
0 0.0
0.6117413251719067
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2084,	 Acc = 0.5622
4718 0.403
24253 0.573
11362 0.617
1141 0.464
29 0.0
0 0.0
0 0.0
0 0.0
0.582737528884056
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3003,	 Acc1 = 0.2408,	 Acc2 = 0.2341

 ===== Epoch 146	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705  1.5093285   2.2889707
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137 -0.2512236  -0.01203157
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 1 1
train:	 Loss = 1.2359,	 Acc = 0.5291
40153 0.279
78848 0.594
41024 0.635
5027 0.658
814 0.509
86 0.453
0 0.0
0 0.0
0.6088919625752192
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2038,	 Acc = 0.5424
4718 0.382
24253 0.568
11362 0.559
1141 0.52
29 0.0
0 0.0
0 0.0
0 0.0
0.5629740383308414
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2811,	 Acc1 = 0.3080,	 Acc2 = 0.3151

 ===== Epoch 147	 =====
[ 3.4761512   2.1616716  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
  2.8283947   2.3270311 ] [-3.5275936e+00 -2.4211202e+00  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -1.1342351e-02 -8.0557177e-03
  1.4306194e-02  2.5413027e-02 -2.1558569e-03  7.0730671e-03
  9.1990769e-02  3.2372734e-01] 6 4
train:	 Loss = 1.2355,	 Acc = 0.5312
40156 0.281
78851 0.594
41019 0.64
5026 0.661
814 0.522
86 0.465
0 0.0
0 0.0
0.6112753982638558
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2073,	 Acc = 0.5531
4718 0.4
24253 0.575
11362 0.579
1141 0.489
29 0.0
0 0.0
0 0.0
0 0.0
0.5726790811472068
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2927,	 Acc1 = 0.2701,	 Acc2 = 0.2694

 ===== Epoch 148	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 1 1
train:	 Loss = 1.2346,	 Acc = 0.5305
40150 0.281
78859 0.593
41016 0.638
5027 0.66
814 0.515
86 0.5
0 0.0
0 0.0
0.6100777412123813
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2108,	 Acc = 0.5557
4718 0.387
24253 0.581
11362 0.579
1141 0.506
29 0.0
0 0.0
0 0.0
0 0.0
0.5774092700829142
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2877,	 Acc1 = 0.3058,	 Acc2 = 0.3124

 ===== Epoch 149	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 6 1
train:	 Loss = 1.2349,	 Acc = 0.5314
40157 0.283
78846 0.596
41022 0.636
5027 0.655
814 0.523
86 0.488
0 0.0
0 0.0
0.6107714933025955
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.1915,	 Acc = 0.5620
4718 0.391
24253 0.568
11362 0.626
1141 0.521
29 0.0
0 0.0
0 0.0
0 0.0
0.5839608536088079
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 1
Testing:	 Loss = 1.2771,	 Acc1 = 0.2470,	 Acc2 = 0.2416

 ===== Epoch 150	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705  2.9291615   2.9891088
 -0.36039132 -0.36746153  1.9372091   2.27312    -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137 -0.3428876   0.00310081
 -0.01134235 -0.00805572 -0.12956241  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 0 0
train:	 Loss = 1.2354,	 Acc = 0.5325
40157 0.282
78856 0.598
41014 0.638
5026 0.664
813 0.508
86 0.465
0 0.0
0 0.0
0.6125839659763902
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2677,	 Acc = 0.5272
4718 0.387
24253 0.563
11362 0.519
1141 0.427
29 0.0
0 0.0
0 0.0
0 0.0
0.5451134973494631
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3318,	 Acc1 = 0.3105,	 Acc2 = 0.3181

 ===== Epoch 151	 =====
[ 1.0586287   1.0871661  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
  0.9751658   1.2441726 ] [-1.3150970e+00 -1.4042319e+00  1.5873009e-02  2.0491373e-02
  4.9740973e+00  2.8112385e+00 -1.1342351e-02 -8.0557177e-03
  3.6400127e+00  2.6890764e+00 -2.1558569e-03  7.0730671e-03
 -1.6594094e+00 -1.8862377e+00] 5 5
train:	 Loss = 1.2333,	 Acc = 0.5307
40160 0.283
78851 0.595
41015 0.635
5026 0.663
814 0.516
86 0.407
0 0.0
0 0.0
0.6098400534215213
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2282,	 Acc = 0.5414
4718 0.394
24253 0.562
11362 0.563
1141 0.508
29 0.0
0 0.0
0 0.0
0 0.0
0.5603914639119206
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3001,	 Acc1 = 0.3055,	 Acc2 = 0.3121

 ===== Epoch 152	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153  1.972355    3.3107903  -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -1.1342351e-02 -8.0557177e-03
 -2.9005184e+00 -4.2252331e+00 -2.1558569e-03  7.0730671e-03
 -1.2275568e-02 -1.3029240e-02] 4 1
train:	 Loss = 1.2351,	 Acc = 0.5307
40155 0.28
78852 0.595
41020 0.636
5025 0.661
814 0.529
86 0.465
0 0.0
0 0.0
0.610690239035907
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2224,	 Acc = 0.5385
4718 0.389
24253 0.559
11362 0.564
1141 0.486
29 0.0
0 0.0
0 0.0
0 0.0
0.5576185945358162
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3084,	 Acc1 = 0.2606,	 Acc2 = 0.2580

 ===== Epoch 153	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705  2.3774972  -4.4954333
 -0.36039132 -0.36746153  1.703868    1.8059223  -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.35251787  0.3316897
 -0.01134235 -0.00805572  0.04674645  0.67310387 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
train:	 Loss = 1.2345,	 Acc = 0.5307
40151 0.281
78852 0.595
41023 0.636
5026 0.661
814 0.51
86 0.465
0 0.0
0 0.0
0.6104959420036407
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2049,	 Acc = 0.5566
4718 0.407
24253 0.576
11362 0.585
1141 0.492
29 0.0
0 0.0
0 0.0
0 0.0
0.5756966154682616
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2845,	 Acc1 = 0.2727,	 Acc2 = 0.2726

 ===== Epoch 154	 =====
[-0.36759388 -0.3845501   1.0927074   1.5487261  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 0 0
train:	 Loss = 1.2341,	 Acc = 0.5302
40160 0.281
78849 0.593
41018 0.637
5026 0.666
813 0.523
86 0.523
0 0.0
0 0.0
0.6099513482574409
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2466,	 Acc = 0.5352
4718 0.396
24253 0.565
11362 0.541
1141 0.43
29 0.0
0 0.0
0 0.0
0 0.0
0.553051515563409
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3214,	 Acc1 = 0.3115,	 Acc2 = 0.3193

 ===== Epoch 155	 =====
[-0.36759388 -0.3845501   1.978056   -4.0749736  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02 -3.0295928e+00  3.5076659e+00
  3.0645475e+00  1.1337060e+00 -1.1342351e-02 -8.0557177e-03
  1.4306194e-02  2.5413027e-02 -2.1558569e-03  7.0730671e-03
 -1.2275568e-02 -1.3029240e-02] 2 1
train:	 Loss = 1.2348,	 Acc = 0.5318
40153 0.28
78855 0.596
41019 0.639
5026 0.662
813 0.508
86 0.477
0 0.0
0 0.0
0.6120954856556888
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2381,	 Acc = 0.5556
4718 0.413
24253 0.566
11362 0.605
1141 0.446
29 0.0
0 0.0
0 0.0
0 0.0
0.5738208508903085
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3110,	 Acc1 = 0.2893,	 Acc2 = 0.2925

 ===== Epoch 156	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 0 1
train:	 Loss = 1.2344,	 Acc = 0.5323
40153 0.282
78856 0.596
41020 0.638
5023 0.671
814 0.521
86 0.442
0 0.0
0 0.0
0.6123180629416768
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2637,	 Acc = 0.5285
4718 0.385
24253 0.559
11362 0.531
1141 0.458
29 0.0
0 0.0
0 0.0
0 0.0
0.5469348919396493
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 1
Testing:	 Loss = 1.3475,	 Acc1 = 0.2959,	 Acc2 = 0.3004

 ===== Epoch 157	 =====
[-0.36759388 -0.3845501   3.120058    2.5625377   2.7144327   0.85961556
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02 -4.4790726e+00 -2.6898685e+00
 -4.1825185e+00 -1.1772250e+00 -1.1342351e-02 -8.0557177e-03
  2.5222063e+00  4.2255898e+00 -2.1558569e-03  7.0730671e-03
 -1.2275568e-02 -1.3029240e-02] 6 6
train:	 Loss = 1.2342,	 Acc = 0.5316
40157 0.283
78849 0.595
41020 0.638
5026 0.662
814 0.512
86 0.453
0 0.0
0 0.0
0.611081521523113
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.1798,	 Acc = 0.5644
4718 0.401
24253 0.576
11362 0.615
1141 0.488
29 0.0
0 0.0
0 0.0
0 0.0
0.5852929183090934
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2571,	 Acc1 = 0.2996,	 Acc2 = 0.3049

 ===== Epoch 158	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705  1.7448759   1.8662037
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.13386029  0.51544005
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
train:	 Loss = 1.2340,	 Acc = 0.5318
40160 0.281
78851 0.596
41017 0.638
5024 0.668
814 0.523
86 0.372
0 0.0
0 0.0
0.6117400152632918
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.1898,	 Acc = 0.5644
4718 0.405
24253 0.57
11362 0.625
1141 0.51
29 0.0
0 0.0
0 0.0
0 0.0
0.5848851434008427
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2667,	 Acc1 = 0.2711,	 Acc2 = 0.2706

 ===== Epoch 159	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493  1.6116403   2.5228224
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.01973958  0.00707307
 -0.01227557 -0.01302924] 3 0
train:	 Loss = 1.2338,	 Acc = 0.5321
40155 0.283
78852 0.596
41020 0.638
5026 0.662
813 0.518
86 0.419
0 0.0
0 0.0
0.6117474979530514
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2262,	 Acc = 0.5458
4718 0.405
24253 0.566
11362 0.572
1141 0.451
29 0.0
0 0.0
0 0.0
0 0.0
0.5638439581351095
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2993,	 Acc1 = 0.2727,	 Acc2 = 0.2726

 ===== Epoch 160	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705  1.7734036   1.5910697
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137 -0.13293496 -0.18929662
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 5 5
train:	 Loss = 1.2342,	 Acc = 0.5312
40149 0.28
78859 0.595
41020 0.638
5024 0.662
814 0.533
86 0.465
0 0.0
0 0.0
0.611344721509026
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2558,	 Acc = 0.5369
4718 0.4
24253 0.568
11362 0.543
1141 0.389
29 0.0
0 0.0
0 0.0
0 0.0
0.5544923202392279
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 6
Testing:	 Loss = 1.3276,	 Acc1 = 0.2987,	 Acc2 = 0.3039

 ===== Epoch 161	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153  1.1438487   1.3756087  -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [ 2.6362982e+00  1.3717287e+00  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -1.1342351e-02 -8.0557177e-03
 -1.8863811e+00 -2.0185981e+00 -2.1558569e-03  7.0730671e-03
  2.7251527e+00  1.8752131e+00] 6 6
train:	 Loss = 1.2340,	 Acc = 0.5310
40158 0.282
78847 0.594
41021 0.639
5026 0.66
814 0.52
86 0.465
0 0.0
0 0.0
0.6104822169578835
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2594,	 Acc = 0.5190
4718 0.374
24253 0.549
11362 0.522
1141 0.457
29 0.0
0 0.0
0 0.0
0 0.0
0.5376376240315346
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3438,	 Acc1 = 0.2697,	 Acc2 = 0.2689

 ===== Epoch 162	 =====
[ 1.938217    2.039453   -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [ 0.05445819  0.01025492  0.01587301  0.02049137  0.00430383  0.00310081
  1.4852263   1.5853235   0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 5 0
train:	 Loss = 1.2344,	 Acc = 0.5312
40158 0.282
78851 0.596
41017 0.637
5026 0.66
814 0.507
86 0.477
0 0.0
0 0.0
0.6108796921951762
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2062,	 Acc = 0.5580
4718 0.406
24253 0.584
11362 0.577
1141 0.455
29 0.0
0 0.0
0 0.0
0 0.0
0.5775180100584477
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2769,	 Acc1 = 0.3107,	 Acc2 = 0.3183

 ===== Epoch 163	 =====
[ 1.1549563   2.9204457  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
  1.0709921   3.2189507 ] [-2.2346947e-01  4.2472124e-01  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -1.1342351e-02 -8.0557177e-03
  1.4306194e-02  2.5413027e-02 -2.1558569e-03  7.0730671e-03
 -1.7747902e+00 -4.1563382e+00] 1 1
train:	 Loss = 1.2336,	 Acc = 0.5320
40157 0.281
78850 0.596
41021 0.638
5024 0.664
814 0.517
86 0.453
0 0.0
0 0.0
0.6121387972494933
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.1850,	 Acc = 0.5532
4718 0.408
24253 0.573
11362 0.578
1141 0.506
29 0.0
0 0.0
0 0.0
0 0.0
0.5718091613429387
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2724,	 Acc1 = 0.2680,	 Acc2 = 0.2669

 ===== Epoch 164	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153  1.038413    2.4427865  -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.58837223  1.0151743  -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
train:	 Loss = 1.2329,	 Acc = 0.5324
40152 0.28
78859 0.598
41019 0.638
5023 0.662
813 0.53
86 0.512
0 0.0
0 0.0
0.6131399046104928
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2446,	 Acc = 0.5316
4718 0.383
24253 0.548
11362 0.566
1141 0.474
29 0.0
0 0.0
0 0.0
0 0.0
0.5506864210955552
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3092,	 Acc1 = 0.2963,	 Acc2 = 0.3009

 ===== Epoch 165	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153  0.8102583   1.9460816  -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  3.6667550e+00 -3.5205827e+00 -1.1342351e-02 -8.0557177e-03
  8.7046736e-01  3.5066035e-01 -2.1558569e-03  7.0730671e-03
 -1.2275568e-02 -1.3029240e-02] 2 2
train:	 Loss = 1.2345,	 Acc = 0.5308
40148 0.283
78856 0.594
41022 0.635
5026 0.664
814 0.534
86 0.523
0 0.0
0 0.0
0.6099567581316969
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.1786,	 Acc = 0.5676
4718 0.406
24253 0.579
11362 0.617
1141 0.511
29 0.0
0 0.0
0 0.0
0 0.0
0.5883104526301481
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2609,	 Acc1 = 0.2804,	 Acc2 = 0.2818

 ===== Epoch 166	 =====
[-0.36759388 -0.3845501   3.3931363   2.2192965  -0.4375299  -0.36171135
  3.0263457   3.4607682  -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.8787247e-01 -6.4869199e+00
  5.8144221e+00  8.0578518e+00 -3.8650411e-01  1.0845621e-01
  1.4306194e-02  2.5413027e-02 -2.1558569e-03  7.0730671e-03
 -1.2275568e-02 -1.3029240e-02] 3 3
train:	 Loss = 1.2349,	 Acc = 0.5309
40161 0.282
78847 0.595
41020 0.637
5025 0.66
813 0.523
86 0.419
0 0.0
0 0.0
0.6105126757876159
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.1955,	 Acc = 0.5639
4718 0.414
24253 0.566
11362 0.629
1141 0.499
29 0.0
0 0.0
0 0.0
0 0.0
0.5830637488106565
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2763,	 Acc1 = 0.2837,	 Acc2 = 0.2858

 ===== Epoch 167	 =====
[ 0.49345022  2.2533355  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.05512719 -0.29577544  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 3 5
train:	 Loss = 1.2333,	 Acc = 0.5318
40158 0.282
78847 0.595
41023 0.638
5024 0.661
814 0.527
86 0.465
0 0.0
0 0.0
0.6115395010890822
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2374,	 Acc = 0.5451
4718 0.391
24253 0.551
11362 0.607
1141 0.45
29 0.0
0 0.0
0 0.0
0 0.0
0.5647682479271442
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3151,	 Acc1 = 0.2639,	 Acc2 = 0.2619

 ===== Epoch 168	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  2.5797431e+00  2.6693008e+00
  4.3038260e-03  3.1008099e-03 -1.1342351e-02 -8.0557177e-03
  1.4306194e-02  2.5413027e-02 -2.1558569e-03  7.0730671e-03
 -1.2275568e-02 -1.3029240e-02] 5 5
train:	 Loss = 1.2340,	 Acc = 0.5315
40157 0.283
78852 0.595
41018 0.637
5025 0.66
814 0.515
86 0.488
0 0.0
0 0.0
0.6109543304582853
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.1581,	 Acc = 0.5770
4718 0.392
24253 0.593
11362 0.628
1141 0.513
29 0.0
0 0.0
0 0.0
0 0.0
0.6007068098409678
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2421,	 Acc1 = 0.2796,	 Acc2 = 0.2808

 ===== Epoch 169	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 1 1
train:	 Loss = 1.2343,	 Acc = 0.5316
40154 0.28
78854 0.597
41019 0.636
5026 0.655
813 0.53
86 0.442
0 0.0
0 0.0
0.6118539245457002
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.1847,	 Acc = 0.5699
4718 0.394
24253 0.574
11362 0.637
1141 0.562
29 0.0
0 0.0
0 0.0
0 0.0
0.592415386706538
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2724,	 Acc1 = 0.2612,	 Acc2 = 0.2587

 ===== Epoch 170	 =====
[-0.36759388 -0.3845501   1.7435775   1.9578876  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.10765287  0.2730621   0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 0 4
train:	 Loss = 1.2351,	 Acc = 0.5317
40158 0.282
78850 0.596
41018 0.636
5026 0.668
814 0.516
86 0.442
0 0.0
0 0.0
0.6114679555463695
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2189,	 Acc = 0.5349
4718 0.397
24253 0.546
11362 0.57
1141 0.533
29 0.0
0 0.0
0 0.0
0 0.0
0.5525350006796248
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 1
Testing:	 Loss = 1.2947,	 Acc1 = 0.2666,	 Acc2 = 0.2652

 ===== Epoch 171	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153  1.9735073   1.7616614  -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.72659874  0.15999813 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 2 2
train:	 Loss = 1.2342,	 Acc = 0.5319
40155 0.282
78851 0.596
41019 0.638
5027 0.66
814 0.527
86 0.453
0 0.0
0 0.0
0.6116441568558869
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2374,	 Acc = 0.5541
4718 0.395
24253 0.581
11362 0.573
1141 0.471
29 0.0
0 0.0
0 0.0
0 0.0
0.5745004757373929
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 6
Testing:	 Loss = 1.3078,	 Acc1 = 0.3016,	 Acc2 = 0.3074

 ===== Epoch 172	 =====
[-0.36759388 -0.3845501   2.0408852   2.1260984  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.0852424   0.6848161   0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 6 4
train:	 Loss = 1.2333,	 Acc = 0.5319
40157 0.282
78846 0.595
41023 0.639
5026 0.662
814 0.52
86 0.477
0 0.0
0 0.0
0.6115505385746651
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2081,	 Acc = 0.5525
4718 0.415
24253 0.578
11362 0.569
1141 0.431
29 0.0
0 0.0
0 0.0
0 0.0
0.5700693217344026
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2916,	 Acc1 = 0.2787,	 Acc2 = 0.2798

 ===== Epoch 173	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 6 1
train:	 Loss = 1.2346,	 Acc = 0.5321
40157 0.282
78848 0.595
41020 0.64
5027 0.661
814 0.526
86 0.477
0 0.0
0 0.0
0.6121387972494933
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2375,	 Acc = 0.5299
4718 0.405
24253 0.558
11362 0.529
1141 0.475
29 0.0
0 0.0
0 0.0
0 0.0
0.5459290471659644
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 1
Testing:	 Loss = 1.3228,	 Acc1 = 0.2872,	 Acc2 = 0.2900

 ===== Epoch 174	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493  1.4096328   2.0495024
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303  0.01786778 -0.06509945
 -0.01227557 -0.01302924] 4 3
train:	 Loss = 1.2339,	 Acc = 0.5307
40154 0.281
78851 0.595
41023 0.636
5024 0.664
814 0.521
86 0.477
0 0.0
0 0.0
0.6105581964737118
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2358,	 Acc = 0.5325
4718 0.397
24253 0.565
11362 0.528
1141 0.467
29 0.0
0 0.0
0 0.0
0 0.0
0.5498980562729373
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3069,	 Acc1 = 0.3051,	 Acc2 = 0.3116

 ===== Epoch 175	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153  1.6030418   2.1477141  -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01359983  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 0 0
train:	 Loss = 1.2336,	 Acc = 0.5318
40155 0.282
78854 0.595
41022 0.639
5022 0.662
813 0.506
86 0.442
0 0.0
0 0.0
0.6114056774008919
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2338,	 Acc = 0.5446
4718 0.393
24253 0.557
11362 0.592
1141 0.453
29 0.0
0 0.0
0 0.0
0 0.0
0.5640886230800598
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3041,	 Acc1 = 0.2857,	 Acc2 = 0.2883

 ===== Epoch 176	 =====
[-0.36759388 -0.3845501   1.8803223   2.0556319  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.18583415  0.13722576  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 1 1
train:	 Loss = 1.2363,	 Acc = 0.5300
40154 0.281
78851 0.594
41023 0.636
5024 0.661
814 0.522
86 0.465
0 0.0
0 0.0
0.6094453011971573
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2304,	 Acc = 0.5404
4718 0.394
24253 0.553
11362 0.58
1141 0.492
29 0.0
0 0.0
0 0.0
0 0.0
0.5591681391871687
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 6
Testing:	 Loss = 1.3128,	 Acc1 = 0.2614,	 Acc2 = 0.2589

 ===== Epoch 177	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 0 1
train:	 Loss = 1.2342,	 Acc = 0.5304
40154 0.281
78852 0.595
41023 0.635
5023 0.663
814 0.512
86 0.407
0 0.0
0 0.0
0.6099302055676561
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.1880,	 Acc = 0.5735
4718 0.392
24253 0.59
11362 0.622
1141 0.512
29 0.0
0 0.0
0 0.0
0 0.0
0.5967649857278782
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 1
Testing:	 Loss = 1.2683,	 Acc1 = 0.2682,	 Acc2 = 0.2671

 ===== Epoch 178	 =====
[-0.36759388 -0.3845501   0.5276598   1.4850787  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.05392199  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 0 0
train:	 Loss = 1.2338,	 Acc = 0.5319
40154 0.282
78856 0.596
41018 0.638
5024 0.659
814 0.521
86 0.407
0 0.0
0 0.0
0.6117187872621186
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2257,	 Acc = 0.5317
4718 0.404
24253 0.555
11362 0.541
1141 0.485
29 0.0
0 0.0
0 0.0
0 0.0
0.5480766616827512
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3044,	 Acc1 = 0.2630,	 Acc2 = 0.2609

 ===== Epoch 179	 =====
[-0.36759388 -0.3845501   2.6194828   2.6511896  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.12172689 -0.2235896   0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 5 3
train:	 Loss = 1.2360,	 Acc = 0.5312
40161 0.281
78845 0.595
41020 0.637
5026 0.663
814 0.507
86 0.488
0 0.0
0 0.0
0.6109181101986628
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2652,	 Acc = 0.5286
4718 0.387
24253 0.559
11362 0.527
1141 0.492
29 0.0
0 0.0
0 0.0
0 0.0
0.5466902269946989
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3407,	 Acc1 = 0.2959,	 Acc2 = 0.3004

 ===== Epoch 180	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705  2.4950774   1.8326508
 -0.36039132 -0.36746153  1.9112821   0.8543144  -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.08982103  0.31223378
 -0.01134235 -0.00805572  0.47623974  0.3338372  -0.00215586  0.00707307
 -0.01227557 -0.01302924] 3 2
train:	 Loss = 1.2339,	 Acc = 0.5316
40154 0.283
78854 0.595
41019 0.638
5025 0.66
814 0.517
86 0.453
0 0.0
0 0.0
0.6109636083244566
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2110,	 Acc = 0.5653
4718 0.406
24253 0.57
11362 0.629
1141 0.504
29 0.0
0 0.0
0 0.0
0 0.0
0.585700693217344
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2906,	 Acc1 = 0.2587,	 Acc2 = 0.2557

 ===== Epoch 181	 =====
[ 2.6757197   3.4602447  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493  2.2716851   1.5516576
 -0.39281774 -0.38534638] [-2.7950454e+00 -3.6500609e+00  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -1.1342351e-02 -8.0557177e-03
  1.4306194e-02  2.5413027e-02 -5.1974893e-02 -2.4682840e-02
 -1.2275568e-02 -1.3029240e-02] 2 3
train:	 Loss = 1.2332,	 Acc = 0.5322
40154 0.282
78849 0.595
41023 0.64
5026 0.669
814 0.529
86 0.453
0 0.0
0 0.0
0.6121241991128635
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.1829,	 Acc = 0.5662
4718 0.404
24253 0.575
11362 0.623
1141 0.493
29 0.0
0 0.0
0 0.0
0 0.0
0.5870055729237461
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2648,	 Acc1 = 0.2952,	 Acc2 = 0.2997

 ===== Epoch 182	 =====
[ 2.7821345   3.2998326  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
  2.5927856   3.5197446 ] [ 1.2377557  -0.06203572  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.05355867 -0.0761711 ] 2 2
train:	 Loss = 1.2340,	 Acc = 0.5307
40156 0.28
78855 0.596
41015 0.636
5026 0.657
814 0.523
86 0.453
0 0.0
0 0.0
0.6107189417787529
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.1902,	 Acc = 0.5593
4718 0.407
24253 0.572
11362 0.607
1141 0.46
29 0.0
0 0.0
0 0.0
0 0.0
0.5788772597526165
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 1
Testing:	 Loss = 1.2653,	 Acc1 = 0.2961,	 Acc2 = 0.3007

 ===== Epoch 183	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.799852    1.6004523  -0.4089302  -0.41695493  3.9771485   1.7919964
  4.5660176  -4.80048   ] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03  1.2141365e+00  4.8948172e-01
  1.4306194e-02  2.5413027e-02  7.1972346e-01  3.9391774e-01
 -5.9830093e+00  5.0623732e+00] 2 2
train:	 Loss = 1.2348,	 Acc = 0.5320
40154 0.282
78854 0.597
41020 0.636
5024 0.659
814 0.509
86 0.477
0 0.0
0 0.0
0.6118380260417494
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2233,	 Acc = 0.5488
4718 0.404
24253 0.563
11362 0.585
1141 0.504
29 0.0
0 0.0
0 0.0
0 0.0
0.5673780073399484
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3061,	 Acc1 = 0.2736,	 Acc2 = 0.2736

 ===== Epoch 184	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705  0.4973664   2.3941033
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137 -0.03922253 -0.05742872
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 5 5
train:	 Loss = 1.2352,	 Acc = 0.5301
40149 0.28
78860 0.594
41017 0.637
5026 0.665
814 0.516
86 0.419
0 0.0
0 0.0
0.6100887896155099
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2293,	 Acc = 0.5536
4718 0.4
24253 0.577
11362 0.576
1141 0.481
29 0.0
0 0.0
0 0.0
0 0.0
0.5732227810248742
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3104,	 Acc1 = 0.2635,	 Acc2 = 0.2614

 ===== Epoch 185	 =====
[-0.36759388 -0.3845501   2.3386014   2.5057096  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.658001    0.0120016   0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 2 2
train:	 Loss = 1.2341,	 Acc = 0.5306
40151 0.282
78854 0.594
41022 0.637
5025 0.658
814 0.511
86 0.442
0 0.0
0 0.0
0.609899762323034
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2030,	 Acc = 0.5571
4718 0.404
24253 0.553
11362 0.638
1141 0.493
29 0.0
0 0.0
0 0.0
0 0.0
0.57681120021748
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2874,	 Acc1 = 0.2536,	 Acc2 = 0.2495

 ===== Epoch 186	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 1
train:	 Loss = 1.2345,	 Acc = 0.5309
40154 0.282
78856 0.594
41020 0.636
5022 0.663
814 0.537
86 0.453
0 0.0
0 0.0
0.610295871158524
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2651,	 Acc = 0.5176
4718 0.393
24253 0.559
11362 0.491
1141 0.436
29 0.0
0 0.0
0 0.0
0 0.0
0.5336142449367949
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3351,	 Acc1 = 0.2985,	 Acc2 = 0.3037

 ===== Epoch 187	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [ 1.88055     1.607878    0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 6 4
train:	 Loss = 1.2337,	 Acc = 0.5327
40150 0.281
78857 0.598
41019 0.639
5026 0.657
814 0.527
86 0.512
0 0.0
0 0.0
0.6130824629179187
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2707,	 Acc = 0.5346
4718 0.408
24253 0.563
11362 0.54
1141 0.424
29 0.0
0 0.0
0 0.0
0 0.0
0.5509310860405057
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3419,	 Acc1 = 0.2734,	 Acc2 = 0.2734

 ===== Epoch 188	 =====
[-0.36759388 -0.3845501   1.755076    1.6555626  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.6069237  -0.00710039  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 2 2
train:	 Loss = 1.2345,	 Acc = 0.5311
40161 0.282
78851 0.595
41016 0.637
5025 0.657
814 0.498
85 0.435
0 0.0
0 0.0
0.610703468451638
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2228,	 Acc = 0.5497
4718 0.411
24253 0.565
11362 0.584
1141 0.477
29 0.0
0 0.0
0 0.0
0 0.0
0.5674595623215984
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3037,	 Acc1 = 0.2711,	 Acc2 = 0.2706

 ===== Epoch 189	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153  0.8096813   2.6591728  -0.42787513 -0.42500633
  2.2116776   1.071543  ] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -1.1342351e-02 -8.0557177e-03
 -1.4773418e+00 -3.4822114e+00 -2.1558569e-03  7.0730671e-03
 -1.1612644e-03  4.1092351e-02] 1 4
train:	 Loss = 1.2348,	 Acc = 0.5320
40154 0.282
78854 0.596
41019 0.637
5026 0.666
813 0.53
86 0.419
0 0.0
0 0.0
0.6116790410022417
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2159,	 Acc = 0.5501
4718 0.4
24253 0.563
11362 0.596
1141 0.457
29 0.0
0 0.0
0 0.0
0 0.0
0.5693353268995515
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3066,	 Acc1 = 0.2608,	 Acc2 = 0.2582

 ===== Epoch 190	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 1 1
train:	 Loss = 1.2343,	 Acc = 0.5321
40150 0.282
78856 0.598
41022 0.637
5024 0.655
814 0.504
86 0.453
0 0.0
0 0.0
0.6119139600324319
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2520,	 Acc = 0.5222
4718 0.401
24253 0.548
11362 0.527
1141 0.442
29 0.0
0 0.0
0 0.0
0 0.0
0.5377463640070681
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3310,	 Acc1 = 0.2639,	 Acc2 = 0.2619

 ===== Epoch 191	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
  1.1936326   1.4848077 ] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.12289341 -0.26258993] 3 5
train:	 Loss = 1.2341,	 Acc = 0.5317
40158 0.281
78848 0.597
41021 0.637
5025 0.66
814 0.506
86 0.477
0 0.0
0 0.0
0.6117302892029827
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2335,	 Acc = 0.5455
4718 0.401
24253 0.565
11362 0.57
1141 0.509
29 0.0
0 0.0
0 0.0
0 0.0
0.564034253092293
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3060,	 Acc1 = 0.2810,	 Acc2 = 0.2826

 ===== Epoch 192	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493  2.032014    1.6105161
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
  1.8231461   2.155287    0.01430619  0.02541303  0.8227799   0.1860609
 -0.01227557 -0.01302924] 2 2
train:	 Loss = 1.2343,	 Acc = 0.5317
40157 0.282
78850 0.595
41021 0.638
5025 0.662
813 0.539
86 0.512
0 0.0
0 0.0
0.6114789936006996
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.1984,	 Acc = 0.5454
4718 0.413
24253 0.561
11362 0.58
1141 0.438
29 0.0
0 0.0
0 0.0
0 0.0
0.5623487834715237
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2730,	 Acc1 = 0.2748,	 Acc2 = 0.2751

 ===== Epoch 193	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153  1.33225     2.6591728  -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572 -0.29317856  0.5160879  -0.00215586  0.00707307
 -0.01227557 -0.01302924] 1 1
train:	 Loss = 1.2331,	 Acc = 0.5316
40156 0.284
78860 0.595
41011 0.636
5025 0.662
814 0.518
86 0.488
0 0.0
0 0.0
0.6107189417787529
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2108,	 Acc = 0.5470
4718 0.398
24253 0.567
11362 0.573
1141 0.493
29 0.0
0 0.0
0 0.0
0 0.0
0.566127497621313
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2942,	 Acc1 = 0.2688,	 Acc2 = 0.2679

 ===== Epoch 194	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 5 1
train:	 Loss = 1.2355,	 Acc = 0.5314
40152 0.283
78859 0.595
41016 0.636
5025 0.661
814 0.521
86 0.465
0 0.0
0 0.0
0.6107154213036566
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.1876,	 Acc = 0.5708
4718 0.399
24253 0.576
11362 0.636
1141 0.531
29 0.0
0 0.0
0 0.0
0 0.0
0.5927959766209052
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 1
Testing:	 Loss = 1.2717,	 Acc1 = 0.2567,	 Acc2 = 0.2532

 ===== Epoch 195	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153  3.6846745   2.7181873  -0.42787513 -0.42500633
  3.347528    1.4324958 ] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -1.1342351e-02 -8.0557177e-03
 -4.9964924e+00 -3.5495036e+00  1.9867194e+00  2.7409680e+00
 -4.5158753e+00 -2.1027241e+00] 6 5
train:	 Loss = 1.2339,	 Acc = 0.5317
40155 0.281
78858 0.596
41012 0.639
5027 0.663
814 0.518
86 0.477
0 0.0
0 0.0
0.6118587883653823
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.1840,	 Acc = 0.5658
4718 0.407
24253 0.581
11362 0.606
1141 0.508
29 0.0
0 0.0
0 0.0
0 0.0
0.5861900231072448
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2759,	 Acc1 = 0.2459,	 Acc2 = 0.2403

 ===== Epoch 196	 =====
[-0.36759388 -0.3845501   3.7947466   1.9351565  -0.4375299  -0.36171135
  2.7186756   3.2115934  -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  4.7145199e-02 -6.5187569e+00
  4.3038260e-03  3.1008099e-03  9.4861485e-02  1.5569077e-01
  1.4306194e-02  2.5413027e-02 -2.1558569e-03  7.0730671e-03
 -1.2275568e-02 -1.3029240e-02] 2 2
train:	 Loss = 1.2339,	 Acc = 0.5313
40151 0.281
78858 0.595
41019 0.637
5024 0.665
814 0.528
86 0.5
0 0.0
0 0.0
0.6112670010572253
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2699,	 Acc = 0.5319
4718 0.41
24253 0.553
11362 0.548
1141 0.45
29 0.0
0 0.0
0 0.0
0 0.0
0.5475329618050836
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3421,	 Acc1 = 0.2707,	 Acc2 = 0.2701

 ===== Epoch 197	 =====
[-0.36759388 -0.3845501   1.9595766   1.303229   -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01795793  1.4531407   0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 6 4
train:	 Loss = 1.2351,	 Acc = 0.5303
40151 0.282
78857 0.594
41018 0.636
5027 0.657
813 0.519
86 0.5
0 0.0
0 0.0
0.6094625638905891
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2150,	 Acc = 0.5494
4718 0.409
24253 0.543
11362 0.628
1141 0.49
29 0.0
0 0.0
0 0.0
0 0.0
0.5674051923338317
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2912,	 Acc1 = 0.2606,	 Acc2 = 0.2580

 ===== Epoch 198	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [ 2.2809875e+00  2.4319913e+00  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -1.1342351e-02 -8.0557177e-03
  1.4306194e-02  2.5413027e-02 -2.1558569e-03  7.0730671e-03
  2.9516797e+00  3.1500771e+00] 3 6
train:	 Loss = 1.2340,	 Acc = 0.5319
40161 0.28
78847 0.597
41017 0.638
5027 0.657
814 0.518
86 0.442
0 0.0
0 0.0
0.6122218600694803
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2202,	 Acc = 0.5482
4718 0.415
24253 0.561
11362 0.585
1141 0.474
29 0.0
0 0.0
0 0.0
0 0.0
0.5653119478048118
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3031,	 Acc1 = 0.2482,	 Acc2 = 0.2430

 ===== Epoch 199	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 3 1
train:	 Loss = 1.2340,	 Acc = 0.5311
40151 0.281
78853 0.595
41021 0.637
5027 0.664
814 0.523
86 0.442
0 0.0
0 0.0
0.6110205801225745
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2078,	 Acc = 0.5528
4718 0.391
24253 0.574
11362 0.584
1141 0.461
29 0.0
0 0.0
0 0.0
0 0.0
0.573494630963708
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 1
Testing:	 Loss = 1.2853,	 Acc1 = 0.3006,	 Acc2 = 0.3062

 ===== Epoch 200	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493  1.6618567   1.1151239
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303  0.255239    0.4025784
 -0.01227557 -0.01302924] 2 2
train:	 Loss = 1.2351,	 Acc = 0.5319
40158 0.282
78852 0.597
41016 0.637
5026 0.656
814 0.52
86 0.477
0 0.0
0 0.0
0.6117620872219661
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.1877,	 Acc = 0.5511
4718 0.416
24253 0.558
11362 0.598
1141 0.512
29 0.0
0 0.0
0 0.0
0 0.0
0.5684382221014
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 1
Testing:	 Loss = 1.2787,	 Acc1 = 0.2738,	 Acc2 = 0.2739

 ===== Epoch 201	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 0 1
train:	 Loss = 1.2342,	 Acc = 0.5315
40157 0.281
78848 0.597
41022 0.636
5025 0.655
814 0.526
86 0.488
0 0.0
0 0.0
0.6113677014189753
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.1730,	 Acc = 0.5585
4718 0.395
24253 0.579
11362 0.595
1141 0.454
29 0.0
0 0.0
0 0.0
0 0.0
0.5794753296180508
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 3
Testing:	 Loss = 1.2591,	 Acc1 = 0.2936,	 Acc2 = 0.2977

 ===== Epoch 202	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493  3.0249336   1.7355903
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.40265587  0.03016827
 -0.01227557 -0.01302924] 2 0
train:	 Loss = 1.2343,	 Acc = 0.5305
40154 0.281
78850 0.595
41026 0.635
5023 0.658
813 0.514
86 0.43
0 0.0
0 0.0
0.6101368861190162
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2544,	 Acc = 0.5437
4718 0.401
24253 0.562
11362 0.574
1141 0.457
29 0.0
0 0.0
0 0.0
0 0.0
0.5619953785510399
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3265,	 Acc1 = 0.2954,	 Acc2 = 0.3000

 ===== Epoch 203	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705  0.08101621  2.2598915
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137 -0.01054636 -0.28657624
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 5 5
train:	 Loss = 1.2338,	 Acc = 0.5321
40154 0.281
78853 0.597
41019 0.637
5026 0.655
814 0.538
86 0.419
0 0.0
0 0.0
0.612052655845085
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.1919,	 Acc = 0.5567
4718 0.405
24253 0.564
11362 0.608
1141 0.522
29 0.0
0 0.0
0 0.0
0 0.0
0.5760772053826287
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2717,	 Acc1 = 0.2717,	 Acc2 = 0.2714

 ===== Epoch 204	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153  1.0810483   3.0894864  -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -1.1342351e-02 -8.0557177e-03
 -1.8095099e+00 -3.9728861e+00 -2.1558569e-03  7.0730671e-03
 -1.2275568e-02 -1.3029240e-02] 1 1
train:	 Loss = 1.2336,	 Acc = 0.5311
40158 0.282
78847 0.595
41023 0.638
5025 0.66
813 0.523
86 0.477
0 0.0
0 0.0
0.6107048030907675
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.1931,	 Acc = 0.5560
4718 0.392
24253 0.581
11362 0.583
1141 0.448
29 0.0
0 0.0
0 0.0
0 0.0
0.5770286801685469
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2686,	 Acc1 = 0.3004,	 Acc2 = 0.3059

 ===== Epoch 205	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 5 1
train:	 Loss = 1.2346,	 Acc = 0.5313
40152 0.281
78853 0.596
41021 0.637
5026 0.665
814 0.526
86 0.477
0 0.0
0 0.0
0.61120826709062
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.1887,	 Acc = 0.5678
4718 0.39
24253 0.584
11362 0.616
1141 0.497
29 0.0
0 0.0
0 0.0
0 0.0
0.5905939921163518
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2671,	 Acc1 = 0.2963,	 Acc2 = 0.3009

 ===== Epoch 206	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493  1.5838684   2.5620615
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  4.2637658e+00  1.4297937e+00
  4.3038260e-03  3.1008099e-03  3.6286073e+00  3.7801559e+00
  1.4306194e-02  2.5413027e-02 -2.5849445e+00 -3.5091720e+00
 -1.2275568e-02 -1.3029240e-02] 6 6
train:	 Loss = 1.2354,	 Acc = 0.5317
40155 0.283
78844 0.596
41026 0.636
5027 0.661
814 0.502
86 0.488
0 0.0
0 0.0
0.6111592486307305
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.1909,	 Acc = 0.5638
4718 0.4
24253 0.572
11362 0.626
1141 0.468
29 0.0
0 0.0
0 0.0
0 0.0
0.5848579584069593
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2748,	 Acc1 = 0.2742,	 Acc2 = 0.2744

 ===== Epoch 207	 =====
[ 0.90271217  3.1317818  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-1.1724033e+00 -3.3392115e+00  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -1.1342351e-02 -8.0557177e-03
  1.4306194e-02  2.5413027e-02 -2.1558569e-03  7.0730671e-03
 -1.2275568e-02 -1.3029240e-02] 6 1
train:	 Loss = 1.2348,	 Acc = 0.5316
40158 0.281
78846 0.597
41022 0.636
5026 0.663
814 0.512
86 0.384
0 0.0
0 0.0
0.6114679555463695
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2051,	 Acc = 0.5472
4718 0.404
24253 0.568
11362 0.568
1141 0.494
29 0.0
0 0.0
0 0.0
0 0.0
0.5655022427619953
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2901,	 Acc1 = 0.2686,	 Acc2 = 0.2676

 ===== Epoch 208	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493  1.5214778   2.0225255
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303  0.00370538  0.01284687
 -0.01227557 -0.01302924] 0 0
train:	 Loss = 1.2338,	 Acc = 0.5325
40157 0.282
78845 0.596
41027 0.64
5024 0.663
813 0.519
86 0.407
0 0.0
0 0.0
0.6123693310544934
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2312,	 Acc = 0.5481
4718 0.41
24253 0.558
11362 0.591
1141 0.497
29 0.0
0 0.0
0 0.0
0 0.0
0.5658012776947126
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3092,	 Acc1 = 0.2503,	 Acc2 = 0.2455

 ===== Epoch 209	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  0.9540435   3.5400507  -0.4089302  -0.41695493  2.292228    3.1015975
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03  3.4173276e-02  2.7850062e-01
  1.4306194e-02  2.5413027e-02 -3.4943762e+00 -4.1442900e+00
 -1.2275568e-02 -1.3029240e-02] 6 4
train:	 Loss = 1.2341,	 Acc = 0.5316
40155 0.281
78854 0.596
41017 0.638
5026 0.661
814 0.522
86 0.465
0 0.0
0 0.0
0.6116600554862198
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2345,	 Acc = 0.5518
4718 0.36
24253 0.565
11362 0.61
1141 0.501
29 0.0
0 0.0
0 0.0
0 0.0
0.5764306103031127
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 1
Testing:	 Loss = 1.3219,	 Acc1 = 0.2389,	 Acc2 = 0.2319

 ===== Epoch 210	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [ 2.9257669e+00  3.1428492e+00  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -1.1342351e-02 -8.0557177e-03
  1.4306194e-02  2.5413027e-02 -2.1558569e-03  7.0730671e-03
 -1.2275568e-02 -1.3029240e-02] 3 1
train:	 Loss = 1.2355,	 Acc = 0.5311
40155 0.282
78850 0.595
41021 0.638
5026 0.659
814 0.518
86 0.442
0 0.0
0 0.0
0.610690239035907
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2342,	 Acc = 0.5477
4718 0.403
24253 0.573
11362 0.558
1141 0.52
29 0.0
0 0.0
0 0.0
0 0.0
0.5662906075846132
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3042,	 Acc1 = 0.2909,	 Acc2 = 0.2945

 ===== Epoch 211	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153  2.4874322   0.93054146 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.7119117e+00  2.3832080e+00 -1.1342351e-02 -8.0557177e-03
  1.1617341e+00  3.8991433e-01 -2.1558569e-03  7.0730671e-03
 -1.2275568e-02 -1.3029240e-02] 2 2
train:	 Loss = 1.2334,	 Acc = 0.5318
40156 0.281
78852 0.597
41019 0.636
5025 0.664
814 0.526
86 0.523
0 0.0
0 0.0
0.6117682597220897
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2062,	 Acc = 0.5541
4718 0.401
24253 0.584
11362 0.566
1141 0.449
29 0.0
0 0.0
0 0.0
0 0.0
0.573712110914775
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2856,	 Acc1 = 0.2831,	 Acc2 = 0.2850

 ===== Epoch 212	 =====
[-0.36759388 -0.3845501   1.6893727   3.0444393   2.2645423   0.7679042
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.11547051  0.02473626  0.20862411  0.02039496
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 3 0
train:	 Loss = 1.2334,	 Acc = 0.5315
40158 0.282
78856 0.596
41014 0.638
5025 0.658
813 0.518
86 0.36
0 0.0
0 0.0
0.611277167432469
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2615,	 Acc = 0.5319
4718 0.391
24253 0.56
11362 0.54
1141 0.455
29 0.0
0 0.0
0 0.0
0 0.0
0.5499524262607041
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 1
Testing:	 Loss = 1.3430,	 Acc1 = 0.2903,	 Acc2 = 0.2937

 ===== Epoch 213	 =====
[-0.36759388 -0.3845501   2.5135357   1.4737129  -0.4375299  -0.36171135
  1.5416766   3.4635994  -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.2441621  -0.46979305  0.00430383  0.00310081
  0.00934754 -0.78270257  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 5 5
train:	 Loss = 1.2339,	 Acc = 0.5308
40152 0.278
78852 0.596
41021 0.637
5027 0.661
814 0.518
86 0.453
0 0.0
0 0.0
0.6113195548489666
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.1897,	 Acc = 0.5609
4718 0.406
24253 0.571
11362 0.609
1141 0.521
29 0.0
0 0.0
0 0.0
0 0.0
0.5808073943183363
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2694,	 Acc1 = 0.2886,	 Acc2 = 0.2917

 ===== Epoch 214	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 0 1
train:	 Loss = 1.2338,	 Acc = 0.5309
40157 0.282
78853 0.594
41018 0.638
5024 0.662
814 0.511
86 0.453
0 0.0
0 0.0
0.610461465082078
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2335,	 Acc = 0.5532
4718 0.413
24253 0.573
11362 0.581
1141 0.444
29 0.0
0 0.0
0 0.0
0 0.0
0.5711567214897376
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3129,	 Acc1 = 0.2719,	 Acc2 = 0.2716

 ===== Epoch 215	 =====
[-0.36759388 -0.3845501   3.0991151   2.4784322  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  5.3399961e-02  7.5674899e-02
  3.5551214e+00  1.0839852e+00 -1.1342351e-02 -8.0557177e-03
  1.4306194e-02  2.5413027e-02 -2.1558569e-03  7.0730671e-03
 -1.2275568e-02 -1.3029240e-02] 2 2
train:	 Loss = 1.2348,	 Acc = 0.5312
40157 0.282
78856 0.595
41015 0.637
5026 0.658
813 0.523
85 0.424
0 0.0
0 0.0
0.6106443022377678
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2114,	 Acc = 0.5460
4718 0.384
24253 0.564
11362 0.584
1141 0.472
29 0.0
0 0.0
0 0.0
0 0.0
0.5667527524806307
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2950,	 Acc1 = 0.2527,	 Acc2 = 0.2485

 ===== Epoch 216	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705  0.92451125  0.8148783
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137 -0.2911676   0.01390965
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 1 1
train:	 Loss = 1.2343,	 Acc = 0.5315
40148 0.281
78856 0.596
41024 0.636
5025 0.661
813 0.515
86 0.477
0 0.0
0 0.0
0.6114272996089155
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2114,	 Acc = 0.5618
4718 0.417
24253 0.568
11362 0.621
1141 0.447
29 0.0
0 0.0
0 0.0
0 0.0
0.580426804403969
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2957,	 Acc1 = 0.2534,	 Acc2 = 0.2493

 ===== Epoch 217	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705  3.6554613   2.4768672
 -0.36039132 -0.36746153  3.8869045   1.8059223  -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
 -5.4325080e+00 -2.7401841e+00 -1.1342351e-02 -8.0557177e-03
 -5.2440329e+00 -2.5092731e+00 -2.1558569e-03  7.0730671e-03
  3.7381744e+00  1.6647403e+00] 2 2
train:	 Loss = 1.2350,	 Acc = 0.5307
40155 0.283
78850 0.595
41023 0.633
5024 0.661
814 0.534
86 0.419
0 0.0
0 0.0
0.6096250308035963
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.1831,	 Acc = 0.5514
4718 0.406
24253 0.557
11362 0.605
1141 0.5
29 0.0
0 0.0
0 0.0
0 0.0
0.5700421367405192
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2774,	 Acc1 = 0.2474,	 Acc2 = 0.2420

 ===== Epoch 218	 =====
[-0.36759388 -0.3845501   2.9985073   1.7078445  -0.4375299  -0.36171135
  2.1749113   3.044533   -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.01175138  0.04808314  0.00430383  0.00310081
  0.16382565  0.01398708  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 2 0
train:	 Loss = 1.2343,	 Acc = 0.5321
40155 0.281
78855 0.598
41016 0.636
5026 0.659
814 0.522
86 0.512
0 0.0
0 0.0
0.6120893185052108
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2550,	 Acc = 0.5294
4718 0.388
24253 0.546
11362 0.559
1141 0.479
29 0.0
0 0.0
0 0.0
0 0.0
0.5475873317928504
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3472,	 Acc1 = 0.2346,	 Acc2 = 0.2266

 ===== Epoch 219	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153  1.0194004   1.9387047  -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [ 1.3631623e+00  1.8223403e+00  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -1.1342351e-02 -8.0557177e-03
 -1.7340493e+00 -2.6606812e+00 -2.1558569e-03  7.0730671e-03
  2.1270781e+00  2.4855843e+00] 6 6
train:	 Loss = 1.2342,	 Acc = 0.5316
40156 0.281
78851 0.596
41021 0.638
5024 0.662
814 0.522
86 0.419
0 0.0
0 0.0
0.6116331202899933
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2312,	 Acc = 0.5399
4718 0.372
24253 0.565
11362 0.567
1141 0.446
29 0.0
0 0.0
0 0.0
0 0.0
0.5613701236917221
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3110,	 Acc1 = 0.2767,	 Acc2 = 0.2773

 ===== Epoch 220	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  2.7767584e+00  1.4817507e+00 -1.1342351e-02 -8.0557177e-03
  1.4306194e-02  2.5413027e-02 -2.1558569e-03  7.0730671e-03
 -1.2275568e-02 -1.3029240e-02] 4 1
train:	 Loss = 1.2342,	 Acc = 0.5318
40160 0.281
78852 0.597
41016 0.637
5024 0.658
814 0.539
86 0.43
0 0.0
0 0.0
0.6118513100992113
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.1814,	 Acc = 0.5687
4718 0.407
24253 0.582
11362 0.618
1141 0.491
29 0.0
0 0.0
0 0.0
0 0.0
0.5895337773549001
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 1
Testing:	 Loss = 1.2692,	 Acc1 = 0.2763,	 Acc2 = 0.2768

 ===== Epoch 221	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 1 1
train:	 Loss = 1.2347,	 Acc = 0.5306
40153 0.281
78851 0.594
41024 0.636
5025 0.665
814 0.533
85 0.424
0 0.0
0 0.0
0.6101956295360058
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2041,	 Acc = 0.5491
4718 0.39
24253 0.571
11362 0.578
1141 0.464
29 0.0
0 0.0
0 0.0
0 0.0
0.5694984368628517
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 1
Testing:	 Loss = 1.2817,	 Acc1 = 0.2855,	 Acc2 = 0.2880

 ===== Epoch 222	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  1.5693836   1.305973   -0.4089302  -0.41695493  2.0084276   3.2585533
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
  0.09555221 -0.21273883  0.01430619  0.02541303 -0.16430932 -0.23253968
 -0.01227557 -0.01302924] 0 5
train:	 Loss = 1.2327,	 Acc = 0.5325
40152 0.28
78856 0.597
41021 0.639
5026 0.665
811 0.512
86 0.442
0 0.0
0 0.0
0.6129491255961844
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2621,	 Acc = 0.5334
4718 0.404
24253 0.565
11362 0.535
1141 0.385
29 0.0
0 0.0
0 0.0
0 0.0
0.5499252412668207
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3317,	 Acc1 = 0.2989,	 Acc2 = 0.3042

 ===== Epoch 223	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.039622    1.5947893  -0.4089302  -0.41695493  2.2423916   3.0182147
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.0733485e+00 -2.1902926e+00
  1.4306194e-02  2.5413027e-02 -1.1435400e-02 -2.4439056e+00
 -1.2275568e-02 -1.3029240e-02] 5 5
train:	 Loss = 1.2346,	 Acc = 0.5317
40152 0.281
78855 0.596
41018 0.639
5027 0.661
814 0.515
86 0.384
0 0.0
0 0.0
0.6117488076311606
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2231,	 Acc = 0.5432
4718 0.413
24253 0.567
11362 0.556
1141 0.462
29 0.0
0 0.0
0 0.0
0 0.0
0.5599021340220198
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3036,	 Acc1 = 0.2855,	 Acc2 = 0.2880

 ===== Epoch 224	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [ 1.1931947   1.4440192   0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
  1.3554039   2.1368008 ] 5 6
train:	 Loss = 1.2344,	 Acc = 0.5310
40157 0.281
78848 0.595
41024 0.638
5023 0.658
814 0.522
86 0.453
0 0.0
0 0.0
0.6108907349258714
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2090,	 Acc = 0.5507
4718 0.406
24253 0.566
11362 0.586
1141 0.483
29 0.0
0 0.0
0 0.0
0 0.0
0.5692537719179013
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3015,	 Acc1 = 0.2715,	 Acc2 = 0.2711

 ===== Epoch 225	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493  1.5941397   2.581681
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.16479851 -0.03045664
 -0.01227557 -0.01302924] 3 3
train:	 Loss = 1.2335,	 Acc = 0.5322
40152 0.282
78854 0.597
41021 0.636
5025 0.668
814 0.536
86 0.477
0 0.0
0 0.0
0.6121303656597774
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2463,	 Acc = 0.5484
4718 0.399
24253 0.574
11362 0.57
1141 0.424
29 0.0
0 0.0
0 0.0
0 0.0
0.5675411173032486
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 6
Testing:	 Loss = 1.3214,	 Acc1 = 0.2983,	 Acc2 = 0.3034

 ===== Epoch 226	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705  2.0444171   2.9309502
 -0.36039132 -0.36746153  1.0055721   1.5428162  -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137 -0.11501216  0.0938951
 -0.01134235 -0.00805572  0.05309497  0.12635185 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 6 3
train:	 Loss = 1.2340,	 Acc = 0.5325
40159 0.282
78845 0.597
41026 0.638
5025 0.661
811 0.515
86 0.453
0 0.0
0 0.0
0.6124903611488716
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2063,	 Acc = 0.5583
4718 0.408
24253 0.567
11362 0.611
1141 0.493
29 0.0
0 0.0
0 0.0
0 0.0
0.5775995650400979
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2893,	 Acc1 = 0.2740,	 Acc2 = 0.2741

 ===== Epoch 227	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493  1.8364733   1.6007065
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03  3.7630861e+00  2.9708705e+00
  1.4306194e-02  2.5413027e-02 -2.9092529e+00 -2.3775067e+00
 -1.2275568e-02 -1.3029240e-02] 6 5
train:	 Loss = 1.2336,	 Acc = 0.5321
40150 0.281
78856 0.597
41022 0.638
5024 0.658
814 0.532
86 0.43
0 0.0
0 0.0
0.6121047360137358
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2197,	 Acc = 0.5424
4718 0.387
24253 0.557
11362 0.586
1141 0.465
29 0.0
0 0.0
0 0.0
0 0.0
0.5623487834715237
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 1
Testing:	 Loss = 1.3037,	 Acc1 = 0.2725,	 Acc2 = 0.2724

 ===== Epoch 228	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
  1.4578142   1.2075542 ] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.02444845  0.77474064] 4 4
train:	 Loss = 1.2351,	 Acc = 0.5317
40159 0.282
78851 0.596
41017 0.637
5025 0.661
814 0.501
86 0.453
0 0.0
0 0.0
0.6113615225012521
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2084,	 Acc = 0.5594
4718 0.409
24253 0.576
11362 0.597
1141 0.47
29 0.0
0 0.0
0 0.0
0 0.0
0.5786325948076662
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 6
Testing:	 Loss = 1.2906,	 Acc1 = 0.2872,	 Acc2 = 0.2900

 ===== Epoch 229	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493  2.004623    1.8165207
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303  1.2770064  -0.06798635
 -0.01227557 -0.01302924] 2 2
train:	 Loss = 1.2338,	 Acc = 0.5310
40158 0.281
78851 0.597
41017 0.634
5026 0.655
814 0.529
86 0.442
0 0.0
0 0.0
0.6107763486334802
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2233,	 Acc = 0.5522
4718 0.417
24253 0.572
11362 0.576
1141 0.479
29 0.0
0 0.0
0 0.0
0 0.0
0.5696071768383852
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2986,	 Acc1 = 0.2767,	 Acc2 = 0.2773

 ===== Epoch 230	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705  2.6835918   2.9891088
 -0.36039132 -0.36746153  1.945851    2.27312    -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137 -0.01976421  0.00310081
 -0.01134235 -0.00805572 -0.13731931  0.0141976  -0.00215586  0.00707307
 -0.01227557 -0.01302924] 0 0
train:	 Loss = 1.2347,	 Acc = 0.5320
40149 0.282
78856 0.596
41020 0.636
5027 0.67
814 0.523
86 0.512
0 0.0
0 0.0
0.6117580661828415
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2784,	 Acc = 0.5265
4718 0.382
24253 0.546
11362 0.555
1141 0.449
29 0.0
0 0.0
0 0.0
0 0.0
0.5450863123555797
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3547,	 Acc1 = 0.2740,	 Acc2 = 0.2741

 ===== Epoch 231	 =====
[-0.36759388 -0.3845501   1.6840346   2.814854   -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01274563  0.02261382  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 0 0
train:	 Loss = 1.2355,	 Acc = 0.5314
40151 0.284
78856 0.596
41020 0.636
5025 0.657
814 0.521
86 0.453
0 0.0
0 0.0
0.6105674835653134
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2114,	 Acc = 0.5569
4718 0.407
24253 0.56
11362 0.617
1141 0.525
29 0.0
0 0.0
0 0.0
0 0.0
0.5762131303520457
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2996,	 Acc1 = 0.2503,	 Acc2 = 0.2455

 ===== Epoch 232	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 3 1
train:	 Loss = 1.2352,	 Acc = 0.5304
40155 0.281
78859 0.594
41018 0.637
5022 0.656
812 0.543
86 0.5
0 0.0
0 0.0
0.6099509527254227
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2240,	 Acc = 0.5518
4718 0.417
24253 0.568
11362 0.583
1141 0.469
29 0.0
0 0.0
0 0.0
0 0.0
0.5690362919668344
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2948,	 Acc1 = 0.2798,	 Acc2 = 0.2811

 ===== Epoch 233	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [ 1.2697468e+00  1.7452302e+00  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -1.1342351e-02 -8.0557177e-03
  1.4306194e-02  2.5413027e-02 -2.1558569e-03  7.0730671e-03
  2.0408077e+00  2.3623075e+00] 6 6
train:	 Loss = 1.2352,	 Acc = 0.5307
40158 0.28
78849 0.595
41018 0.637
5027 0.66
814 0.516
86 0.453
0 0.0
0 0.0
0.610728651605005
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2090,	 Acc = 0.5434
4718 0.397
24253 0.549
11362 0.599
1141 0.484
29 0.0
0 0.0
0 0.0
0 0.0
0.5622128585021068
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 6
Testing:	 Loss = 1.2950,	 Acc1 = 0.2785,	 Acc2 = 0.2796

 ===== Epoch 234	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153  1.3996599   2.9001484  -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572 -0.00191393  0.00578603 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 0 0
train:	 Loss = 1.2339,	 Acc = 0.5313
40160 0.282
78848 0.594
41019 0.639
5025 0.661
814 0.518
86 0.465
0 0.0
0 0.0
0.6110086491986771
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2366,	 Acc = 0.5444
4718 0.404
24253 0.563
11362 0.573
1141 0.463
29 0.0
0 0.0
0 0.0
0 0.0
0.5624847084409406
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3220,	 Acc1 = 0.2697,	 Acc2 = 0.2689

 ===== Epoch 235	 =====
[ 2.0715268   2.1769488  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.13157365 -0.02107102  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 3 0
train:	 Loss = 1.2338,	 Acc = 0.5325
40160 0.281
78847 0.597
41018 0.639
5027 0.66
814 0.522
86 0.465
0 0.0
0 0.0
0.612789366573391
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2055,	 Acc = 0.5537
4718 0.413
24253 0.575
11362 0.577
1141 0.471
29 0.0
0 0.0
0 0.0
0 0.0
0.5718091613429387
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2911,	 Acc1 = 0.2531,	 Acc2 = 0.2490

 ===== Epoch 236	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705  2.6523647   2.9891088
 -0.36039132 -0.36746153  1.9372091   2.2632842  -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.33664504  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.03662845 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 0 0
train:	 Loss = 1.2347,	 Acc = 0.5306
40157 0.28
78847 0.595
41022 0.637
5026 0.658
814 0.521
86 0.523
0 0.0
0 0.0
0.6105091617313884
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2542,	 Acc = 0.5368
4718 0.387
24253 0.546
11362 0.589
1141 0.454
29 0.0
0 0.0
0 0.0
0 0.0
0.5560146798966971
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3348,	 Acc1 = 0.2746,	 Acc2 = 0.2749

 ===== Epoch 237	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 5 1
train:	 Loss = 1.2347,	 Acc = 0.5313
40156 0.281
78855 0.596
41018 0.636
5023 0.667
814 0.506
86 0.5
0 0.0
0 0.0
0.6111800057235525
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2329,	 Acc = 0.5453
4718 0.354
24253 0.567
11362 0.583
1141 0.5
29 0.0
0 0.0
0 0.0
0 0.0
0.5697974717955688
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3115,	 Acc1 = 0.2791,	 Acc2 = 0.2803

 ===== Epoch 238	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 0 1
train:	 Loss = 1.2348,	 Acc = 0.5308
40160 0.281
78853 0.596
41016 0.635
5024 0.66
813 0.51
86 0.453
0 0.0
0 0.0
0.6107463113711524
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2420,	 Acc = 0.5279
4718 0.379
24253 0.551
11362 0.551
1141 0.441
29 0.0
0 0.0
0 0.0
0 0.0
0.5470708169090662
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 6
Testing:	 Loss = 1.3180,	 Acc1 = 0.2558,	 Acc2 = 0.2522

 ===== Epoch 239	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705  1.7510432   1.0788281
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
 -2.9028268e+00 -1.3890784e+00 -1.1342351e-02 -8.0557177e-03
  1.4306194e-02  2.5413027e-02 -2.1558569e-03  7.0730671e-03
 -1.2275568e-02 -1.3029240e-02] 5 5
train:	 Loss = 1.2338,	 Acc = 0.5318
40162 0.281
78849 0.597
41017 0.638
5025 0.662
813 0.509
86 0.395
0 0.0
0 0.0
0.6120438826615788
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2325,	 Acc = 0.5490
4718 0.397
24253 0.564
11362 0.587
1141 0.49
29 0.0
0 0.0
0 0.0
0 0.0
0.5684925920891668
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 1
Testing:	 Loss = 1.3138,	 Acc1 = 0.2527,	 Acc2 = 0.2485

 ===== Epoch 240	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  2.1541314   1.7278713  -0.4089302  -0.41695493  3.0013473   2.1255279
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.3903462e+00  1.1050599e+00
  4.3038260e-03  3.1008099e-03 -3.0154502e+00 -2.3382943e+00
  1.4306194e-02  2.5413027e-02 -4.4047832e+00 -2.9953034e+00
 -1.2275568e-02 -1.3029240e-02] 6 6
train:	 Loss = 1.2340,	 Acc = 0.5310
40162 0.283
78852 0.594
41014 0.636
5024 0.662
814 0.529
86 0.523
0 0.0
0 0.0
0.6101836393989983
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2424,	 Acc = 0.5373
4718 0.39
24253 0.554
11362 0.578
1141 0.413
29 0.0
0 0.0
0 0.0
0 0.0
0.5562049748538807
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3155,	 Acc1 = 0.2769,	 Acc2 = 0.2776

 ===== Epoch 241	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493  1.0284419   2.0691218
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -1.8718569  -2.9289045
 -0.01227557 -0.01302924] 5 5
train:	 Loss = 1.2345,	 Acc = 0.5314
40159 0.283
78851 0.596
41017 0.636
5026 0.657
813 0.512
86 0.453
0 0.0
0 0.0
0.6108289014492062
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2231,	 Acc = 0.5612
4718 0.412
24253 0.58
11362 0.598
1141 0.418
29 0.0
0 0.0
0 0.0
0 0.0
0.5803996194100857
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3012,	 Acc1 = 0.2808,	 Acc2 = 0.2823

 ===== Epoch 242	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493  1.698758    1.6080637
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303  0.37294847  0.37370944
 -0.01227557 -0.01302924] 2 2
train:	 Loss = 1.2338,	 Acc = 0.5320
40159 0.282
78855 0.597
41016 0.637
5023 0.655
814 0.538
85 0.424
0 0.0
0 0.0
0.611870294849475
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2362,	 Acc = 0.5388
4718 0.371
24253 0.56
11362 0.569
1141 0.501
29 0.0
0 0.0
0 0.0
0 0.0
0.5603099089302704
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 1
Testing:	 Loss = 1.3069,	 Acc1 = 0.2956,	 Acc2 = 0.3002

 ===== Epoch 243	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 1 1
train:	 Loss = 1.2338,	 Acc = 0.5304
40156 0.279
78855 0.595
41014 0.636
5027 0.666
814 0.531
86 0.5
0 0.0
0 0.0
0.6105917517250151
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2405,	 Acc = 0.5547
4718 0.398
24253 0.58
11362 0.576
1141 0.472
29 0.0
0 0.0
0 0.0
0 0.0
0.5748266956639935
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3135,	 Acc1 = 0.2874,	 Acc2 = 0.2903

 ===== Epoch 244	 =====
[ 2.1375098   3.223446   -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
  2.0090358   3.451739  ] [ 0.05235793  0.09459399  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.10595607  0.10423422] 4 3
train:	 Loss = 1.2343,	 Acc = 0.5304
40159 0.28
78850 0.593
41021 0.639
5024 0.663
812 0.531
86 0.477
0 0.0
0 0.0
0.6102406334215735
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.1856,	 Acc = 0.5677
4718 0.401
24253 0.578
11362 0.623
1141 0.5
29 0.0
0 0.0
0 0.0
0 0.0
0.5889900774772325
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2691,	 Acc1 = 0.2765,	 Acc2 = 0.2771

 ===== Epoch 245	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  1.3771617   1.0794507  -0.4089302  -0.41695493  1.8699514   3.0304768
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -2.0872023  -1.6171799   0.01430619  0.02541303 -0.19996442 -1.7221804
 -0.01227557 -0.01302924] 5 5
train:	 Loss = 1.2331,	 Acc = 0.5330
40157 0.283
78851 0.598
41019 0.639
5025 0.66
814 0.523
86 0.453
0 0.0
0 0.0
0.6129496402877698
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2252,	 Acc = 0.5384
4718 0.419
24253 0.556
11362 0.555
1141 0.503
29 0.0
0 0.0
0 0.0
0 0.0
0.5537311404104934
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3062,	 Acc1 = 0.2855,	 Acc2 = 0.2880

 ===== Epoch 246	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -1.1342351e-02 -8.0557177e-03
  1.4306194e-02  2.5413027e-02  2.1747591e+00  4.0429602e+00
 -1.2275568e-02 -1.3029240e-02] 5 5
train:	 Loss = 1.2341,	 Acc = 0.5305
40158 0.281
78853 0.594
41014 0.637
5027 0.66
814 0.509
86 0.488
0 0.0
0 0.0
0.6101483377585576
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2104,	 Acc = 0.5549
4718 0.402
24253 0.578
11362 0.579
1141 0.48
29 0.0
0 0.0
0 0.0
0 0.0
0.5746092157129264
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2898,	 Acc1 = 0.2769,	 Acc2 = 0.2776

 ===== Epoch 247	 =====
[ 0.21360587  1.5403934  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.01098861 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 0 0
train:	 Loss = 1.2345,	 Acc = 0.5308
40152 0.282
78854 0.595
41021 0.636
5026 0.662
813 0.509
86 0.442
0 0.0
0 0.0
0.6101510333863275
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2317,	 Acc = 0.5444
4718 0.409
24253 0.561
11362 0.575
1141 0.453
29 0.0
0 0.0
0 0.0
0 0.0
0.5617507136060894
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 6
Testing:	 Loss = 1.3095,	 Acc1 = 0.2948,	 Acc2 = 0.2992

 ===== Epoch 248	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 1 1
train:	 Loss = 1.2337,	 Acc = 0.5311
40160 0.28
78847 0.595
41020 0.638
5025 0.662
814 0.523
86 0.442
0 0.0
0 0.0
0.6112232892393793
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2014,	 Acc = 0.5650
4718 0.389
24253 0.577
11362 0.624
1141 0.472
29 0.0
0 0.0
0 0.0
0 0.0
0.5876308277830637
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 1
Testing:	 Loss = 1.2886,	 Acc1 = 0.2653,	 Acc2 = 0.2637

 ===== Epoch 249	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 1 1
train:	 Loss = 1.2351,	 Acc = 0.5307
40156 0.281
78850 0.596
41022 0.634
5024 0.659
814 0.528
86 0.453
0 0.0
0 0.0
0.6105281566981462
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2214,	 Acc = 0.5566
4718 0.406
24253 0.565
11362 0.608
1141 0.5
29 0.0
0 0.0
0 0.0
0 0.0
0.5759684654070952
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3049,	 Acc1 = 0.2523,	 Acc2 = 0.2480

 ===== Epoch 250	 =====
[ 2.9577436   3.4831605  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-3.0531514e+00 -3.6717484e+00  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -1.1342351e-02 -8.0557177e-03
  1.4306194e-02  2.5413027e-02  2.5176282e+00  3.9188237e+00
 -1.2275568e-02 -1.3029240e-02] 6 6
train:	 Loss = 1.2346,	 Acc = 0.5314
40160 0.28
78850 0.597
41016 0.636
5026 0.661
814 0.525
86 0.5
0 0.0
0 0.0
0.6116048715339608
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2156,	 Acc = 0.5480
4718 0.37
24253 0.562
11362 0.595
1141 0.524
29 0.0
0 0.0
0 0.0
0 0.0
0.5708305015631372
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2959,	 Acc1 = 0.2672,	 Acc2 = 0.2659

 ===== Epoch 251	 =====
[-0.36759388 -0.3845501   1.5932816   1.8192272  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.03885373  0.16481751  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
train:	 Loss = 1.2356,	 Acc = 0.5311
40157 0.281
78851 0.596
41017 0.636
5027 0.653
814 0.532
86 0.407
0 0.0
0 0.0
0.6110179259906992
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2038,	 Acc = 0.5596
4718 0.38
24253 0.567
11362 0.625
1141 0.511
29 0.0
0 0.0
0 0.0
0 0.0
0.5826287889085225
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2831,	 Acc1 = 0.2612,	 Acc2 = 0.2587

 ===== Epoch 252	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [ 2.8628292e+00  2.9862194e+00  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -1.1342351e-02 -8.0557177e-03
  1.4306194e-02  2.5413027e-02 -2.1558569e-03  7.0730671e-03
 -1.2275568e-02 -1.3029240e-02] 6 5
train:	 Loss = 1.2340,	 Acc = 0.5302
40156 0.281
78851 0.594
41021 0.637
5024 0.655
814 0.531
86 0.419
0 0.0
0 0.0
0.6098127126458711
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2499,	 Acc = 0.5358
4718 0.393
24253 0.553
11362 0.57
1141 0.436
29 0.0
0 0.0
0 0.0
0 0.0
0.5540845453309773
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3379,	 Acc1 = 0.2560,	 Acc2 = 0.2525

 ===== Epoch 253	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705  1.4626828   0.85737866
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137 -0.10221039  0.11983633
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 3 3
train:	 Loss = 1.2340,	 Acc = 0.5319
40149 0.282
78855 0.595
41023 0.639
5025 0.672
814 0.514
86 0.43
0 0.0
0 0.0
0.6116865257585272
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2402,	 Acc = 0.5322
4718 0.367
24253 0.546
11362 0.58
1141 0.466
29 0.0
0 0.0
0 0.0
0 0.0
0.5534321054777762
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3191,	 Acc1 = 0.2723,	 Acc2 = 0.2721

 ===== Epoch 254	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 0 1
train:	 Loss = 1.2349,	 Acc = 0.5308
40155 0.28
78854 0.595
41017 0.637
5026 0.661
814 0.512
86 0.43
0 0.0
0 0.0
0.6108015294482381
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2196,	 Acc = 0.5573
4718 0.419
24253 0.572
11362 0.592
1141 0.486
29 0.0
0 0.0
0 0.0
0 0.0
0.5750169906211771
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3003,	 Acc1 = 0.2750,	 Acc2 = 0.2753

 ===== Epoch 255	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 3 1
train:	 Loss = 1.2338,	 Acc = 0.5315
40152 0.281
78858 0.596
41017 0.638
5025 0.662
814 0.515
86 0.477
0 0.0
0 0.0
0.6114069952305247
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2433,	 Acc = 0.5329
4718 0.407
24253 0.555
11362 0.551
1141 0.429
29 0.0
0 0.0
0 0.0
0 0.0
0.5490825064564361
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3194,	 Acc1 = 0.2595,	 Acc2 = 0.2567

 ===== Epoch 256	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705  1.7275279   0.9893536
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137 -0.03359019  0.6646021
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
train:	 Loss = 1.2344,	 Acc = 0.5303
40154 0.282
78858 0.594
41015 0.636
5025 0.661
814 0.507
86 0.488
0 0.0
0 0.0
0.6096042862366652
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2184,	 Acc = 0.5546
4718 0.408
24253 0.582
11362 0.567
1141 0.477
29 0.0
0 0.0
0 0.0
0 0.0
0.5734130759820579
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3002,	 Acc1 = 0.2705,	 Acc2 = 0.2699

 ===== Epoch 257	 =====
[-0.36759388 -0.3845501   1.0980452   3.0126154  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02 -1.9126441e+00 -3.1101124e+00
  2.3450742e+00  1.3801477e+00 -1.1342351e-02 -8.0557177e-03
  1.4306194e-02  2.5413027e-02 -2.1558569e-03  7.0730671e-03
 -1.2275568e-02 -1.3029240e-02] 6 6
train:	 Loss = 1.2342,	 Acc = 0.5321
40156 0.282
78851 0.596
41022 0.639
5023 0.665
814 0.527
86 0.465
0 0.0
0 0.0
0.6120862348564342
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2085,	 Acc = 0.5451
4718 0.413
24253 0.559
11362 0.578
1141 0.477
29 0.0
0 0.0
0 0.0
0 0.0
0.56207693353269
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2877,	 Acc1 = 0.2738,	 Acc2 = 0.2739

 ===== Epoch 258	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [ 1.9983916e+00  2.6223567e+00  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -1.1342351e-02 -8.0557177e-03
  1.4306194e-02  2.5413027e-02 -2.1558569e-03  7.0730671e-03
 -1.2275568e-02 -1.3029240e-02] 5 5
train:	 Loss = 1.2335,	 Acc = 0.5316
40155 0.28
78849 0.597
41022 0.638
5026 0.659
814 0.515
86 0.488
0 0.0
0 0.0
0.6119621294625468
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2071,	 Acc = 0.5517
4718 0.403
24253 0.575
11362 0.573
1141 0.479
29 0.0
0 0.0
0 0.0
0 0.0
0.5707761315753704
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2845,	 Acc1 = 0.2897,	 Acc2 = 0.2930

 ===== Epoch 259	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493  2.0183194   1.6448501
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303  0.25865728  0.01284687
 -0.01227557 -0.01302924] 3 3
train:	 Loss = 1.2342,	 Acc = 0.5315
40157 0.282
78850 0.595
41021 0.637
5025 0.662
813 0.523
86 0.512
0 0.0
0 0.0
0.6111451170555269
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2295,	 Acc = 0.5398
4718 0.401
24253 0.563
11362 0.557
1141 0.455
29 0.0
0 0.0
0 0.0
0 0.0
0.5575642245480494
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 3
Testing:	 Loss = 1.3100,	 Acc1 = 0.2769,	 Acc2 = 0.2776

 ===== Epoch 260	 =====
[-0.36759388 -0.3845501   2.3406556   2.1215522  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.66534644  0.2857968   0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 1 1
train:	 Loss = 1.2339,	 Acc = 0.5324
40154 0.282
78852 0.596
41020 0.64
5027 0.658
813 0.529
86 0.488
0 0.0
0 0.0
0.612195742380642
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2121,	 Acc = 0.5569
4718 0.404
24253 0.576
11362 0.587
1141 0.488
29 0.0
0 0.0
0 0.0
0 0.0
0.576457795296996
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2925,	 Acc1 = 0.2824,	 Acc2 = 0.2843

 ===== Epoch 261	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  1.3142424   1.7335345  -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03  1.1003406e-01 -4.9929518e-01
  1.4306194e-02  2.5413027e-02  2.8741720e+00  4.2739124e+00
 -1.2275568e-02 -1.3029240e-02] 5 5
train:	 Loss = 1.2331,	 Acc = 0.5325
40158 0.282
78843 0.597
41025 0.638
5026 0.661
814 0.515
86 0.465
0 0.0
0 0.0
0.6125649872012974
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2251,	 Acc = 0.5427
4718 0.398
24253 0.564
11362 0.569
1141 0.445
29 0.0
0 0.0
0 0.0
0 0.0
0.561207013728422
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 1
Testing:	 Loss = 1.2968,	 Acc1 = 0.2884,	 Acc2 = 0.2915

 ===== Epoch 262	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493  1.1886028   3.2242193
 -0.39281774 -0.38534638] [ 2.3271906e+00  1.8994502e+00  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -1.1342351e-02 -8.0557177e-03
  1.4306194e-02  2.5413027e-02 -2.0774806e+00 -4.2886353e+00
 -1.2275568e-02 -1.3029240e-02] 6 6
train:	 Loss = 1.2346,	 Acc = 0.5315
40155 0.281
78852 0.597
41021 0.637
5024 0.651
814 0.506
86 0.465
0 0.0
0 0.0
0.6114931198677234
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2037,	 Acc = 0.5516
4718 0.372
24253 0.559
11362 0.619
1141 0.488
29 0.0
0 0.0
0 0.0
0 0.0
0.5746092157129264
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 1
Testing:	 Loss = 1.2920,	 Acc1 = 0.2554,	 Acc2 = 0.2517

 ===== Epoch 263	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 6 1
train:	 Loss = 1.2343,	 Acc = 0.5316
40157 0.281
78845 0.596
41025 0.638
5025 0.663
814 0.517
86 0.43
0 0.0
0 0.0
0.6115902857824238
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2215,	 Acc = 0.5439
4718 0.395
24253 0.556
11362 0.589
1141 0.473
29 0.0
0 0.0
0 0.0
0 0.0
0.5630012233247248
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2991,	 Acc1 = 0.2758,	 Acc2 = 0.2763

 ===== Epoch 264	 =====
[-0.36759388 -0.3845501   1.4302561   2.3534105  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.2540663  -0.6926496   0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 5 5
train:	 Loss = 1.2342,	 Acc = 0.5301
40156 0.281
78849 0.594
41022 0.635
5025 0.667
814 0.506
86 0.477
0 0.0
0 0.0
0.609772965754078
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2308,	 Acc = 0.5551
4718 0.418
24253 0.578
11362 0.571
1141 0.486
29 0.0
0 0.0
0 0.0
0 0.0
0.5726518961533233
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3139,	 Acc1 = 0.2954,	 Acc2 = 0.3000

 ===== Epoch 265	 =====
[ 1.5992234   2.5792518  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [ 1.7561351e+00  1.7483983e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03  1.5086726e+00  3.2763207e+00
  1.4306194e-02  2.5413027e-02 -2.1558569e-03  7.0730671e-03
 -1.2275568e-02 -1.3029240e-02] 2 2
train:	 Loss = 1.2347,	 Acc = 0.5325
40159 0.282
78859 0.596
41008 0.639
5026 0.67
814 0.531
86 0.465
0 0.0
0 0.0
0.6125937055321043
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2331,	 Acc = 0.5521
4718 0.409
24253 0.58
11362 0.562
1141 0.474
29 0.0
0 0.0
0 0.0
0 0.0
0.5704227266548865
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3069,	 Acc1 = 0.3142,	 Acc2 = 0.3226

 ===== Epoch 266	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 6 1
train:	 Loss = 1.2347,	 Acc = 0.5311
40158 0.281
78855 0.596
41015 0.636
5024 0.657
814 0.529
86 0.465
0 0.0
0 0.0
0.6107763486334802
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2057,	 Acc = 0.5600
4718 0.409
24253 0.568
11362 0.612
1141 0.507
29 0.0
0 0.0
0 0.0
0 0.0
0.579339404648634
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2836,	 Acc1 = 0.2911,	 Acc2 = 0.2947

 ===== Epoch 267	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705  2.9291615   2.9891088
 -0.36039132 -0.36746153  1.8196747   2.27312    -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.15817478  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 0 0
train:	 Loss = 1.2327,	 Acc = 0.5317
40158 0.28
78845 0.598
41024 0.636
5026 0.659
813 0.513
86 0.43
0 0.0
0 0.0
0.6121198149355295
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.1758,	 Acc = 0.5687
4718 0.377
24253 0.574
11362 0.641
1141 0.542
29 0.0
0 0.0
0 0.0
0 0.0
0.5932581215169227
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2738,	 Acc1 = 0.2622,	 Acc2 = 0.2599

 ===== Epoch 268	 =====
[ 2.3665946   3.302379   -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
  2.5105858   3.5249763 ] [-2.5121362e+00 -3.5006602e+00  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -1.1342351e-02 -8.0557177e-03
  1.4306194e-02  2.5413027e-02  2.7300885e+00  2.1347189e+00
 -3.5081468e+00 -4.5081286e+00] 2 5
train:	 Loss = 1.2351,	 Acc = 0.5313
40154 0.283
78857 0.596
41017 0.636
5024 0.66
814 0.507
86 0.465
0 0.0
0 0.0
0.6107410292691458
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2213,	 Acc = 0.5559
4718 0.399
24253 0.574
11362 0.586
1141 0.528
29 0.0
0 0.0
0 0.0
0 0.0
0.5760772053826287
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 1
Testing:	 Loss = 1.3079,	 Acc1 = 0.2754,	 Acc2 = 0.2758

 ===== Epoch 269	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [ 1.9261396e+00  3.6151481e+00  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -1.1342351e-02 -8.0557177e-03
  1.4306194e-02  2.5413027e-02 -2.1558569e-03  7.0730671e-03
  2.9463871e+00  4.5782857e+00] 3 5
train:	 Loss = 1.2332,	 Acc = 0.5320
40157 0.282
78846 0.596
41023 0.638
5026 0.667
814 0.511
86 0.442
0 0.0
0 0.0
0.611852617353631
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2325,	 Acc = 0.5439
4718 0.415
24253 0.56
11362 0.575
1141 0.443
29 0.0
0 0.0
0 0.0
0 0.0
0.5603370939241539
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 6
Testing:	 Loss = 1.3040,	 Acc1 = 0.2855,	 Acc2 = 0.2880

 ===== Epoch 270	 =====
[-0.36759388 -0.3845501   1.2680517   2.3852344  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02 -2.1284239e+00 -2.5243180e+00
  4.3038260e-03  3.1008099e-03 -1.1342351e-02 -8.0557177e-03
  1.4306194e-02  2.5413027e-02 -2.1558569e-03  7.0730671e-03
 -1.2275568e-02 -1.3029240e-02] 6 6
train:	 Loss = 1.2349,	 Acc = 0.5309
40155 0.281
78856 0.594
41017 0.638
5025 0.661
813 0.526
86 0.442
0 0.0
0 0.0
0.6106822897207406
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2089,	 Acc = 0.5581
4718 0.409
24253 0.578
11362 0.588
1141 0.468
29 0.0
0 0.0
0 0.0
0 0.0
0.5772189751257306
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 1
Testing:	 Loss = 1.2875,	 Acc1 = 0.2793,	 Acc2 = 0.2806

 ===== Epoch 271	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705  2.4310825   2.5350256
 -0.36039132 -0.36746153  1.6209023   1.7493668  -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  2.8133953e-01 -6.4627500e+00 -1.1342351e-02 -8.0557177e-03
  1.4830102e-01  7.3759258e-01 -2.1558569e-03  7.0730671e-03
 -1.2275568e-02 -1.3029240e-02] 4 4
train:	 Loss = 1.2337,	 Acc = 0.5319
40151 0.28
78860 0.597
41017 0.639
5025 0.656
813 0.512
86 0.43
0 0.0
0 0.0
0.6122367866710121
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.1850,	 Acc = 0.5620
4718 0.41
24253 0.576
11362 0.607
1141 0.465
29 0.0
0 0.0
0 0.0
0 0.0
0.5814598341715373
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2696,	 Acc1 = 0.2798,	 Acc2 = 0.2811

 ===== Epoch 272	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705  2.2344725   1.4971213
 -0.36039132 -0.36746153  0.9370116   1.6485504  -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.11747457  0.09173334
 -0.01134235 -0.00805572 -0.06397554  0.22168298 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 3
train:	 Loss = 1.2343,	 Acc = 0.5306
40160 0.281
78857 0.594
41011 0.638
5024 0.657
814 0.522
86 0.453
0 0.0
0 0.0
0.6103329305520224
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2395,	 Acc = 0.5422
4718 0.413
24253 0.556
11362 0.572
1141 0.504
29 0.0
0 0.0
0 0.0
0 0.0
0.5587059942911513
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3256,	 Acc1 = 0.2841,	 Acc2 = 0.2863

 ===== Epoch 273	 =====
[ 0.23854917  2.8771596  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.01427341 -0.3873436   0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 5 5
train:	 Loss = 1.2347,	 Acc = 0.5317
40154 0.281
78857 0.595
41017 0.639
5024 0.665
814 0.526
86 0.453
0 0.0
0 0.0
0.6116869902542171
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2388,	 Acc = 0.5459
4718 0.401
24253 0.571
11362 0.563
1141 0.452
29 0.0
0 0.0
0 0.0
0 0.0
0.5644963979883104
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3204,	 Acc1 = 0.2723,	 Acc2 = 0.2721

 ===== Epoch 274	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153  3.2099261   2.0641105  -0.42787513 -0.42500633
  3.675448    0.9433785 ] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -1.1342351e-02 -8.0557177e-03
 -4.4153738e+00 -2.8036778e+00  3.5242531e+00  1.4332019e+00
  1.9731405e-01  2.0856860e+00] 6 6
train:	 Loss = 1.2348,	 Acc = 0.5312
40162 0.281
78844 0.595
41021 0.639
5026 0.658
813 0.512
86 0.488
0 0.0
0 0.0
0.6111058112727562
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2360,	 Acc = 0.5451
4718 0.405
24253 0.567
11362 0.568
1141 0.45
29 0.0
0 0.0
0 0.0
0 0.0
0.5630012233247248
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3174,	 Acc1 = 0.2579,	 Acc2 = 0.2547

 ===== Epoch 275	 =====
[ 4.905075    2.2609742  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
  2.1505766   3.150945  ] [-4.83533287e+00 -2.51509809e+00  1.58730093e-02  2.04913728e-02
  4.30382602e-03  3.10080987e-03 -1.13423513e-02 -8.05571768e-03
  1.43061941e-02  2.54130270e-02 -2.15585693e-03  7.07306713e-03
  7.18793273e-02 -1.00225136e-01] 2 2
train:	 Loss = 1.2342,	 Acc = 0.5322
40158 0.281
78852 0.596
41016 0.639
5026 0.66
814 0.529
86 0.465
0 0.0
0 0.0
0.6122549565162091
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2270,	 Acc = 0.5466
4718 0.398
24253 0.569
11362 0.57
1141 0.46
29 0.0
0 0.0
0 0.0
0 0.0
0.5655837977436455
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3023,	 Acc1 = 0.2928,	 Acc2 = 0.2967

 ===== Epoch 276	 =====
[-0.36759388 -0.3845501   1.4516106   1.4600743  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.03515771  1.1666108   0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 6 4
train:	 Loss = 1.2360,	 Acc = 0.5309
40157 0.281
78854 0.595
41017 0.637
5025 0.661
813 0.51
86 0.488
0 0.0
0 0.0
0.6106840494455265
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2522,	 Acc = 0.5430
4718 0.411
24253 0.549
11362 0.596
1141 0.454
29 0.0
0 0.0
0 0.0
0 0.0
0.5599565040097866
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3331,	 Acc1 = 0.2585,	 Acc2 = 0.2555

 ===== Epoch 277	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 1 1
train:	 Loss = 1.2353,	 Acc = 0.5312
40159 0.281
78852 0.595
41016 0.638
5025 0.657
814 0.517
86 0.419
0 0.0
0 0.0
0.6109719936721439
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.1708,	 Acc = 0.5665
4718 0.411
24253 0.573
11362 0.624
1141 0.514
29 0.0
0 0.0
0 0.0
0 0.0
0.586353133070545
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2607,	 Acc1 = 0.2606,	 Acc2 = 0.2580

 ===== Epoch 278	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 1 1
train:	 Loss = 1.2343,	 Acc = 0.5307
40147 0.279
78855 0.595
41026 0.638
5025 0.665
813 0.526
86 0.477
0 0.0
0 0.0
0.611017050196733
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.1825,	 Acc = 0.5632
4718 0.396
24253 0.571
11362 0.623
1141 0.508
29 0.0
0 0.0
0 0.0
0 0.0
0.584613293462009
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 1
Testing:	 Loss = 1.2644,	 Acc1 = 0.2740,	 Acc2 = 0.2741

 ===== Epoch 279	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 3 1
train:	 Loss = 1.2350,	 Acc = 0.5308
40162 0.282
78846 0.595
41020 0.635
5024 0.661
814 0.505
86 0.453
0 0.0
0 0.0
0.6101279910962716
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2007,	 Acc = 0.5594
4718 0.408
24253 0.581
11362 0.586
1141 0.479
29 0.0
0 0.0
0 0.0
0 0.0
0.5787685197770831
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2736,	 Acc1 = 0.2917,	 Acc2 = 0.2955

 ===== Epoch 280	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  2.2769668e+00  1.0904706e+00 -1.1342351e-02 -8.0557177e-03
  1.4306194e-02  2.5413027e-02 -2.1558569e-03  7.0730671e-03
 -1.2275568e-02 -1.3029240e-02] 6 1
train:	 Loss = 1.2338,	 Acc = 0.5311
40162 0.282
78853 0.595
41012 0.636
5025 0.662
814 0.522
86 0.5
0 0.0
0 0.0
0.6107639716988632
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2076,	 Acc = 0.5613
4718 0.396
24253 0.574
11362 0.612
1141 0.483
29 0.0
0 0.0
0 0.0
0 0.0
0.5824656789452223
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2921,	 Acc1 = 0.2692,	 Acc2 = 0.2684

 ===== Epoch 281	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 6 1
train:	 Loss = 1.2339,	 Acc = 0.5317
40152 0.282
78855 0.596
41020 0.638
5026 0.659
813 0.513
86 0.488
0 0.0
0 0.0
0.6113990461049285
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2644,	 Acc = 0.5206
4718 0.393
24253 0.549
11362 0.525
1141 0.41
29 0.0
0 0.0
0 0.0
0 0.0
0.5369308141905668
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3336,	 Acc1 = 0.3070,	 Acc2 = 0.3139

 ===== Epoch 282	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705  0.63730574  0.78579915
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137 -0.18823731  0.02039496
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 1 1
train:	 Loss = 1.2344,	 Acc = 0.5307
40155 0.28
78855 0.595
41016 0.637
5026 0.668
814 0.516
86 0.442
0 0.0
0 0.0
0.6108015294482381
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2494,	 Acc = 0.5262
4718 0.405
24253 0.55
11362 0.536
1141 0.428
29 0.0
0 0.0
0 0.0
0 0.0
0.5416610031262743
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 6
Testing:	 Loss = 1.3297,	 Acc1 = 0.3068,	 Acc2 = 0.3136

 ===== Epoch 283	 =====
[ 2.3002508   2.0114443  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
  2.136511    2.2119446 ] [-0.21004671  0.11628119  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
  0.21054715  0.12828825] 4 3
train:	 Loss = 1.2344,	 Acc = 0.5322
40155 0.283
78853 0.597
41020 0.636
5024 0.661
814 0.517
86 0.465
0 0.0
0 0.0
0.6117951938440503
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.1840,	 Acc = 0.5573
4718 0.418
24253 0.565
11362 0.607
1141 0.486
29 0.0
0 0.0
0 0.0
0 0.0
0.575152915590594
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 6
Testing:	 Loss = 1.2727,	 Acc1 = 0.2672,	 Acc2 = 0.2659

 ===== Epoch 284	 =====
[-0.36759388 -0.3845501   2.0753784   1.7328488  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.22070916  0.21151127  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 5 2
train:	 Loss = 1.2340,	 Acc = 0.5317
40157 0.282
78849 0.596
41020 0.637
5026 0.669
814 0.527
86 0.419
0 0.0
0 0.0
0.611407448626734
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.1942,	 Acc = 0.5636
4718 0.407
24253 0.578
11362 0.606
1141 0.506
29 0.0
0 0.0
0 0.0
0 0.0
0.5837433736577409
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2741,	 Acc1 = 0.2814,	 Acc2 = 0.2831

 ===== Epoch 285	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153  1.3864087   2.8829355  -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -1.1342351e-02 -8.0557177e-03
 -2.1832879e+00 -3.7373621e+00 -2.1558569e-03  7.0730671e-03
  2.8696437e+00  2.9606516e+00] 6 6
train:	 Loss = 1.2356,	 Acc = 0.5312
40158 0.28
78851 0.596
41018 0.637
5025 0.66
814 0.522
86 0.477
0 0.0
0 0.0
0.6113169149561982
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2304,	 Acc = 0.5551
4718 0.407
24253 0.576
11362 0.582
1141 0.466
29 0.0
0 0.0
0 0.0
0 0.0
0.5741470708169091
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3088,	 Acc1 = 0.2911,	 Acc2 = 0.2947

 ===== Epoch 286	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493  2.192175    1.9636669
 -0.39281774 -0.38534638] [ 1.9273572e+00  3.6633418e+00  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -1.1342351e-02 -8.0557177e-03
  1.4306194e-02  2.5413027e-02  3.0701178e-01 -2.7295631e-01
 -1.2275568e-02 -1.3029240e-02] 4 5
train:	 Loss = 1.2352,	 Acc = 0.5321
40154 0.282
78854 0.596
41020 0.639
5024 0.661
814 0.51
86 0.43
0 0.0
0 0.0
0.6117744320259464
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2126,	 Acc = 0.5537
4718 0.389
24253 0.568
11362 0.602
1141 0.46
29 0.0
0 0.0
0 0.0
0 0.0
0.5748266956639935
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2954,	 Acc1 = 0.2653,	 Acc2 = 0.2637

 ===== Epoch 287	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493  1.473544    2.5522518
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303  0.02568428  0.03305517
 -0.01227557 -0.01302924] 3 0
train:	 Loss = 1.2346,	 Acc = 0.5309
40159 0.281
78849 0.594
41017 0.638
5027 0.664
814 0.517
86 0.442
0 0.0
0 0.0
0.610518868299508
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2141,	 Acc = 0.5436
4718 0.407
24253 0.566
11362 0.56
1141 0.494
29 0.0
0 0.0
0 0.0
0 0.0
0.5610982737528885
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2926,	 Acc1 = 0.2959,	 Acc2 = 0.3004

 ===== Epoch 288	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  1.8251034   3.07568    -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.0030664  -1.9509708   0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 5 5
train:	 Loss = 1.2338,	 Acc = 0.5318
40155 0.283
78850 0.596
41023 0.637
5024 0.66
814 0.521
86 0.477
0 0.0
0 0.0
0.6110638568487324
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2003,	 Acc = 0.5553
4718 0.392
24253 0.576
11362 0.587
1141 0.48
29 0.0
0 0.0
0 0.0
0 0.0
0.5762131303520457
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2812,	 Acc1 = 0.2711,	 Acc2 = 0.2706

 ===== Epoch 289	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 0 1
train:	 Loss = 1.2339,	 Acc = 0.5321
40156 0.283
78850 0.596
41022 0.638
5024 0.663
814 0.533
86 0.488
0 0.0
0 0.0
0.6117126140735795
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2038,	 Acc = 0.5498
4718 0.411
24253 0.567
11362 0.584
1141 0.432
29 0.0
0 0.0
0 0.0
0 0.0
0.5676226722848987
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2920,	 Acc1 = 0.2657,	 Acc2 = 0.2642

 ===== Epoch 290	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705  2.9291615   2.9891088
 -0.36039132 -0.36746153  1.9372091   2.27312    -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572 -0.12956241  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 0 0
train:	 Loss = 1.2350,	 Acc = 0.5321
40152 0.281
78856 0.596
41021 0.639
5024 0.657
813 0.536
86 0.5
0 0.0
0 0.0
0.6121462639109698
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2170,	 Acc = 0.5603
4718 0.415
24253 0.571
11362 0.605
1141 0.507
29 0.0
0 0.0
0 0.0
0 0.0
0.5789588147342667
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2936,	 Acc1 = 0.2597,	 Acc2 = 0.2570

 ===== Epoch 291	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705  2.06215     1.9109409
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [ 3.5517163e+00  2.1428287e+00  1.5873009e-02  2.0491373e-02
 -3.3160768e+00 -2.1932566e+00 -1.1342351e-02 -8.0557177e-03
  4.7691016e+00  3.8190308e+00 -2.1558569e-03  7.0730671e-03
  5.2068954e+00  2.7622058e+00] 6 5
train:	 Loss = 1.2340,	 Acc = 0.5321
40155 0.281
78855 0.597
41017 0.639
5025 0.655
814 0.507
86 0.453
0 0.0
0 0.0
0.6123277979602058
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2407,	 Acc = 0.5363
4718 0.381
24253 0.554
11362 0.57
1141 0.474
29 0.0
0 0.0
0 0.0
0 0.0
0.5561506048661139
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3247,	 Acc1 = 0.2703,	 Acc2 = 0.2696

 ===== Epoch 292	 =====
[-0.36759388 -0.3845501   1.0980452   3.0103424  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 0 0
train:	 Loss = 1.2349,	 Acc = 0.5298
40154 0.278
78853 0.594
41018 0.636
5027 0.664
814 0.521
86 0.43
0 0.0
0 0.0
0.6101289368670408
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.1891,	 Acc = 0.5584
4718 0.388
24253 0.574
11362 0.602
1141 0.5
29 0.0
0 0.0
0 0.0
0 0.0
0.5802636944406687
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 1
Testing:	 Loss = 1.2702,	 Acc1 = 0.2946,	 Acc2 = 0.2990

 ===== Epoch 293	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153  2.3831491   1.7346132  -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.8852791  -0.364323   -0.00215586  0.00707307
 -0.01227557 -0.01302924] 2 2
train:	 Loss = 1.2343,	 Acc = 0.5317
40156 0.281
78850 0.596
41019 0.637
5027 0.667
814 0.522
86 0.419
0 0.0
0 0.0
0.6118080066138828
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2062,	 Acc = 0.5553
4718 0.399
24253 0.567
11362 0.601
1141 0.507
29 0.0
0 0.0
0 0.0
0 0.0
0.5753432105477776
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2854,	 Acc1 = 0.2787,	 Acc2 = 0.2798

 ===== Epoch 294	 =====
[ 2.8064582   2.7956808  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-2.9146960e+00 -3.0211325e+00  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -1.1342351e-02 -8.0557177e-03
  1.4306194e-02  2.5413027e-02 -2.1558569e-03  7.0730671e-03
 -1.2275568e-02 -1.3029240e-02] 3 2
train:	 Loss = 1.2356,	 Acc = 0.5313
40153 0.283
78853 0.596
41023 0.636
5025 0.658
812 0.516
86 0.453
0 0.0
0 0.0
0.6106884792406935
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2037,	 Acc = 0.5602
4718 0.397
24253 0.573
11362 0.607
1141 0.505
29 0.0
0 0.0
0 0.0
0 0.0
0.5810520592632866
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2797,	 Acc1 = 0.2895,	 Acc2 = 0.2927

 ===== Epoch 295	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153  2.6602778   3.3034136  -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.8358359e+00  2.4956203e+00 -1.1342351e-02 -8.0557177e-03
 -5.1987037e-02 -1.9569134e+00 -2.1558569e-03  7.0730671e-03
 -1.2275568e-02 -1.3029240e-02] 5 5
train:	 Loss = 1.2346,	 Acc = 0.5317
40153 0.282
78853 0.596
41019 0.637
5027 0.656
814 0.518
86 0.5
0 0.0
0 0.0
0.6115310932519337
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.1681,	 Acc = 0.5660
4718 0.404
24253 0.575
11362 0.621
1141 0.508
29 0.0
0 0.0
0 0.0
0 0.0
0.5867337229849123
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 1
Testing:	 Loss = 1.2533,	 Acc1 = 0.2597,	 Acc2 = 0.2570

 ===== Epoch 296	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153  1.2078015   1.9460816  -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  1.0601774  -0.07272194 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 2 2
train:	 Loss = 1.2350,	 Acc = 0.5318
40160 0.281
78849 0.596
41021 0.638
5023 0.663
813 0.522
86 0.407
0 0.0
0 0.0
0.6117002671076062
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.1831,	 Acc = 0.5651
4718 0.404
24253 0.583
11362 0.599
1141 0.53
29 0.0
0 0.0
0 0.0
0 0.0
0.5857822481989942
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 1
Testing:	 Loss = 1.2708,	 Acc1 = 0.2455,	 Acc2 = 0.2398

 ===== Epoch 297	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -1.1342351e-02 -8.0557177e-03
  2.7351892e+00  1.4441643e+00 -2.1558569e-03  7.0730671e-03
 -1.2275568e-02 -1.3029240e-02] 2 1
train:	 Loss = 1.2345,	 Acc = 0.5321
40152 0.282
78854 0.597
41020 0.637
5026 0.664
814 0.504
86 0.477
0 0.0
0 0.0
0.6120190779014308
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2257,	 Acc = 0.5372
4718 0.366
24253 0.562
11362 0.562
1141 0.479
29 0.0
0 0.0
0 0.0
0 0.0
0.5591409541932854
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.2979,	 Acc1 = 0.2822,	 Acc2 = 0.2840

 ===== Epoch 298	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 0 1
train:	 Loss = 1.2346,	 Acc = 0.5315
40158 0.282
78847 0.596
41022 0.636
5025 0.664
814 0.509
86 0.453
0 0.0
0 0.0
0.6110227832806017
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2269,	 Acc = 0.5420
4718 0.398
24253 0.554
11362 0.582
1141 0.501
29 0.0
0 0.0
0 0.0
0 0.0
0.5605273888813375
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3081,	 Acc1 = 0.2701,	 Acc2 = 0.2694

 ===== Epoch 299	 =====
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493  1.5313692   1.5663722
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227  0.01587301  0.02049137  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303  0.02519509  0.03594207
 -0.01227557 -0.01302924] 3 0
train:	 Loss = 1.2342,	 Acc = 0.5312
40159 0.282
78852 0.596
41018 0.636
5023 0.648
814 0.509
86 0.477
0 0.0
0 0.0
0.6107255570659734
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.1931,	 Acc = 0.5609
4718 0.408
24253 0.571
11362 0.615
1141 0.46
29 0.0
0 0.0
0 0.0
0 0.0
0.5806170993611527
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 6
Testing:	 Loss = 1.2814,	 Acc1 = 0.2622,	 Acc2 = 0.2599

 ===== Epoch 300	 =====
[-0.36759388 -0.3845501   1.4495565   2.4852514  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.13371275  0.21787861  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 1 3
train:	 Loss = 1.2352,	 Acc = 0.5318
40154 0.281
78854 0.597
41019 0.637
5026 0.66
813 0.534
86 0.442
0 0.0
0 0.0
0.6118936708055772
0.6111730324860677
[-0.36759388 -0.3845501  -0.42137614 -0.34023705 -0.4375299  -0.36171135
  3.234153    1.3852559  -0.4089302  -0.41695493  2.3542376   2.8539014
 -0.39281774 -0.38534638] [-9.8298509e-03 -1.1432271e-02  1.5873009e-02  2.0491373e-02
  4.3038260e-03  3.1008099e-03 -4.3057551e+00 -1.9572688e+00
  1.4306194e-02  2.5413027e-02 -1.7310117e-01 -5.7896775e-01
 -1.2275568e-02 -1.3029240e-02] 5 5
val:	 Loss = 1.2367,	 Acc = 0.5381
4718 0.386
24253 0.56
11362 0.567
1141 0.424
29 0.0
0 0.0
0 0.0
0 0.0
0.5575914095419329
0.6111730324860677
[-0.36759388 -0.3845501   1.0902433   2.3397717  -0.4375299  -0.36171135
 -0.36039132 -0.36746153 -0.4089302  -0.41695493 -0.42787513 -0.42500633
 -0.39281774 -0.38534638] [-0.00982985 -0.01143227 -0.00966646  0.21363372  0.00430383  0.00310081
 -0.01134235 -0.00805572  0.01430619  0.02541303 -0.00215586  0.00707307
 -0.01227557 -0.01302924] 4 4
Testing:	 Loss = 1.3075,	 Acc1 = 0.2888,	 Acc2 = 0.2920
