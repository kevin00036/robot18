(0.24300000000005184, array([-1.000000e+00, -1.000000e+00,  1.189783e+03,  1.790000e-01,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 4, array([  0.   ,   0.   , -15.851,   0.091,   0.   ,   0.   ,   0.   ,
         0.   ,   0.   ,   0.   ,   0.   ,   0.   ,   0.   ,   0.   ]), 1)
((0.236, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.364685e+03, -3.300000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), array([0., 0., 0.])), (0.24, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.543573e+03, -3.290000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), array([0., 0., 0.])))
(0.236, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.364685e+03, -3.300000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([0.   , 0.   , 0.   , 0.   , 5.823, 3.129, 0.   , 0.   , 0.   ,
       0.   , 0.   , 0.   , 0.   , 0.   ]))
(0.236, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.364685e+03, -3.300000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([ 0.   ,  0.   ,  0.   ,  0.   , -8.411, -0.011,  0.   ,  0.   ,
        0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ]))
(0.236, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.364685e+03, -3.300000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  6.2433e+01,
       -3.0000e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
        0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]))
(0.236, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.364685e+03, -3.300000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([ 0.00000e+00,  0.00000e+00,  0.00000e+00,  0.00000e+00,
       -1.19043e+02, -4.00000e-03,  0.00000e+00,  0.00000e+00,
        0.00000e+00,  0.00000e+00,  0.00000e+00,  0.00000e+00,
        0.00000e+00,  0.00000e+00]))
(0.236, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.364685e+03, -3.300000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  5.0464e+01,
       -1.3000e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
        0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]))
(0.236, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.364685e+03, -3.300000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([ 0.00000e+00,  0.00000e+00,  0.00000e+00,  0.00000e+00,
       -1.53656e+02, -1.00000e-02,  0.00000e+00,  0.00000e+00,
        0.00000e+00,  0.00000e+00,  0.00000e+00,  0.00000e+00,
        0.00000e+00,  0.00000e+00]))
(0.236, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.364685e+03, -3.300000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([ 0.000000e+00,  0.000000e+00,  2.399328e+03,  1.498000e+00,
       -1.455700e+01, -6.000000e-03,  0.000000e+00,  0.000000e+00,
        0.000000e+00,  0.000000e+00,  0.000000e+00,  0.000000e+00,
        0.000000e+00,  0.000000e+00]))
(0.236, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.364685e+03, -3.300000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  1.9086e+01,
       -9.0000e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
        0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]))
(0.236, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.364685e+03, -3.300000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([ 0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  7.052e+01,
       -2.000e-02,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,
        0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00]))
(0.236, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.364685e+03, -3.300000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  4.1406e+01,
       -1.7000e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
        0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]))
14 1 14

 ===== Epoch 1	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096  1.4243863   2.2074687
 -0.3977158  -0.3905835 ] [ 2.328966    2.6398308   0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -2.1962483  -2.8960352
  2.1422496   3.3513505 ] 5 2
train:	 Loss = 1.4025,	 Acc = 0.4661
48802 0.28
95390 0.515
49907 0.541
5744 0.568
901 0.527
88 0.307
0 0.0
0 0.0
0.5256725646254029
0.0
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3833,	 Acc = 0.4603
5486 0.352
29114 0.501
14300 0.431
1295 0.351
29 0.0
0 0.0
0 0.0
0 0.0
0.47353480262863784
0.47353480262863784
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.4559,	 Acc1 = 0.2156,	 Acc2 = 0.2038

 ===== Epoch 2	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096  1.0083141   3.104465
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.30101705  0.12624612
 -0.01747011 -0.0158521 ] 1 1
train:	 Loss = 1.3222,	 Acc = 0.4920
48801 0.284
95391 0.547
49907 0.575
5743 0.614
902 0.553
88 0.33
0 0.0
0 0.0
0.5586492228558649
0.47353480262863784
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.2972,	 Acc = 0.4976
5486 0.411
29114 0.514
14300 0.495
1295 0.52
29 0.69
0 0.0
0 0.0
0 0.0
0.5081809647279717
0.5081809647279717
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3610,	 Acc1 = 0.2515,	 Acc2 = 0.2470

 ===== Epoch 3	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 1 1
train:	 Loss = 1.3043,	 Acc = 0.4992
48808 0.285
95388 0.559
49902 0.581
5744 0.61
902 0.584
88 0.443
0 0.0
0 0.0
0.5679366415829079
0.5081809647279717
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 6
val:	 Loss = 1.2522,	 Acc = 0.5162
5486 0.384
29114 0.541
14300 0.523
1295 0.461
29 0.0
0 0.0
0 0.0
0 0.0
0.5324109258348607
0.5324109258348607
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3291,	 Acc1 = 0.2338,	 Acc2 = 0.2256

 ===== Epoch 4	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096  1.8700689   1.9653046
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
  5.4487996   1.9071306   0.0244112   0.04163946  1.0382324   0.66438615
 -0.01747011 -0.0158521 ] 2 2
train:	 Loss = 1.2907,	 Acc = 0.5020
48800 0.287
95391 0.561
49910 0.585
5741 0.622
902 0.565
88 0.58
0 0.0
0 0.0
0.5710639865291518
0.5324109258348607
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3014,	 Acc = 0.4897
5486 0.389
29114 0.518
14300 0.472
1295 0.486
29 0.0
0 0.0
0 0.0
0 0.0
0.5020564173633153
0.5324109258348607
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3737,	 Acc1 = 0.2152,	 Acc2 = 0.2033

 ===== Epoch 5	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  2.6491079e+00  2.6448283e+00 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.7470114e-02 -1.5852105e-02] 5 6
train:	 Loss = 1.2843,	 Acc = 0.5044
48802 0.288
95388 0.563
49908 0.59
5744 0.624
902 0.558
88 0.455
0 0.0
0 0.0
0.573985397618891
0.5324109258348607
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.2691,	 Acc = 0.4946
5486 0.364
29114 0.519
14300 0.496
1295 0.49
29 0.0
0 0.0
0 0.0
0 0.0
0.5106397246188922
0.5324109258348607
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 1
Testing:	 Loss = 1.3496,	 Acc1 = 0.2288,	 Acc2 = 0.2197

 ===== Epoch 6	 =====
[ 2.5296457   1.5640268  -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-2.6212647e+00 -1.8244867e+00  2.2577252e+00  1.7111124e+00
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.7470114e-02 -1.5852105e-02] 6 5
train:	 Loss = 1.2815,	 Acc = 0.5031
48799 0.288
95395 0.56
49905 0.59
5743 0.623
902 0.544
88 0.455
0 0.0
0 0.0
0.5721915636736761
0.5324109258348607
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3270,	 Acc = 0.4758
5486 0.364
29114 0.511
14300 0.444
1295 0.511
29 0.0
0 0.0
0 0.0
0 0.0
0.4895167419196209
0.5324109258348607
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3932,	 Acc1 = 0.2257,	 Acc2 = 0.2160

 ===== Epoch 7	 =====
[ 2.3645332   2.1485214  -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  2.1628535   2.3289073 ] [-0.08691356 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 0 0
train:	 Loss = 1.2789,	 Acc = 0.5043
48800 0.285
95400 0.563
49902 0.592
5740 0.626
902 0.551
88 0.523
0 0.0
0 0.0
0.5745566722795201
0.5324109258348607
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.2601,	 Acc = 0.5030
5486 0.36
29114 0.514
14300 0.53
1295 0.582
29 0.0
0 0.0
0 0.0
0 0.0
0.5204971165452188
0.5324109258348607
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3306,	 Acc1 = 0.2519,	 Acc2 = 0.2475

 ===== Epoch 8	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143  2.9485798   3.0035195
 -0.3580977  -0.36433512  1.9468825   2.2423005  -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 0 0
train:	 Loss = 1.2752,	 Acc = 0.5037
48800 0.284
95393 0.562
49906 0.593
5743 0.627
902 0.532
88 0.477
0 0.0
0 0.0
0.5743593454009682
0.5324109258348607
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 4
val:	 Loss = 1.3371,	 Acc = 0.4739
5486 0.358
29114 0.486
14300 0.483
1295 0.61
29 0.0
0 0.0
0 0.0
0 0.0
0.48810854307300283
0.5324109258348607
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3912,	 Acc1 = 0.2622,	 Acc2 = 0.2599

 ===== Epoch 9	 =====
[-0.36866695 -0.38685238  0.64375806  1.7323469  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [ 5.1597655e-01  1.9226047e+00 -1.2413778e+00 -1.8508177e+00
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.7470114e-02 -1.5852105e-02] 6 6
train:	 Loss = 1.2747,	 Acc = 0.5039
48799 0.285
95394 0.562
49905 0.593
5744 0.627
902 0.522
88 0.5
0 0.0
0 0.0
0.5742174396348161
0.5324109258348607
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.2718,	 Acc = 0.5066
5486 0.361
29114 0.546
14300 0.479
1295 0.556
29 0.0
0 0.0
0 0.0
0 0.0
0.524475837095981
0.5324109258348607
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3362,	 Acc1 = 0.2534,	 Acc2 = 0.2493

 ===== Epoch 10	 =====
[-0.36866695 -0.38685238  1.3578576   2.9522464  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-1.7616821e-02 -2.3137011e-02 -2.0896840e+00 -2.9562440e+00
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.7470114e-02 -1.5852105e-02] 6 6
train:	 Loss = 1.2711,	 Acc = 0.5059
48801 0.287
95392 0.564
49906 0.594
5744 0.619
901 0.522
88 0.534
0 0.0
0 0.0
0.5759877919634812
0.5324109258348607
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3309,	 Acc = 0.4864
5486 0.402
29114 0.518
14300 0.454
1295 0.505
29 0.0
0 0.0
0 0.0
0 0.0
0.4967365550538692
0.5324109258348607
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 6
Testing:	 Loss = 1.3962,	 Acc1 = 0.2569,	 Acc2 = 0.2535

 ===== Epoch 11	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [ 2.9505892e+00  9.5185719e+00  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.7470114e-02 -1.5852105e-02] 2 1
train:	 Loss = 1.2712,	 Acc = 0.5065
48798 0.289
95393 0.565
49909 0.593
5742 0.62
902 0.529
88 0.545
0 0.0
0 0.0
0.5763447649867792
0.5324109258348607
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3064,	 Acc = 0.4946
5486 0.382
29114 0.521
14300 0.481
1295 0.534
29 0.0
0 0.0
0 0.0
0 0.0
0.5084268407170638
0.5324109258348607
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3743,	 Acc1 = 0.2509,	 Acc2 = 0.2463

 ===== Epoch 12	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
  2.0983746   3.1820962  -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-1.7616821e-02 -2.3137011e-02  1.7287642e+00  1.6611495e+00
  1.0224475e-02  6.4758179e-03 -2.7161517e+00 -3.6653204e+00
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.7470114e-02 -1.5852105e-02] 6 6
train:	 Loss = 1.2708,	 Acc = 0.5062
48797 0.287
95396 0.565
49908 0.594
5741 0.623
902 0.542
88 0.511
0 0.0
0 0.0
0.5766501134607163
0.5324109258348607
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 4
val:	 Loss = 1.2925,	 Acc = 0.5136
5486 0.363
29114 0.54
14300 0.513
1295 0.58
29 0.0
0 0.0
0 0.0
0 0.0
0.5320085833072555
0.5324109258348607
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3464,	 Acc1 = 0.2926,	 Acc2 = 0.2965

 ===== Epoch 13	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
  2.0242379   1.5747621  -0.41367632 -0.42256096  2.7437444   1.9875443
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  1.1505984   3.0913343   0.01022447  0.00647582
 -2.634657   -2.0092032   0.0244112   0.04163946 -3.7533364  -2.6541436
 -0.01747011 -0.0158521 ] 6 5
train:	 Loss = 1.2685,	 Acc = 0.5062
48802 0.287
95394 0.564
49904 0.595
5742 0.621
902 0.56
88 0.523
0 0.0
0 0.0
0.576557258435835
0.5324109258348607
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3302,	 Acc = 0.5026
5486 0.377
29114 0.528
14300 0.493
1295 0.577
29 0.0
0 0.0
0 0.0
0 0.0
0.5180160042916536
0.5324109258348607
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.4062,	 Acc1 = 0.2197,	 Acc2 = 0.2087

 ===== Epoch 14	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  3.0339985e+00  1.2011217e+00 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.7470114e-02 -1.5852105e-02] 3 2
train:	 Loss = 1.2693,	 Acc = 0.5061
48806 0.286
95383 0.565
49909 0.595
5744 0.61
902 0.55
88 0.489
0 0.0
0 0.0
0.5766776735558391
0.5324109258348607
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3253,	 Acc = 0.4975
5486 0.359
29114 0.528
14300 0.483
1295 0.556
29 0.0
0 0.0
0 0.0
0 0.0
0.5143949215432071
0.5324109258348607
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3828,	 Acc1 = 0.2862,	 Acc2 = 0.2888

 ===== Epoch 15	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143  1.6614231   1.6010976
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
 -2.5727878e+00 -1.8361497e+00 -1.5871393e-02 -1.1254287e-02
  1.8296560e+00  1.9586754e+00 -1.1927608e-02 -1.4941680e-03
 -1.7470114e-02 -1.5852105e-02] 6 5
train:	 Loss = 1.2687,	 Acc = 0.5054
48805 0.286
95385 0.565
49908 0.594
5744 0.614
902 0.552
88 0.511
0 0.0
0 0.0
0.5758187690344478
0.5324109258348607
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3035,	 Acc = 0.4773
5486 0.356
29114 0.505
14300 0.461
1295 0.572
29 0.0
0 0.0
0 0.0
0 0.0
0.4921319683490545
0.5324109258348607
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3756,	 Acc1 = 0.2387,	 Acc2 = 0.2316

 ===== Epoch 16	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 0 1
train:	 Loss = 1.2694,	 Acc = 0.5052
48807 0.287
95387 0.562
49905 0.596
5743 0.617
902 0.561
88 0.511
0 0.0
0 0.0
0.5751685578029929
0.5324109258348607
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.2558,	 Acc = 0.5166
5486 0.406
29114 0.538
14300 0.519
1295 0.482
29 0.034
0 0.0
0 0.0
0 0.0
0.5301980419330323
0.5324109258348607
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3377,	 Acc1 = 0.2199,	 Acc2 = 0.2090

 ===== Epoch 17	 =====
[ 2.2033982   2.8278685  -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-2.3281860e+00 -2.9914582e+00  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.7470114e-02 -1.5852105e-02] 4 1
train:	 Loss = 1.2688,	 Acc = 0.5049
48806 0.287
95393 0.563
49900 0.594
5743 0.616
902 0.555
88 0.5
0 0.0
0 0.0
0.5749213950245353
0.5324109258348607
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3127,	 Acc = 0.4888
5486 0.409
29114 0.513
14300 0.47
1295 0.494
29 0.0
0 0.0
0 0.0
0 0.0
0.49850239170280297
0.5324109258348607
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3817,	 Acc1 = 0.2410,	 Acc2 = 0.2343

 ===== Epoch 18	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [ 5.0030512e-01  2.0717309e+00  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.7470114e-02 -1.5852105e-02] 5 6
train:	 Loss = 1.2677,	 Acc = 0.5048
48799 0.285
95388 0.563
49913 0.596
5742 0.611
902 0.558
88 0.489
0 0.0
0 0.0
0.575322462886347
0.5324109258348607
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3023,	 Acc = 0.4869
5486 0.365
29114 0.5
14300 0.502
1295 0.542
29 0.0
0 0.0
0 0.0
0 0.0
0.501832893736868
0.5324109258348607
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3679,	 Acc1 = 0.2381,	 Acc2 = 0.2309

 ===== Epoch 19	 =====
[ 1.3321291   2.2561915  -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.20000657  0.322457    0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 1 1
train:	 Loss = 1.2679,	 Acc = 0.5049
48804 0.287
95385 0.562
49911 0.596
5742 0.607
902 0.547
88 0.511
0 0.0
0 0.0
0.5748480543057858
0.5324109258348607
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3431,	 Acc = 0.4597
5486 0.353
29114 0.484
14300 0.448
1295 0.499
29 0.0
0 0.0
0 0.0
0 0.0
0.4728195270240064
0.5324109258348607
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.4067,	 Acc1 = 0.2684,	 Acc2 = 0.2674

 ===== Epoch 20	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096  1.8573791   2.985854
 -0.3977158  -0.3905835 ] [ 3.1494691   1.4089478   0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -2.7072604  -3.7521667
 -0.01747011 -0.0158521 ] 1 2
train:	 Loss = 1.2676,	 Acc = 0.5050
48796 0.288
95395 0.562
49908 0.595
5744 0.618
901 0.555
88 0.523
0 0.0
0 0.0
0.5747454550238101
0.5324109258348607
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3461,	 Acc = 0.4690
5486 0.364
29114 0.489
14300 0.468
1295 0.495
29 0.0
0 0.0
0 0.0
0 0.0
0.48187223389512274
0.5324109258348607
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.4129,	 Acc1 = 0.2534,	 Acc2 = 0.2493

 ===== Epoch 21	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  0.7661219   1.0486867 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.3152765e+00 -1.5751606e+00] 5 6
train:	 Loss = 1.2673,	 Acc = 0.5042
48788 0.288
95399 0.562
49913 0.594
5742 0.606
902 0.537
88 0.466
0 0.0
0 0.0
0.5737155034069086
0.5324109258348607
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3261,	 Acc = 0.5081
5486 0.363
29114 0.547
14300 0.483
1295 0.527
29 0.0
0 0.0
0 0.0
0 0.0
0.5259063883052438
0.5324109258348607
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3811,	 Acc1 = 0.2862,	 Acc2 = 0.2888

 ===== Epoch 22	 =====
[-0.36866695 -0.38685238  2.2496526   1.5324764  -0.43314552 -0.35780522
  1.5490655   3.8942428  -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [ 2.4143436e+00  1.8444909e+00 -3.1490817e+00 -1.6697025e+00
  1.0224475e-02  6.4758179e-03  1.5160923e-01 -2.9020920e+00
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.7470114e-02 -1.5852105e-02] 5 5
train:	 Loss = 1.2672,	 Acc = 0.5051
48804 0.288
95387 0.563
49910 0.594
5741 0.62
902 0.537
88 0.489
0 0.0
0 0.0
0.5748612097771463
0.5324109258348607
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.2890,	 Acc = 0.5177
5486 0.408
29114 0.541
14300 0.513
1295 0.511
29 0.0
0 0.0
0 0.0
0 0.0
0.5310474317135321
0.5324109258348607
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3539,	 Acc1 = 0.2645,	 Acc2 = 0.2627

 ===== Epoch 23	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096  1.6097355   2.479286
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946  0.05070065  0.35454795
 -0.01747011 -0.0158521 ] 6 4
train:	 Loss = 1.2677,	 Acc = 0.5051
48807 0.286
95384 0.564
49912 0.594
5740 0.612
901 0.541
88 0.511
0 0.0
0 0.0
0.5754514060187469
0.5324109258348607
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.2750,	 Acc = 0.5189
5486 0.382
29114 0.549
14300 0.51
1295 0.535
29 0.0
0 0.0
0 0.0
0 0.0
0.5356967231436363
0.5356967231436363
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3515,	 Acc1 = 0.2300,	 Acc2 = 0.2212

 ===== Epoch 24	 =====
[-0.36866695 -0.38685238  2.5097766   2.214793   -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.31799453  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 0 3
train:	 Loss = 1.2673,	 Acc = 0.5046
48797 0.287
95394 0.563
49910 0.593
5741 0.615
902 0.541
88 0.5
0 0.0
0 0.0
0.5744466734633472
0.5356967231436363
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3266,	 Acc = 0.4891
5486 0.366
29114 0.519
14300 0.474
1295 0.524
29 0.0
0 0.0
0 0.0
0 0.0
0.5041351870892753
0.5356967231436363
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3915,	 Acc1 = 0.2725,	 Acc2 = 0.2724

 ===== Epoch 25	 =====
[-0.36866695 -0.38685238  2.0007126   1.4773397  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.38884908  0.21431072  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 2 2
train:	 Loss = 1.2672,	 Acc = 0.5055
48797 0.288
95395 0.562
49909 0.597
5741 0.611
902 0.547
88 0.5
0 0.0
0 0.0
0.5754530206860262
0.5356967231436363
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.2794,	 Acc = 0.5165
5486 0.396
29114 0.536
14300 0.526
1295 0.483
29 0.345
0 0.0
0 0.0
0 0.0
0.5312486029773347
0.5356967231436363
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 1
Testing:	 Loss = 1.3622,	 Acc1 = 0.2261,	 Acc2 = 0.2165

 ===== Epoch 26	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
  3.4952881   1.560462   -0.41367632 -0.42256096  3.370929    1.980131
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  2.203598    1.5466515   0.01022447  0.00647582
  0.03418038  2.3668356   0.0244112   0.04163946 -4.493531   -2.64599
 -0.01747011 -0.0158521 ] 6 6
train:	 Loss = 1.2662,	 Acc = 0.5056
48800 0.285
95396 0.562
49904 0.598
5742 0.624
902 0.543
88 0.5
0 0.0
0 0.0
0.5763589244369607
0.5356967231436363
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3093,	 Acc = 0.4916
5486 0.4
29114 0.516
14300 0.48
1295 0.479
29 0.0
0 0.0
0 0.0
0 0.0
0.5028163976932362
0.5356967231436363
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3732,	 Acc1 = 0.2482,	 Acc2 = 0.2430

 ===== Epoch 27	 =====
[-0.36866695 -0.38685238  3.3829334  -4.360259    2.2426572   1.0715431
 -0.3580977  -0.36433512  0.97589785  0.91961336 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-1.7616821e-02 -2.3137011e-02  2.1466134e-01  6.8177252e+00
  7.2317910e-01  4.7082585e-01 -1.5871393e-02 -1.1254287e-02
  5.6429195e-01  8.3317280e-01 -1.1927608e-02 -1.4941680e-03
 -1.7470114e-02 -1.5852105e-02] 2 2
train:	 Loss = 1.2673,	 Acc = 0.5050
48799 0.285
95393 0.563
49906 0.595
5744 0.621
902 0.557
88 0.489
0 0.0
0 0.0
0.5754737458314971
0.5356967231436363
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3115,	 Acc = 0.4866
5486 0.365
29114 0.509
14300 0.483
1295 0.541
29 0.0
0 0.0
0 0.0
0 0.0
0.5015199606598417
0.5356967231436363
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3732,	 Acc1 = 0.2705,	 Acc2 = 0.2699

 ===== Epoch 28	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 1 1
train:	 Loss = 1.2667,	 Acc = 0.5057
48801 0.286
95387 0.564
49913 0.595
5741 0.613
902 0.527
88 0.523
0 0.0
0 0.0
0.5761061888693754
0.5356967231436363
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.2860,	 Acc = 0.5159
5486 0.344
29114 0.543
14300 0.525
1295 0.528
29 0.552
0 0.0
0 0.0
0 0.0
0.5369037507264518
0.5369037507264518
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3583,	 Acc1 = 0.2503,	 Acc2 = 0.2455

 ===== Epoch 29	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
  3.999063    1.7120433  -0.41367632 -0.42256096  3.3870792   2.1259239
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  2.5524666   1.2552019   0.01022447  0.00647582
  0.10540774  1.5535113   0.0244112   0.04163946 -4.5125914  -2.8063447
 -0.01747011 -0.0158521 ] 6 4
train:	 Loss = 1.2662,	 Acc = 0.5062
48794 0.288
95394 0.564
49911 0.596
5743 0.621
902 0.535
88 0.523
0 0.0
0 0.0
0.5762177876583485
0.5369037507264518
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3365,	 Acc = 0.5094
5486 0.406
29114 0.54
14300 0.489
1295 0.486
29 0.69
0 0.0
0 0.0
0 0.0
0.5220170772050605
0.5369037507264518
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3948,	 Acc1 = 0.2734,	 Acc2 = 0.2734

 ===== Epoch 30	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143  0.7529787   2.3168938
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
 -1.4524983e+00 -2.5094573e+00 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.7470114e-02 -1.5852105e-02] 5 6
train:	 Loss = 1.2649,	 Acc = 0.5066
48800 0.286
95397 0.565
49902 0.597
5743 0.621
902 0.548
88 0.534
0 0.0
0 0.0
0.5773587139549569
0.5369037507264518
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3157,	 Acc = 0.5150
5486 0.41
29114 0.541
14300 0.504
1295 0.507
29 0.0
0 0.0
0 0.0
0 0.0
0.5278286914926907
0.5369037507264518
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 1
Testing:	 Loss = 1.3868,	 Acc1 = 0.2688,	 Acc2 = 0.2679

 ===== Epoch 31	 =====
[ 0.10957076  1.1384736  -0.41997173 -0.34217143 -0.43314552 -0.35780522
  3.409476    1.2572992  -0.41367632 -0.42256096  3.9865782   1.5526371
  3.7863793   3.2128067 ] [-0.4472332  -1.4315511   0.02226749  0.02903207  0.01022447  0.00647582
  1.6929426   0.84332573  0.0244112   0.04163946  0.6701776   0.6263358
  1.7190752   0.36550054] 2 2
train:	 Loss = 1.2660,	 Acc = 0.5061
48806 0.286
95386 0.565
49909 0.595
5741 0.627
902 0.52
88 0.557
0 0.0
0 0.0
0.5768355412889901
0.5369037507264518
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3741,	 Acc = 0.4682
5486 0.366
29114 0.495
14300 0.453
1295 0.487
29 0.0
0 0.0
0 0.0
0 0.0
0.4807322634002414
0.5369037507264518
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.4276,	 Acc1 = 0.2791,	 Acc2 = 0.2803

 ===== Epoch 32	 =====
[-0.36866695 -0.38685238  1.7248483   3.057925   -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.13888447  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 0 0
train:	 Loss = 1.2646,	 Acc = 0.5069
48801 0.287
95395 0.566
49903 0.597
5743 0.617
902 0.538
88 0.534
0 0.0
0 0.0
0.5774940637106906
0.5369037507264518
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.2948,	 Acc = 0.5083
5486 0.403
29114 0.541
14300 0.485
1295 0.479
29 0.0
0 0.0
0 0.0
0 0.0
0.5213018016004292
0.5369037507264518
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3604,	 Acc1 = 0.2612,	 Acc2 = 0.2587

 ===== Epoch 33	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512  1.5573293   3.2117841  -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [ 1.9635650e+00  2.4575653e+00  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
 -2.2530620e+00 -3.8818414e+00 -1.1927608e-02 -1.4941680e-03
  2.0101733e+00  3.4106722e+00] 6 6
train:	 Loss = 1.2639,	 Acc = 0.5062
48800 0.286
95394 0.565
49907 0.597
5741 0.618
902 0.535
88 0.545
0 0.0
0 0.0
0.5769837928857082
0.5369037507264518
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3297,	 Acc = 0.4928
5486 0.408
29114 0.529
14300 0.457
1295 0.456
29 0.0
0 0.0
0 0.0
0 0.0
0.5031516831329071
0.5369037507264518
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.4005,	 Acc1 = 0.2387,	 Acc2 = 0.2316

 ===== Epoch 34	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143  2.705786    2.9967878
 -0.3580977  -0.36433512  1.9492015   2.2252495  -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207 -0.033232    0.01491855
 -0.01587139 -0.01125429 -0.11491252  0.06004721 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 0 0
train:	 Loss = 1.2638,	 Acc = 0.5078
48800 0.286
95393 0.566
49909 0.6
5740 0.622
902 0.529
88 0.534
0 0.0
0 0.0
0.579121500736687
0.5369037507264518
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3596,	 Acc = 0.4973
5486 0.363
29114 0.534
14300 0.475
1295 0.502
29 0.0
0 0.0
0 0.0
0 0.0
0.5137690553891546
0.5369037507264518
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.4279,	 Acc1 = 0.2331,	 Acc2 = 0.2249

 ===== Epoch 35	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512  1.0715483   2.0230708  -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.35128614 -0.03462122 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 2 2
train:	 Loss = 1.2627,	 Acc = 0.5089
48805 0.286
95387 0.567
49907 0.601
5743 0.629
902 0.543
88 0.568
0 0.0
0 0.0
0.5806271254448223
0.5369037507264518
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3139,	 Acc = 0.5040
5486 0.367
29114 0.534
14300 0.494
1295 0.527
29 0.0
0 0.0
0 0.0
0 0.0
0.5208547543475346
0.5369037507264518
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3776,	 Acc1 = 0.2705,	 Acc2 = 0.2699

 ===== Epoch 36	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512  1.9492015   3.0972974  -0.4264405  -0.4242137
  3.1937335   2.5401044 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.33588052  0.09423304  4.2050943   1.3655989
  0.39979     0.14798827] 2 2
train:	 Loss = 1.2616,	 Acc = 0.5092
48801 0.288
95390 0.568
49909 0.598
5742 0.63
902 0.529
88 0.545
0 0.0
0 0.0
0.5802303477580231
0.5369037507264518
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3262,	 Acc = 0.4843
5486 0.348
29114 0.514
14300 0.475
1295 0.502
29 0.0
0 0.0
0 0.0
0 0.0
0.5010952657695918
0.5369037507264518
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3922,	 Acc1 = 0.2595,	 Acc2 = 0.2567

 ===== Epoch 37	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143  2.7704535   1.6392434
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-1.7616821e-02 -2.3137011e-02  3.3702652e+00  1.1573582e+00
 -3.9404383e+00 -1.8720312e+00  1.4465936e+00  2.3167396e+00
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.7470114e-02 -1.5852105e-02] 5 5
train:	 Loss = 1.2616,	 Acc = 0.5104
48801 0.287
95392 0.57
49908 0.6
5741 0.624
902 0.54
88 0.568
0 0.0
0 0.0
0.5819471028934888
0.5369037507264518
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.2831,	 Acc = 0.5121
5486 0.365
29114 0.539
14300 0.513
1295 0.538
29 0.0
0 0.0
0 0.0
0 0.0
0.5301086324824534
0.5369037507264518
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3590,	 Acc1 = 0.2292,	 Acc2 = 0.2202

 ===== Epoch 38	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096  1.2563428   1.5476949
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946  0.662916    0.42793068
 -0.01747011 -0.0158521 ] 2 2
train:	 Loss = 1.2595,	 Acc = 0.5108
48800 0.288
95392 0.57
49908 0.6
5742 0.628
902 0.541
88 0.557
0 0.0
0 0.0
0.5822721532308988
0.5369037507264518
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3844,	 Acc = 0.4845
5486 0.37
29114 0.516
14300 0.466
1295 0.483
29 0.0
0 0.0
0 0.0
0 0.0
0.49861415351602667
0.5369037507264518
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.4446,	 Acc1 = 0.2560,	 Acc2 = 0.2525

 ===== Epoch 39	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
  3.9815514   1.9522854  -0.41367632 -0.42256096  3.94274     2.368088
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -4.786236   -2.3981845   0.0244112   0.04163946 -0.00920452 -0.34122896
 -0.01747011 -0.0158521 ] 3 0
train:	 Loss = 1.2591,	 Acc = 0.5103
48802 0.286
95386 0.569
49910 0.606
5744 0.618
902 0.5
88 0.534
0 0.0
0 0.0
0.5823850555811353
0.5369037507264518
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3211,	 Acc = 0.5030
5486 0.409
29114 0.538
14300 0.471
1295 0.486
29 0.0
0 0.0
0 0.0
0 0.0
0.5144843309937861
0.5369037507264518
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3827,	 Acc1 = 0.2787,	 Acc2 = 0.2798

 ===== Epoch 40	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 1 1
train:	 Loss = 1.2586,	 Acc = 0.5108
48805 0.287
95390 0.57
49905 0.603
5742 0.623
902 0.525
88 0.511
0 0.0
0 0.0
0.5825873035710762
0.5369037507264518
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.2857,	 Acc = 0.5007
5486 0.409
29114 0.516
14300 0.506
1295 0.496
29 0.0
0 0.0
0 0.0
0 0.0
0.5120255711028656
0.5369037507264518
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3555,	 Acc1 = 0.2459,	 Acc2 = 0.2403

 ===== Epoch 41	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143  2.972588   -4.899408
 -0.3580977  -0.36433512  1.9932581   1.95243    -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
 -4.1897101e+00  4.2784963e+00 -1.5871393e-02 -1.1254287e-02
 -2.7567725e+00 -2.5222974e+00 -1.1927608e-02 -1.4941680e-03
 -1.7470114e-02 -1.5852105e-02] 3 6
train:	 Loss = 1.2571,	 Acc = 0.5122
48801 0.287
95401 0.571
49899 0.605
5741 0.63
902 0.544
88 0.545
0 0.0
0 0.0
0.5844005498878518
0.5369037507264518
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3663,	 Acc = 0.5131
5486 0.388
29114 0.533
14300 0.522
1295 0.51
29 0.0
0 0.0
0 0.0
0 0.0
0.5285439670973222
0.5369037507264518
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.4377,	 Acc1 = 0.2259,	 Acc2 = 0.2162

 ===== Epoch 42	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143  1.6927884   1.2645164
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 0 3
train:	 Loss = 1.2574,	 Acc = 0.5124
48797 0.287
95398 0.572
49905 0.605
5742 0.625
902 0.523
88 0.5
0 0.0
0 0.0
0.5846285394810405
0.5369037507264518
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3385,	 Acc = 0.4949
5486 0.397
29114 0.541
14300 0.446
1295 0.419
29 0.0
0 0.0
0 0.0
0 0.0
0.5069515847825116
0.5369037507264518
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.4093,	 Acc1 = 0.2494,	 Acc2 = 0.2445

 ===== Epoch 43	 =====
[ 2.7296004   3.0842257  -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  2.2558517   3.2701693 ] [-7.7089384e-02 -1.3202982e+00  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -5.7092085e-02 -1.5666863e+00] 5 5
train:	 Loss = 1.2582,	 Acc = 0.5115
48799 0.287
95390 0.57
49912 0.605
5741 0.631
902 0.518
88 0.614
0 0.0
0 0.0
0.5834588543276789
0.5369037507264518
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3673,	 Acc = 0.4903
5486 0.408
29114 0.514
14300 0.474
1295 0.489
29 0.0
0 0.0
0 0.0
0 0.0
0.500335285439671
0.5369037507264518
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.4278,	 Acc1 = 0.2703,	 Acc2 = 0.2696

 ===== Epoch 44	 =====
[-0.36866695 -0.38685238  1.6987525   3.0740066  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.1179677  -0.10003845  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 5 3
train:	 Loss = 1.2570,	 Acc = 0.5123
48800 0.287
95393 0.573
49906 0.605
5743 0.624
902 0.509
88 0.534
0 0.0
0 0.0
0.5847190065249421
0.5369037507264518
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3132,	 Acc = 0.5149
5486 0.344
29114 0.554
14300 0.504
1295 0.483
29 0.069
0 0.0
0 0.0
0 0.0
0.5357637802315705
0.5369037507264518
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3776,	 Acc1 = 0.2501,	 Acc2 = 0.2453

 ===== Epoch 45	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
  4.038175    1.920825   -0.41367632 -0.42256096  3.9885006   2.3606749
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.3456989  -0.01420111  0.0244112   0.04163946 -0.16350591  0.00394159
 -0.01747011 -0.0158521 ] 3 3
train:	 Loss = 1.2581,	 Acc = 0.5117
48805 0.286
95386 0.571
49911 0.608
5740 0.617
902 0.509
88 0.5
0 0.0
0 0.0
0.5842449038657607
0.5369037507264518
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3320,	 Acc = 0.4962
5486 0.401
29114 0.528
14300 0.467
1295 0.524
29 0.0
0 0.0
0 0.0
0 0.0
0.507823326925656
0.5369037507264518
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3956,	 Acc1 = 0.2670,	 Acc2 = 0.2657

 ===== Epoch 46	 =====
[ 1.7415719   2.2561915  -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.23384137  0.00526798  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 2 1
train:	 Loss = 1.2589,	 Acc = 0.5119
48791 0.288
95399 0.571
49909 0.604
5743 0.619
902 0.537
88 0.523
0 0.0
0 0.0
0.5836978183516288
0.5369037507264518
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3803,	 Acc = 0.4806
5486 0.377
29114 0.535
14300 0.416
1295 0.412
29 0.0
0 0.0
0 0.0
0 0.0
0.49333899593187
0.5369037507264518
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.4392,	 Acc1 = 0.2655,	 Acc2 = 0.2639

 ===== Epoch 47	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  4.1136985e+00  3.0331938e+00 -1.5871393e-02 -1.1254287e-02
  2.9187953e+00  2.8580256e+00 -1.1927608e-02 -1.4941680e-03
 -1.7470114e-02 -1.5852105e-02] 5 6
train:	 Loss = 1.2572,	 Acc = 0.5123
48807 0.287
95392 0.571
49901 0.607
5742 0.621
902 0.52
88 0.557
0 0.0
0 0.0
0.5846735734254235
0.5369037507264518
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3199,	 Acc = 0.5123
5486 0.392
29114 0.562
14300 0.462
1295 0.456
29 0.0
0 0.0
0 0.0
0 0.0
0.5270463588001252
0.5369037507264518
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3795,	 Acc1 = 0.2657,	 Acc2 = 0.2642

 ===== Epoch 48	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
  1.6839128   1.1400381  -0.41367632 -0.42256096  1.9085232   3.0822253
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
  0.09129001 -0.01714794  0.0244112   0.04163946 -0.00965907 -0.00149417
 -0.01747011 -0.0158521 ] 0 0
train:	 Loss = 1.2569,	 Acc = 0.5121
48802 0.286
95391 0.572
49908 0.606
5741 0.622
902 0.524
88 0.568
0 0.0
0 0.0
0.5846477668881142
0.5369037507264518
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3437,	 Acc = 0.4816
5486 0.375
29114 0.516
14300 0.456
1295 0.466
29 0.0
0 0.0
0 0.0
0 0.0
0.4946801376905539
0.5369037507264518
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.4004,	 Acc1 = 0.2868,	 Acc2 = 0.2895

 ===== Epoch 49	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
  3.3166604   2.4098895  -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -4.055354   -2.869677    1.2891064   3.6863747   3.946411    1.7026157
  3.2903252   3.834397  ] 5 6
train:	 Loss = 1.2576,	 Acc = 0.5117
48795 0.286
95394 0.57
49911 0.608
5742 0.628
902 0.513
88 0.58
0 0.0
0 0.0
0.5842919815571209
0.5369037507264518
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3316,	 Acc = 0.4997
5486 0.402
29114 0.54
14300 0.457
1295 0.496
29 0.0
0 0.0
0 0.0
0 0.0
0.5116902856631946
0.5369037507264518
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3906,	 Acc1 = 0.2769,	 Acc2 = 0.2776

 ===== Epoch 50	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
  1.6699023   2.241148   -0.41367632 -0.42256096  2.7164414   2.2321794
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
  0.34539917  0.07125689  0.0244112   0.04163946  0.04661671  0.04742764
 -0.01747011 -0.0158521 ] 2 2
train:	 Loss = 1.2584,	 Acc = 0.5117
48801 0.285
95395 0.572
49904 0.606
5742 0.622
902 0.514
88 0.534
0 0.0
0 0.0
0.584518946793746
0.5369037507264518
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3062,	 Acc = 0.4806
5486 0.369
29114 0.508
14300 0.466
1295 0.504
29 0.0
0 0.0
0 0.0
0 0.0
0.4943001475255935
0.5369037507264518
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3767,	 Acc1 = 0.2451,	 Acc2 = 0.2393

 ===== Epoch 51	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  2.0755575   2.7200131 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946  3.7862103   1.5341073
  0.00649935  0.6112611 ] 4 6
train:	 Loss = 1.2575,	 Acc = 0.5124
48800 0.288
95391 0.571
49910 0.607
5742 0.626
901 0.527
88 0.5
0 0.0
0 0.0
0.584284887392128
0.5369037507264518
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.2720,	 Acc = 0.5126
5486 0.371
29114 0.536
14300 0.522
1295 0.495
29 0.0
0 0.0
0 0.0
0 0.0
0.5299968706692297
0.5369037507264518
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3517,	 Acc1 = 0.2470,	 Acc2 = 0.2416

 ===== Epoch 52	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143  1.9305487   1.8838258
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.04938225  0.30830336
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
train:	 Loss = 1.2573,	 Acc = 0.5121
48807 0.285
95384 0.571
49910 0.607
5741 0.631
902 0.517
88 0.545
0 0.0
0 0.0
0.5850616674888999
0.5369037507264518
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3122,	 Acc = 0.5161
5486 0.406
29114 0.554
14300 0.488
1295 0.449
29 0.0
0 0.0
0 0.0
0 0.0
0.5295051186910457
0.5369037507264518
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3734,	 Acc1 = 0.2639,	 Acc2 = 0.2619

 ===== Epoch 53	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
  3.927263    2.0495262  -0.41367632 -0.42256096  3.9565842   2.484228
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
  0.41085008 -0.55052376  0.0244112   0.04163946  0.1028911  -0.42004746
 -0.01747011 -0.0158521 ] 3 5
train:	 Loss = 1.2569,	 Acc = 0.5128
48798 0.288
95393 0.572
49909 0.606
5742 0.624
902 0.533
88 0.523
0 0.0
0 0.0
0.5849217938092796
0.5369037507264518
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3318,	 Acc = 0.5154
5486 0.41
29114 0.549
14300 0.49
1295 0.493
29 0.0
0 0.0
0 0.0
0 0.0
0.5283427958335196
0.5369037507264518
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.4015,	 Acc1 = 0.2529,	 Acc2 = 0.2488

 ===== Epoch 54	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096  1.1671294   3.181068
 -0.3977158  -0.3905835 ] [ 5.3072896   1.8302884   0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.04778092  0.08004218
 -0.01747011 -0.0158521 ] 5 6
train:	 Loss = 1.2566,	 Acc = 0.5125
48811 0.286
95380 0.572
49907 0.607
5744 0.627
902 0.531
88 0.557
0 0.0
0 0.0
0.5851428421073405
0.5369037507264518
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3223,	 Acc = 0.4966
5486 0.375
29114 0.525
14300 0.482
1295 0.537
29 0.0
0 0.0
0 0.0
0 0.0
0.511489114399392
0.5369037507264518
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3809,	 Acc1 = 0.2814,	 Acc2 = 0.2831

 ===== Epoch 55	 =====
[-0.36866695 -0.38685238  1.014891    3.0808988  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-1.7616821e-02 -2.3137011e-02 -1.6822609e+00 -3.0728240e+00
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.7470114e-02 -1.5852105e-02] 1 1
train:	 Loss = 1.2572,	 Acc = 0.5119
48804 0.286
95391 0.571
49904 0.608
5743 0.623
902 0.51
88 0.534
0 0.0
0 0.0
0.5845304812271424
0.5369037507264518
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3372,	 Acc = 0.4883
5486 0.361
29114 0.511
14300 0.487
1295 0.539
29 0.0
0 0.0
0 0.0
0 0.0
0.5039340158254727
0.5369037507264518
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.4012,	 Acc1 = 0.2534,	 Acc2 = 0.2493

 ===== Epoch 56	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096  2.171933    1.1325562
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.04142705  0.21050036
 -0.01747011 -0.0158521 ] 3 2
train:	 Loss = 1.2586,	 Acc = 0.5110
48803 0.287
95396 0.57
49902 0.603
5741 0.627
902 0.514
88 0.591
0 0.0
0 0.0
0.5830992771116037
0.5369037507264518
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.4194,	 Acc = 0.4771
5486 0.408
29114 0.526
14300 0.407
1295 0.439
29 0.0
0 0.0
0 0.0
0 0.0
0.48558272609414815
0.5369037507264518
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.4670,	 Acc1 = 0.2816,	 Acc2 = 0.2833

 ===== Epoch 57	 =====
[ 1.2446692   2.222865   -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.0666122   2.4775276 ] [ 0.27242693  0.2561787   0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
  0.0921035   0.2977045 ] 4 4
train:	 Loss = 1.2579,	 Acc = 0.5128
48800 0.287
95392 0.572
49908 0.606
5743 0.629
901 0.536
88 0.523
0 0.0
0 0.0
0.585264944222269
0.5369037507264518
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3090,	 Acc = 0.5073
5486 0.406
29114 0.552
14300 0.464
1295 0.429
29 0.0
0 0.0
0 0.0
0 0.0
0.5197818409405874
0.5369037507264518
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3709,	 Acc1 = 0.2779,	 Acc2 = 0.2788

 ===== Epoch 58	 =====
[ 2.3740628   2.115195   -0.41997173 -0.34217143 -0.43314552 -0.35780522
  0.8421443   1.1571983  -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-2.7354032e-01 -7.4746418e-01  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.3352389e+00 -1.5789665e+00
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.7470114e-02 -1.5852105e-02] 5 5
train:	 Loss = 1.2573,	 Acc = 0.5124
48799 0.287
95390 0.571
49909 0.607
5744 0.626
902 0.518
88 0.5
0 0.0
0 0.0
0.5847677806791947
0.5369037507264518
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3086,	 Acc = 0.5000
5486 0.407
29114 0.532
14300 0.479
1295 0.408
29 0.0
0 0.0
0 0.0
0 0.0
0.5113550002235236
0.5369037507264518
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3671,	 Acc1 = 0.2727,	 Acc2 = 0.2726

 ===== Epoch 59	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  3.6003826   2.6235404 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  3.4425998   3.944083    4.384357    1.7053337
  0.03046881  0.2751058 ] 2 2
train:	 Loss = 1.2579,	 Acc = 0.5122
48804 0.288
95390 0.572
49910 0.605
5738 0.621
902 0.513
88 0.523
0 0.0
0 0.0
0.5840437287868024
0.5369037507264518
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 6
val:	 Loss = 1.3137,	 Acc = 0.5073
5486 0.375
29114 0.543
14300 0.486
1295 0.502
29 0.0
0 0.0
0 0.0
0 0.0
0.5234476284143234
0.5369037507264518
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3765,	 Acc1 = 0.2748,	 Acc2 = 0.2751

 ===== Epoch 60	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096  0.8564209   1.3648363
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  3.5646276   1.561224    0.01022447  0.00647582
  2.4097667   3.6103966   0.0244112   0.04163946 -1.5259432  -1.9692382
 -0.01747011 -0.0158521 ] 6 6
train:	 Loss = 1.2574,	 Acc = 0.5123
48799 0.287
95394 0.571
49906 0.608
5743 0.624
902 0.508
88 0.557
0 0.0
0 0.0
0.5847414706017773
0.5369037507264518
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3310,	 Acc = 0.4965
5486 0.368
29114 0.529
14300 0.478
1295 0.525
29 0.0
0 0.0
0 0.0
0 0.0
0.5123161518172471
0.5369037507264518
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3908,	 Acc1 = 0.2562,	 Acc2 = 0.2527

 ===== Epoch 61	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096  1.7758566   1.199275
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -1.4728042   0.5040313
 -0.01747011 -0.0158521 ] 1 1
train:	 Loss = 1.2586,	 Acc = 0.5116
48803 0.286
95391 0.571
49907 0.605
5742 0.623
901 0.526
88 0.568
0 0.0
0 0.0
0.5838886002012774
0.5369037507264518
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.2902,	 Acc = 0.5109
5486 0.315
29114 0.546
14300 0.519
1295 0.479
29 0.0
0 0.0
0 0.0
0 0.0
0.5348920380884259
0.5369037507264518
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3681,	 Acc1 = 0.2470,	 Acc2 = 0.2416

 ===== Epoch 62	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 0 1
train:	 Loss = 1.2586,	 Acc = 0.5123
48798 0.288
95388 0.57
49914 0.607
5744 0.631
900 0.514
88 0.557
0 0.0
0 0.0
0.5842508912480103
0.5369037507264518
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3511,	 Acc = 0.4774
5486 0.354
29114 0.509
14300 0.46
1295 0.491
29 0.0
0 0.0
0 0.0
0 0.0
0.4925119585140149
0.5369037507264518
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.4071,	 Acc1 = 0.2756,	 Acc2 = 0.2761

 ===== Epoch 63	 =====
[-0.36866695 -0.38685238  1.1105745   3.051033   -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.09509188  0.03111386  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 0 0
train:	 Loss = 1.2587,	 Acc = 0.5106
48804 0.286
95389 0.569
49909 0.605
5741 0.626
901 0.518
88 0.568
0 0.0
0 0.0
0.582787381271871
0.5369037507264518
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.2780,	 Acc = 0.5315
5486 0.401
29114 0.567
14300 0.513
1295 0.496
29 0.0
0 0.0
0 0.0
0 0.0
0.5474540658947651
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3435,	 Acc1 = 0.2494,	 Acc2 = 0.2445

 ===== Epoch 64	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096  0.48072517  2.5978968
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -1.0825521  -3.32546
 -0.01747011 -0.0158521 ] 1 1
train:	 Loss = 1.2574,	 Acc = 0.5123
48805 0.288
95391 0.571
49902 0.606
5744 0.622
902 0.512
88 0.545
0 0.0
0 0.0
0.584258059423655
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.2958,	 Acc = 0.5007
5486 0.366
29114 0.531
14300 0.495
1295 0.464
29 0.0
0 0.0
0 0.0
0 0.0
0.5173007286870223
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3684,	 Acc1 = 0.2453,	 Acc2 = 0.2396

 ===== Epoch 65	 =====
[-0.36866695 -0.38685238  1.8801767   0.90299904 -0.43314552 -0.35780522
  1.3120637   3.9743233  -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.00061761  0.03111386  0.01022447  0.00647582
  0.12915021 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 0 0
train:	 Loss = 1.2568,	 Acc = 0.5128
48798 0.289
95401 0.572
49902 0.605
5741 0.624
902 0.51
88 0.523
0 0.0
0 0.0
0.5846850046700081
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.2782,	 Acc = 0.5176
5486 0.394
29114 0.536
14300 0.526
1295 0.533
29 0.69
0 0.0
0 0.0
0 0.0
0.5327015065492423
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3432,	 Acc1 = 0.2763,	 Acc2 = 0.2768

 ===== Epoch 66	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  2.208914    3.1945553 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
  0.38364756 -0.02432661] 2 2
train:	 Loss = 1.2565,	 Acc = 0.5131
48799 0.288
95392 0.572
49911 0.606
5741 0.629
901 0.523
88 0.545
0 0.0
0 0.0
0.5853400248630232
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.2824,	 Acc = 0.5183
5486 0.405
29114 0.557
14300 0.491
1295 0.44
29 0.0
0 0.0
0 0.0
0 0.0
0.5321874022084134
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3497,	 Acc1 = 0.2542,	 Acc2 = 0.2502

 ===== Epoch 67	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512  1.1005323   3.1874254  -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
 -1.7252388e+00 -3.8555446e+00 -1.1927608e-02 -1.4941680e-03
 -1.7470114e-02 -1.5852105e-02] 1 1
train:	 Loss = 1.2575,	 Acc = 0.5123
48807 0.288
95387 0.572
49905 0.605
5743 0.627
902 0.524
88 0.58
0 0.0
0 0.0
0.5843315244203257
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.2985,	 Acc = 0.4975
5486 0.389
29114 0.519
14300 0.497
1295 0.481
29 0.0
0 0.0
0 0.0
0 0.0
0.5107961911574054
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3708,	 Acc1 = 0.2486,	 Acc2 = 0.2435

 ===== Epoch 68	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096  1.4447672   1.5995873
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946  0.00032559  0.1045031
 -0.01747011 -0.0158521 ] 3 3
train:	 Loss = 1.2581,	 Acc = 0.5111
48804 0.284
95389 0.571
49906 0.606
5743 0.617
902 0.519
88 0.511
0 0.0
0 0.0
0.5840437287868024
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3685,	 Acc = 0.4773
5486 0.389
29114 0.51
14300 0.455
1295 0.382
29 0.0
0 0.0
0 0.0
0 0.0
0.4881532477982923
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 0
Testing:	 Loss = 1.4408,	 Acc1 = 0.2519,	 Acc2 = 0.2475

 ===== Epoch 69	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096  1.5889695   1.5798187
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.00920452  0.08819582
 -0.01747011 -0.0158521 ] 3 0
train:	 Loss = 1.2566,	 Acc = 0.5126
48795 0.289
95394 0.572
49911 0.605
5742 0.623
902 0.53
88 0.58
0 0.0
0 0.0
0.5844169511368943
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.4298,	 Acc = 0.4747
5486 0.369
29114 0.521
14300 0.427
1295 0.413
29 0.0
0 0.0
0 0.0
0 0.0
0.48770620054539765
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 0
Testing:	 Loss = 1.4955,	 Acc1 = 0.2630,	 Acc2 = 0.2609

 ===== Epoch 70	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143  2.7522533  -4.0601993
 -0.3580977  -0.36433512  1.7596422   2.476146   -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
 -3.9179943e+00  3.4891009e+00 -1.5871393e-02 -1.1254287e-02
 -2.4868321e+00 -3.0876784e+00 -1.1927608e-02 -1.4941680e-03
 -1.7470114e-02 -1.5852105e-02] 4 6
train:	 Loss = 1.2577,	 Acc = 0.5119
48797 0.288
95390 0.571
49913 0.605
5742 0.62
902 0.511
88 0.602
0 0.0
0 0.0
0.5839050218699642
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3365,	 Acc = 0.4808
5486 0.365
29114 0.509
14300 0.466
1295 0.515
29 0.0
0 0.0
0 0.0
0 0.0
0.49501542313022484
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3928,	 Acc1 = 0.2890,	 Acc2 = 0.2922

 ===== Epoch 71	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 5 1
train:	 Loss = 1.2571,	 Acc = 0.5117
48798 0.286
95393 0.57
49909 0.606
5743 0.626
901 0.533
88 0.489
0 0.0
0 0.0
0.584112764250102
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3188,	 Acc = 0.5108
5486 0.403
29114 0.543
14300 0.496
1295 0.419
29 0.0
0 0.0
0 0.0
0 0.0
0.5239840851177969
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.4023,	 Acc1 = 0.2387,	 Acc2 = 0.2316

 ===== Epoch 72	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512  2.086009    3.216656   -0.4264405  -0.4242137
  2.558536    1.4215412 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429 -2.8639452  -3.8871007   2.178747    2.485365
 -3.3140144  -1.9791119 ] 6 6
train:	 Loss = 1.2575,	 Acc = 0.5120
48797 0.287
95392 0.572
49909 0.604
5744 0.622
902 0.535
88 0.545
0 0.0
0 0.0
0.5841681191830829
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3483,	 Acc = 0.4963
5486 0.406
29114 0.527
14300 0.476
1295 0.419
29 0.0
0 0.0
0 0.0
0 0.0
0.5073092225848272
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 1
Testing:	 Loss = 1.4141,	 Acc1 = 0.2513,	 Acc2 = 0.2468

 ===== Epoch 73	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143  2.521851    1.8434361
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  4.0561971e-01  2.6186836e-01 -1.5871393e-02 -1.1254287e-02
  3.7681370e+00  1.5800016e+00 -1.1927608e-02 -1.4941680e-03
 -1.7470114e-02 -1.5852105e-02] 2 2
train:	 Loss = 1.2566,	 Acc = 0.5126
48800 0.287
95393 0.572
49906 0.607
5743 0.626
902 0.532
88 0.568
0 0.0
0 0.0
0.584982109029678
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.2914,	 Acc = 0.4980
5486 0.371
29114 0.525
14300 0.487
1295 0.556
29 0.0
0 0.0
0 0.0
0 0.0
0.513567884125352
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3531,	 Acc1 = 0.2773,	 Acc2 = 0.2781

 ===== Epoch 74	 =====
[-0.36866695 -0.38685238  2.2434402   3.0808988   2.0459437   0.81574136
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.5581161   0.03527742  0.43522793  0.2787538
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 2 2
train:	 Loss = 1.2582,	 Acc = 0.5111
48804 0.287
95390 0.569
49907 0.606
5741 0.619
902 0.528
88 0.489
0 0.0
0 0.0
0.5829715578709185
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3014,	 Acc = 0.5156
5486 0.406
29114 0.551
14300 0.486
1295 0.527
29 0.0
0 0.0
0 0.0
0 0.0
0.5290357190755063
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3633,	 Acc1 = 0.2765,	 Acc2 = 0.2771

 ===== Epoch 75	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
  1.3646016   1.8922248  -0.41367632 -0.42256096  2.1846228   1.9924864
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.07554988 -0.01125429  0.0244112   0.04163946  0.2862374  -0.03954447
 -0.01747011 -0.0158521 ] 2 3
train:	 Loss = 1.2577,	 Acc = 0.5116
48803 0.287
95388 0.571
49908 0.605
5743 0.623
902 0.523
88 0.568
0 0.0
0 0.0
0.5837833571226543
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3338,	 Acc = 0.4824
5486 0.403
29114 0.521
14300 0.445
1295 0.376
29 0.0
0 0.0
0 0.0
0 0.0
0.4921319683490545
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.4058,	 Acc1 = 0.2496,	 Acc2 = 0.2448

 ===== Epoch 76	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 2 1
train:	 Loss = 1.2564,	 Acc = 0.5123
48807 0.288
95382 0.571
49913 0.606
5742 0.625
900 0.51
88 0.489
0 0.0
0 0.0
0.5843446801512909
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3425,	 Acc = 0.4989
5486 0.407
29114 0.547
14300 0.447
1295 0.387
29 0.0
0 0.0
0 0.0
0 0.0
0.5102150297286423
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.4017,	 Acc1 = 0.2600,	 Acc2 = 0.2572

 ===== Epoch 77	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096  1.4374613   2.795582
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
  3.701484    1.4710001   0.0244112   0.04163946  0.6302403   0.05014552
 -0.01747011 -0.0158521 ] 2 2
train:	 Loss = 1.2564,	 Acc = 0.5120
48799 0.289
95389 0.57
49911 0.606
5744 0.626
901 0.516
88 0.511
0 0.0
0 0.0
0.5837679977373333
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3400,	 Acc = 0.5058
5486 0.402
29114 0.544
14300 0.478
1295 0.402
29 0.0
0 0.0
0 0.0
0 0.0
0.5184630515445483
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3984,	 Acc1 = 0.2682,	 Acc2 = 0.2671

 ===== Epoch 78	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096  0.6572289   0.84591293
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946  0.9048067   0.99324936
 -0.01747011 -0.0158521 ] 2 2
train:	 Loss = 1.2576,	 Acc = 0.5123
48798 0.286
95395 0.571
49907 0.609
5742 0.625
902 0.497
88 0.557
0 0.0
0 0.0
0.5849152163331887
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3282,	 Acc = 0.5148
5486 0.386
29114 0.551
14300 0.494
1295 0.498
29 0.0
0 0.0
0 0.0
0 0.0
0.5306450891859269
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.4021,	 Acc1 = 0.2496,	 Acc2 = 0.2448

 ===== Epoch 79	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
  2.9658275   1.903665   -0.41367632 -0.42256096  2.4103475   3.2848525
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
  0.48913532 -0.76564217  0.0244112   0.04163946 -0.364553   -0.68911743
 -0.01747011 -0.0158521 ] 5 5
train:	 Loss = 1.2573,	 Acc = 0.5119
48802 0.287
95395 0.571
49903 0.605
5742 0.621
902 0.516
88 0.523
0 0.0
0 0.0
0.5840097349207394
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3414,	 Acc = 0.4877
5486 0.361
29114 0.509
14300 0.492
1295 0.507
29 0.0
0 0.0
0 0.0
0 0.0
0.5032187402208413
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.4021,	 Acc1 = 0.2791,	 Acc2 = 0.2803

 ===== Epoch 80	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512  5.6951694   2.8999906   0.6914526   2.4125671
  3.8271754   2.0186298 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
 -7.0342875e+00 -3.5452423e+00 -1.3312497e+00 -3.1216190e+00
 -5.7582028e-02  1.2468488e+00] 4 4
train:	 Loss = 1.2568,	 Acc = 0.5121
48806 0.287
95394 0.571
49902 0.607
5742 0.625
900 0.506
88 0.545
0 0.0
0 0.0
0.5842290134582242
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3848,	 Acc = 0.4689
5486 0.367
29114 0.503
14300 0.449
1295 0.377
29 0.0
0 0.0
0 0.0
0 0.0
0.4814251866422281
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 6
Testing:	 Loss = 1.4534,	 Acc1 = 0.2680,	 Acc2 = 0.2669

 ===== Epoch 81	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096  1.4366921   1.9900153
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.0269039  -0.05041598
 -0.01747011 -0.0158521 ] 3 3
train:	 Loss = 1.2575,	 Acc = 0.5122
48796 0.287
95395 0.571
49908 0.606
5744 0.627
902 0.517
87 0.575
0 0.0
0 0.0
0.5845852298139914
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3187,	 Acc = 0.4854
5486 0.365
29114 0.514
14300 0.474
1295 0.502
29 0.0
0 0.0
0 0.0
0 0.0
0.5002235236264473
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3816,	 Acc1 = 0.2802,	 Acc2 = 0.2816

 ===== Epoch 82	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143  1.9266763   2.6085975
 -0.3580977  -0.36433512  1.2860326   0.8928186  -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
 -2.8998966e+00 -2.7838459e+00 -1.5871393e-02 -1.1254287e-02
  2.9101225e-02  2.5897985e+00 -1.1927608e-02 -1.4941680e-03
 -1.7470114e-02 -1.5852105e-02] 4 6
train:	 Loss = 1.2582,	 Acc = 0.5109
48801 0.286
95392 0.57
49909 0.604
5740 0.624
902 0.522
88 0.568
0 0.0
0 0.0
0.5829074333524084
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3041,	 Acc = 0.5149
5486 0.406
29114 0.55
14300 0.487
1295 0.493
29 0.0
0 0.0
0 0.0
0 0.0
0.5282086816576512
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3758,	 Acc1 = 0.2430,	 Acc2 = 0.2368

 ===== Epoch 83	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  1.8216177e+00  2.5266860e+00 -1.1927608e-02 -1.4941680e-03
 -1.7470114e-02 -1.5852105e-02] 4 6
train:	 Loss = 1.2570,	 Acc = 0.5109
48800 0.287
95393 0.57
49906 0.603
5743 0.618
902 0.52
88 0.489
0 0.0
0 0.0
0.5826668069880026
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3383,	 Acc = 0.5008
5486 0.395
29114 0.54
14300 0.467
1295 0.442
29 0.0
0 0.0
0 0.0
0 0.0
0.5137914077517993
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.4040,	 Acc1 = 0.2585,	 Acc2 = 0.2555

 ===== Epoch 84	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512  1.4054507   2.8683238  -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
 -2.0775683e+00 -3.5110567e+00 -1.1927608e-02 -1.4941680e-03
  2.6597879e+00  3.3372264e+00] 6 6
train:	 Loss = 1.2573,	 Acc = 0.5116
48799 0.287
95390 0.57
49909 0.607
5744 0.624
902 0.522
88 0.545
0 0.0
0 0.0
0.5836232923115376
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.4259,	 Acc = 0.4551
5486 0.368
29114 0.488
14300 0.422
1295 0.461
29 0.0
0 0.0
0 0.0
0 0.0
0.46575618042827127
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.4856,	 Acc1 = 0.2581,	 Acc2 = 0.2550

 ===== Epoch 85	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512  1.2234267   1.2655096  -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429 -0.07338355  0.9383599  -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
train:	 Loss = 1.2571,	 Acc = 0.5130
48805 0.286
95391 0.573
49907 0.608
5740 0.626
901 0.508
88 0.591
0 0.0
0 0.0
0.5860011708446526
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3296,	 Acc = 0.4873
5486 0.34
29114 0.524
14300 0.47
1295 0.485
29 0.207
0 0.0
0 0.0
0 0.0
0.5053198623094461
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3888,	 Acc1 = 0.2785,	 Acc2 = 0.2796

 ===== Epoch 86	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512  2.7445383   2.0060194  -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.30908838  0.14682661 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 2 3
train:	 Loss = 1.2569,	 Acc = 0.5127
48794 0.289
95394 0.572
49911 0.606
5744 0.622
901 0.512
88 0.534
0 0.0
0 0.0
0.5845709625225273
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3955,	 Acc = 0.4861
5486 0.402
29114 0.535
14300 0.426
1295 0.429
29 0.0
0 0.0
0 0.0
0 0.0
0.496423621976843
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.4509,	 Acc1 = 0.2961,	 Acc2 = 0.3007

 ===== Epoch 87	 =====
[-0.36866695 -0.38685238  1.2182685   2.6972392  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-1.7616821e-02 -2.3137011e-02 -1.9238609e+00 -2.7251661e+00
  1.7193536e+00  2.4253173e+00 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.7470114e-02 -1.5852105e-02] 6 6
train:	 Loss = 1.2579,	 Acc = 0.5120
48799 0.287
95392 0.571
49910 0.607
5741 0.629
902 0.514
88 0.534
0 0.0
0 0.0
0.5843402419211619
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3136,	 Acc = 0.5096
5486 0.407
29114 0.55
14300 0.471
1295 0.48
29 0.0
0 0.0
0 0.0
0 0.0
0.5222182484688631
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3769,	 Acc1 = 0.2536,	 Acc2 = 0.2495

 ===== Epoch 88	 =====
[ 1.7327583   2.6817448  -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  2.1799617   2.8477745 ] [ 0.2515248  -0.051542    0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.2933601   0.00674657] 3 3
train:	 Loss = 1.2578,	 Acc = 0.5117
48811 0.287
95384 0.571
49905 0.606
5742 0.621
902 0.507
88 0.545
0 0.0
0 0.0
0.5839193269350945
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.2913,	 Acc = 0.5216
5486 0.405
29114 0.563
14300 0.488
1295 0.455
29 0.0
0 0.0
0 0.0
0 0.0
0.53580848495686
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3543,	 Acc1 = 0.2455,	 Acc2 = 0.2398

 ===== Epoch 89	 =====
[ 3.1629665   1.8614012  -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  2.0496767   2.0420961 ] [-0.8703881   0.5094565   0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
  0.11069114  0.58301276] 4 4
train:	 Loss = 1.2567,	 Acc = 0.5127
48804 0.287
95392 0.571
49907 0.608
5740 0.631
901 0.517
88 0.591
0 0.0
0 0.0
0.5852145657378904
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3508,	 Acc = 0.4889
5486 0.403
29114 0.528
14300 0.454
1295 0.375
29 0.0
0 0.0
0 0.0
0 0.0
0.49944119093388173
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.4160,	 Acc1 = 0.2692,	 Acc2 = 0.2684

 ===== Epoch 90	 =====
[-0.36866695 -0.38685238  1.9394095   2.106817   -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00233638  0.03527742  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 3 0
train:	 Loss = 1.2575,	 Acc = 0.5123
48795 0.287
95401 0.571
49903 0.608
5743 0.625
902 0.521
88 0.534
0 0.0
0 0.0
0.5845748074481869
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3133,	 Acc = 0.5104
5486 0.401
29114 0.553
14300 0.473
1295 0.435
29 0.0
0 0.0
0 0.0
0 0.0
0.5237829138539943
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3727,	 Acc1 = 0.2630,	 Acc2 = 0.2609

 ===== Epoch 91	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
  2.5618718   1.2058188  -0.41367632 -0.42256096  1.5558999   3.1687124
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
  0.13941763  0.18028949  0.0244112   0.04163946  0.41603237  0.14527129
 -0.01747011 -0.0158521 ] 2 2
train:	 Loss = 1.2564,	 Acc = 0.5115
48804 0.285
95386 0.57
49909 0.608
5743 0.624
902 0.514
88 0.523
0 0.0
0 0.0
0.5841226616149656
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3734,	 Acc = 0.4802
5486 0.371
29114 0.516
14300 0.446
1295 0.521
29 0.69
0 0.0
0 0.0
0 0.0
0.49358487192096206
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.4312,	 Acc1 = 0.2775,	 Acc2 = 0.2783

 ===== Epoch 92	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 1 1
train:	 Loss = 1.2566,	 Acc = 0.5123
48795 0.287
95393 0.571
49910 0.608
5744 0.625
902 0.511
88 0.489
0 0.0
0 0.0
0.5846471582575294
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.2773,	 Acc = 0.5050
5486 0.363
29114 0.529
14300 0.512
1295 0.502
29 0.0
0 0.0
0 0.0
0 0.0
0.5224641244579552
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3462,	 Acc1 = 0.2453,	 Acc2 = 0.2396

 ===== Epoch 93	 =====
[-0.36866695 -0.38685238  1.5980997   2.7960258  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.04539436 -0.21661828  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 5 3
train:	 Loss = 1.2593,	 Acc = 0.5111
48799 0.286
95392 0.57
49911 0.605
5741 0.621
901 0.516
88 0.58
0 0.0
0 0.0
0.5832220636309222
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.2932,	 Acc = 0.5106
5486 0.406
29114 0.544
14300 0.49
1295 0.424
29 0.0
0 0.0
0 0.0
0 0.0
0.523313514238455
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3691,	 Acc1 = 0.2564,	 Acc2 = 0.2530

 ===== Epoch 94	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  2.5187414e+00  2.4443135e+00 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.7470114e-02 -1.5852105e-02] 1 6
train:	 Loss = 1.2569,	 Acc = 0.5113
48802 0.287
95389 0.569
49908 0.607
5744 0.619
901 0.526
88 0.557
0 0.0
0 0.0
0.583154640531474
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3540,	 Acc = 0.4799
5486 0.381
29114 0.519
14300 0.438
1295 0.487
29 0.0
0 0.0
0 0.0
0 0.0
0.49202020653583084
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.4102,	 Acc1 = 0.2870,	 Acc2 = 0.2898

 ===== Epoch 95	 =====
[ 2.5575416   2.307463   -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  2.1457453   2.4697053 ] [ 0.02663833  0.61834234  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
  0.27211717  0.7638022 ] 4 4
train:	 Loss = 1.2588,	 Acc = 0.5109
48800 0.287
95392 0.569
49907 0.605
5743 0.623
902 0.522
88 0.557
0 0.0
0 0.0
0.582693117238476
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3045,	 Acc = 0.5075
5486 0.407
29114 0.557
14300 0.452
1295 0.45
29 0.0
0 0.0
0 0.0
0 0.0
0.5199159551164558
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3634,	 Acc1 = 0.2610,	 Acc2 = 0.2584

 ===== Epoch 96	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 3 1
train:	 Loss = 1.2585,	 Acc = 0.5122
48804 0.287
95392 0.572
49905 0.606
5741 0.621
902 0.509
88 0.557
0 0.0
0 0.0
0.5845370589628226
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3553,	 Acc = 0.4922
5486 0.408
29114 0.543
14300 0.432
1295 0.393
29 0.0
0 0.0
0 0.0
0 0.0
0.5025705217041442
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.4093,	 Acc1 = 0.2884,	 Acc2 = 0.2915

 ===== Epoch 97	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096  0.7968176   2.017197
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946  0.24266936 -0.4662514
 -0.01747011 -0.0158521 ] 2 5
train:	 Loss = 1.2573,	 Acc = 0.5114
48802 0.286
95390 0.57
49910 0.606
5740 0.626
902 0.523
88 0.557
0 0.0
0 0.0
0.5836479642175887
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.2678,	 Acc = 0.5158
5486 0.406
29114 0.544
14300 0.508
1295 0.431
29 0.69
0 0.0
0 0.0
0 0.0
0.5292368903393089
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3439,	 Acc1 = 0.2342,	 Acc2 = 0.2261

 ===== Epoch 98	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
  2.1298966   2.3040686  -0.41367632 -0.42256096  2.8010402   2.6250787
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -2.7508025  -2.7606444   0.0244112   0.04163946 -3.8209565  -3.3553565
 -0.01747011 -0.0158521 ] 6 6
train:	 Loss = 1.2570,	 Acc = 0.5121
48805 0.288
95387 0.571
49906 0.607
5744 0.624
902 0.516
88 0.523
0 0.0
0 0.0
0.5841396594026061
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.2977,	 Acc = 0.5227
5486 0.41
29114 0.562
14300 0.492
1295 0.46
29 0.0
0 0.0
0 0.0
0 0.0
0.5365014081988466
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3530,	 Acc1 = 0.2798,	 Acc2 = 0.2811

 ===== Epoch 99	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 3 1
train:	 Loss = 1.2564,	 Acc = 0.5134
48798 0.287
95394 0.573
49908 0.609
5742 0.62
902 0.502
88 0.58
0 0.0
0 0.0
0.5858952602707289
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3691,	 Acc = 0.5025
5486 0.396
29114 0.545
14300 0.464
1295 0.44
29 0.0
0 0.0
0 0.0
0 0.0
0.5155572444007331
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.4245,	 Acc1 = 0.2608,	 Acc2 = 0.2582

 ===== Epoch 100	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143  0.3812366   2.8486922
  5.0848384   1.7292033  -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  3.6489088e-02 -1.5047725e+00 -5.9990258e+00 -2.1683321e+00
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.7470114e-02 -1.5852105e-02] 5 5
train:	 Loss = 1.2572,	 Acc = 0.5125
48805 0.288
95391 0.572
49903 0.607
5743 0.623
902 0.514
88 0.523
0 0.0
0 0.0
0.5846527261604847
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3089,	 Acc = 0.5179
5486 0.403
29114 0.554
14300 0.495
1295 0.466
29 0.0
0 0.0
0 0.0
0 0.0
0.5319415262193213
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3763,	 Acc1 = 0.2583,	 Acc2 = 0.2552

 ===== Epoch 101	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  1.8959686e+00  1.5352972e+00 -1.1927608e-02 -1.4941680e-03
 -1.7470114e-02 -1.5852105e-02] 6 6
train:	 Loss = 1.2574,	 Acc = 0.5120
48800 0.288
95392 0.57
49906 0.607
5744 0.626
902 0.525
88 0.545
0 0.0
0 0.0
0.5838573458219323
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3036,	 Acc = 0.5185
5486 0.4
29114 0.569
14300 0.471
1295 0.427
29 0.0
0 0.0
0 0.0
0 0.0
0.5330367919889132
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3679,	 Acc1 = 0.2484,	 Acc2 = 0.2433

 ===== Epoch 102	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [ 4.2057586e+00  2.4338946e+00  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03  2.4059165e+00  1.7804168e+00
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.7470114e-02 -1.5852105e-02] 6 6
train:	 Loss = 1.2577,	 Acc = 0.5132
48801 0.289
95392 0.571
49910 0.608
5740 0.626
902 0.524
87 0.54
0 0.0
0 0.0
0.5851175089291
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3237,	 Acc = 0.5119
5486 0.406
29114 0.551
14300 0.481
1295 0.439
29 0.0
0 0.0
0 0.0
0 0.0
0.5248558272609415
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3848,	 Acc1 = 0.2709,	 Acc2 = 0.2704

 ===== Epoch 103	 =====
[ 1.1225603   0.86417127 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  0.7805979   1.1529818 ] [-1.3572344  -1.1782732   0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946  2.4819047   2.8876107
 -1.3314189  -1.6881541 ] 6 6
train:	 Loss = 1.2573,	 Acc = 0.5123
48805 0.288
95392 0.572
49904 0.605
5742 0.618
901 0.517
88 0.5
0 0.0
0 0.0
0.5843304149920737
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3409,	 Acc = 0.4938
5486 0.347
29114 0.538
14300 0.462
1295 0.475
29 0.655
0 0.0
0 0.0
0 0.0
0.5117796951137735
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3953,	 Acc1 = 0.2903,	 Acc2 = 0.2937

 ===== Epoch 104	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  2.1839106   3.0615792 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.00719792 -0.5017236 ] 5 5
train:	 Loss = 1.2573,	 Acc = 0.5120
48808 0.288
95391 0.57
49900 0.606
5744 0.629
901 0.515
88 0.568
0 0.0
0 0.0
0.58389464821344
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3408,	 Acc = 0.4927
5486 0.41
29114 0.538
14300 0.438
1295 0.436
29 0.0
0 0.0
0 0.0
0 0.0
0.5027716929679467
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3947,	 Acc1 = 0.2738,	 Acc2 = 0.2739

 ===== Epoch 105	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096  0.88103133  3.2601418
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.34095427 -0.05041598
 -0.01747011 -0.0158521 ] 1 1
train:	 Loss = 1.2570,	 Acc = 0.5124
48804 0.287
95386 0.571
49911 0.608
5741 0.626
902 0.522
88 0.568
0 0.0
0 0.0
0.5848264793327544
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3560,	 Acc = 0.4885
5486 0.371
29114 0.507
14300 0.491
1295 0.54
29 0.0
0 0.0
0 0.0
0 0.0
0.502905807143815
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.4044,	 Acc1 = 0.2938,	 Acc2 = 0.2980

 ===== Epoch 106	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512  1.2790778   1.9865324  -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.67012286 -0.07143673 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 2 2
train:	 Loss = 1.2580,	 Acc = 0.5121
48797 0.286
95394 0.572
49908 0.606
5744 0.621
901 0.516
88 0.568
0 0.0
0 0.0
0.5846680040780083
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3536,	 Acc = 0.4943
5486 0.369
29114 0.539
14300 0.454
1295 0.472
29 0.0
0 0.0
0 0.0
0 0.0
0.5096338682998793
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.4029,	 Acc1 = 0.2969,	 Acc2 = 0.3017

 ===== Epoch 107	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143  1.7644261   2.8666432
 -0.3580977  -0.36433512  1.3376265   0.84166497 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  5.2247558e-02 -1.3253646e+00 -1.5871393e-02 -1.1254287e-02
 -1.9991982e+00 -1.3231639e+00 -1.1927608e-02 -1.4941680e-03
 -1.7470114e-02 -1.5852105e-02] 5 5
train:	 Loss = 1.2569,	 Acc = 0.5125
48806 0.287
95389 0.571
49904 0.607
5743 0.628
902 0.521
88 0.58
0 0.0
0 0.0
0.5847749727020378
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3126,	 Acc = 0.4891
5486 0.351
29114 0.516
14300 0.486
1295 0.52
29 0.0
0 0.0
0 0.0
0 0.0
0.5060351379140775
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3844,	 Acc1 = 0.2610,	 Acc2 = 0.2584

 ===== Epoch 108	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946  2.2863038   1.474314
 -0.01747011 -0.0158521 ] 6 2
train:	 Loss = 1.2581,	 Acc = 0.5119
48807 0.287
95384 0.571
49907 0.606
5744 0.62
902 0.542
88 0.534
0 0.0
0 0.0
0.5840157868771584
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3390,	 Acc = 0.5075
5486 0.371
29114 0.548
14300 0.477
1295 0.517
29 0.414
0 0.0
0 0.0
0 0.0
0.5241629040189548
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3941,	 Acc1 = 0.2845,	 Acc2 = 0.2868

 ===== Epoch 109	 =====
[ 2.2432141   1.3461231  -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-2.3639538e+00 -1.6232847e+00  4.1418090e+00  2.2981753e+00
  5.0004740e+00  1.0449314e+00  2.3147969e+00  3.0416591e+00
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.7470114e-02 -1.5852105e-02] 6 5
train:	 Loss = 1.2582,	 Acc = 0.5120
48803 0.288
95385 0.571
49910 0.605
5744 0.625
902 0.525
88 0.534
0 0.0
0 0.0
0.5839806878950726
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3514,	 Acc = 0.5030
5486 0.385
29114 0.549
14300 0.459
1295 0.465
29 0.31
0 0.0
0 0.0
0 0.0
0.5174571952255353
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.4151,	 Acc1 = 0.2486,	 Acc2 = 0.2435

 ===== Epoch 110	 =====
[-0.36866695 -0.38685238  2.7682445   1.0821934  -0.43314552 -0.35780522
  2.9179597   3.019075   -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-1.7616821e-02 -2.3137011e-02  2.1662965e-01 -4.7236776e+00
  3.2956851e+00  1.2792170e+00 -3.6170816e+00 -3.4973512e+00
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.7470114e-02 -1.5852105e-02] 6 2
train:	 Loss = 1.2585,	 Acc = 0.5117
48800 0.288
95393 0.569
49909 0.606
5741 0.628
901 0.515
88 0.557
0 0.0
0 0.0
0.5834166491264997
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3241,	 Acc = 0.5049
5486 0.357
29114 0.549
14300 0.471
1295 0.53
29 0.0
0 0.0
0 0.0
0 0.0
0.5230899906120077
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3845,	 Acc1 = 0.2602,	 Acc2 = 0.2575

 ===== Epoch 111	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096  3.0517602   1.7009009
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
  3.746402    1.5034151   0.0244112   0.04163946  0.40468827  0.10993885
 -0.01747011 -0.0158521 ] 2 2
train:	 Loss = 1.2576,	 Acc = 0.5111
48804 0.285
95388 0.571
49908 0.604
5743 0.628
901 0.522
88 0.568
0 0.0
0 0.0
0.5835503986107822
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3506,	 Acc = 0.4924
5486 0.369
29114 0.532
14300 0.458
1295 0.507
29 0.0
0 0.0
0 0.0
0 0.0
0.5074433367606956
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.4070,	 Acc1 = 0.2816,	 Acc2 = 0.2833

 ===== Epoch 112	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 1 1
train:	 Loss = 1.2588,	 Acc = 0.5124
48798 0.289
95395 0.57
49908 0.609
5741 0.62
902 0.529
88 0.534
0 0.0
0 0.0
0.5841522291066472
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3612,	 Acc = 0.4803
5486 0.402
29114 0.524
14300 0.43
1295 0.405
29 0.0
0 0.0
0 0.0
0 0.0
0.4899861415351603
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.4284,	 Acc1 = 0.2608,	 Acc2 = 0.2582

 ===== Epoch 113	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512  1.3434236   2.9146059  -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  2.9618919e+00  2.4970803e+00 -1.5871393e-02 -1.1254287e-02
 -2.0058968e+00 -3.5610206e+00 -1.1927608e-02 -1.4941680e-03
 -1.7470114e-02 -1.5852105e-02] 5 6
train:	 Loss = 1.2576,	 Acc = 0.5117
48801 0.286
95389 0.571
49911 0.606
5741 0.621
902 0.517
88 0.534
0 0.0
0 0.0
0.5840650919878183
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3423,	 Acc = 0.5064
5486 0.408
29114 0.532
14300 0.492
1295 0.503
29 0.138
0 0.0
0 0.0
0 0.0
0.5183512897313246
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.4076,	 Acc1 = 0.2540,	 Acc2 = 0.2500

 ===== Epoch 114	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
  2.160253    2.793133   -0.41367632 -0.42256096  2.8698728   2.7906399
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.04859785 -0.07608449  0.0244112   0.04163946 -0.03597946  0.38988036
 -0.01747011 -0.0158521 ] 3 3
train:	 Loss = 1.2571,	 Acc = 0.5123
48803 0.286
95391 0.572
49905 0.606
5743 0.622
902 0.527
88 0.511
0 0.0
0 0.0
0.5848949871406113
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.2992,	 Acc = 0.5155
5486 0.375
29114 0.552
14300 0.498
1295 0.507
29 0.0
0 0.0
0 0.0
0 0.0
0.5327685636371764
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3690,	 Acc1 = 0.2550,	 Acc2 = 0.2512

 ===== Epoch 115	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
  1.3033081   2.9704542  -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
  0.05407254 -1.1310487   0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 5 5
train:	 Loss = 1.2567,	 Acc = 0.5132
48803 0.288
95389 0.572
49912 0.609
5739 0.621
901 0.501
88 0.545
0 0.0
0 0.0
0.5856185333061456
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3051,	 Acc = 0.5126
5486 0.366
29114 0.548
14300 0.503
1295 0.458
29 0.0
0 0.0
0 0.0
0 0.0
0.5306003844606375
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3677,	 Acc1 = 0.2511,	 Acc2 = 0.2465

 ===== Epoch 116	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 0 1
train:	 Loss = 1.2572,	 Acc = 0.5111
48805 0.286
95390 0.571
49905 0.603
5742 0.627
902 0.512
88 0.591
0 0.0
0 0.0
0.5832845481394753
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.2891,	 Acc = 0.5114
5486 0.36
29114 0.554
14300 0.49
1295 0.442
29 0.0
0 0.0
0 0.0
0 0.0
0.5299298135812955
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3581,	 Acc1 = 0.2546,	 Acc2 = 0.2507

 ===== Epoch 117	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096  1.2390385   1.1078457
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946  0.86078554  1.1753472
 -0.01747011 -0.0158521 ] 2 2
train:	 Loss = 1.2577,	 Acc = 0.5115
48803 0.285
95386 0.571
49910 0.607
5743 0.621
902 0.518
88 0.523
0 0.0
0 0.0
0.5841582855902492
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.2744,	 Acc = 0.5123
5486 0.365
29114 0.546
14300 0.505
1295 0.471
29 0.0
0 0.0
0 0.0
0 0.0
0.5302874513836112
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3420,	 Acc1 = 0.2569,	 Acc2 = 0.2535

 ===== Epoch 118	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096  2.1111755   1.3796625
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.02735845  0.0148131
 -0.01747011 -0.0158521 ] 3 0
train:	 Loss = 1.2585,	 Acc = 0.5113
48799 0.287
95397 0.57
49903 0.606
5743 0.619
902 0.531
88 0.511
0 0.0
0 0.0
0.5832483737083396
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.2979,	 Acc = 0.4978
5486 0.41
29114 0.539
14300 0.46
1295 0.383
29 0.0
0 0.0
0 0.0
0 0.0
0.5086280119808664
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3707,	 Acc1 = 0.2356,	 Acc2 = 0.2279

 ===== Epoch 119	 =====
[-0.36866695 -0.38685238  2.713154    1.5715315  -0.43314552 -0.35780522
  1.626122    3.0676951  -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.8563028   0.297582    0.01022447  0.00647582
  1.0210941   0.10072516  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 2 2
train:	 Loss = 1.2571,	 Acc = 0.5125
48801 0.287
95398 0.572
49903 0.607
5740 0.62
902 0.53
88 0.5
0 0.0
0 0.0
0.5848938703290776
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.2726,	 Acc = 0.5298
5486 0.396
29114 0.563
14300 0.519
1295 0.478
29 0.0
0 0.0
0 0.0
0 0.0
0.546135276498726
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3428,	 Acc1 = 0.2494,	 Acc2 = 0.2445

 ===== Epoch 120	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 0 1
train:	 Loss = 1.2576,	 Acc = 0.5118
48800 0.287
95394 0.571
49904 0.605
5744 0.623
902 0.529
88 0.568
0 0.0
0 0.0
0.583837613134077
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3130,	 Acc = 0.4978
5486 0.25
29114 0.552
14300 0.485
1295 0.477
29 0.0
0 0.0
0 0.0
0 0.0
0.5281863292950065
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3816,	 Acc1 = 0.2199,	 Acc2 = 0.2413

 ===== Epoch 121	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143  0.3061142   2.6355238
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207 -0.08623657  0.18588379
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 1 6
train:	 Loss = 1.2581,	 Acc = 0.5117
48798 0.287
95392 0.57
49910 0.607
5742 0.622
902 0.524
88 0.523
0 0.0
0 0.0
0.5839088624912848
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3403,	 Acc = 0.5101
5486 0.4
29114 0.556
14300 0.464
1295 0.474
29 0.0
0 0.0
0 0.0
0 0.0
0.5236264473154812
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3939,	 Acc1 = 0.2911,	 Acc2 = 0.2947

 ===== Epoch 122	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 3 1
train:	 Loss = 1.2585,	 Acc = 0.5120
48798 0.287
95393 0.572
49909 0.604
5744 0.625
900 0.53
88 0.534
0 0.0
0 0.0
0.584106186774011
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3077,	 Acc = 0.4946
5486 0.407
29114 0.526
14300 0.476
1295 0.373
29 0.0
0 0.0
0 0.0
0 0.0
0.505252805221512
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 6
Testing:	 Loss = 1.3824,	 Acc1 = 0.2445,	 Acc2 = 0.2386

 ===== Epoch 123	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096  2.2503793   1.7280825
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946  0.55853516  0.1045031
 -0.01747011 -0.0158521 ] 2 2
train:	 Loss = 1.2576,	 Acc = 0.5127
48805 0.287
95386 0.572
49910 0.607
5743 0.623
900 0.528
88 0.511
0 0.0
0 0.0
0.5851131706867859
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.2979,	 Acc = 0.4982
5486 0.363
29114 0.518
14300 0.507
1295 0.537
29 0.0
0 0.0
0 0.0
0 0.0
0.5147972640708123
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3608,	 Acc1 = 0.2583,	 Acc2 = 0.2552

 ===== Epoch 124	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096  1.6858739   1.1276141
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
  1.8778081   2.3137927   0.0244112   0.04163946  0.57668906  0.9497633
 -0.01747011 -0.0158521 ] 2 2
train:	 Loss = 1.2585,	 Acc = 0.5119
48803 0.287
95387 0.571
49912 0.605
5741 0.623
901 0.524
88 0.591
0 0.0
0 0.0
0.5841188194357655
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3812,	 Acc = 0.4695
5486 0.357
29114 0.495
14300 0.457
1295 0.518
29 0.0
0 0.0
0 0.0
0 0.0
0.483347489829675
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.4411,	 Acc1 = 0.2633,	 Acc2 = 0.2612

 ===== Epoch 125	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-1.7616821e-02 -2.3137011e-02  3.4647396e+00  3.1641967e+00
  3.1700957e+00  1.1715722e+00 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.7470114e-02 -1.5852105e-02] 2 2
train:	 Loss = 1.2568,	 Acc = 0.5118
48805 0.287
95392 0.57
49902 0.605
5743 0.63
902 0.523
88 0.511
0 0.0
0 0.0
0.5838107704552481
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3193,	 Acc = 0.4825
5486 0.375
29114 0.511
14300 0.468
1295 0.472
29 0.0
0 0.0
0 0.0
0 0.0
0.49566364164692206
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3792,	 Acc1 = 0.2624,	 Acc2 = 0.2602

 ===== Epoch 126	 =====
[ 0.66574013  1.2512709  -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [ 0.45438066  0.26564705  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 2 2
train:	 Loss = 1.2567,	 Acc = 0.5121
48803 0.288
95392 0.571
49906 0.605
5741 0.622
902 0.518
88 0.5
0 0.0
0 0.0
0.5838688671240355
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.2746,	 Acc = 0.5252
5486 0.369
29114 0.56
14300 0.516
1295 0.524
29 0.0
0 0.0
0 0.0
0 0.0
0.5443694398497921
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3455,	 Acc1 = 0.2567,	 Acc2 = 0.2532

 ===== Epoch 127	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512  2.3996224   2.1131985  -0.4264405  -0.4242137
  3.7819924   1.405897  ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
 -3.2263217e+00 -2.6958563e+00 -1.1927608e-02 -1.4941680e-03
 -4.6783023e+00 -1.9621630e+00] 5 5
train:	 Loss = 1.2585,	 Acc = 0.5111
48799 0.287
95389 0.57
49913 0.605
5742 0.619
901 0.539
88 0.568
0 0.0
0 0.0
0.5832286411502766
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.4068,	 Acc = 0.4767
5486 0.41
29114 0.523
14300 0.419
1295 0.367
29 0.0
0 0.0
0 0.0
0 0.0
0.48480039340158254
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.4488,	 Acc1 = 0.3062,	 Acc2 = 0.3129

 ===== Epoch 128	 =====
[ 3.0226016   1.0795114  -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512  1.7967423   2.6734529  -0.4264405  -0.4242137
  2.862974    1.2129513 ] [-8.3044338e-01  1.0160122e+00  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
 -2.5297010e+00 -3.3006825e+00 -1.1927608e-02 -1.4941680e-03
 -7.9279929e-01  1.2581482e+00] 1 6
train:	 Loss = 1.2569,	 Acc = 0.5125
48800 0.287
95392 0.572
49907 0.606
5743 0.628
902 0.522
88 0.568
0 0.0
0 0.0
0.5847847821511261
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3957,	 Acc = 0.4908
5486 0.41
29114 0.534
14300 0.442
1295 0.416
29 0.0
0 0.0
0 0.0
0 0.0
0.500670570879342
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.4541,	 Acc1 = 0.2734,	 Acc2 = 0.2734

 ===== Epoch 129	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
  2.5881407   1.7520837  -0.41367632 -0.42256096  2.5956967   2.882069
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
  0.1330005   0.04473544  0.0244112   0.04163946 -0.212067    0.17516793
 -0.01747011 -0.0158521 ] 3 0
train:	 Loss = 1.2576,	 Acc = 0.5123
48805 0.287
95381 0.572
49912 0.606
5744 0.624
902 0.506
88 0.523
0 0.0
0 0.0
0.5846987706131148
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.2973,	 Acc = 0.4953
5486 0.37
29114 0.512
14300 0.506
1295 0.546
29 0.0
0 0.0
0 0.0
0 0.0
0.5106844293441817
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3634,	 Acc1 = 0.2688,	 Acc2 = 0.2679

 ===== Epoch 130	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096  1.6408827   3.0377462
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
  3.7528193   2.4346125   0.0244112   0.04163946 -2.4517543  -3.8092422
 -0.01747011 -0.0158521 ] 2 5
train:	 Loss = 1.2574,	 Acc = 0.5122
48798 0.287
95395 0.571
49909 0.608
5742 0.618
900 0.509
88 0.557
0 0.0
0 0.0
0.5845731875764631
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3904,	 Acc = 0.4948
5486 0.396
29114 0.546
14300 0.432
1295 0.466
29 0.0
0 0.0
0 0.0
0 0.0
0.5069292324198668
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.4421,	 Acc1 = 0.2678,	 Acc2 = 0.2667

 ===== Epoch 131	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096  1.4613028   1.6712481
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01238216 -0.05041598
 -0.01747011 -0.0158521 ] 3 3
train:	 Loss = 1.2569,	 Acc = 0.5131
48798 0.288
95389 0.573
49913 0.605
5742 0.627
902 0.527
88 0.523
0 0.0
0 0.0
0.5854808792770039
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.2781,	 Acc = 0.4987
5486 0.369
29114 0.512
14300 0.522
1295 0.507
29 0.069
0 0.0
0 0.0
0 0.0
0.514573740444365
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3575,	 Acc1 = 0.2335,	 Acc2 = 0.2254

 ===== Epoch 132	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096  2.2411497   1.7799749
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946  0.5476442   0.04742764
 -0.01747011 -0.0158521 ] 2 2
train:	 Loss = 1.2578,	 Acc = 0.5120
48798 0.289
95388 0.571
49914 0.605
5742 0.622
902 0.519
88 0.545
0 0.0
0 0.0
0.583744425589013
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 6
val:	 Loss = 1.5797,	 Acc = 0.4325
5486 0.254
29114 0.479
14300 0.414
1295 0.357
29 0.0
0 0.0
0 0.0
0 0.0
0.4544682372926818
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.6729,	 Acc1 = 0.1842,	 Acc2 = 0.1983

 ===== Epoch 133	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 6 0
train:	 Loss = 1.2562,	 Acc = 0.5119
48798 0.289
95391 0.57
49910 0.606
5743 0.62
902 0.518
88 0.591
0 0.0
0 0.0
0.5835010589736507
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3293,	 Acc = 0.5041
5486 0.349
29114 0.537
14300 0.495
1295 0.522
29 0.0
0 0.0
0 0.0
0 0.0
0.5231123429746524
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3909,	 Acc1 = 0.2575,	 Acc2 = 0.2542

 ===== Epoch 134	 =====
[ 0.21228911  1.5512089  -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01795044 -0.01840284  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 0 0
train:	 Loss = 1.2573,	 Acc = 0.5120
48799 0.287
95390 0.57
49912 0.609
5741 0.615
902 0.541
88 0.545
0 0.0
0 0.0
0.5843205093630988
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3140,	 Acc = 0.5104
5486 0.373
29114 0.548
14300 0.484
1295 0.54
29 0.0
0 0.0
0 0.0
0 0.0
0.5272028253386383
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3795,	 Acc1 = 0.2470,	 Acc2 = 0.2416

 ===== Epoch 135	 =====
[-0.36866695 -0.38685238  2.156456    1.0615171  -0.43314552 -0.35780522
  3.5694249   3.396598   -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -3.0383697  -1.2429371   0.01022447  0.00647582
 -4.333205   -3.8863328   1.3292958   3.6048546   4.051246    1.6102079
  3.035469    3.7327032 ] 5 5
train:	 Loss = 1.2579,	 Acc = 0.5119
48803 0.287
95390 0.571
49907 0.606
5742 0.627
902 0.506
88 0.58
0 0.0
0 0.0
0.5840333094343843
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3217,	 Acc = 0.5182
5486 0.406
29114 0.565
14300 0.476
1295 0.425
29 0.0
0 0.0
0 0.0
0 0.0
0.5319191738566766
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3750,	 Acc1 = 0.2548,	 Acc2 = 0.2510

 ===== Epoch 136	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143  3.2413259   2.3191373
 -0.3580977  -0.36433512  3.104527    1.409227   -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207 -0.39997405  0.5489211
 -0.01587139 -0.01125429  0.26018995  0.83054316 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
train:	 Loss = 1.2574,	 Acc = 0.5129
48805 0.288
95386 0.572
49909 0.606
5742 0.628
902 0.521
88 0.591
0 0.0
0 0.0
0.5850079262236313
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3015,	 Acc = 0.5104
5486 0.406
29114 0.545
14300 0.489
1295 0.419
29 0.0
0 0.0
0 0.0
0 0.0
0.5231346953372972
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3694,	 Acc1 = 0.2606,	 Acc2 = 0.2580

 ===== Epoch 137	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512  1.481389    1.3507656  -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429 -0.00707097  1.306515   -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 6
train:	 Loss = 1.2570,	 Acc = 0.5126
48800 0.286
95389 0.572
49911 0.608
5742 0.619
902 0.529
88 0.489
0 0.0
0 0.0
0.5854293832877289
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3046,	 Acc = 0.4963
5486 0.357
29114 0.529
14300 0.483
1295 0.524
29 0.0
0 0.0
0 0.0
0 0.0
0.5133890652241943
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3644,	 Acc1 = 0.2614,	 Acc2 = 0.2589

 ===== Epoch 138	 =====
[ 1.1938503   3.09448    -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  0.8143752   3.4266114 ] [-1.4212765e+00 -3.2376349e+00  3.9208763e+00  1.6424135e+00
  1.0224475e-02  6.4758179e-03  3.5282290e+00  3.9316013e+00
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.3690841e+00 -4.1514091e+00] 6 5
train:	 Loss = 1.2573,	 Acc = 0.5127
48799 0.288
95389 0.57
49911 0.61
5743 0.626
902 0.519
88 0.568
0 0.0
0 0.0
0.5848598659501556
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.4240,	 Acc = 0.4707
5486 0.395
29114 0.517
14300 0.41
1295 0.428
29 0.0
0 0.0
0 0.0
0 0.0
0.4799946354329653
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.4780,	 Acc1 = 0.2560,	 Acc2 = 0.2525

 ===== Epoch 139	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096  1.4539968   3.0155067
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
  1.3144064   3.778366    0.0244112   0.04163946 -2.231194   -3.7847812
 -0.01747011 -0.0158521 ] 6 6
train:	 Loss = 1.2577,	 Acc = 0.5118
48800 0.289
95396 0.571
49905 0.604
5741 0.624
902 0.524
88 0.489
0 0.0
0 0.0
0.5834232266891181
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3717,	 Acc = 0.4806
5486 0.379
29114 0.526
14300 0.438
1295 0.378
29 0.0
0 0.0
0 0.0
0 0.0
0.4931154723054227
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.4414,	 Acc1 = 0.2350,	 Acc2 = 0.2271

 ===== Epoch 140	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 1 1
train:	 Loss = 1.2583,	 Acc = 0.5121
48792 0.287
95399 0.571
49908 0.607
5743 0.622
902 0.523
88 0.523
0 0.0
0 0.0
0.5844711917916338
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3596,	 Acc = 0.5143
5486 0.411
29114 0.548
14300 0.484
1295 0.53
29 0.0
0 0.0
0 0.0
0 0.0
0.5269569493495463
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.4132,	 Acc1 = 0.2820,	 Acc2 = 0.2838

 ===== Epoch 141	 =====
[-0.36866695 -0.38685238  2.3188255   1.4612582  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.01020809  0.5432324   0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 6 4
train:	 Loss = 1.2572,	 Acc = 0.5122
48800 0.289
95393 0.571
49907 0.605
5743 0.624
901 0.509
88 0.545
0 0.0
0 0.0
0.583903388760261
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.2951,	 Acc = 0.5179
5486 0.386
29114 0.563
14300 0.486
1295 0.436
29 0.0
0 0.0
0 0.0
0 0.0
0.5341097053958603
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3662,	 Acc1 = 0.2385,	 Acc2 = 0.2314

 ===== Epoch 142	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096  1.2790301   1.3302413
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.68087053  0.35726583
 -0.01747011 -0.0158521 ] 1 1
train:	 Loss = 1.2582,	 Acc = 0.5116
48804 0.286
95389 0.57
49908 0.608
5741 0.623
902 0.529
88 0.568
0 0.0
0 0.0
0.5839055963375168
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.2966,	 Acc = 0.5054
5486 0.369
29114 0.537
14300 0.495
1295 0.49
29 0.0
0 0.0
0 0.0
0 0.0
0.5221064866556395
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3554,	 Acc1 = 0.2738,	 Acc2 = 0.2739

 ===== Epoch 143	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143  1.6927884   1.2645164
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 0 0
train:	 Loss = 1.2585,	 Acc = 0.5118
48802 0.287
95392 0.571
49907 0.606
5741 0.625
902 0.517
88 0.614
0 0.0
0 0.0
0.5841281326054069
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3053,	 Acc = 0.5144
5486 0.416
29114 0.539
14300 0.505
1295 0.507
29 0.0
0 0.0
0 0.0
0 0.0
0.5265546068219411
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3641,	 Acc1 = 0.2754,	 Acc2 = 0.2758

 ===== Epoch 144	 =====
[ 2.4019878   2.2561915  -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  2.2018962   2.571393  ] [-0.15119281  0.09758419  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01796006 -0.0158521 ] 0 0
train:	 Loss = 1.2581,	 Acc = 0.5128
48802 0.288
95390 0.573
49907 0.604
5743 0.621
902 0.528
88 0.557
0 0.0
0 0.0
0.5849306058014866
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.2962,	 Acc = 0.4998
5486 0.382
29114 0.532
14300 0.484
1295 0.452
29 0.0
0 0.0
0 0.0
0 0.0
0.5142161026420493
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3622,	 Acc1 = 0.2531,	 Acc2 = 0.2490

 ===== Epoch 145	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
  1.609776    3.6368403  -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
  0.07332399 -0.9807605   0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 5 5
train:	 Loss = 1.2563,	 Acc = 0.5136
48804 0.288
95389 0.572
49910 0.609
5742 0.631
899 0.525
88 0.591
0 0.0
0 0.0
0.5859446943984002
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3995,	 Acc = 0.4740
5486 0.364
29114 0.511
14300 0.438
1295 0.507
29 0.0
0 0.0
0 0.0
0 0.0
0.4874826769189503
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.4490,	 Acc1 = 0.3010,	 Acc2 = 0.3067

 ===== Epoch 146	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512  1.8181909   3.041272   -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
 -2.5544846e+00 -3.6977639e+00 -1.1927608e-02 -1.4941680e-03
 -1.7470114e-02 -1.5852105e-02] 4 6
train:	 Loss = 1.2574,	 Acc = 0.5121
48802 0.288
95388 0.572
49908 0.605
5744 0.62
902 0.52
88 0.545
0 0.0
0 0.0
0.5841544432019996
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3279,	 Acc = 0.4806
5486 0.369
29114 0.504
14300 0.477
1295 0.495
29 0.0
0 0.0
0 0.0
0 0.0
0.4943224998882382
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3887,	 Acc1 = 0.2583,	 Acc2 = 0.2552

 ===== Epoch 147	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143  3.0302851   2.5726955
 -0.3580977  -0.36433512  3.2807534   1.7210206  -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [ 1.4976809e+00  1.2503532e+00  2.2267485e-02  2.9032072e-02
 -4.2608619e+00 -2.7500749e+00 -1.5871393e-02 -1.1254287e-02
  2.9368064e-01  1.2302543e+00 -1.1927608e-02 -1.4941680e-03
  2.0938222e+00  1.5519309e+00] 4 6
train:	 Loss = 1.2577,	 Acc = 0.5129
48797 0.288
95397 0.572
49905 0.606
5743 0.625
902 0.519
88 0.568
0 0.0
0 0.0
0.5850889597789982
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3590,	 Acc = 0.4763
5486 0.366
29114 0.503
14300 0.464
1295 0.48
29 0.0
0 0.0
0 0.0
0 0.0
0.4897402655460682
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.4102,	 Acc1 = 0.2853,	 Acc2 = 0.2878

 ===== Epoch 148	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 5 1
train:	 Loss = 1.2562,	 Acc = 0.5119
48798 0.287
95389 0.57
49914 0.608
5741 0.624
902 0.507
88 0.534
0 0.0
0 0.0
0.5841193417261928
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3702,	 Acc = 0.4998
5486 0.404
29114 0.528
14300 0.48
1295 0.498
29 0.0
0 0.0
0 0.0
0 0.0
0.5115785238499709
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.4197,	 Acc1 = 0.2841,	 Acc2 = 0.2863

 ===== Epoch 149	 =====
[-0.36866695 -0.38685238  2.145686    0.9535411  -0.43314552 -0.35780522
  5.412322    2.3870094  -0.41367632 -0.42256096  4.694133    2.3977408
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.02054096  0.23721033  0.01022447  0.00647582
  0.22797032  0.42192936  0.0244112   0.04163946 -0.5846603   0.47685248
 -0.01747011 -0.0158521 ] 3 3
train:	 Loss = 1.2577,	 Acc = 0.5127
48804 0.29
95392 0.571
49903 0.606
5743 0.624
902 0.529
88 0.523
0 0.0
0 0.0
0.584155550293367
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3575,	 Acc = 0.4848
5486 0.41
29114 0.525
14300 0.439
1295 0.409
29 0.0
0 0.0
0 0.0
0 0.0
0.493920157360633
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.4125,	 Acc1 = 0.2686,	 Acc2 = 0.2676

 ===== Epoch 150	 =====
[ 3.1323116   1.8896006  -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512  1.7654394   3.2848608  -0.4264405  -0.4242137
  2.8120887   2.0420961 ] [-3.162659   -2.1251063   0.02226749  0.02903207  0.01022447  0.00647582
  1.6544994   2.1104617  -2.4935308  -3.9607317   2.9021502   3.8252788
 -3.5967531  -2.6514225 ] 6 6
train:	 Loss = 1.2569,	 Acc = 0.5130
48798 0.288
95394 0.572
49907 0.608
5743 0.627
902 0.514
88 0.489
0 0.0
0 0.0
0.5851520054724602
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3536,	 Acc = 0.5020
5486 0.405
29114 0.543
14300 0.463
1295 0.432
29 0.034
0 0.0
0 0.0
0 0.0
0.5138808172023783
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.4200,	 Acc1 = 0.2641,	 Acc2 = 0.2622

 ===== Epoch 151	 =====
[ 2.2698562   2.2561915  -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  2.201457    2.430595  ] [ 0.24082023  0.02420464  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.07959474  0.02652041] 4 0
train:	 Loss = 1.2572,	 Acc = 0.5119
48803 0.287
95384 0.571
49911 0.605
5744 0.623
902 0.521
88 0.534
0 0.0
0 0.0
0.5839741102026587
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.2805,	 Acc = 0.5290
5486 0.393
29114 0.565
14300 0.514
1295 0.477
29 0.0
0 0.0
0 0.0
0 0.0
0.5457329339711208
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3461,	 Acc1 = 0.2538,	 Acc2 = 0.2498

 ===== Epoch 152	 =====
[-0.36866695 -0.38685238  1.1085026   1.566937   -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02079201  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 0 0
train:	 Loss = 1.2593,	 Acc = 0.5110
48805 0.286
95392 0.569
49908 0.606
5737 0.624
902 0.512
88 0.557
0 0.0
0 0.0
0.5831069481079019
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3378,	 Acc = 0.4950
5486 0.4
29114 0.542
14300 0.442
1295 0.43
29 0.0
0 0.0
0 0.0
0 0.0
0.5066386517054853
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3974,	 Acc1 = 0.2600,	 Acc2 = 0.2572

 ===== Epoch 153	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512  3.4795883   2.3080697  -0.4264405  -0.4242137
  4.064936    1.4085044 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
 -4.4742093e+00 -2.9062307e+00 -1.1927608e-02 -1.4941680e-03
 -4.9938154e+00 -1.9649878e+00] 2 6
train:	 Loss = 1.2582,	 Acc = 0.5119
48799 0.289
95394 0.57
49908 0.606
5741 0.621
902 0.514
88 0.511
0 0.0
0 0.0
0.5835772496760572
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3666,	 Acc = 0.4751
5486 0.368
29114 0.508
14300 0.448
1295 0.496
29 0.0
0 0.0
0 0.0
0 0.0
0.4881979525235817
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.4283,	 Acc1 = 0.2752,	 Acc2 = 0.2756

 ===== Epoch 154	 =====
[ 1.6869937   2.6407275  -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.1622428   2.8712406 ] [-0.03129864  0.6585827   0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
  0.1654787   0.7242545 ] 4 4
train:	 Loss = 1.2567,	 Acc = 0.5124
48806 0.286
95394 0.573
49899 0.605
5743 0.622
902 0.507
88 0.557
0 0.0
0 0.0
0.5849591517240472
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3414,	 Acc = 0.5029
5486 0.37
29114 0.538
14300 0.48
1295 0.541
29 0.0
0 0.0
0 0.0
0 0.0
0.5192006795118244
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3999,	 Acc1 = 0.2672,	 Acc2 = 0.2659

 ===== Epoch 155	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143  2.4215581   1.1253961
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.02693805  0.00014377
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 0 0
train:	 Loss = 1.2584,	 Acc = 0.5117
48804 0.287
95385 0.57
49911 0.607
5742 0.622
902 0.52
88 0.557
0 0.0
0 0.0
0.5838792853947957
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.2989,	 Acc = 0.5040
5486 0.379
29114 0.533
14300 0.492
1295 0.527
29 0.0
0 0.0
0 0.0
0 0.0
0.5193571460503376
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3589,	 Acc1 = 0.2546,	 Acc2 = 0.2507

 ===== Epoch 156	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143  1.745065    2.5569882
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207 -0.092444   -0.28901964
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 5 5
train:	 Loss = 1.2573,	 Acc = 0.5127
48796 0.288
95397 0.572
49906 0.606
5743 0.631
902 0.511
88 0.534
0 0.0
0 0.0
0.5848483253966166
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.2981,	 Acc = 0.4962
5486 0.407
29114 0.513
14300 0.497
1295 0.494
29 0.0
0 0.0
0 0.0
0 0.0
0.5072198131342482
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3701,	 Acc1 = 0.2577,	 Acc2 = 0.2545

 ===== Epoch 157	 =====
[ 3.7933352   1.676824   -0.41997173 -0.34217143 -0.43314552 -0.35780522
  2.7615135   1.468941   -0.41367632 -0.42256096  1.8408439   3.2848525
 -0.3977158  -0.3905835 ] [-3.7564776  -1.9286385   0.02226749  0.02903207  0.01022447  0.00647582
  0.8978927   0.9906671   0.0244112   0.04163946 -2.6877456  -4.0810304
 -0.01747011 -0.0158521 ] 2 2
train:	 Loss = 1.2561,	 Acc = 0.5131
48799 0.29
95391 0.572
49911 0.606
5741 0.618
902 0.529
88 0.545
0 0.0
0 0.0
0.5847809357179033
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3624,	 Acc = 0.5100
5486 0.355
29114 0.551
14300 0.482
1295 0.561
29 0.0
0 0.0
0 0.0
0 0.0
0.5289463096249274
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.4191,	 Acc1 = 0.2767,	 Acc2 = 0.2773

 ===== Epoch 158	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.7215496   0.96003604] [ 2.3538051e+00  1.6385548e+00  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  3.0386944e+00  4.0203438e+00 -1.1927608e-02 -1.4941680e-03
  7.8818876e-01  6.3103497e-01] 2 2
train:	 Loss = 1.2567,	 Acc = 0.5130
48806 0.288
95392 0.572
49903 0.607
5741 0.628
902 0.517
88 0.545
0 0.0
0 0.0
0.585222264612632
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.4541,	 Acc = 0.4629
5486 0.383
29114 0.511
14300 0.404
1295 0.378
29 0.0
0 0.0
0 0.0
0 0.0
0.47266306048549334
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.5044,	 Acc1 = 0.2845,	 Acc2 = 0.2868

 ===== Epoch 159	 =====
[ 2.4438      2.1228857  -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [ 2.3669513e-02  4.7875457e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03  1.1668180e+00  1.7037995e+00
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.7470114e-02 -1.5852105e-02] 3 2
train:	 Loss = 1.2583,	 Acc = 0.5116
48799 0.286
95389 0.572
49914 0.606
5740 0.62
902 0.509
88 0.523
0 0.0
0 0.0
0.5841034512244052
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3449,	 Acc = 0.4829
5486 0.369
29114 0.522
14300 0.445
1295 0.517
29 0.0
0 0.0
0 0.0
0 0.0
0.4968036121418034
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.4060,	 Acc1 = 0.2606,	 Acc2 = 0.2580

 ===== Epoch 160	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143  1.8794345   0.77310777
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01213468  0.04657877
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 2 4
train:	 Loss = 1.2582,	 Acc = 0.5122
48805 0.286
95383 0.572
49912 0.606
5742 0.625
902 0.514
88 0.602
0 0.0
0 0.0
0.5848697928657409
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3213,	 Acc = 0.5045
5486 0.411
29114 0.534
14300 0.482
1295 0.497
29 0.0
0 0.0
0 0.0
0 0.0
0.515981939290983
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3832,	 Acc1 = 0.2711,	 Acc2 = 0.2706

 ===== Epoch 161	 =====
[-0.36866695 -0.38685238  1.1072606   1.5646394  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.04539284  0.01237781  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 0
train:	 Loss = 1.2562,	 Acc = 0.5130
48801 0.288
95385 0.572
49913 0.607
5743 0.624
902 0.525
88 0.58
0 0.0
0 0.0
0.5852951042879413
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3591,	 Acc = 0.5026
5486 0.398
29114 0.552
14300 0.449
1295 0.447
29 0.0
0 0.0
0 0.0
0 0.0
0.5154231302248647
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.4079,	 Acc1 = 0.2996,	 Acc2 = 0.3049

 ===== Epoch 162	 =====
[ 3.4124262   2.4946039  -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  3.173555    2.626148  ] [-3.4142950e+00 -2.6837375e+00  2.2267485e-02  2.9032072e-02
  2.8558798e+00  3.0310831e+00 -1.5871393e-02 -1.1254287e-02
  3.7996171e+00  1.6168171e+00 -1.1927608e-02 -1.4941680e-03
 -3.9998276e+00 -3.2841852e+00] 5 6
train:	 Loss = 1.2581,	 Acc = 0.5123
48798 0.287
95385 0.571
49916 0.607
5743 0.624
902 0.518
88 0.568
0 0.0
0 0.0
0.5847705118591894
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3166,	 Acc = 0.5065
5486 0.395
29114 0.542
14300 0.479
1295 0.48
29 0.0
0 0.0
0 0.0
0 0.0
0.5201171263802584
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3848,	 Acc1 = 0.2628,	 Acc2 = 0.2607

 ===== Epoch 163	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096  1.4870672   3.2576706
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -2.2702234  -4.0511336
 -0.01747011 -0.0158521 ] 5 1
train:	 Loss = 1.2590,	 Acc = 0.5134
48798 0.289
95397 0.573
49909 0.606
5738 0.624
902 0.525
88 0.545
0 0.0
0 0.0
0.5853361748030046
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3521,	 Acc = 0.4963
5486 0.407
29114 0.545
14300 0.44
1295 0.412
29 0.0
0 0.0
0 0.0
0 0.0
0.507242165496893
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.4088,	 Acc1 = 0.2760,	 Acc2 = 0.2766

 ===== Epoch 164	 =====
[ 2.6037753   1.2230716  -0.41997173 -0.34217143  4.4483256   2.8531797
 -0.3580977  -0.36433512  5.379818    2.3056338   0.43688694  1.4488524
  3.3977156   1.2781355 ] [ 0.16670717  0.3650645   0.02226749  0.02903207 -6.0095825  -3.01391
 -0.01587139 -0.01125429  0.1503378   0.37560868  0.24857149  0.44152007
  0.6316561   0.43047175] 2 2
train:	 Loss = 1.2566,	 Acc = 0.5123
48799 0.287
95393 0.572
49909 0.606
5741 0.616
902 0.531
88 0.523
0 0.0
0 0.0
0.5847085830050055
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3419,	 Acc = 0.5095
5486 0.392
29114 0.549
14300 0.484
1295 0.402
29 0.0
0 0.0
0 0.0
0 0.0
0.523894675667218
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.4045,	 Acc1 = 0.2507,	 Acc2 = 0.2460

 ===== Epoch 165	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512  1.8790584   2.8731956  -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
 -2.6248162e+00 -3.5163162e+00 -1.1927608e-02 -1.4941680e-03
 -1.7470114e-02 -1.5852105e-02] 4 1
train:	 Loss = 1.2579,	 Acc = 0.5125
48801 0.288
95395 0.571
49904 0.607
5742 0.622
902 0.529
88 0.489
0 0.0
0 0.0
0.5845847228525761
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.2832,	 Acc = 0.5294
5486 0.405
29114 0.563
14300 0.511
1295 0.514
29 0.0
0 0.0
0 0.0
0 0.0
0.5445706111135947
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3476,	 Acc1 = 0.2657,	 Acc2 = 0.2642

 ===== Epoch 166	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512  1.0587944   2.8098624  -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429 -0.18189599  0.35720092 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 1 1
train:	 Loss = 1.2583,	 Acc = 0.5112
48801 0.287
95393 0.57
49905 0.605
5743 0.63
902 0.511
88 0.568
0 0.0
0 0.0
0.5831047615288987
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3327,	 Acc = 0.4998
5486 0.363
29114 0.533
14300 0.483
1295 0.522
29 0.0
0 0.0
0 0.0
0 0.0
0.5164960436318119
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3959,	 Acc1 = 0.2542,	 Acc2 = 0.2502

 ===== Epoch 167	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143  3.0163455   2.5996218
 -0.3580977  -0.36433512  2.0599227   1.9158915  -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
 -4.2436714e+00 -2.7754030e+00 -1.5871393e-02 -1.1254287e-02
 -2.8338027e+00 -2.4828522e+00 -1.1927608e-02 -1.4941680e-03
 -1.7470114e-02 -1.5852105e-02] 6 5
train:	 Loss = 1.2579,	 Acc = 0.5114
48808 0.287
95380 0.57
49911 0.606
5743 0.615
902 0.525
88 0.545
0 0.0
0 0.0
0.5832565910645687
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.2938,	 Acc = 0.5080
5486 0.41
29114 0.525
14300 0.515
1295 0.459
29 0.0
0 0.0
0 0.0
0 0.0
0.51998301220439
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3588,	 Acc1 = 0.2540,	 Acc2 = 0.2500

 ===== Epoch 168	 =====
[ 2.3382742   1.8255113  -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.4925625   2.2611156 ] [-0.3254397   0.3603303   0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.08546349  0.43612137] 6 1
train:	 Loss = 1.2579,	 Acc = 0.5119
48798 0.287
95397 0.571
49905 0.607
5743 0.624
901 0.517
88 0.557
0 0.0
0 0.0
0.584112764250102
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.2584,	 Acc = 0.5274
5486 0.4
29114 0.56
14300 0.514
1295 0.49
29 0.0
0 0.0
0 0.0
0 0.0
0.543050650453753
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3316,	 Acc1 = 0.2437,	 Acc2 = 0.2376

 ===== Epoch 169	 =====
[-0.36866695 -0.38685238  1.4253736   1.6680208  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.14036147  0.29341847  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 2 4
train:	 Loss = 1.2587,	 Acc = 0.5114
48802 0.287
95388 0.569
49910 0.607
5742 0.622
902 0.514
88 0.5
0 0.0
0 0.0
0.5832993488127344
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3209,	 Acc = 0.5002
5486 0.378
29114 0.532
14300 0.481
1295 0.535
29 0.0
0 0.0
0 0.0
0 0.0
0.5152666636863517
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3801,	 Acc1 = 0.2901,	 Acc2 = 0.2935

 ===== Epoch 170	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143  1.7671374   0.78881484
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-1.7616821e-02 -2.3137011e-02  3.7151954e+00  3.1725237e+00
  1.1042492e+00  4.0539470e-01 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.7470114e-02 -1.5852105e-02] 2 2
train:	 Loss = 1.2568,	 Acc = 0.5122
48797 0.288
95395 0.571
49906 0.606
5744 0.622
902 0.512
88 0.591
0 0.0
0 0.0
0.5842207386457066
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.2831,	 Acc = 0.5226
5486 0.409
29114 0.555
14300 0.505
1295 0.483
29 0.0
0 0.0
0 0.0
0 0.0
0.5364790558362019
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3512,	 Acc1 = 0.2484,	 Acc2 = 0.2433

 ===== Epoch 171	 =====
[ 2.0793803   2.1613393  -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  2.3111255   2.3210852 ] [ 1.0307635   0.30115324  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.07323605  0.38244954] 4 2
train:	 Loss = 1.2564,	 Acc = 0.5129
48797 0.286
95391 0.572
49912 0.609
5742 0.622
902 0.507
88 0.534
0 0.0
0 0.0
0.5856940835991712
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.2894,	 Acc = 0.5148
5486 0.364
29114 0.541
14300 0.517
1295 0.556
29 0.0
0 0.0
0 0.0
0 0.0
0.5332379632527158
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3516,	 Acc1 = 0.2486,	 Acc2 = 0.2435

 ===== Epoch 172	 =====
[-0.36866695 -0.38685238  2.7508469   1.7231575  -0.43314552 -0.35780522
  1.953604    3.147776   -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-1.7616821e-02 -2.3137011e-02 -3.7444694e+00 -1.8424904e+00
  1.0224475e-02  6.4758179e-03 -2.5570126e+00 -3.6299586e+00
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.7470114e-02 -1.5852105e-02] 2 5
train:	 Loss = 1.2579,	 Acc = 0.5114
48799 0.287
95387 0.571
49913 0.604
5743 0.62
902 0.525
88 0.545
0 0.0
0 0.0
0.5833996566534897
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.4249,	 Acc = 0.4721
5486 0.35
29114 0.525
14300 0.417
1295 0.412
29 0.103
0 0.0
0 0.0
0 0.0
0.4871473914792794
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.4837,	 Acc1 = 0.2562,	 Acc2 = 0.2527

 ===== Epoch 173	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143  2.6705472   3.0035195
 -0.3580977  -0.36433512  1.9445637   2.2423005  -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.02932655  0.00647582
 -0.01587139 -0.01125429 -0.10955366  0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 0 0
train:	 Loss = 1.2567,	 Acc = 0.5118
48802 0.287
95393 0.57
49906 0.607
5742 0.627
902 0.532
87 0.563
0 0.0
0 0.0
0.5839834243241465
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3235,	 Acc = 0.5136
5486 0.408
29114 0.559
14300 0.47
1295 0.446
29 0.0
0 0.0
0 0.0
0 0.0
0.5265099020966516
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3870,	 Acc1 = 0.2616,	 Acc2 = 0.2592

 ===== Epoch 174	 =====
[-0.36866695 -0.38685238  1.6809413   1.6289656  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [ 3.0122151   2.9120452  -2.4734876  -1.7571374   0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946  3.2143853   1.452571
  3.2208633   4.1225305 ] 5 5
train:	 Loss = 1.2569,	 Acc = 0.5118
48804 0.289
95391 0.57
49908 0.606
5739 0.628
902 0.519
88 0.534
0 0.0
0 0.0
0.5833793774830952
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3433,	 Acc = 0.4963
5486 0.406
29114 0.539
14300 0.447
1295 0.464
29 0.0
0 0.0
0 0.0
0 0.0
0.5073315749474719
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.4009,	 Acc1 = 0.2626,	 Acc2 = 0.2604

 ===== Epoch 175	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.7721024e+00  2.2347918e+00 -1.1927608e-02 -1.4941680e-03
  4.1649570e+00  1.8315893e+00] 2 6
train:	 Loss = 1.2578,	 Acc = 0.5116
48801 0.288
95389 0.57
49910 0.604
5742 0.63
902 0.517
88 0.591
0 0.0
0 0.0
0.5834270642171663
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.2796,	 Acc = 0.5208
5486 0.4
29114 0.558
14300 0.501
1295 0.422
29 0.0
0 0.0
0 0.0
0 0.0
0.5355402566051232
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3511,	 Acc1 = 0.2503,	 Acc2 = 0.2455

 ===== Epoch 176	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 1 1
train:	 Loss = 1.2573,	 Acc = 0.5121
48805 0.287
95387 0.572
49907 0.606
5744 0.624
901 0.507
88 0.511
0 0.0
0 0.0
0.5842646372026021
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3166,	 Acc = 0.5048
5486 0.378
29114 0.54
14300 0.484
1295 0.476
29 0.0
0 0.0
0 0.0
0 0.0
0.5202959452814162
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3858,	 Acc1 = 0.2760,	 Acc2 = 0.2766

 ===== Epoch 177	 =====
[-0.36866695 -0.38685238  1.8192884   3.0648172  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-1.7616821e-02 -2.3137011e-02 -2.6378353e+00 -3.0582516e+00
  2.9552059e+00  1.0343779e+00 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.7470114e-02 -1.5852105e-02] 2 2
train:	 Loss = 1.2574,	 Acc = 0.5124
48801 0.288
95391 0.571
49908 0.606
5742 0.623
902 0.525
88 0.511
0 0.0
0 0.0
0.584518946793746
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3585,	 Acc = 0.4926
5486 0.362
29114 0.536
14300 0.456
1295 0.475
29 0.655
0 0.0
0 0.0
0 0.0
0.5086056596182217
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.4163,	 Acc1 = 0.2608,	 Acc2 = 0.2582

 ===== Epoch 178	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  1.9796975e+00  2.8028023e+00 -1.1927608e-02 -1.4941680e-03
 -1.7470114e-02 -1.5852105e-02] 6 5
train:	 Loss = 1.2566,	 Acc = 0.5133
48798 0.286
95398 0.572
49906 0.61
5740 0.626
902 0.525
88 0.534
0 0.0
0 0.0
0.5862372890274544
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.2965,	 Acc = 0.5196
5486 0.402
29114 0.553
14300 0.501
1295 0.491
29 0.0
0 0.0
0 0.0
0 0.0
0.5340650006705708
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3620,	 Acc1 = 0.2645,	 Acc2 = 0.2627

 ===== Epoch 179	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512  1.3434236   2.91217    -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429 -0.05529848  0.04426914 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 0 0
train:	 Loss = 1.2567,	 Acc = 0.5131
48796 0.287
95393 0.572
49910 0.608
5743 0.628
902 0.532
88 0.568
0 0.0
0 0.0
0.585709963429714
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3032,	 Acc = 0.4976
5486 0.362
29114 0.522
14300 0.503
1295 0.483
29 0.0
0 0.0
0 0.0
0 0.0
0.5142831597299835
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3690,	 Acc1 = 0.2597,	 Acc2 = 0.2570

 ===== Epoch 180	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 3 1
train:	 Loss = 1.2570,	 Acc = 0.5132
48801 0.288
95386 0.572
49913 0.608
5742 0.625
902 0.517
88 0.602
0 0.0
0 0.0
0.5853477251350053
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3313,	 Acc = 0.4854
5486 0.398
29114 0.521
14300 0.459
1295 0.362
29 0.0
0 0.0
0 0.0
0 0.0
0.4961553936251062
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 0
Testing:	 Loss = 1.4024,	 Acc1 = 0.2552,	 Acc2 = 0.2515

 ===== Epoch 181	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096  2.0015812   2.251948
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
  1.6654096   1.8776622   0.0244112   0.04163946  0.20318805  0.6970006
 -0.01747011 -0.0158521 ] 6 6
train:	 Loss = 1.2582,	 Acc = 0.5117
48801 0.286
95393 0.571
49906 0.605
5742 0.628
902 0.528
88 0.489
0 0.0
0 0.0
0.5841374456525313
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3041,	 Acc = 0.5111
5486 0.41
29114 0.548
14300 0.483
1295 0.425
29 0.0
0 0.0
0 0.0
0 0.0
0.523559390227547
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3774,	 Acc1 = 0.2579,	 Acc2 = 0.2547

 ===== Epoch 182	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143  2.3212655  -4.9465294
 -0.3580977  -0.36433512  1.3753054   0.9951259  -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  3.1966478e-01  6.8092041e+00 -1.5871393e-02 -1.1254287e-02
  4.0219516e-01  3.7297902e-01 -1.1927608e-02 -1.4941680e-03
 -1.7470114e-02 -1.5852105e-02] 4 2
train:	 Loss = 1.2568,	 Acc = 0.5119
48796 0.287
95397 0.571
49906 0.606
5743 0.625
902 0.501
88 0.545
0 0.0
0 0.0
0.5840985029861349
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3111,	 Acc = 0.5097
5486 0.363
29114 0.544
14300 0.494
1295 0.522
29 0.69
0 0.0
0 0.0
0 0.0
0.5277169296794672
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3812,	 Acc1 = 0.2571,	 Acc2 = 0.2537

 ===== Epoch 183	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
  1.6815777   2.7473724  -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.00560396 -0.06429718  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 5 3
train:	 Loss = 1.2570,	 Acc = 0.5131
48806 0.288
95389 0.573
49908 0.606
5741 0.624
900 0.52
88 0.523
0 0.0
0 0.0
0.5852880428347783
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.2953,	 Acc = 0.4973
5486 0.371
29114 0.524
14300 0.498
1295 0.434
29 0.0
0 0.0
0 0.0
0 0.0
0.5127855514327865
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3746,	 Acc1 = 0.2393,	 Acc2 = 0.2324

 ===== Epoch 184	 =====
[ 3.222642    2.5920198  -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-3.2438054e+00 -2.7736864e+00  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.7470114e-02 -1.5852105e-02] 6 1
train:	 Loss = 1.2566,	 Acc = 0.5124
48803 0.288
95394 0.572
49905 0.606
5740 0.624
902 0.524
88 0.523
0 0.0
0 0.0
0.5845134809806024
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3637,	 Acc = 0.4946
5486 0.406
29114 0.543
14300 0.437
1295 0.433
29 0.0
0 0.0
0 0.0
0 0.0
0.5054539764853145
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.4167,	 Acc1 = 0.2793,	 Acc2 = 0.2806

 ===== Epoch 185	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143  2.0378122   1.8165095
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207 -0.14640662 -0.0272951
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 3 0
train:	 Loss = 1.2585,	 Acc = 0.5117
48801 0.289
95395 0.57
49903 0.604
5743 0.624
902 0.519
88 0.557
0 0.0
0 0.0
0.5831179167406647
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.2764,	 Acc = 0.5248
5486 0.37
29114 0.563
14300 0.51
1295 0.487
29 0.0
0 0.0
0 0.0
0 0.0
0.5438106307836739
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3545,	 Acc1 = 0.2474,	 Acc2 = 0.2420

 ===== Epoch 186	 =====
[-0.36866695 -0.38685238  2.4360476   1.7530233  -0.43314552 -0.35780522
  0.9425499   3.725501   -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.124615   -0.08962953  0.01022447  0.00647582
  0.5109536  -0.50337446  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 2 2
train:	 Loss = 1.2583,	 Acc = 0.5115
48806 0.285
95391 0.572
49903 0.605
5742 0.624
902 0.52
88 0.523
0 0.0
0 0.0
0.5841303461250049
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3009,	 Acc = 0.5003
5486 0.366
29114 0.524
14300 0.503
1295 0.521
29 0.0
0 0.0
0 0.0
0 0.0
0.5168089767088381
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3694,	 Acc1 = 0.2546,	 Acc2 = 0.2507

 ===== Epoch 187	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 6 1
train:	 Loss = 1.2567,	 Acc = 0.5125
48803 0.287
95388 0.573
49908 0.605
5743 0.621
902 0.518
88 0.545
0 0.0
0 0.0
0.5847831663695743
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3090,	 Acc = 0.5007
5486 0.366
29114 0.544
14300 0.467
1295 0.49
29 0.0
0 0.0
0 0.0
0 0.0
0.5172336715990881
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3714,	 Acc1 = 0.2754,	 Acc2 = 0.2758

 ===== Epoch 188	 =====
[ 3.1129842   1.8280747  -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512  1.9578962   3.2531943  -0.4264405  -0.4242137
  2.8002448   1.9795191 ] [-0.15262964 -0.03023826  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.15301724  0.04163946 -0.01192761 -0.00149417
  0.03389237 -0.02715144] 0 0
train:	 Loss = 1.2569,	 Acc = 0.5127
48806 0.287
95391 0.572
49907 0.606
5738 0.629
902 0.522
88 0.545
0 0.0
0 0.0
0.5851696420349151
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3390,	 Acc = 0.5105
5486 0.392
29114 0.553
14300 0.474
1295 0.459
29 0.0
0 0.0
0 0.0
0 0.0
0.5249452367115204
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.4016,	 Acc1 = 0.2767,	 Acc2 = 0.2773

 ===== Epoch 189	 =====
[ 1.9422733   0.9615871  -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.7934922   1.1295153 ] [-0.49727783  0.09758419  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.38141096  0.14798827] 1 1
train:	 Loss = 1.2581,	 Acc = 0.5113
48806 0.285
95380 0.572
49913 0.604
5743 0.625
902 0.502
88 0.523
0 0.0
0 0.0
0.5838935445252785
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.2865,	 Acc = 0.5116
5486 0.372
29114 0.533
14300 0.523
1295 0.508
29 0.0
0 0.0
0 0.0
0 0.0
0.5287004336358353
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3508,	 Acc1 = 0.2558,	 Acc2 = 0.2522

 ===== Epoch 190	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512  1.3909569   2.8464007  -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [ 1.8773876e+00  3.5227525e+00  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
 -2.0608208e+00 -3.4873896e+00 -1.1927608e-02 -1.4941680e-03
 -1.7470114e-02 -1.5852105e-02] 6 5
train:	 Loss = 1.2563,	 Acc = 0.5133
48801 0.286
95392 0.572
49905 0.609
5744 0.635
902 0.501
88 0.614
0 0.0
0 0.0
0.5862817451703929
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3045,	 Acc = 0.4944
5486 0.249
29114 0.542
14300 0.498
1295 0.424
29 0.0
0 0.0
0 0.0
0 0.0
0.5244981894586258
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 0
Testing:	 Loss = 1.3813,	 Acc1 = 0.2133,	 Acc2 = 0.2333

 ===== Epoch 191	 =====
[-0.36866695 -0.38685238  0.95110244  1.3785532  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.01783955 -0.19996402  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 5 3
train:	 Loss = 1.2583,	 Acc = 0.5110
48799 0.287
95389 0.571
49910 0.603
5744 0.621
902 0.498
88 0.557
0 0.0
0 0.0
0.5830642031664178
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3270,	 Acc = 0.5099
5486 0.402
29114 0.547
14300 0.481
1295 0.469
29 0.0
0 0.0
0 0.0
0 0.0
0.5231570476999419
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3892,	 Acc1 = 0.2628,	 Acc2 = 0.2607

 ===== Epoch 192	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
  1.3278247   3.147776   -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.8691238e+00 -3.6299586e+00
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
  2.8970342e+00  3.8541710e+00] 5 6
train:	 Loss = 1.2571,	 Acc = 0.5125
48802 0.286
95391 0.572
49907 0.606
5742 0.632
902 0.509
88 0.58
0 0.0
0 0.0
0.5851016246793396
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.2987,	 Acc = 0.5250
5486 0.38
29114 0.561
14300 0.509
1295 0.514
29 0.0
0 0.0
0 0.0
0 0.0
0.5428047744646609
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3697,	 Acc1 = 0.2449,	 Acc2 = 0.2391

 ===== Epoch 193	 =====
[ 2.4411483   2.5510025  -0.41997173 -0.34217143 -0.43314552 -0.35780522
  1.3669368   1.9236852  -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [ 1.6152999e+00  5.4496276e-01  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.0314064e-01  1.0466568e+00
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.7470114e-02 -1.5852105e-02] 4 4
train:	 Loss = 1.2575,	 Acc = 0.5119
48801 0.286
95388 0.571
49915 0.607
5738 0.621
902 0.52
88 0.545
0 0.0
0 0.0
0.584492636370214
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3221,	 Acc = 0.5022
5486 0.393
29114 0.538
14300 0.479
1295 0.438
29 0.0
0 0.0
0 0.0
0 0.0
0.5156243014886673
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3931,	 Acc1 = 0.2498,	 Acc2 = 0.2450

 ===== Epoch 194	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 6 1
train:	 Loss = 1.2584,	 Acc = 0.5121
48798 0.289
95392 0.571
49909 0.606
5743 0.622
902 0.511
88 0.534
0 0.0
0 0.0
0.5838365102542852
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3002,	 Acc = 0.5202
5486 0.371
29114 0.569
14300 0.481
1295 0.509
29 0.0
0 0.0
0 0.0
0 0.0
0.5385354731995172
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3584,	 Acc1 = 0.2694,	 Acc2 = 0.2686

 ===== Epoch 195	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
  1.2700337   1.7663838  -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [ 3.0020404e+00  3.3168163e+00  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.8055969e+00 -2.2066407e+00
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
  2.8583908e+00  4.1168809e+00] 5 6
train:	 Loss = 1.2573,	 Acc = 0.5118
48806 0.286
95385 0.571
49908 0.606
5743 0.626
902 0.509
88 0.523
0 0.0
0 0.0
0.5842290134582242
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3314,	 Acc = 0.4809
5486 0.41
29114 0.522
14300 0.438
1295 0.333
29 0.0
0 0.0
0 0.0
0 0.0
0.48956144664491036
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.4074,	 Acc1 = 0.2513,	 Acc2 = 0.2468

 ===== Epoch 196	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096  1.137519    1.8417515
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.02191087  0.05829916
 -0.01747011 -0.0158521 ] 2 0
train:	 Loss = 1.2563,	 Acc = 0.5126
48804 0.288
95383 0.572
49912 0.606
5743 0.627
902 0.518
88 0.58
0 0.0
0 0.0
0.5848001683900335
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3253,	 Acc = 0.4971
5486 0.407
29114 0.537
14300 0.462
1295 0.394
29 0.0
0 0.0
0 0.0
0 0.0
0.5082033170906165
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3916,	 Acc1 = 0.2418,	 Acc2 = 0.2353

 ===== Epoch 197	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03  2.0055025e+00  2.2813776e+00
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.7470114e-02 -1.5852105e-02] 4 6
train:	 Loss = 1.2574,	 Acc = 0.5127
48799 0.287
95392 0.572
49908 0.607
5743 0.625
902 0.522
88 0.58
0 0.0
0 0.0
0.5850243039340143
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3706,	 Acc = 0.4833
5486 0.363
29114 0.524
14300 0.446
1295 0.492
29 0.0
0 0.0
0 0.0
0 0.0
0.49803299208726365
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.4289,	 Acc1 = 0.2758,	 Acc2 = 0.2763

 ===== Epoch 198	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 3 1
train:	 Loss = 1.2570,	 Acc = 0.5131
48798 0.288
95396 0.572
49906 0.607
5742 0.625
902 0.538
88 0.534
0 0.0
0 0.0
0.5854216819921859
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3176,	 Acc = 0.5064
5486 0.38
29114 0.554
14300 0.463
1295 0.469
29 0.0
0 0.0
0 0.0
0 0.0
0.5219500201171264
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3809,	 Acc1 = 0.2697,	 Acc2 = 0.2689

 ===== Epoch 199	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096  1.8381524   1.2363409
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946  0.25946105  0.20778248
 -0.01747011 -0.0158521 ] 2 2
train:	 Loss = 1.2563,	 Acc = 0.5123
48802 0.286
95387 0.572
49910 0.606
5743 0.626
902 0.525
88 0.5
0 0.0
0 0.0
0.5848911399065974
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3513,	 Acc = 0.4934
5486 0.393
29114 0.523
14300 0.472
1295 0.49
29 0.0
0 0.0
0 0.0
0 0.0
0.505744557199696
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.4144,	 Acc1 = 0.2796,	 Acc2 = 0.2808

 ===== Epoch 200	 =====
[ 1.4554793   2.289518   -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.6851403   2.47492   ] [ 0.09130781  0.09758419  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
  0.03438081  0.16211244] 1 4
train:	 Loss = 1.2566,	 Acc = 0.5123
48800 0.288
95395 0.571
49904 0.607
5743 0.619
902 0.509
88 0.511
0 0.0
0 0.0
0.584350663018312
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3677,	 Acc = 0.5078
5486 0.389
29114 0.547
14300 0.474
1295 0.507
29 0.966
0 0.0
0 0.0
0 0.0
0.5223747150073763
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.4285,	 Acc1 = 0.2352,	 Acc2 = 0.2274

 ===== Epoch 201	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096  3.0940604   1.594645
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.40267485 -0.00149417
 -0.01747011 -0.0158521 ] 0 1
train:	 Loss = 1.2587,	 Acc = 0.5118
48804 0.288
95389 0.57
49909 0.607
5741 0.614
901 0.518
88 0.568
0 0.0
0 0.0
0.5835635540821428
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3380,	 Acc = 0.4802
5486 0.372
29114 0.497
14300 0.485
1295 0.513
29 0.0
0 0.0
0 0.0
0 0.0
0.4934954624703831
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.4026,	 Acc1 = 0.2560,	 Acc2 = 0.2525

 ===== Epoch 202	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096  1.0894525   1.2165724
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946  0.701491    0.5502352
 -0.01747011 -0.0158521 ] 2 2
train:	 Loss = 1.2574,	 Acc = 0.5127
48805 0.288
95395 0.572
49902 0.606
5741 0.621
901 0.532
88 0.523
0 0.0
0 0.0
0.5848500595288995
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3639,	 Acc = 0.4900
5486 0.389
29114 0.542
14300 0.426
1295 0.462
29 0.0
0 0.0
0 0.0
0 0.0
0.5024811122535652
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.4191,	 Acc1 = 0.2804,	 Acc2 = 0.2818

 ===== Epoch 203	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096  1.2240413   1.2215146
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01737379  0.9986851
 -0.01747011 -0.0158521 ] 6 4
train:	 Loss = 1.2570,	 Acc = 0.5112
48800 0.287
95384 0.571
49917 0.604
5741 0.621
902 0.523
88 0.557
0 0.0
0 0.0
0.5832850978741317
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.2984,	 Acc = 0.4961
5486 0.362
29114 0.513
14300 0.516
1295 0.485
29 0.0
0 0.0
0 0.0
0 0.0
0.5125173230810497
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3728,	 Acc1 = 0.2381,	 Acc2 = 0.2309

 ===== Epoch 204	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512  1.8199291   2.0035837  -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.31444725 -0.01358379 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 2 3
train:	 Loss = 1.2582,	 Acc = 0.5118
48802 0.287
95390 0.571
49908 0.605
5742 0.62
902 0.527
88 0.511
0 0.0
0 0.0
0.5838847595869237
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3262,	 Acc = 0.5116
5486 0.381
29114 0.553
14300 0.486
1295 0.439
29 0.0
0 0.0
0 0.0
0 0.0
0.527649872591533
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3857,	 Acc1 = 0.2750,	 Acc2 = 0.2753

 ===== Epoch 205	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 2 1
train:	 Loss = 1.2578,	 Acc = 0.5119
48803 0.288
95388 0.571
49908 0.606
5744 0.623
901 0.511
88 0.557
0 0.0
0 0.0
0.5838622894316216
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.4242,	 Acc = 0.4698
5486 0.4
29114 0.514
14300 0.415
1295 0.39
29 0.0
0 0.0
0 0.0
0 0.0
0.47836291295989986
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.4725,	 Acc1 = 0.2727,	 Acc2 = 0.2726

 ===== Epoch 206	 =====
[-0.36866695 -0.38685238  2.486581    1.872486   -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.21933258  0.19973825  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 0 3
train:	 Loss = 1.2565,	 Acc = 0.5129
48803 0.288
95389 0.572
49908 0.607
5743 0.622
901 0.515
88 0.557
0 0.0
0 0.0
0.5850396963737182
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3453,	 Acc = 0.4900
5486 0.407
29114 0.521
14300 0.465
1295 0.435
29 0.0
0 0.0
0 0.0
0 0.0
0.5001341141758684
0.5474540658947651
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.4064,	 Acc1 = 0.2628,	 Acc2 = 0.2607

 ===== Epoch 207	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.7968860e+00  2.1164560e+00 -1.1927608e-02 -1.4941680e-03
  4.1869698e+00  1.7270706e+00] 2 6
train:	 Loss = 1.2574,	 Acc = 0.5113
48799 0.287
95394 0.57
49908 0.606
5741 0.621
902 0.518
88 0.534
0 0.0
0 0.0
0.583386501614781
0.5474540658947651
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.2763,	 Acc = 0.5326
5486 0.406
29114 0.566
14300 0.521
1295 0.458
29 0.0
0 0.0
0 0.0
0 0.0
0.5480575796861729
0.5480575796861729
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3427,	 Acc1 = 0.2573,	 Acc2 = 0.2540

 ===== Epoch 208	 =====
[-0.36866695 -0.38685238  2.5089478   2.164251   -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.45478594  0.36419907  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 2 2
train:	 Loss = 1.2573,	 Acc = 0.5123
48803 0.287
95387 0.572
49911 0.606
5743 0.621
901 0.504
87 0.563
0 0.0
0 0.0
0.5846055686743976
0.5480575796861729
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3002,	 Acc = 0.5174
5486 0.4
29114 0.553
14300 0.49
1295 0.523
29 0.0
0 0.0
0 0.0
0 0.0
0.5318521167687424
0.5480575796861729
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3605,	 Acc1 = 0.2705,	 Acc2 = 0.2699

 ===== Epoch 209	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
  2.9045336   1.7349236  -0.41367632 -0.42256096  2.4730272   2.9932668
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.02742226  0.13314027  0.0244112   0.04163946 -0.13491419  0.24311492
 -0.01747011 -0.0158521 ] 3 3
train:	 Loss = 1.2579,	 Acc = 0.5121
48803 0.287
95395 0.571
49901 0.605
5743 0.626
902 0.522
88 0.523
0 0.0
0 0.0
0.58430957251577
0.5480575796861729
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3790,	 Acc = 0.4825
5486 0.397
29114 0.53
14300 0.435
1295 0.32
29 0.0
0 0.0
0 0.0
0 0.0
0.49300371049219904
0.5480575796861729
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.4550,	 Acc1 = 0.2335,	 Acc2 = 0.2254

 ===== Epoch 210	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512  1.5608075   3.0485797  -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [ 1.9635650e+00  2.4575653e+00  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
 -2.2570810e+00 -3.7056527e+00 -1.1927608e-02 -1.4941680e-03
  2.0101733e+00  3.4106722e+00] 6 6
train:	 Loss = 1.2575,	 Acc = 0.5123
48804 0.287
95388 0.572
49911 0.606
5739 0.624
902 0.51
88 0.58
0 0.0
0 0.0
0.5847672797116321
0.5480575796861729
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3998,	 Acc = 0.4693
5486 0.363
29114 0.504
14300 0.438
1295 0.493
29 0.0
0 0.0
0 0.0
0 0.0
0.48238633823595156
0.5480575796861729
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.4490,	 Acc1 = 0.2901,	 Acc2 = 0.2935

 ===== Epoch 211	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-1.7616821e-02 -2.3137011e-02  2.6833546e+00  3.1496241e+00
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.7470114e-02 -1.5852105e-02] 2 5
train:	 Loss = 1.2566,	 Acc = 0.5125
48800 0.287
95391 0.572
49909 0.607
5743 0.619
901 0.506
88 0.523
0 0.0
0 0.0
0.5847650494632709
0.5480575796861729
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.2979,	 Acc = 0.5213
5486 0.395
29114 0.561
14300 0.493
1295 0.486
29 0.0
0 0.0
0 0.0
0 0.0
0.5367696365505834
0.5480575796861729
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3655,	 Acc1 = 0.2554,	 Acc2 = 0.2517

 ===== Epoch 212	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096  1.8012363   1.2931753
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
  1.7738543   2.720455    0.0244112   0.04163946 -2.6410012  -1.8904197
 -0.01747011 -0.0158521 ] 6 5
train:	 Loss = 1.2576,	 Acc = 0.5113
48797 0.285
95390 0.571
49912 0.606
5744 0.628
901 0.531
88 0.557
0 0.0
0 0.0
0.5839510638997599
0.5480575796861729
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3729,	 Acc = 0.4951
5486 0.393
29114 0.527
14300 0.47
1295 0.486
29 0.0
0 0.0
0 0.0
0 0.0
0.5075998032992087
0.5480575796861729
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.4295,	 Acc1 = 0.2595,	 Acc2 = 0.2567

 ===== Epoch 213	 =====
[-0.36866695 -0.38685238  3.9185066   2.1872249  -0.43314552 -0.35780522
  3.7112756   3.6311202  -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-1.7616821e-02 -2.3137011e-02 -5.1315770e+00 -2.2630103e+00
  1.0224475e-02  6.4758179e-03 -4.4891348e+00 -4.1279721e+00
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.7470114e-02 -1.5852105e-02] 3 1
train:	 Loss = 1.2564,	 Acc = 0.5129
48807 0.287
95391 0.572
49904 0.607
5740 0.626
902 0.513
88 0.545
0 0.0
0 0.0
0.5853576714356191
0.5480575796861729
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.4625,	 Acc = 0.4587
5486 0.353
29114 0.5
14300 0.416
1295 0.472
29 0.0
0 0.0
0 0.0
0 0.0
0.47167955652912513
0.5480575796861729
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.5078,	 Acc1 = 0.2812,	 Acc2 = 0.2828

 ===== Epoch 214	 =====
[ 2.1333847   2.2126107  -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  2.0588884   2.3523736 ] [ 0.0867721  -0.00893452  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.03997125  0.06324326] 3 0
train:	 Loss = 1.2581,	 Acc = 0.5121
48805 0.287
95392 0.57
49904 0.607
5742 0.629
901 0.512
88 0.557
0 0.0
0 0.0
0.5842514816447079
0.5480575796861729
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3906,	 Acc = 0.4859
5486 0.381
29114 0.532
14300 0.44
1295 0.414
29 0.0
0 0.0
0 0.0
0 0.0
0.4988153247798292
0.5480575796861729
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.4529,	 Acc1 = 0.2738,	 Acc2 = 0.2739

 ===== Epoch 215	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
  2.7224033   1.8178642  -0.41367632 -0.42256096  2.8383403   2.7634583
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -3.402116   -2.2596836   0.0244112   0.04163946 -0.09906226 -1.2924865
 -0.01747011 -0.0158521 ] 5 5
train:	 Loss = 1.2573,	 Acc = 0.5109
48801 0.286
95387 0.57
49912 0.605
5742 0.626
902 0.501
88 0.523
0 0.0
0 0.0
0.5831902704053779
0.5480575796861729
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.2982,	 Acc = 0.5117
5486 0.405
29114 0.538
14300 0.507
1295 0.447
29 0.0
0 0.0
0 0.0
0 0.0
0.5248334748982968
0.5480575796861729
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 0
Testing:	 Loss = 1.3791,	 Acc1 = 0.2509,	 Acc2 = 0.2463

 ===== Epoch 216	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096  2.443418    1.8689332
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
  4.1654234   1.9041836   0.0244112   0.04163946  1.1539589   0.20234673
 -0.01747011 -0.0158521 ] 2 2
train:	 Loss = 1.2575,	 Acc = 0.5125
48802 0.288
95387 0.572
49912 0.605
5741 0.627
902 0.53
88 0.568
0 0.0
0 0.0
0.5845819903966323
0.5480575796861729
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3092,	 Acc = 0.4952
5486 0.367
29114 0.519
14300 0.496
1295 0.504
29 0.0
0 0.0
0 0.0
0 0.0
0.510907952970629
0.5480575796861729
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3761,	 Acc1 = 0.2595,	 Acc2 = 0.2567

 ===== Epoch 217	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143  1.9305487   1.8838258
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.33924365  0.54047835
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 2
train:	 Loss = 1.2581,	 Acc = 0.5117
48803 0.284
95391 0.571
49907 0.608
5741 0.624
902 0.539
88 0.534
0 0.0
0 0.0
0.584677923290951
0.5480575796861729
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3221,	 Acc = 0.5056
5486 0.394
29114 0.544
14300 0.477
1295 0.456
29 0.0
0 0.0
0 0.0
0 0.0
0.5192677365997586
0.5480575796861729
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3905,	 Acc1 = 0.2622,	 Acc2 = 0.2599

 ===== Epoch 218	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 2 1
train:	 Loss = 1.2573,	 Acc = 0.5121
48804 0.287
95395 0.571
49905 0.606
5738 0.627
902 0.527
88 0.545
0 0.0
0 0.0
0.5841950167074487
0.5480575796861729
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3103,	 Acc = 0.5192
5486 0.403
29114 0.561
14300 0.484
1295 0.481
29 0.0
0 0.0
0 0.0
0 0.0
0.5334614868791632
0.5480575796861729
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3721,	 Acc1 = 0.2655,	 Acc2 = 0.2639

 ===== Epoch 219	 =====
[-0.36866695 -0.38685238  2.4749825   3.1222515   2.1303601   0.8628627
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-1.7616821e-02 -2.3137011e-02 -6.2714745e-03 -6.6056089e+00
  2.2177033e-01  5.2910820e-02 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.7470114e-02 -1.5852105e-02] 2 2
train:	 Loss = 1.2567,	 Acc = 0.5126
48800 0.287
95391 0.572
49909 0.606
5742 0.625
902 0.518
88 0.534
0 0.0
0 0.0
0.584916333403494
0.5480575796861729
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3822,	 Acc = 0.4823
5486 0.398
29114 0.512
14300 0.455
1295 0.485
29 0.0
0 0.0
0 0.0
0 0.0
0.49266842505252806
0.5480575796861729
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.4366,	 Acc1 = 0.2763,	 Acc2 = 0.2768

 ===== Epoch 220	 =====
[-0.36866695 -0.38685238  3.3692632   2.1665485  -0.43314552 -0.35780522
  2.369235    3.8885226  -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-1.7616821e-02 -2.3137011e-02 -4.4791098e+00 -2.2442744e+00
  4.1351857e+00 -3.5225847e+00 -3.0138955e+00 -4.3931866e+00
  2.9067380e+00  2.9605832e+00 -1.1927608e-02 -1.4941680e-03
 -1.7470114e-02 -1.5852105e-02] 6 5
train:	 Loss = 1.2572,	 Acc = 0.5130
48806 0.287
95389 0.572
49906 0.607
5741 0.626
902 0.496
88 0.58
0 0.0
0 0.0
0.5853932879902122
0.5480575796861729
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3029,	 Acc = 0.5180
5486 0.4
29114 0.562
14300 0.477
1295 0.493
29 0.0
0 0.0
0 0.0
0 0.0
0.5325003352854397
0.5480575796861729
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3607,	 Acc1 = 0.2577,	 Acc2 = 0.2545

 ===== Epoch 221	 =====
[ 2.0512593   1.0590029  -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  2.0093186   1.2364177 ] [-0.31065708  0.10231835  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
  0.12194322  0.10561576] 2 3
train:	 Loss = 1.2574,	 Acc = 0.5118
48801 0.286
95391 0.572
49910 0.606
5740 0.619
902 0.53
88 0.58
0 0.0
0 0.0
0.5844202827055008
0.5480575796861729
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.2924,	 Acc = 0.5196
5486 0.402
29114 0.561
14300 0.484
1295 0.465
29 0.552
0 0.0
0 0.0
0 0.0
0.5339532388573472
0.5480575796861729
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3580,	 Acc1 = 0.2492,	 Acc2 = 0.2443

 ===== Epoch 222	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096  2.2542245   1.4735631
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.3091863   0.4252128
 -0.01747011 -0.0158521 ] 1 1
train:	 Loss = 1.2568,	 Acc = 0.5128
48804 0.289
95390 0.571
49908 0.607
5740 0.623
902 0.51
88 0.511
0 0.0
0 0.0
0.5845699476412239
0.5480575796861729
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3448,	 Acc = 0.4885
5486 0.414
29114 0.517
14300 0.46
1295 0.5
29 0.0
0 0.0
0 0.0
0 0.0
0.4976753542849479
0.5480575796861729
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.4079,	 Acc1 = 0.2707,	 Acc2 = 0.2701

 ===== Epoch 223	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512  1.9474614   1.7697383  -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.587067    0.05478786 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 2 2
train:	 Loss = 1.2569,	 Acc = 0.5123
48798 0.287
95393 0.572
49907 0.605
5744 0.626
902 0.528
88 0.545
0 0.0
0 0.0
0.5847770893352803
0.5480575796861729
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.4001,	 Acc = 0.4690
5486 0.406
29114 0.509
14300 0.424
1295 0.334
29 0.0
0 0.0
0 0.0
0 0.0
0.47670883812418974
0.5480575796861729
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 6
Testing:	 Loss = 1.4529,	 Acc1 = 0.2901,	 Acc2 = 0.2935

 ===== Epoch 224	 =====
[ 2.0373094   2.2331195  -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-2.1789830e+00 -2.4422951e+00  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.7470114e-02 -1.5852105e-02] 3 6
train:	 Loss = 1.2586,	 Acc = 0.5116
48805 0.286
95394 0.571
49900 0.605
5743 0.619
902 0.543
88 0.545
0 0.0
0 0.0
0.5838962815815611
0.5480575796861729
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3243,	 Acc = 0.5103
5486 0.4
29114 0.551
14300 0.477
1295 0.449
29 0.0
0 0.0
0 0.0
0 0.0
0.5238276185792838
0.5480575796861729
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3821,	 Acc1 = 0.2758,	 Acc2 = 0.2763

 ===== Epoch 225	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 6 1
train:	 Loss = 1.2581,	 Acc = 0.5111
48805 0.287
95386 0.571
49908 0.604
5743 0.62
902 0.513
88 0.523
0 0.0
0 0.0
0.5831464147815848
0.5480575796861729
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3426,	 Acc = 0.4862
5486 0.359
29114 0.532
14300 0.45
1295 0.402
29 0.0
0 0.0
0 0.0
0 0.0
0.5018105413742232
0.5480575796861729
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3983,	 Acc1 = 0.2950,	 Acc2 = 0.2995

 ===== Epoch 226	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143  2.9392862   2.3909416
 -0.3580977  -0.36433512  3.1369894   1.4457653  -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
 -4.1486421e+00 -2.5791097e+00 -1.5871393e-02 -1.1254287e-02
 -1.5845001e-01  1.4117023e+00 -1.1927608e-02 -1.4941680e-03
  2.1980138e+00  1.3852657e+00] 4 6
train:	 Loss = 1.2588,	 Acc = 0.5105
48801 0.286
95402 0.569
49898 0.606
5741 0.615
902 0.52
88 0.557
0 0.0
0 0.0
0.5825982858759069
0.5480575796861729
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3354,	 Acc = 0.4901
5486 0.37
29114 0.522
14300 0.47
1295 0.5
29 0.0
0 0.0
0 0.0
0 0.0
0.5048057579686173
0.5480575796861729
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3930,	 Acc1 = 0.2936,	 Acc2 = 0.2977

 ===== Epoch 227	 =====
[ 1.6358048   2.5638204  -0.41997173 -0.34217143 -0.43314552 -0.35780522
  1.3622667   2.5099902  -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [ 1.9946188e+00  2.0173581e-01  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -6.2466627e-03  4.2782301e-01
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.7470114e-02 -1.5852105e-02] 4 2
train:	 Loss = 1.2576,	 Acc = 0.5125
48804 0.286
95389 0.573
49905 0.605
5744 0.624
902 0.511
88 0.58
0 0.0
0 0.0
0.5851619438524482
0.5480575796861729
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3070,	 Acc = 0.5263
5486 0.41
29114 0.561
14300 0.507
1295 0.462
29 0.0
0 0.0
0 0.0
0 0.0
0.5406142429254772
0.5480575796861729
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3791,	 Acc1 = 0.2432,	 Acc2 = 0.2371

 ===== Epoch 228	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096  1.5755106   1.6070004
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946  0.14782137 -0.0096478
 -0.01747011 -0.0158521 ] 3 3
train:	 Loss = 1.2574,	 Acc = 0.5116
48798 0.288
95400 0.57
49902 0.606
5742 0.621
902 0.52
88 0.545
0 0.0
0 0.0
0.5834681715931963
0.5480575796861729
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3075,	 Acc = 0.5096
5486 0.373
29114 0.547
14300 0.489
1295 0.486
29 0.069
0 0.0
0 0.0
0 0.0
0.5263310831954937
0.5480575796861729
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3741,	 Acc1 = 0.2690,	 Acc2 = 0.2681

 ===== Epoch 229	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 1 1
train:	 Loss = 1.2567,	 Acc = 0.5118
48805 0.288
95394 0.57
49904 0.607
5740 0.626
901 0.503
88 0.489
0 0.0
0 0.0
0.583850237128931
0.5480575796861729
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3080,	 Acc = 0.4971
5486 0.411
29114 0.527
14300 0.475
1295 0.455
29 0.0
0 0.0
0 0.0
0 0.0
0.5076445080244982
0.5480575796861729
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3694,	 Acc1 = 0.2583,	 Acc2 = 0.2552

 ===== Epoch 230	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 6 1
train:	 Loss = 1.2569,	 Acc = 0.5121
48793 0.287
95398 0.571
49908 0.608
5744 0.618
901 0.514
88 0.511
0 0.0
0 0.0
0.5843369135550747
0.5480575796861729
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.2682,	 Acc = 0.5091
5486 0.369
29114 0.535
14300 0.514
1295 0.489
29 0.0
0 0.0
0 0.0
0 0.0
0.5263757879207832
0.5480575796861729
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3411,	 Acc1 = 0.2494,	 Acc2 = 0.2445

 ===== Epoch 231	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512  1.3526974   2.9389648  -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.44975182 -0.10562256 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 2 2
train:	 Loss = 1.2578,	 Acc = 0.5116
48805 0.286
95386 0.571
49910 0.605
5741 0.627
902 0.512
88 0.511
0 0.0
0 0.0
0.5839949482657686
0.5480575796861729
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3035,	 Acc = 0.4810
5486 0.371
29114 0.498
14300 0.488
1295 0.504
29 0.0
0 0.0
0 0.0
0 0.0
0.4944342617014618
0.5480575796861729
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3690,	 Acc1 = 0.2606,	 Acc2 = 0.2580

 ===== Epoch 232	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [ 2.5346930e+00  2.9783235e+00  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03  2.3500903e+00  2.4788151e+00
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.7470114e-02 -1.5852105e-02] 6 5
train:	 Loss = 1.2564,	 Acc = 0.5120
48799 0.285
95397 0.572
49904 0.606
5742 0.627
902 0.518
88 0.523
0 0.0
0 0.0
0.5849848388178882
0.5480575796861729
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.2710,	 Acc = 0.5217
5486 0.404
29114 0.549
14300 0.52
1295 0.441
29 0.0
0 0.0
0 0.0
0 0.0
0.5361214180338861
0.5480575796861729
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 1
Testing:	 Loss = 1.3476,	 Acc1 = 0.2453,	 Acc2 = 0.2396

 ===== Epoch 233	 =====
[ 0.5649645   2.0100884  -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-8.5632801e-01 -2.2363591e+00  2.0407274e+00  1.3509640e+00
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.7470114e-02 -1.5852105e-02] 5 6
train:	 Loss = 1.2580,	 Acc = 0.5126
48796 0.286
95394 0.572
49912 0.607
5740 0.631
902 0.539
88 0.58
0 0.0
0 0.0
0.5852824331079481
0.5480575796861729
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3918,	 Acc = 0.4767
5486 0.384
29114 0.509
14300 0.451
1295 0.44
29 0.0
0 0.0
0 0.0
0 0.0
0.48804148598506863
0.5480575796861729
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 0
Testing:	 Loss = 1.4512,	 Acc1 = 0.2756,	 Acc2 = 0.2761

 ===== Epoch 234	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512  2.993227    1.9914042  -0.4264405  -0.4242137
  3.6898713   0.8948516 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
  1.4267014   3.2096283  -3.9122245  -2.5643723   3.138596    3.175706
 -4.575577   -1.4084954 ] 6 5
train:	 Loss = 1.2583,	 Acc = 0.5110
48796 0.286
95388 0.57
49916 0.607
5742 0.622
902 0.517
88 0.568
0 0.0
0 0.0
0.5833881449130469
0.5480575796861729
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3841,	 Acc = 0.4742
5486 0.366
29114 0.506
14300 0.448
1295 0.51
29 0.0
0 0.0
0 0.0
0 0.0
0.4873932674683714
0.5480575796861729
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.4457,	 Acc1 = 0.2564,	 Acc2 = 0.2530

 ===== Epoch 235	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 1
train:	 Loss = 1.2573,	 Acc = 0.5133
48799 0.288
95395 0.574
49906 0.606
5743 0.622
902 0.509
87 0.529
0 0.0
0 0.0
0.5856491682726777
0.5480575796861729
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3390,	 Acc = 0.4923
5486 0.366
29114 0.524
14300 0.474
1295 0.532
29 0.0
0 0.0
0 0.0
0 0.0
0.5077786222003666
0.5480575796861729
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3953,	 Acc1 = 0.2668,	 Acc2 = 0.2654

 ===== Epoch 236	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 1
train:	 Loss = 1.2572,	 Acc = 0.5121
48799 0.286
95393 0.572
49906 0.605
5744 0.626
902 0.516
88 0.534
0 0.0
0 0.0
0.5846559628501707
0.5480575796861729
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3288,	 Acc = 0.5122
5486 0.41
29114 0.557
14300 0.465
1295 0.47
29 0.069
0 0.0
0 0.0
0 0.0
0.5246993607224284
0.5480575796861729
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3952,	 Acc1 = 0.2501,	 Acc2 = 0.2453

 ===== Epoch 237	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 0 1
train:	 Loss = 1.2563,	 Acc = 0.5118
48807 0.286
95386 0.571
49906 0.605
5743 0.631
902 0.519
88 0.511
0 0.0
0 0.0
0.5841012991284328
0.5480575796861729
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3539,	 Acc = 0.4828
5486 0.374
29114 0.519
14300 0.455
1295 0.449
29 0.0
0 0.0
0 0.0
0 0.0
0.49613304126246144
0.5480575796861729
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.4219,	 Acc1 = 0.2610,	 Acc2 = 0.2584

 ===== Epoch 238	 =====
[-0.36866695 -0.38685238  1.8188735   3.133738   -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-1.7616821e-02 -2.3137011e-02 -2.6373427e+00 -3.1207049e+00
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.7470114e-02 -1.5852105e-02] 1 5
train:	 Loss = 1.2564,	 Acc = 0.5130
48795 0.286
95392 0.572
49912 0.607
5744 0.629
901 0.528
88 0.489
0 0.0
0 0.0
0.585706111012451
0.5480575796861729
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3614,	 Acc = 0.4988
5486 0.399
29114 0.545
14300 0.45
1295 0.436
29 0.0
0 0.0
0 0.0
0 0.0
0.5110197147838527
0.5480575796861729
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.4204,	 Acc1 = 0.2678,	 Acc2 = 0.2667

 ===== Epoch 239	 =====
[ 3.3327167   0.86160773 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512  2.1097765   2.8317857  -0.4264405  -0.4242137
  2.5313387   0.9939319 ] [-3.3426893  -1.1759061   0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429 -2.8914082  -3.4716115   2.2368367   4.048145
 -3.2836864  -1.5158392 ] 6 6
train:	 Loss = 1.2570,	 Acc = 0.5130
48798 0.287
95388 0.573
49913 0.606
5744 0.62
901 0.503
88 0.636
0 0.0
0 0.0
0.5854611468487312
0.5480575796861729
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3081,	 Acc = 0.5126
5486 0.407
29114 0.549
14300 0.483
1295 0.467
29 0.0
0 0.0
0 0.0
0 0.0
0.5255487505029282
0.5480575796861729
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3700,	 Acc1 = 0.2567,	 Acc2 = 0.2532

 ===== Epoch 240	 =====
[-0.36866695 -0.38685238  3.3133457   1.112059   -0.43314552 -0.35780522
  2.9109542   2.9161139  -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -4.412683   -1.2887363   0.01022447  0.00647582
 -3.609381   -3.3912654   0.0244112   0.04163946  2.4097464   3.376829
 -0.01747011 -0.0158521 ] 5 6
train:	 Loss = 1.2572,	 Acc = 0.5125
48800 0.286
95389 0.572
49910 0.607
5743 0.625
902 0.52
88 0.534
0 0.0
0 0.0
0.585330719848453
0.5480575796861729
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.2808,	 Acc = 0.5077
5486 0.368
29114 0.533
14300 0.506
1295 0.551
29 0.0
0 0.0
0 0.0
0 0.0
0.524811122535652
0.5480575796861729
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3428,	 Acc1 = 0.2771,	 Acc2 = 0.2778

 ===== Epoch 241	 =====
[-0.36866695 -0.38685238  1.1151303   3.051033   -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.01931502  0.02278672  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 0 0
train:	 Loss = 1.2571,	 Acc = 0.5125
48805 0.286
95389 0.572
49904 0.607
5744 0.633
902 0.527
88 0.545
0 0.0
0 0.0
0.585363126286778
0.5480575796861729
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3056,	 Acc = 0.5096
5486 0.386
29114 0.545
14300 0.49
1295 0.476
29 0.0
0 0.0
0 0.0
0 0.0
0.5247887701730073
0.5480575796861729
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3694,	 Acc1 = 0.2736,	 Acc2 = 0.2736

 ===== Epoch 242	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 6 1
train:	 Loss = 1.2582,	 Acc = 0.5121
48798 0.287
95394 0.572
49908 0.606
5742 0.617
902 0.517
88 0.58
0 0.0
0 0.0
0.5844613704829182
0.5480575796861729
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3119,	 Acc = 0.4991
5486 0.357
29114 0.538
14300 0.477
1295 0.466
29 0.621
0 0.0
0 0.0
0 0.0
0.5165854530823908
0.5480575796861729
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3894,	 Acc1 = 0.2496,	 Acc2 = 0.2448

 ===== Epoch 243	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
  3.0177805   2.255448   -0.41367632 -0.42256096  3.4213042   3.027862
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
  1.037138   -0.1733298   0.0244112   0.04163946  0.8222105  -0.31133235
 -0.01747011 -0.0158521 ] 2 2
train:	 Loss = 1.2575,	 Acc = 0.5125
48799 0.287
95387 0.573
49913 0.606
5743 0.619
902 0.517
88 0.557
0 0.0
0 0.0
0.5849124861049904
0.5480575796861729
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3240,	 Acc = 0.5010
5486 0.406
29114 0.543
14300 0.458
1295 0.432
29 0.69
0 0.0
0 0.0
0 0.0
0.5126514372569181
0.5480575796861729
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3905,	 Acc1 = 0.2503,	 Acc2 = 0.2455

 ===== Epoch 244	 =====
[-0.36866695 -0.38685238  2.752504    1.7024812  -0.43314552 -0.35780522
  2.2110386   3.8570623  -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.35342258  0.22680141  0.01022447  0.00647582
  0.1638008   0.00642668  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 3 2
train:	 Loss = 1.2573,	 Acc = 0.5120
48803 0.287
95389 0.571
49907 0.608
5743 0.62
902 0.531
88 0.489
0 0.0
0 0.0
0.5843029948233561
0.5480575796861729
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.2881,	 Acc = 0.5040
5486 0.359
29114 0.536
14300 0.491
1295 0.553
29 0.0
0 0.0
0 0.0
0 0.0
0.5218382583039027
0.5480575796861729
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3628,	 Acc1 = 0.2560,	 Acc2 = 0.2525

 ===== Epoch 245	 =====
[-0.36866695 -0.38685238  2.2508945   1.1740879  -0.43314552 -0.35780522
  3.1386173   3.5510397  -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-1.7616821e-02 -2.3137011e-02 -6.5810546e-02  1.2427113e+00
  1.0224475e-02  6.4758179e-03 -3.8596399e+00 -4.0454612e+00
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.7470114e-02 -1.5852105e-02] 6 4
train:	 Loss = 1.2562,	 Acc = 0.5131
48806 0.288
95392 0.573
49906 0.606
5741 0.622
899 0.523
88 0.557
0 0.0
0 0.0
0.5852683093681343
0.5480575796861729
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3173,	 Acc = 0.5075
5486 0.353
29114 0.546
14300 0.488
1295 0.518
29 0.0
0 0.0
0 0.0
0 0.0
0.5264651973713621
0.5480575796861729
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3783,	 Acc1 = 0.2597,	 Acc2 = 0.2570

 ===== Epoch 246	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143  1.7826262   2.114945
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.05654478 -1.0277584
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 5 5
train:	 Loss = 1.2571,	 Acc = 0.5130
48804 0.288
95387 0.571
49910 0.608
5741 0.63
902 0.527
88 0.545
0 0.0
0 0.0
0.5852079880022101
0.5480575796861729
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3076,	 Acc = 0.5340
5486 0.405
29114 0.566
14300 0.52
1295 0.513
29 0.0
0 0.0
0 0.0
0 0.0
0.5498234163351067
0.5498234163351067
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3814,	 Acc1 = 0.2340,	 Acc2 = 0.2259

 ===== Epoch 247	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  3.2490206e+00  2.4109802e+00 -1.1927608e-02 -1.4941680e-03
  4.8038096e+00  2.2920372e+00] 6 6
train:	 Loss = 1.2567,	 Acc = 0.5123
48806 0.288
95384 0.571
49910 0.606
5742 0.623
902 0.523
88 0.545
0 0.0
0 0.0
0.5843342586136582
0.5498234163351067
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3472,	 Acc = 0.4974
5486 0.406
29114 0.549
14300 0.433
1295 0.446
29 0.0
0 0.0
0 0.0
0 0.0
0.5085833072555769
0.5498234163351067
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3985,	 Acc1 = 0.2847,	 Acc2 = 0.2870

 ===== Epoch 248	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143  2.9485798   3.0035195
 -0.3580977  -0.36433512  1.8286258   2.2423005  -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.16105549  0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 0 0
train:	 Loss = 1.2581,	 Acc = 0.5132
48801 0.286
95387 0.572
49910 0.609
5744 0.626
902 0.53
88 0.511
0 0.0
0 0.0
0.5859923305115404
0.5498234163351067
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3209,	 Acc = 0.4880
5486 0.351
29114 0.523
14300 0.467
1295 0.511
29 0.0
0 0.0
0 0.0
0 0.0
0.5048281103312621
0.5498234163351067
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3821,	 Acc1 = 0.2602,	 Acc2 = 0.2575

 ===== Epoch 249	 =====
[ 4.50087     2.343353   -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512  1.5567504   3.2044766  -0.4264405  -0.4242137
  3.0950325   2.4931717 ] [-4.3920789e+00 -2.5440798e+00  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  1.2756273e-01  5.7417538e-02 -1.1927608e-02 -1.4941680e-03
  8.8678420e-02  6.3243255e-02] 2 0
train:	 Loss = 1.2559,	 Acc = 0.5118
48801 0.288
95391 0.571
49910 0.606
5740 0.619
902 0.525
88 0.523
0 0.0
0 0.0
0.5837756773289658
0.5498234163351067
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3763,	 Acc = 0.4954
5486 0.354
29114 0.535
14300 0.469
1295 0.49
29 0.69
0 0.0
0 0.0
0 0.0
0.5127184943448523
0.5498234163351067
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.4338,	 Acc1 = 0.2921,	 Acc2 = 0.2960

 ===== Epoch 250	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512  1.5260264   3.1728098  -0.4264405  -0.4242137
  2.8120887   2.341944  ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  1.7398374   0.13893758  4.6906915   1.9635321
  1.4079636   0.7779263 ] 2 2
train:	 Loss = 1.2558,	 Acc = 0.5142
48806 0.288
95386 0.573
49911 0.609
5740 0.619
901 0.521
88 0.511
0 0.0
0 0.0
0.5866496520332048
0.5498234163351067
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.2878,	 Acc = 0.4911
5486 0.36
29114 0.503
14300 0.519
1295 0.477
29 0.0
0 0.0
0 0.0
0 0.0
0.5071304036836694
0.5498234163351067
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3742,	 Acc1 = 0.2399,	 Acc2 = 0.2331

 ===== Epoch 251	 =====
[ 4.50087     2.343353   -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512  1.5567504   3.2044766  -0.4264405  -0.4242137
  3.0950325   2.4931717 ] [-4.392079   -2.5440798   0.02226749  0.02903207  0.01022447  0.00647582
  4.2757945   1.4798404   1.7016566   0.09423304  4.687968    2.0070183
  0.61551213  0.62256044] 2 2
train:	 Loss = 1.2573,	 Acc = 0.5129
48799 0.287
95392 0.571
49908 0.609
5743 0.624
902 0.538
88 0.534
0 0.0
0 0.0
0.5853597574210863
0.5498234163351067
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3578,	 Acc = 0.4996
5486 0.399
29114 0.542
14300 0.461
1295 0.407
29 0.0
0 0.0
0 0.0
0 0.0
0.5120255711028656
0.5498234163351067
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.4255,	 Acc1 = 0.2562,	 Acc2 = 0.2527

 ===== Epoch 252	 =====
[ 0.6867743   3.4585073  -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-9.6575373e-01 -3.5737605e+00  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.7470114e-02 -1.5852105e-02] 6 6
train:	 Loss = 1.2578,	 Acc = 0.5125
48800 0.288
95395 0.571
49905 0.607
5742 0.623
902 0.521
88 0.568
0 0.0
0 0.0
0.5845545674594822
0.5498234163351067
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.2785,	 Acc = 0.5095
5486 0.408
29114 0.533
14300 0.512
1295 0.388
29 0.0
0 0.0
0 0.0
0 0.0
0.5219053153918369
0.5498234163351067
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 6
Testing:	 Loss = 1.3641,	 Acc1 = 0.2251,	 Acc2 = 0.2152

 ===== Epoch 253	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
  2.2612412   1.1915187  -0.41367632 -0.42256096  3.0471468   1.7280825
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  1.4930044   2.6279428
 -2.895183   -1.6143285   0.0244112   0.04163946 -4.111408   -2.3687663
 -0.01747011 -0.0158521 ] 5 6
train:	 Loss = 1.2576,	 Acc = 0.5122
48802 0.287
95388 0.571
49910 0.607
5742 0.625
902 0.512
88 0.534
0 0.0
0 0.0
0.5846148786423733
0.5498234163351067
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3270,	 Acc = 0.4969
5486 0.412
29114 0.51
14300 0.506
1295 0.477
29 0.0
0 0.0
0 0.0
0 0.0
0.5072645178595377
0.5498234163351067
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3949,	 Acc1 = 0.2503,	 Acc2 = 0.2455

 ===== Epoch 254	 =====
[-0.36866695 -0.38685238  1.8549094   1.4336898  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -2.680151   -1.5801859   0.01022447  0.00647582
  3.359465    1.9601735   0.0244112   0.04163946  3.8084464   3.2354991
 -0.01747011 -0.0158521 ] 5 5
train:	 Loss = 1.2571,	 Acc = 0.5127
48801 0.287
95390 0.572
49908 0.607
5743 0.623
902 0.524
88 0.523
0 0.0
0 0.0
0.5851767073820471
0.5498234163351067
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.2787,	 Acc = 0.5020
5486 0.369
29114 0.517
14300 0.525
1295 0.486
29 0.0
0 0.0
0 0.0
0 0.0
0.5183289373686799
0.5498234163351067
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3592,	 Acc1 = 0.2333,	 Acc2 = 0.2251

 ===== Epoch 255	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096  3.1582782   1.4191996
  3.3029625   3.416182  ] [-0.01761682 -0.02313701  1.1801215   1.3530457   0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -4.2425637  -2.0290315
 -4.144131   -4.14011   ] 6 5
train:	 Loss = 1.2570,	 Acc = 0.5115
48802 0.286
95386 0.57
49910 0.607
5744 0.619
902 0.512
88 0.477
0 0.0
0 0.0
0.5839044925343683
0.5498234163351067
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3467,	 Acc = 0.4949
5486 0.37
29114 0.537
14300 0.457
1295 0.502
29 0.0
0 0.0
0 0.0
0 0.0
0.5102597344539318
0.5498234163351067
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.4021,	 Acc1 = 0.2866,	 Acc2 = 0.2893

 ===== Epoch 256	 =====
[-0.36866695 -0.38685238  1.5678616   0.84786236 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.2215504   1.1282133   0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 6 4
train:	 Loss = 1.2567,	 Acc = 0.5123
48805 0.288
95387 0.572
49908 0.604
5744 0.62
900 0.524
88 0.523
0 0.0
0 0.0
0.5842251705289192
0.5498234163351067
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.2852,	 Acc = 0.5153
5486 0.408
29114 0.553
14300 0.484
1295 0.48
29 0.0
0 0.0
0 0.0
0 0.0
0.5283875005588091
0.5498234163351067
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3516,	 Acc1 = 0.2637,	 Acc2 = 0.2617

 ===== Epoch 257	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143  1.7946303   1.0064707
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207 -0.17123637  0.3251888
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
train:	 Loss = 1.2580,	 Acc = 0.5117
48796 0.288
95394 0.57
49911 0.606
5741 0.626
902 0.513
88 0.58
0 0.0
0 0.0
0.5835065379252282
0.5498234163351067
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3511,	 Acc = 0.4972
5486 0.358
29114 0.541
14300 0.465
1295 0.459
29 0.0
0 0.0
0 0.0
0 0.0
0.514238455004694
0.5498234163351067
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.4174,	 Acc1 = 0.2668,	 Acc2 = 0.2654

 ===== Epoch 258	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143  2.1330702   1.4350508
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207 -0.18508463  0.19643721
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 2 4
train:	 Loss = 1.2567,	 Acc = 0.5118
48802 0.285
95386 0.572
49912 0.606
5742 0.628
902 0.523
88 0.557
0 0.0
0 0.0
0.584608300993225
0.5498234163351067
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3554,	 Acc = 0.4961
5486 0.415
29114 0.529
14300 0.466
1295 0.439
29 0.0
0 0.0
0 0.0
0 0.0
0.5060127855514328
0.5498234163351067
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.4349,	 Acc1 = 0.2486,	 Acc2 = 0.2435

 ===== Epoch 259	 =====
[ 1.8002137   2.9765556  -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.21103512 -0.4018702   0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 5 5
train:	 Loss = 1.2563,	 Acc = 0.5127
48803 0.289
95382 0.571
49914 0.607
5744 0.62
902 0.52
87 0.529
0 0.0
0 0.0
0.5846713455985372
0.5498234163351067
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 6
val:	 Loss = 1.3563,	 Acc = 0.4841
5486 0.401
29114 0.525
14300 0.439
1295 0.427
29 0.0
0 0.0
0 0.0
0 0.0
0.4942330904376593
0.5498234163351067
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.4161,	 Acc1 = 0.2585,	 Acc2 = 0.2555

 ===== Epoch 260	 =====
[ 2.2626486   2.2869544  -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.8233217   2.4775276 ] [-1.1770234   0.9355314   0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -1.1983204   1.125381  ] 1 1
train:	 Loss = 1.2556,	 Acc = 0.5127
48803 0.288
95388 0.572
49907 0.605
5744 0.625
902 0.522
88 0.534
0 0.0
0 0.0
0.5848686763709555
0.5498234163351067
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3098,	 Acc = 0.5163
5486 0.411
29114 0.55
14300 0.493
1295 0.46
29 0.0
0 0.0
0 0.0
0 0.0
0.5291921856140194
0.5498234163351067
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3793,	 Acc1 = 0.2416,	 Acc2 = 0.2351

 ===== Epoch 261	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512  1.3440025   2.9146059  -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429 -0.05596731  0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 0 0
train:	 Loss = 1.2570,	 Acc = 0.5127
48796 0.287
95397 0.571
49908 0.609
5741 0.624
902 0.51
88 0.557
0 0.0
0 0.0
0.5849996053566261
0.5498234163351067
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3267,	 Acc = 0.4868
5486 0.352
29114 0.523
14300 0.466
1295 0.478
29 0.0
0 0.0
0 0.0
0 0.0
0.5033081496714202
0.5498234163351067
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3822,	 Acc1 = 0.2985,	 Acc2 = 0.3037

 ===== Epoch 262	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096  1.6266547   2.6226075
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.03053469 -0.12379871
 -0.01747011 -0.0158521 ] 3 3
train:	 Loss = 1.2570,	 Acc = 0.5119
48797 0.287
95393 0.571
49911 0.607
5742 0.619
901 0.515
88 0.568
0 0.0
0 0.0
0.5841089222876311
0.5498234163351067
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.2691,	 Acc = 0.5036
5486 0.372
29114 0.526
14300 0.514
1295 0.453
29 0.0
0 0.0
0 0.0
0 0.0
0.5197147838526532
0.5498234163351067
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3437,	 Acc1 = 0.2468,	 Acc2 = 0.2413

 ===== Epoch 263	 =====
[-0.36866695 -0.38685238  2.9181888   2.4812872  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [ 5.7560363e+00  2.3060720e+00 -3.9432611e+00 -2.5294785e+00
  1.0224475e-02  6.4758179e-03  3.6187057e+00  2.1399300e+00
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.7470114e-02 -1.5852105e-02] 5 6
train:	 Loss = 1.2562,	 Acc = 0.5124
48806 0.288
95389 0.571
49906 0.607
5741 0.626
902 0.537
88 0.511
0 0.0
0 0.0
0.5845776380355991
0.5498234163351067
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.2878,	 Acc = 0.5182
5486 0.398
29114 0.557
14300 0.492
1295 0.452
29 0.0
0 0.0
0 0.0
0 0.0
0.5329026778130448
0.5498234163351067
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3558,	 Acc1 = 0.2534,	 Acc2 = 0.2493

 ===== Epoch 264	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143  0.8621776   2.0207021
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207 -0.3546089   0.3526277
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 1 1
train:	 Loss = 1.2579,	 Acc = 0.5115
48802 0.287
95391 0.57
49908 0.607
5741 0.614
902 0.514
88 0.557
0 0.0
0 0.0
0.5836611195158851
0.5498234163351067
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3651,	 Acc = 0.4940
5486 0.398
29114 0.542
14300 0.443
1295 0.396
29 0.0
0 0.0
0 0.0
0 0.0
0.5056775001117618
0.5498234163351067
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.4288,	 Acc1 = 0.2670,	 Acc2 = 0.2657

 ===== Epoch 265	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946  1.8329284   2.5397224
 -0.01747011 -0.0158521 ] 6 6
train:	 Loss = 1.2583,	 Acc = 0.5116
48805 0.285
95384 0.571
49912 0.607
5742 0.622
902 0.516
87 0.529
0 0.0
0 0.0
0.5843106816552323
0.5498234163351067
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3236,	 Acc = 0.4938
5486 0.315
29114 0.506
14300 0.532
1295 0.538
29 0.69
0 0.0
0 0.0
0 0.0
0.5157584156645357
0.5498234163351067
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3937,	 Acc1 = 0.2434,	 Acc2 = 0.2373

 ===== Epoch 266	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 2 1
train:	 Loss = 1.2577,	 Acc = 0.5127
48802 0.288
95393 0.571
49906 0.607
5742 0.625
901 0.514
88 0.557
0 0.0
0 0.0
0.5849042952048937
0.5498234163351067
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3594,	 Acc = 0.4830
5486 0.365
29114 0.525
14300 0.446
1295 0.456
29 0.0
0 0.0
0 0.0
0 0.0
0.4973624212079217
0.5498234163351067
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.4218,	 Acc1 = 0.2857,	 Acc2 = 0.2883

 ===== Epoch 267	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096  1.5666665   1.6045294
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946  0.5721506   0.5801319
 -0.01747011 -0.0158521 ] 2 2
train:	 Loss = 1.2578,	 Acc = 0.5119
48805 0.286
95391 0.571
49902 0.606
5744 0.628
902 0.525
88 0.545
0 0.0
0 0.0
0.5843961927815454
0.5498234163351067
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3393,	 Acc = 0.4977
5486 0.402
29114 0.537
14300 0.459
1295 0.456
29 0.0
0 0.0
0 0.0
0 0.0
0.5094550493987214
0.5498234163351067
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3913,	 Acc1 = 0.2798,	 Acc2 = 0.2811

 ===== Epoch 268	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143  2.2926102   2.1643102
 -0.3580977  -0.36433512  1.2251651   0.8173061  -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
 -3.3511646e+00 -2.3659308e+00 -1.5871393e-02 -1.1254287e-02
 -1.2964727e-01  2.0349362e+00 -1.1927608e-02 -1.4941680e-03
 -1.7470114e-02 -1.5852105e-02] 6 6
train:	 Loss = 1.2568,	 Acc = 0.5132
48791 0.287
95399 0.574
49908 0.605
5744 0.624
902 0.529
88 0.58
0 0.0
0 0.0
0.585651238810584
0.5498234163351067
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3156,	 Acc = 0.4987
5486 0.377
29114 0.534
14300 0.478
1295 0.458
29 0.0
0 0.0
0 0.0
0 0.0
0.5136125888506415
0.5498234163351067
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3836,	 Acc1 = 0.2503,	 Acc2 = 0.2455

 ===== Epoch 269	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096  2.2596085   2.293956
 -0.3977158  -0.3905835 ] [ 1.5560732   2.9641209   0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -3.181966   -2.9911609
  1.5924248   3.7948494 ] 5 5
train:	 Loss = 1.2575,	 Acc = 0.5127
48796 0.286
95394 0.572
49910 0.609
5742 0.623
902 0.513
88 0.534
0 0.0
0 0.0
0.5854074035096951
0.5498234163351067
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3301,	 Acc = 0.4892
5486 0.373
29114 0.52
14300 0.471
1295 0.5
29 0.0
0 0.0
0 0.0
0 0.0
0.5034199114846439
0.5498234163351067
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3887,	 Acc1 = 0.2758,	 Acc2 = 0.2763

 ===== Epoch 270	 =====
[-0.36866695 -0.38685238  3.2056499   2.632913    2.5752888   0.84491175
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-1.7616821e-02 -2.3137011e-02 -4.2847471e+00 -2.6668763e+00
 -3.8612726e-01  7.2199702e-01 -1.5871393e-02 -1.1254287e-02
  1.5858389e+00  2.4425364e+00 -1.1927608e-02 -1.4941680e-03
 -1.7470114e-02 -1.5852105e-02] 4 6
train:	 Loss = 1.2575,	 Acc = 0.5128
48804 0.286
95395 0.572
49900 0.608
5743 0.634
902 0.507
88 0.58
0 0.0
0 0.0
0.5856026521430263
0.5498234163351067
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3589,	 Acc = 0.4659
5486 0.242
29114 0.505
14300 0.471
1295 0.488
29 0.0
0 0.0
0 0.0
0 0.0
0.4933166435692253
0.5498234163351067
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.4149,	 Acc1 = 0.2465,	 Acc2 = 0.2734

 ===== Epoch 271	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096  2.2642217   1.4142575
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.28331625  0.29747248
 -0.01747011 -0.0158521 ] 1 1
train:	 Loss = 1.2581,	 Acc = 0.5127
48800 0.287
95391 0.572
49909 0.607
5742 0.623
902 0.523
88 0.557
0 0.0
0 0.0
0.5851333929699011
0.5498234163351067
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.4008,	 Acc = 0.4868
5486 0.394
29114 0.534
14300 0.434
1295 0.421
29 0.0
0 0.0
0 0.0
0 0.0
0.4982118109884215
0.5498234163351067
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.4563,	 Acc1 = 0.2767,	 Acc2 = 0.2773

 ===== Epoch 272	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
  1.2548558   2.4241896  -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [ 2.6925218e+00  2.8433998e+00  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.7889125e+00 -2.8844111e+00
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
  2.9156234e+00  3.5547388e+00] 5 6
train:	 Loss = 1.2562,	 Acc = 0.5127
48803 0.288
95390 0.572
49906 0.606
5743 0.629
902 0.507
88 0.511
0 0.0
0 0.0
0.5848949871406113
0.5498234163351067
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.4366,	 Acc = 0.4533
5486 0.38
29114 0.495
14300 0.403
1295 0.388
29 0.0
0 0.0
0 0.0
0 0.0
0.4622915642183379
0.5498234163351067
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.4923,	 Acc1 = 0.2567,	 Acc2 = 0.2532

 ===== Epoch 273	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946  2.9829323   1.9146103
 -0.01747011 -0.0158521 ] 5 6
train:	 Loss = 1.2585,	 Acc = 0.5118
48805 0.287
95388 0.571
49908 0.605
5742 0.619
901 0.511
88 0.568
0 0.0
0 0.0
0.5838633926868254
0.5498234163351067
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3185,	 Acc = 0.4948
5486 0.344
29114 0.511
14300 0.519
1295 0.508
29 0.0
0 0.0
0 0.0
0 0.0
0.513232598685681
0.5498234163351067
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 1
Testing:	 Loss = 1.3918,	 Acc1 = 0.2389,	 Acc2 = 0.2319

 ===== Epoch 274	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143  2.9485798   3.0035195
 -0.3580977  -0.36433512  1.9468825   2.2423005  -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 0 0
train:	 Loss = 1.2582,	 Acc = 0.5119
48802 0.288
95385 0.571
49915 0.604
5740 0.626
902 0.522
88 0.591
0 0.0
0 0.0
0.5836940077616259
0.5498234163351067
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3418,	 Acc = 0.5035
5486 0.394
29114 0.528
14300 0.495
1295 0.521
29 0.0
0 0.0
0 0.0
0 0.0
0.5168983861594171
0.5498234163351067
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.4048,	 Acc1 = 0.2682,	 Acc2 = 0.2671

 ===== Epoch 275	 =====
[ 0.2785246   3.6456485  -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.0544638  -0.27404773  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 5 5
train:	 Loss = 1.2574,	 Acc = 0.5119
48797 0.288
95395 0.571
49907 0.605
5743 0.618
902 0.537
88 0.557
0 0.0
0 0.0
0.5836879665866412
0.5498234163351067
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3574,	 Acc = 0.4964
5486 0.405
29114 0.542
14300 0.443
1295 0.442
29 0.0
0 0.0
0 0.0
0 0.0
0.5075103938486298
0.5498234163351067
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.4092,	 Acc1 = 0.2824,	 Acc2 = 0.2843

 ===== Epoch 276	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
  1.2793741   2.7559526  -0.41367632 -0.42256096  2.384968    2.1654608
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -1.8158644  -3.226243    0.0244112   0.04163946 -0.4643954   0.05829916
 -0.01747011 -0.0158521 ] 1 1
train:	 Loss = 1.2559,	 Acc = 0.5136
48796 0.288
95395 0.573
49909 0.607
5743 0.626
901 0.529
88 0.591
0 0.0
0 0.0
0.586170380699308
0.5498234163351067
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3343,	 Acc = 0.4898
5486 0.366
29114 0.524
14300 0.468
1295 0.497
29 0.0
0 0.0
0 0.0
0 0.0
0.5050069292324199
0.5498234163351067
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3992,	 Acc1 = 0.2707,	 Acc2 = 0.2701

 ===== Epoch 277	 =====
[ 2.179125    2.4074423  -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [ 2.2990105e+00  2.9405203e-01  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03  1.6981360e+00  3.2066815e+00
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.7470114e-02 -1.5852105e-02] 2 2
train:	 Loss = 1.2576,	 Acc = 0.5123
48796 0.288
95395 0.571
49911 0.605
5741 0.627
901 0.545
88 0.557
0 0.0
0 0.0
0.5843681759583257
0.5498234163351067
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3251,	 Acc = 0.4777
5486 0.343
29114 0.497
14300 0.491
1295 0.48
29 0.0
0 0.0
0 0.0
0 0.0
0.494255442800304
0.5498234163351067
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.4058,	 Acc1 = 0.2201,	 Acc2 = 0.2092

 ===== Epoch 278	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096  1.6039666   3.2972078
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.1408149   0.00122371
 -0.01747011 -0.0158521 ] 0 1
train:	 Loss = 1.2571,	 Acc = 0.5125
48797 0.287
95392 0.573
49910 0.605
5744 0.617
901 0.522
88 0.523
0 0.0
0 0.0
0.5849574111224389
0.5498234163351067
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.2907,	 Acc = 0.5251
5486 0.359
29114 0.555
14300 0.525
1295 0.576
29 0.0
0 0.0
0 0.0
0 0.0
0.5455094103446735
0.5498234163351067
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3604,	 Acc1 = 0.2730,	 Acc2 = 0.2729

 ===== Epoch 279	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 0 1
train:	 Loss = 1.2575,	 Acc = 0.5125
48802 0.287
95400 0.571
49897 0.608
5743 0.618
902 0.531
88 0.523
0 0.0
0 0.0
0.5847990528185226
0.5498234163351067
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3444,	 Acc = 0.5225
5486 0.405
29114 0.551
14300 0.515
1295 0.473
29 0.0
0 0.0
0 0.0
0 0.0
0.5369037507264518
0.5498234163351067
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.4363,	 Acc1 = 0.2146,	 Acc2 = 0.2025

 ===== Epoch 280	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096  0.9271757   2.3186667
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946  0.05660138 -0.18630992
 -0.01747011 -0.0158521 ] 5 3
train:	 Loss = 1.2578,	 Acc = 0.5110
48802 0.285
95397 0.57
49900 0.606
5743 0.624
902 0.519
88 0.534
0 0.0
0 0.0
0.5836611195158851
0.5498234163351067
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.2945,	 Acc = 0.5001
5486 0.386
29114 0.519
14300 0.503
1295 0.537
29 0.0
0 0.0
0 0.0
0 0.0
0.5140819884661809
0.5498234163351067
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3598,	 Acc1 = 0.2620,	 Acc2 = 0.2597

 ===== Epoch 281	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946  2.4451451   3.999223
 -0.01747011 -0.0158521 ] 1 6
train:	 Loss = 1.2568,	 Acc = 0.5125
48802 0.289
95385 0.57
49911 0.609
5744 0.626
902 0.528
88 0.545
0 0.0
0 0.0
0.5843649279747418
0.5498234163351067
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.2776,	 Acc = 0.5090
5486 0.411
29114 0.545
14300 0.475
1295 0.489
29 0.0
0 0.0
0 0.0
0 0.0
0.5210559256113371
0.5498234163351067
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3457,	 Acc1 = 0.2503,	 Acc2 = 0.2455

 ===== Epoch 282	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512  2.0558639   3.2093482  -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
 -2.8291128e+00 -3.8792119e+00 -1.1927608e-02 -1.4941680e-03
 -1.7470114e-02 -1.5852105e-02] 4 1
train:	 Loss = 1.2565,	 Acc = 0.5119
48800 0.288
95388 0.57
49911 0.607
5744 0.621
901 0.523
88 0.5
0 0.0
0 0.0
0.5837126394443275
0.5498234163351067
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3589,	 Acc = 0.4878
5486 0.407
29114 0.531
14300 0.441
1295 0.392
29 0.0
0 0.0
0 0.0
0 0.0
0.4976753542849479
0.5498234163351067
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.4388,	 Acc1 = 0.2193,	 Acc2 = 0.2083

 ===== Epoch 283	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 1 1
train:	 Loss = 1.2571,	 Acc = 0.5127
48803 0.287
95390 0.572
49907 0.607
5742 0.628
902 0.518
88 0.568
0 0.0
0 0.0
0.585052851758546
0.5498234163351067
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3386,	 Acc = 0.5006
5486 0.406
29114 0.534
14300 0.477
1295 0.435
29 0.0
0 0.0
0 0.0
0 0.0
0.512159685278734
0.5498234163351067
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3953,	 Acc1 = 0.2816,	 Acc2 = 0.2833

 ===== Epoch 284	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946  3.6223772   1.6509761
  3.8063965   4.0716834 ] 2 6
train:	 Loss = 1.2576,	 Acc = 0.5122
48798 0.288
95397 0.572
49907 0.605
5740 0.618
902 0.53
88 0.545
0 0.0
0 0.0
0.5842311588197376
0.5498234163351067
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.2857,	 Acc = 0.5152
5486 0.398
29114 0.548
14300 0.499
1295 0.461
29 0.0
0 0.0
0 0.0
0 0.0
0.5295274710536904
0.5498234163351067
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3540,	 Acc1 = 0.2490,	 Acc2 = 0.2440

 ===== Epoch 285	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 1 1
train:	 Loss = 1.2576,	 Acc = 0.5123
48804 0.286
95388 0.572
49907 0.606
5743 0.626
902 0.516
88 0.534
0 0.0
0 0.0
0.5848527902754755
0.5498234163351067
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3224,	 Acc = 0.5147
5486 0.403
29114 0.556
14300 0.48
1295 0.442
29 0.0
0 0.0
0 0.0
0 0.0
0.5284545576467432
0.5498234163351067
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3779,	 Acc1 = 0.2820,	 Acc2 = 0.2838

 ===== Epoch 286	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
  2.0937045   3.142056   -0.41367632 -0.42256096  3.2174976   3.3021502
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -2.711018   -3.6240647   0.0244112   0.04163946 -4.3124537  -4.100055
 -0.01747011 -0.0158521 ] 6 6
train:	 Loss = 1.2569,	 Acc = 0.5108
48800 0.286
95391 0.57
49907 0.605
5744 0.618
902 0.51
88 0.58
0 0.0
0 0.0
0.5828772889917913
0.5498234163351067
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3287,	 Acc = 0.4953
5486 0.397
29114 0.536
14300 0.46
1295 0.395
29 0.0
0 0.0
0 0.0
0 0.0
0.5073315749474719
0.5498234163351067
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 0
Testing:	 Loss = 1.3926,	 Acc1 = 0.2630,	 Acc2 = 0.2609

 ===== Epoch 287	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512  0.88082963  2.977939   -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
 -1.4713750e+00 -3.6293924e+00 -1.1927608e-02 -1.4941680e-03
 -1.7470114e-02 -1.5852105e-02] 1 1
train:	 Loss = 1.2570,	 Acc = 0.5124
48801 0.286
95387 0.572
49912 0.608
5742 0.623
902 0.506
88 0.534
0 0.0
0 0.0
0.5852293282291112
0.5498234163351067
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3240,	 Acc = 0.4767
5486 0.36
29114 0.509
14300 0.46
1295 0.453
29 0.0
0 0.0
0 0.0
0 0.0
0.4909919978541732
0.5498234163351067
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3944,	 Acc1 = 0.2503,	 Acc2 = 0.2455

 ===== Epoch 288	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512  1.0518379   1.2923043  -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  1.5623350e+00  5.4916751e-01 -1.1927608e-02 -1.4941680e-03
 -1.7470114e-02 -1.5852105e-02] 2 2
train:	 Loss = 1.2576,	 Acc = 0.5129
48804 0.288
95391 0.572
49905 0.606
5742 0.625
902 0.524
88 0.545
0 0.0
0 0.0
0.5851027442313258
0.5498234163351067
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3071,	 Acc = 0.4934
5486 0.362
29114 0.521
14300 0.488
1295 0.498
29 0.0
0 0.0
0 0.0
0 0.0
0.5095668112119451
0.5498234163351067
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3701,	 Acc1 = 0.2595,	 Acc2 = 0.2567

 ===== Epoch 289	 =====
[-0.36866695 -0.38685238  2.0968084   1.0408409  -0.43314552 -0.35780522
  3.532063    3.3565576  -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.2377882   0.05401346  0.01022447  0.00647582
  0.02712251  0.0624164   0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 3 3
train:	 Loss = 1.2577,	 Acc = 0.5118
48798 0.288
95387 0.571
49914 0.605
5744 0.621
901 0.518
88 0.534
0 0.0
0 0.0
0.5836326084954682
0.5498234163351067
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3244,	 Acc = 0.4740
5486 0.217
29114 0.519
14300 0.482
1295 0.48
29 0.0
0 0.0
0 0.0
0 0.0
0.5054763288479592
0.5498234163351067
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3911,	 Acc1 = 0.2282,	 Acc2 = 0.2512

 ===== Epoch 290	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 1 0
train:	 Loss = 1.2582,	 Acc = 0.5118
48808 0.286
95384 0.571
49908 0.606
5742 0.619
902 0.525
88 0.545
0 0.0
0 0.0
0.5842103878335
0.5498234163351067
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.2973,	 Acc = 0.5089
5486 0.404
29114 0.548
14300 0.476
1295 0.452
29 0.0
0 0.0
0 0.0
0 0.0
0.5216594394027448
0.5498234163351067
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3602,	 Acc1 = 0.2567,	 Acc2 = 0.2532

 ===== Epoch 291	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  1.7934859e+00  1.8455992e+00 -1.1927608e-02 -1.4941680e-03
 -1.7470114e-02 -1.5852105e-02] 4 6
train:	 Loss = 1.2573,	 Acc = 0.5129
48800 0.288
95383 0.572
49916 0.607
5743 0.62
902 0.508
88 0.591
0 0.0
0 0.0
0.5850413070932435
0.5498234163351067
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3970,	 Acc = 0.4568
5486 0.346
29114 0.483
14300 0.441
1295 0.533
29 0.0
0 0.0
0 0.0
0 0.0
0.4703160624077965
0.5498234163351067
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.4465,	 Acc1 = 0.2818,	 Acc2 = 0.2835

 ===== Epoch 292	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  0.95826036  1.4397929 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
  0.31027234 -0.23336434] 3 2
train:	 Loss = 1.2562,	 Acc = 0.5124
48799 0.288
95394 0.571
49908 0.606
5741 0.626
902 0.513
88 0.58
0 0.0
0 0.0
0.5845244124630836
0.5498234163351067
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3704,	 Acc = 0.4813
5486 0.407
29114 0.517
14300 0.441
1295 0.447
29 0.0
0 0.0
0 0.0
0 0.0
0.4905002458759891
0.5498234163351067
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.4184,	 Acc1 = 0.2701,	 Acc2 = 0.2694

 ===== Epoch 293	 =====
[ 2.8549926   3.3200743  -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  2.3979821   3.4996178 ] [-0.08590788 -0.02076993  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.03214576 -0.02150177] 0 0
train:	 Loss = 1.2564,	 Acc = 0.5116
48802 0.287
95395 0.57
49903 0.607
5742 0.621
902 0.52
88 0.557
0 0.0
0 0.0
0.5837597842531079
0.5498234163351067
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3372,	 Acc = 0.5074
5486 0.4
29114 0.549
14300 0.469
1295 0.461
29 0.0
0 0.0
0 0.0
0 0.0
0.5206312307210872
0.5498234163351067
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3952,	 Acc1 = 0.2752,	 Acc2 = 0.2756

 ===== Epoch 294	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 0 1
train:	 Loss = 1.2571,	 Acc = 0.5131
48802 0.289
95398 0.572
49901 0.607
5741 0.624
902 0.509
88 0.534
0 0.0
0 0.0
0.5851937117674143
0.5498234163351067
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3171,	 Acc = 0.5096
5486 0.372
29114 0.543
14300 0.493
1295 0.532
29 0.0
0 0.0
0 0.0
0 0.0
0.5265546068219411
0.5498234163351067
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3722,	 Acc1 = 0.2872,	 Acc2 = 0.2900

 ===== Epoch 295	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143  2.806853    2.0655797
 -0.3580977  -0.36433512  2.0749936   1.2265352  -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207 -0.01030952 -0.28690898
 -0.01587139 -0.01125429 -0.09079772 -0.3002188  -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 5 5
train:	 Loss = 1.2572,	 Acc = 0.5121
48799 0.286
95394 0.572
49908 0.607
5741 0.617
902 0.512
88 0.545
0 0.0
0 0.0
0.5846362302921077
0.5498234163351067
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 6
val:	 Loss = 1.3439,	 Acc = 0.4789
5486 0.369
29114 0.517
14300 0.446
1295 0.456
29 0.0
0 0.0
0 0.0
0 0.0
0.4923554919755018
0.5498234163351067
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.4094,	 Acc1 = 0.2589,	 Acc2 = 0.2560

 ===== Epoch 296	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512  2.0668793   2.188711   -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.02226749  0.02903207  0.01022447  0.00647582
 -0.01587139 -0.01125429 -2.8418412  -2.7773764   2.326241    2.300549
 -0.01747011 -0.0158521 ] 6 5
train:	 Loss = 1.2581,	 Acc = 0.5114
48801 0.286
95392 0.57
49909 0.607
5740 0.627
902 0.506
88 0.58
0 0.0
0 0.0
0.583874341417211
0.5498234163351067
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3749,	 Acc = 0.4931
5486 0.4
29114 0.54
14300 0.435
1295 0.478
29 0.0
0 0.0
0 0.0
0 0.0
0.5045598819795253
0.5498234163351067
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.4333,	 Acc1 = 0.2583,	 Acc2 = 0.2552

 ===== Epoch 297	 =====
[-0.36866695 -0.38685238  1.2033563   2.6972392  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701  0.09804434 -0.33736166  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 5 5
train:	 Loss = 1.2574,	 Acc = 0.5125
48802 0.288
95392 0.571
49905 0.608
5743 0.626
902 0.508
88 0.545
0 0.0
0 0.0
0.5847595869236335
0.5498234163351067
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.2898,	 Acc = 0.5147
5486 0.4
29114 0.552
14300 0.494
1295 0.395
29 0.0
0 0.0
0 0.0
0 0.0
0.5287004336358353
0.5498234163351067
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3545,	 Acc1 = 0.2509,	 Acc2 = 0.2463

 ===== Epoch 298	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512  0.8732924   2.563838   -0.4264405  -0.4242137
  2.369468    1.0799752 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
 -1.4626658e+00 -3.1823466e+00 -1.1927608e-02 -1.4941680e-03
 -2.7574936e-01  1.2821443e-01] 1 1
train:	 Loss = 1.2567,	 Acc = 0.5122
48804 0.288
95387 0.571
49908 0.606
5744 0.627
901 0.527
88 0.58
0 0.0
0 0.0
0.5842081721788092
0.5498234163351067
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3347,	 Acc = 0.4975
5486 0.407
29114 0.541
14300 0.453
1295 0.406
29 0.0
0 0.0
0 0.0
0 0.0
0.5086503643435111
0.5498234163351067
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3948,	 Acc1 = 0.2820,	 Acc2 = 0.2838

 ===== Epoch 299	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
  1.2863796   3.8999627  -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03  3.8028680e-02 -2.2184281e+00
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.7470114e-02 -1.5852105e-02] 5 5
train:	 Loss = 1.2581,	 Acc = 0.5114
48803 0.287
95390 0.57
49908 0.606
5741 0.625
902 0.52
88 0.58
0 0.0
0 0.0
0.5835794486578219
0.5498234163351067
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3163,	 Acc = 0.5094
5486 0.362
29114 0.539
14300 0.504
1295 0.546
29 0.0
0 0.0
0 0.0
0 0.0
0.5274487013277304
0.5498234163351067
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3823,	 Acc1 = 0.2769,	 Acc2 = 0.2776

 ===== Epoch 300	 =====
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096  2.8260355   1.3055307
  3.424475    3.2675617 ] [-0.01761682 -0.02313701  1.0723604   2.1108148   0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -3.8504555  -1.9040091
 -4.279631   -3.9790945 ] 6 5
train:	 Loss = 1.2565,	 Acc = 0.5127
48798 0.287
95389 0.571
49911 0.608
5744 0.628
902 0.53
88 0.545
0 0.0
0 0.0
0.58500072352237
0.5498234163351067
[-0.36866695 -0.38685238 -0.41997173 -0.34217143 -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
  1.2916504   3.2493103 ] [-1.7616821e-02 -2.3137011e-02  2.2267485e-02  2.9032072e-02
  1.0224475e-02  6.4758179e-03 -1.5871393e-02 -1.1254287e-02
  2.4411201e-02  4.1639462e-02 -1.1927608e-02 -1.4941680e-03
 -1.9012982e+00 -3.9593208e+00] 5 1
val:	 Loss = 1.3211,	 Acc = 0.5038
5486 0.399
29114 0.548
14300 0.466
1295 0.401
29 0.0
0 0.0
0 0.0
0 0.0
0.5166972148956145
0.5498234163351067
[-0.36866695 -0.38685238  1.1047752   2.3664188  -0.43314552 -0.35780522
 -0.3580977  -0.36433512 -0.41367632 -0.42256096 -0.4264405  -0.4242137
 -0.3977158  -0.3905835 ] [-0.01761682 -0.02313701 -0.00184354  0.21847428  0.01022447  0.00647582
 -0.01587139 -0.01125429  0.0244112   0.04163946 -0.01192761 -0.00149417
 -0.01747011 -0.0158521 ] 4 4
Testing:	 Loss = 1.3883,	 Acc1 = 0.2513,	 Acc2 = 0.2468
