(0.24300000000005184, array([-1.000000e+00, -1.000000e+00,  1.189783e+03,  1.790000e-01,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 4, array([  0.   ,   0.   , -15.851,   0.091,   0.   ,   0.   ,   0.   ,
         0.   ,   0.   ,   0.   ,   0.   ,   0.   ,   0.   ,   0.   ]), 1)
((0.236, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.364685e+03, -3.300000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), array([0., 0., 0.])), (0.24, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.543573e+03, -3.290000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), array([0., 0., 0.])))
(0.236, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.364685e+03, -3.300000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([0.   , 0.   , 0.   , 0.   , 5.823, 3.129, 0.   , 0.   , 0.   ,
       0.   , 0.   , 0.   , 0.   , 0.   ]))
(0.236, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.364685e+03, -3.300000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([ 0.   ,  0.   ,  0.   ,  0.   , -8.411, -0.011,  0.   ,  0.   ,
        0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ]))
(0.236, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.364685e+03, -3.300000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  6.2433e+01,
       -3.0000e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
        0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]))
(0.236, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.364685e+03, -3.300000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([ 0.00000e+00,  0.00000e+00,  0.00000e+00,  0.00000e+00,
       -1.19043e+02, -4.00000e-03,  0.00000e+00,  0.00000e+00,
        0.00000e+00,  0.00000e+00,  0.00000e+00,  0.00000e+00,
        0.00000e+00,  0.00000e+00]))
(0.24, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.543573e+03, -3.290000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([   0.   ,    0.   ,    0.   ,    0.   , -173.065,    3.128,
          0.   ,    0.   ,    0.   ,    0.   ,    0.   ,    0.   ,
          0.   ,    0.   ]))
(0.24, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.543573e+03, -3.290000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([ 0.00000e+00,  0.00000e+00,  0.00000e+00,  0.00000e+00,
       -1.87299e+02, -1.20000e-02,  0.00000e+00,  0.00000e+00,
        0.00000e+00,  0.00000e+00,  0.00000e+00,  0.00000e+00,
        0.00000e+00,  0.00000e+00]))
(0.24, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.543573e+03, -3.290000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([ 0.00000e+00,  0.00000e+00,  0.00000e+00,  0.00000e+00,
       -1.16455e+02, -4.00000e-03,  0.00000e+00,  0.00000e+00,
        0.00000e+00,  0.00000e+00,  0.00000e+00,  0.00000e+00,
        0.00000e+00,  0.00000e+00]))
(0.24, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.543573e+03, -3.290000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([ 0.00000e+00,  0.00000e+00,  0.00000e+00,  0.00000e+00,
       -2.97931e+02, -5.00000e-03,  0.00000e+00,  0.00000e+00,
        0.00000e+00,  0.00000e+00,  0.00000e+00,  0.00000e+00,
        0.00000e+00,  0.00000e+00]))
(0.24, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.543573e+03, -3.290000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([ 0.00000e+00,  0.00000e+00,  0.00000e+00,  0.00000e+00,
       -1.28424e+02, -1.40000e-02,  0.00000e+00,  0.00000e+00,
        0.00000e+00,  0.00000e+00,  0.00000e+00,  0.00000e+00,
        0.00000e+00,  0.00000e+00]))
(0.238, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.370508e+03,  2.799000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([  0.   ,   0.   ,   0.   ,   0.   , -14.234,  -3.14 ,   0.   ,
         0.   ,   0.   ,   0.   ,   0.   ,   0.   ,   0.   ,   0.   ]))
14 1 14

 ===== Epoch 1	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 1 4
train:	 Loss = 1.6122,	 Acc = 0.3859
13235 0.246
25968 0.413
13396 0.46
1728 0.465
299 0.462
30 0.233
0 0.0
0 0.0
0.4305786919678424
0.0
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 0
val:	 Loss = 1.4241,	 Acc = 0.4549
1668 0.39
7898 0.478
3716 0.447
378 0.344
10 0.1
0 0.0
0 0.0
0 0.0
0.46392267955340777
0.46392267955340777
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.5120,	 Acc1 = 0.3072,	 Acc2 = 0.3141

 ===== Epoch 2	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722  1.9624783   1.9217265
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249  -0.2770871   0.1499167
 -0.00275372 -0.0031672 ] 2 1
train:	 Loss = 1.4714,	 Acc = 0.4589
13227 0.278
25969 0.495
13403 0.556
1728 0.552
299 0.452
30 0.333
0 0.0
0 0.0
0.5167394820053586
0.46392267955340777
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 0
val:	 Loss = 1.4581,	 Acc = 0.4314
1668 0.37
7898 0.46
3716 0.409
378 0.333
10 0.0
0 0.0
0 0.0
0 0.0
0.4399266788868522
0.46392267955340777
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 6
Testing:	 Loss = 1.5347,	 Acc1 = 0.3183,	 Acc2 = 0.3275

 ===== Epoch 3	 =====
[-0.3673141  -0.38194367  2.4369745   2.232993   -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.56075716  0.10074333  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 2 2
train:	 Loss = 1.4510,	 Acc = 0.4658
13232 0.282
25971 0.505
13396 0.558
1728 0.572
299 0.475
30 0.367
0 0.0
0 0.0
0.5245027037466203
0.46392267955340777
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 0
val:	 Loss = 1.3940,	 Acc = 0.4854
1668 0.405
7898 0.516
3716 0.468
378 0.365
10 0.9
0 0.0
0 0.0
0 0.0
0.49658390268288616
0.49658390268288616
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4775,	 Acc1 = 0.3330,	 Acc2 = 0.3452

 ===== Epoch 4	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436  0.5111809   1.978373
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849 -0.05713348  0.3222783
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 6 1
train:	 Loss = 1.4396,	 Acc = 0.4704
13233 0.277
25970 0.515
13397 0.562
1727 0.572
299 0.508
30 0.233
0 0.0
0 0.0
0.5323612485817059
0.49658390268288616
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 0
val:	 Loss = 1.3629,	 Acc = 0.5079
1668 0.387
7898 0.531
3716 0.528
378 0.368
10 0.1
0 0.0
0 0.0
0 0.0
0.5247458756873854
0.5247458756873854
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 1
Testing:	 Loss = 1.4538,	 Acc1 = 0.3398,	 Acc2 = 0.3534

 ===== Epoch 5	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722  2.3679583   1.0833577
  2.3854957   3.5429585 ] [0.00112635 0.00204053 0.00522043 0.00475849 0.00270004 0.00084708
 0.00060168 0.00094007 0.00291407 0.0077249  0.38177574 0.09775601
 0.08011674 0.03068413] 2 2
train:	 Loss = 1.4284,	 Acc = 0.4759
13232 0.275
25969 0.52
13398 0.575
1729 0.591
298 0.48
30 0.267
0 0.0
0 0.0
0.54009752800309
0.5247458756873854
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 0
val:	 Loss = 1.3825,	 Acc = 0.4861
1668 0.388
7898 0.517
3716 0.485
378 0.299
10 0.0
0 0.0
0 0.0
0 0.0
0.4996667222129645
0.5247458756873854
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4870,	 Acc1 = 0.3218,	 Acc2 = 0.3318

 ===== Epoch 6	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6361516   1.7022088  -0.40214875 -0.40990722  2.578808    2.9022748
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.27738568  0.22926807  0.00291407  0.0077249  -0.35855305  0.21010211
 -0.00275372 -0.0031672 ] 3 2
train:	 Loss = 1.4160,	 Acc = 0.4796
13233 0.277
25965 0.527
13400 0.576
1729 0.585
299 0.485
30 0.133
0 0.0
0 0.0
0.5444559785626343
0.5247458756873854
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 0
val:	 Loss = 1.3751,	 Acc = 0.4916
1668 0.398
7898 0.543
3716 0.448
378 0.259
10 0.0
0 0.0
0 0.0
0 0.0
0.5045825695717381
0.5247458756873854
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 6
Testing:	 Loss = 1.4536,	 Acc1 = 0.3635,	 Acc2 = 0.3820

 ===== Epoch 7	 =====
[ 3.4407792   9.633444   -0.4204066  -0.33581436  0.45458397  2.33354
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [-3.9879711e+00 -1.1231966e+01  5.2204342e-03  4.7584870e-03
  4.0571574e-02  4.8052141e-01  6.0168497e-04  9.4006682e-04
  2.9140669e-03  7.7248965e-03  3.7514956e-03  5.4717045e-03
 -2.7537176e-03 -3.1672046e-03] 6 6
train:	 Loss = 1.4001,	 Acc = 0.4859
13229 0.274
25968 0.536
13401 0.584
1729 0.589
299 0.528
30 0.233
0 0.0
0 0.0
0.5537210032104666
0.5247458756873854
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3413,	 Acc = 0.5080
1668 0.379
7898 0.532
3716 0.531
378 0.344
10 0.5
0 0.0
0 0.0
0 0.0
0.5259956673887686
0.5259956673887686
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4365,	 Acc1 = 0.3627,	 Acc2 = 0.3810

 ===== Epoch 8	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 3 1
train:	 Loss = 1.3948,	 Acc = 0.4893
13234 0.275
25964 0.541
13400 0.589
1729 0.58
299 0.512
30 0.2
0 0.0
0 0.0
0.5576505238762011
0.5259956673887686
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3425,	 Acc = 0.5161
1668 0.378
7898 0.553
3716 0.517
378 0.344
10 0.9
0 0.0
0 0.0
0 0.0
0.5353274454257624
0.5353274454257624
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4214,	 Acc1 = 0.3778,	 Acc2 = 0.3991

 ===== Epoch 9	 =====
[-0.3673141  -0.38194367  1.7043116   3.0953138  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.29049638 -0.11762218  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 1 5
train:	 Loss = 1.3834,	 Acc = 0.4943
13232 0.278
25968 0.547
13399 0.593
1728 0.593
299 0.525
30 0.333
0 0.0
0 0.0
0.5634656237929703
0.5353274454257624
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3209,	 Acc = 0.4960
1668 0.241
7898 0.545
3716 0.518
378 0.407
10 0.0
0 0.0
0 0.0
0 0.0
0.5314947508748542
0.5353274454257624
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 1
Testing:	 Loss = 1.4181,	 Acc1 = 0.3212,	 Acc2 = 0.3633

 ===== Epoch 10	 =====
[-0.3673141  -0.38194367  0.7932687   1.7768708  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.02180681 -0.35038543  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 5 5
train:	 Loss = 1.3759,	 Acc = 0.4972
13230 0.273
25970 0.549
13398 0.606
1729 0.588
299 0.548
30 0.3
0 0.0
0 0.0
0.5689904890648385
0.5353274454257624
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 0
val:	 Loss = 1.3649,	 Acc = 0.5100
1668 0.402
7898 0.548
3716 0.495
378 0.349
10 0.0
0 0.0
0 0.0
0 0.0
0.5250791534744209
0.5353274454257624
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4378,	 Acc1 = 0.3802,	 Acc2 = 0.4021

 ===== Epoch 11	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722  1.5038153   1.7844497
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0494858  -0.03465191
 -0.00275372 -0.0031672 ] 2 2
train:	 Loss = 1.3694,	 Acc = 0.5008
13232 0.281
25973 0.55
13396 0.609
1726 0.595
299 0.582
30 0.3
0 0.0
0 0.0
0.5711181923522596
0.5353274454257624
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 0
val:	 Loss = 1.3536,	 Acc = 0.5417
1668 0.402
7898 0.569
3716 0.559
378 0.437
10 0.0
0 0.0
0 0.0
0 0.0
0.5610731544742543
0.5610731544742543
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 1
Testing:	 Loss = 1.4521,	 Acc1 = 0.3571,	 Acc2 = 0.3743

 ===== Epoch 12	 =====
[-0.3673141  -0.38194367  1.0864964   3.0068123  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00955773  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 0 0
train:	 Loss = 1.3664,	 Acc = 0.5029
13226 0.285
25972 0.55
13400 0.614
1729 0.603
299 0.542
30 0.3
0 0.0
0 0.0
0.5725561187545257
0.5610731544742543
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3385,	 Acc = 0.5181
1668 0.387
7898 0.55
3716 0.519
378 0.434
10 0.0
0 0.0
0 0.0
0 0.0
0.5364105982336277
0.5610731544742543
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4260,	 Acc1 = 0.3600,	 Acc2 = 0.3777

 ===== Epoch 13	 =====
[ 1.9790999   1.8380791  -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
  2.5130053   2.0375426 ] [ 1.7502210e+00  3.8910534e-02  5.2204342e-03  4.7584870e-03
  2.7000366e-03  8.4708212e-04  6.0168497e-04  9.4006682e-04
  2.9140669e-03  7.7248965e-03  3.7514956e-03  5.4717045e-03
 -1.9138247e-01  5.6072637e-02] 1 2
train:	 Loss = 1.3643,	 Acc = 0.5044
13229 0.282
25975 0.554
13396 0.613
1727 0.605
299 0.545
30 0.4
0 0.0
0 0.0
0.5754218263451373
0.5610731544742543
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3210,	 Acc = 0.5276
1668 0.401
7898 0.564
3716 0.521
378 0.399
10 0.1
0 0.0
0 0.0
0 0.0
0.5451591401433095
0.5610731544742543
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4205,	 Acc1 = 0.3518,	 Acc2 = 0.3678

 ===== Epoch 14	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722  2.6263335   1.924178
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249  -0.37927634  0.02152115
 -0.00275372 -0.0031672 ] 2 0
train:	 Loss = 1.3608,	 Acc = 0.5037
13230 0.284
25972 0.552
13397 0.614
1728 0.604
299 0.508
30 0.267
0 0.0
0 0.0
0.5737700960749288
0.5610731544742543
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3004,	 Acc = 0.5296
1668 0.389
7898 0.576
3716 0.516
378 0.339
10 0.0
0 0.0
0 0.0
0 0.0
0.5491584735877354
0.5610731544742543
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 6
Testing:	 Loss = 1.3876,	 Acc1 = 0.3703,	 Acc2 = 0.3902

 ===== Epoch 15	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 1.1263542e-03  2.0405303e-03  9.4594753e-01  2.8099158e+00
  2.7000366e-03  8.4708212e-04  6.0168497e-04  9.4006682e-04
  2.9140669e-03  7.7248965e-03  3.7514956e-03  5.4717045e-03
 -2.7537176e-03 -3.1672046e-03] 5 5
train:	 Loss = 1.3581,	 Acc = 0.5041
13231 0.284
25968 0.553
13399 0.615
1729 0.594
299 0.542
30 0.333
0 0.0
0 0.0
0.5742908871454435
0.5610731544742543
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3647,	 Acc = 0.5127
1668 0.391
7898 0.541
3716 0.52
378 0.392
10 0.0
0 0.0
0 0.0
0 0.0
0.529661723046159
0.5610731544742543
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4412,	 Acc1 = 0.3689,	 Acc2 = 0.3884

 ===== Epoch 16	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722  2.2830899   1.3799736
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249  -0.45216447  0.01750879
 -0.00275372 -0.0031672 ] 3 1
train:	 Loss = 1.3613,	 Acc = 0.5021
13228 0.283
25968 0.549
13402 0.614
1729 0.602
299 0.525
30 0.333
0 0.0
0 0.0
0.5719320266486434
0.5610731544742543
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2975,	 Acc = 0.5574
1668 0.353
7898 0.583
3716 0.61
378 0.426
10 0.0
0 0.0
0 0.0
0 0.0
0.5857357107148808
0.5857357107148808
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4119,	 Acc1 = 0.3398,	 Acc2 = 0.3534

 ===== Epoch 17	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 1 1
train:	 Loss = 1.3648,	 Acc = 0.5004
13229 0.271
25972 0.551
13397 0.615
1729 0.604
299 0.538
30 0.267
0 0.0
0 0.0
0.5737321070799237
0.5857357107148808
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3371,	 Acc = 0.5225
1668 0.399
7898 0.559
3716 0.516
378 0.376
10 0.0
0 0.0
0 0.0
0 0.0
0.539576737210465
0.5857357107148808
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4203,	 Acc1 = 0.3623,	 Acc2 = 0.3805

 ===== Epoch 18	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722  1.3951844   1.7158114
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249  -0.3635544  -0.01057774
 -0.00275372 -0.0031672 ] 1 1
train:	 Loss = 1.3547,	 Acc = 0.5034
13230 0.282
25971 0.553
13398 0.612
1728 0.614
299 0.518
30 0.3
0 0.0
0 0.0
0.5741080480857432
0.5857357107148808
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2938,	 Acc = 0.5454
1668 0.37
7898 0.573
3716 0.577
378 0.444
10 0.0
0 0.0
0 0.0
0 0.0
0.569821696383936
0.5857357107148808
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4043,	 Acc1 = 0.3470,	 Acc2 = 0.3621

 ===== Epoch 19	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084  1.6328775   3.2979493  -0.4259259  -0.4217841
  3.208591    2.5873468 ] [ 1.1263542e-03  2.0405303e-03  5.2204342e-03  4.7584870e-03
  2.7000366e-03  8.4708212e-04  6.0168497e-04  9.4006682e-04
 -7.9087473e-02 -1.1698021e-01  3.7514956e-03  5.4717045e-03
 -6.4580512e+00 -4.8015943e+00] 2 3
train:	 Loss = 1.3571,	 Acc = 0.5048
13232 0.281
25970 0.554
13396 0.617
1729 0.607
299 0.532
30 0.3
0 0.0
0 0.0
0.5762842796446505
0.5857357107148808
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2806,	 Acc = 0.5541
1668 0.379
7898 0.573
3716 0.601
378 0.479
10 0.0
0 0.0
0 0.0
0 0.0
0.5783202799533411
0.5857357107148808
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3875,	 Acc1 = 0.3544,	 Acc2 = 0.3710

 ===== Epoch 20	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084  3.3305044   2.5018144  -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.05946696  0.22206178  0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 3
train:	 Loss = 1.3557,	 Acc = 0.5038
13228 0.283
25975 0.552
13397 0.614
1728 0.611
298 0.5
30 0.367
0 0.0
0 0.0
0.5742975765182968
0.5857357107148808
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2854,	 Acc = 0.5517
1668 0.395
7898 0.563
3716 0.614
378 0.413
10 0.0
0 0.0
0 0.0
0 0.0
0.5734877520413264
0.5857357107148808
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 1
Testing:	 Loss = 1.3978,	 Acc1 = 0.3505,	 Acc2 = 0.3663

 ===== Epoch 21	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084  0.72392744  1.167482   -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 1.1263542e-03  2.0405303e-03  5.2204342e-03  4.7584870e-03
  4.2608657e+00  3.5267005e+00  6.0168497e-04  9.4006682e-04
  1.4146850e-01 -3.5080227e-01  3.7514956e-03  5.4717045e-03
 -2.7537176e-03 -3.1672046e-03] 5 5
train:	 Loss = 1.3577,	 Acc = 0.5003
13230 0.279
25969 0.55
13401 0.61
1727 0.597
299 0.492
30 0.2
0 0.0
0 0.0
0.570849225124318
0.5857357107148808
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2851,	 Acc = 0.5495
1668 0.387
7898 0.568
3716 0.597
378 0.421
10 0.0
0 0.0
0 0.0
0 0.0
0.5720713214464256
0.5857357107148808
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 1
Testing:	 Loss = 1.3914,	 Acc1 = 0.3697,	 Acc2 = 0.3894

 ===== Epoch 22	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722  1.6826029   1.6104025
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.01232709 -0.21922052
 -0.00275372 -0.0031672 ] 2 5
train:	 Loss = 1.3577,	 Acc = 0.5043
13233 0.284
25968 0.555
13397 0.612
1729 0.597
299 0.528
30 0.267
0 0.0
0 0.0
0.5747048741037588
0.5857357107148808
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3187,	 Acc = 0.5372
1668 0.381
7898 0.552
3716 0.589
378 0.418
10 0.0
0 0.0
0 0.0
0 0.0
0.5589068488585236
0.5857357107148808
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 1
Testing:	 Loss = 1.4135,	 Acc1 = 0.3551,	 Acc2 = 0.3718

 ===== Epoch 23	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722  2.4064307   2.2992377
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249  -0.01554249 -0.05471371
 -0.00275372 -0.0031672 ] 2 3
train:	 Loss = 1.3556,	 Acc = 0.5021
13231 0.284
25967 0.551
13400 0.61
1729 0.6
299 0.518
30 0.2
0 0.0
0 0.0
0.5718527459263729
0.5857357107148808
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3203,	 Acc = 0.5032
1668 0.394
7898 0.541
3716 0.487
378 0.36
10 0.0
0 0.0
0 0.0
0 0.0
0.5184135977337111
0.5857357107148808
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 1
Testing:	 Loss = 1.3988,	 Acc1 = 0.3786,	 Acc2 = 0.4001

 ===== Epoch 24	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436  1.8335603   1.0038832
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849 -0.05486236  0.13436468
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 2
train:	 Loss = 1.3540,	 Acc = 0.5040
13232 0.282
25972 0.554
13394 0.613
1729 0.61
299 0.522
30 0.233
0 0.0
0 0.0
0.5748599845500193
0.5857357107148808
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2948,	 Acc = 0.5431
1668 0.389
7898 0.569
3716 0.566
378 0.476
10 0.0
0 0.0
0 0.0
0 0.0
0.5644892517913681
0.5857357107148808
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 1
Testing:	 Loss = 1.3967,	 Acc1 = 0.3646,	 Acc2 = 0.3832

 ===== Epoch 25	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
  1.3100181   1.4144312 ] [ 1.1263542e-03  2.0405303e-03  5.2204342e-03  4.7584870e-03
  2.7000366e-03  8.4708212e-04  6.0168497e-04  9.4006682e-04
  2.9140669e-03  7.7248965e-03  3.7514956e-03  5.4717045e-03
 -3.0500960e+00 -2.9059196e+00] 2 6
train:	 Loss = 1.3551,	 Acc = 0.5032
13228 0.284
25972 0.55
13399 0.613
1728 0.616
299 0.542
30 0.4
0 0.0
0 0.0
0.5732113546393743
0.5857357107148808
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3280,	 Acc = 0.5173
1668 0.39
7898 0.551
3716 0.516
378 0.397
10 0.0
0 0.0
0 0.0
0 0.0
0.5349941676387269
0.5857357107148808
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 1
Testing:	 Loss = 1.4217,	 Acc1 = 0.3588,	 Acc2 = 0.3762

 ===== Epoch 26	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436  2.1559334   2.8241148
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.14736345  0.10716666
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 3
train:	 Loss = 1.3572,	 Acc = 0.5019
13225 0.279
25974 0.551
13399 0.614
1729 0.602
299 0.518
30 0.233
0 0.0
0 0.0
0.5730733025995028
0.5857357107148808
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3270,	 Acc = 0.5368
1668 0.399
7898 0.563
3716 0.557
378 0.413
10 0.0
0 0.0
0 0.0
0 0.0
0.555990668221963
0.5857357107148808
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4122,	 Acc1 = 0.3747,	 Acc2 = 0.3954

 ===== Epoch 27	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084  2.0486944   2.2488368  -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [3.8657546e+00 1.3832475e+00 5.2204342e-03 4.7584870e-03 2.7000366e-03
 8.4708212e-04 6.0168497e-04 9.4006682e-04 7.2663307e-02 1.0365419e+00
 3.7514956e-03 5.4717045e-03 5.2671008e+00 2.2437153e+00] 6 6
train:	 Loss = 1.3519,	 Acc = 0.5038
13228 0.281
25970 0.553
13400 0.615
1729 0.603
299 0.552
30 0.267
0 0.0
0 0.0
0.5748768948537222
0.5857357107148808
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3221,	 Acc = 0.5279
1668 0.388
7898 0.565
3716 0.529
378 0.381
10 0.0
0 0.0
0 0.0
0 0.0
0.5472421263122813
0.5857357107148808
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4141,	 Acc1 = 0.3646,	 Acc2 = 0.3832

 ===== Epoch 28	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
  2.01176     3.1580958 ] [0.00112635 0.00204053 0.00522043 0.00475849 0.00270004 0.00084708
 0.00060168 0.00094007 0.00291407 0.0077249  0.0037515  0.0054717
 0.14562555 0.27610633] 4 4
train:	 Loss = 1.3561,	 Acc = 0.5023
13229 0.282
25971 0.551
13399 0.612
1729 0.602
298 0.55
30 0.4
0 0.0
0 0.0
0.5727424143674416
0.5857357107148808
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2818,	 Acc = 0.5678
1668 0.387
7898 0.586
3716 0.623
378 0.442
10 0.9
0 0.0
0 0.0
0 0.0
0.5929011831361439
0.5929011831361439
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 1
Testing:	 Loss = 1.3846,	 Acc1 = 0.3606,	 Acc2 = 0.3785

 ===== Epoch 29	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  1.3938648   3.3254886  -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.01163175  0.1175331   0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
train:	 Loss = 1.3529,	 Acc = 0.5037
13231 0.279
25967 0.554
13401 0.616
1728 0.591
299 0.545
30 0.267
0 0.0
0 0.0
0.5752806276403138
0.5929011831361439
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2830,	 Acc = 0.5601
1668 0.389
7898 0.574
3716 0.618
378 0.471
10 0.0
0 0.0
0 0.0
0 0.0
0.5839026828861856
0.5929011831361439
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3868,	 Acc1 = 0.3695,	 Acc2 = 0.3892

 ===== Epoch 30	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 3 1
train:	 Loss = 1.3510,	 Acc = 0.5048
13229 0.283
25973 0.555
13397 0.616
1728 0.595
299 0.528
30 0.333
0 0.0
0 0.0
0.5757839090448258
0.5929011831361439
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2879,	 Acc = 0.5566
1668 0.392
7898 0.581
3716 0.59
378 0.458
10 0.0
0 0.0
0 0.0
0 0.0
0.5794867522079653
0.5929011831361439
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 1
Testing:	 Loss = 1.3770,	 Acc1 = 0.3773,	 Acc2 = 0.3986

 ===== Epoch 31	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 5 1
train:	 Loss = 1.3529,	 Acc = 0.5027
13231 0.282
25966 0.552
13401 0.613
1729 0.6
299 0.508
30 0.267
0 0.0
0 0.0
0.5730838865419433
0.5929011831361439
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2973,	 Acc = 0.5565
1668 0.389
7898 0.583
3716 0.59
378 0.431
10 0.0
0 0.0
0 0.0
0 0.0
0.5797367105482419
0.5929011831361439
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 6
Testing:	 Loss = 1.3849,	 Acc1 = 0.3858,	 Acc2 = 0.4088

 ===== Epoch 32	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 3 1
train:	 Loss = 1.3537,	 Acc = 0.5035
13234 0.282
25967 0.554
13399 0.613
1727 0.596
299 0.515
30 0.2
0 0.0
0 0.0
0.5744049056057168
0.5929011831361439
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2918,	 Acc = 0.5583
1668 0.401
7898 0.574
3716 0.613
378 0.402
10 0.0
0 0.0
0 0.0
0 0.0
0.5801533077820363
0.5929011831361439
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3939,	 Acc1 = 0.3625,	 Acc2 = 0.3807

 ===== Epoch 33	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 5 1
train:	 Loss = 1.3551,	 Acc = 0.5035
13232 0.283
25968 0.551
13400 0.615
1728 0.603
298 0.54
30 0.3
0 0.0
0 0.0
0.5738219389725763
0.5929011831361439
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3090,	 Acc = 0.5309
1668 0.387
7898 0.559
3716 0.547
378 0.431
10 0.0
0 0.0
0 0.0
0 0.0
0.5509081819696717
0.5929011831361439
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3931,	 Acc1 = 0.3747,	 Acc2 = 0.3954

 ===== Epoch 34	 =====
[-0.3673141  -0.38194367  2.6133177  -4.116409    2.4461837   1.0194218
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 1.1263542e-03  2.0405303e-03  2.5371045e-01  7.4795780e+00
 -1.8505683e-02  3.5462756e-02  6.0168497e-04  9.4006682e-04
  2.9140669e-03  7.7248965e-03  3.7514956e-03  5.4717045e-03
 -2.7537176e-03 -3.1672046e-03] 2 3
train:	 Loss = 1.3517,	 Acc = 0.5038
13233 0.282
25969 0.553
13397 0.614
1728 0.615
299 0.508
30 0.233
0 0.0
0 0.0
0.5747772976365787
0.5929011831361439
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3119,	 Acc = 0.5402
1668 0.348
7898 0.562
3716 0.589
378 0.455
10 0.0
0 0.0
0 0.0
0 0.0
0.5668221963006166
0.5929011831361439
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4033,	 Acc1 = 0.3637,	 Acc2 = 0.3822

 ===== Epoch 35	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  1.2426009   2.8916807  -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.06678829 -0.805495    0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 5 5
train:	 Loss = 1.3536,	 Acc = 0.5026
13233 0.276
25968 0.553
13398 0.616
1729 0.607
298 0.54
30 0.267
0 0.0
0 0.0
0.5751876976558916
0.5929011831361439
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3469,	 Acc = 0.5051
1668 0.397
7898 0.549
3716 0.472
378 0.413
10 0.0
0 0.0
0 0.0
0 0.0
0.5201633061156474
0.5929011831361439
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4193,	 Acc1 = 0.3924,	 Acc2 = 0.4167

 ===== Epoch 36	 =====
[-0.3673141  -0.38194367  2.0350077   1.0824759  -0.44182304 -0.36572987
  1.4124906   3.815271   -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 1.1263542e-03  2.0405303e-03  1.4560613e-01  1.1034182e-01
  2.7000366e-03  8.4708212e-04 -3.1554019e+00 -7.2666926e+00
  2.9140669e-03  7.7248965e-03  3.7514956e-03  5.4717045e-03
 -2.7537176e-03 -3.1672046e-03] 4 2
train:	 Loss = 1.3525,	 Acc = 0.5052
13231 0.283
25974 0.556
13394 0.614
1729 0.6
299 0.545
29 0.207
0 0.0
0 0.0
0.576077248038624
0.5929011831361439
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3028,	 Acc = 0.5231
1668 0.388
7898 0.557
3716 0.523
378 0.41
10 0.0
0 0.0
0 0.0
0 0.0
0.5418263622729546
0.5929011831361439
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 1
Testing:	 Loss = 1.3945,	 Acc1 = 0.3712,	 Acc2 = 0.3912

 ===== Epoch 37	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 1 1
train:	 Loss = 1.3527,	 Acc = 0.5031
13230 0.282
25969 0.551
13399 0.615
1729 0.604
299 0.528
30 0.233
0 0.0
0 0.0
0.5736011200695216
0.5929011831361439
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 2
val:	 Loss = 1.4134,	 Acc = 0.5001
1668 0.388
7898 0.551
3716 0.457
378 0.37
10 0.0
0 0.0
0 0.0
0 0.0
0.5156640559906682
0.5929011831361439
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4758,	 Acc1 = 0.3934,	 Acc2 = 0.4180

 ===== Epoch 38	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084  1.2829286   2.4596515  -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007 -0.06306478  0.30779654  0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
train:	 Loss = 1.3521,	 Acc = 0.5044
13224 0.283
25974 0.554
13400 0.613
1729 0.608
299 0.542
30 0.167
0 0.0
0 0.0
0.574942073759413
0.5929011831361439
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3444,	 Acc = 0.5175
1668 0.391
7898 0.552
3716 0.512
378 0.413
10 0.0
0 0.0
0 0.0
0 0.0
0.5350774870854857
0.5929011831361439
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4135,	 Acc1 = 0.3899,	 Acc2 = 0.4138

 ===== Epoch 39	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722  1.1371858   3.154766
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0780689  -0.2633565
 -0.00275372 -0.0031672 ] 5 5
train:	 Loss = 1.3519,	 Acc = 0.5029
13234 0.281
25969 0.553
13396 0.612
1728 0.606
299 0.495
30 0.2
0 0.0
0 0.0
0.5738255033557047
0.5929011831361439
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3093,	 Acc = 0.5257
1668 0.363
7898 0.564
3716 0.531
378 0.413
10 0.0
0 0.0
0 0.0
0 0.0
0.5483252791201466
0.5929011831361439
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3895,	 Acc1 = 0.3891,	 Acc2 = 0.4128

 ===== Epoch 40	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 6 1
train:	 Loss = 1.3519,	 Acc = 0.5038
13233 0.282
25974 0.556
13394 0.61
1726 0.593
299 0.535
30 0.2
0 0.0
0 0.0
0.5745600270381189
0.5929011831361439
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2727,	 Acc = 0.5710
1668 0.389
7898 0.59
3716 0.627
378 0.442
10 0.0
0 0.0
0 0.0
0 0.0
0.596233961006499
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 1
Testing:	 Loss = 1.3642,	 Acc1 = 0.3790,	 Acc2 = 0.4006

 ===== Epoch 41	 =====
[ 2.6442876   2.252753   -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
  1.9866987   2.477386  ] [-3.1536210e+00 -2.9532323e+00  5.2204342e-03  4.7584870e-03
  2.7000366e-03  8.4708212e-04  6.0168497e-04  9.4006682e-04
  2.9140669e-03  7.7248965e-03  3.7514956e-03  5.4717045e-03
  7.3802523e-02  1.3223815e-01] 1 4
train:	 Loss = 1.3509,	 Acc = 0.5053
13229 0.284
25972 0.554
13397 0.616
1729 0.604
299 0.518
30 0.3
0 0.0
0 0.0
0.5759770198179931
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3048,	 Acc = 0.5281
1668 0.379
7898 0.557
3716 0.55
378 0.381
10 0.0
0 0.0
0 0.0
0 0.0
0.548741876353941
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4000,	 Acc1 = 0.3722,	 Acc2 = 0.3924

 ===== Epoch 42	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 1
train:	 Loss = 1.3492,	 Acc = 0.5034
13231 0.283
25970 0.553
13399 0.613
1727 0.593
299 0.535
30 0.167
0 0.0
0 0.0
0.5738563669281834
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3156,	 Acc = 0.5124
1668 0.387
7898 0.548
3716 0.505
378 0.405
10 0.0
0 0.0
0 0.0
0 0.0
0.5297450424929179
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 1
Testing:	 Loss = 1.4089,	 Acc1 = 0.3639,	 Acc2 = 0.3825

 ===== Epoch 43	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 6 1
train:	 Loss = 1.3493,	 Acc = 0.5055
13229 0.286
25971 0.556
13399 0.613
1729 0.603
298 0.54
30 0.267
0 0.0
0 0.0
0.5757839090448258
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3285,	 Acc = 0.4985
1668 0.388
7898 0.534
3716 0.484
378 0.405
10 0.0
0 0.0
0 0.0
0 0.0
0.513831028161973
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 1
Testing:	 Loss = 1.4158,	 Acc1 = 0.3584,	 Acc2 = 0.3757

 ===== Epoch 44	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 2 1
train:	 Loss = 1.3504,	 Acc = 0.5048
13230 0.284
25973 0.554
13397 0.615
1727 0.604
299 0.502
30 0.233
0 0.0
0 0.0
0.5752184618355622
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2913,	 Acc = 0.5326
1668 0.388
7898 0.56
3716 0.55
378 0.431
10 0.0
0 0.0
0 0.0
0 0.0
0.5525745709048492
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 1
Testing:	 Loss = 1.3857,	 Acc1 = 0.3648,	 Acc2 = 0.3834

 ===== Epoch 45	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 1 1
train:	 Loss = 1.3525,	 Acc = 0.5040
13232 0.281
25972 0.555
13394 0.613
1729 0.596
299 0.535
30 0.233
0 0.0
0 0.0
0.5751738122827347
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 2
val:	 Loss = 1.3199,	 Acc = 0.5031
1668 0.37
7898 0.544
3716 0.486
378 0.413
10 0.0
0 0.0
0 0.0
0 0.0
0.5216630561573071
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4121,	 Acc1 = 0.3650,	 Acc2 = 0.3837

 ===== Epoch 46	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436  1.6102321   0.8262997
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 1.1263542e-03  2.0405303e-03  5.2204342e-03  4.7584870e-03
 -4.0615706e+00 -1.3269112e+00  6.0168497e-04  9.4006682e-04
  2.9140669e-03  7.7248965e-03  3.7514956e-03  5.4717045e-03
 -2.7537176e-03 -3.1672046e-03] 5 5
train:	 Loss = 1.3502,	 Acc = 0.5052
13225 0.283
25971 0.556
13403 0.614
1728 0.597
299 0.542
30 0.267
0 0.0
0 0.0
0.5759455480195989
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3474,	 Acc = 0.5009
1668 0.341
7898 0.547
3716 0.485
378 0.397
10 0.0
0 0.0
0 0.0
0 0.0
0.523079486752208
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4300,	 Acc1 = 0.3613,	 Acc2 = 0.3792

 ===== Epoch 47	 =====
[-0.3673141  -0.38194367  1.6049396   0.89185774 -0.44182304 -0.36572987
  2.97424     3.387061   -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 1.1263542e-03  2.0405303e-03 -3.7282031e+00 -1.2934366e+00
  2.7000366e-03  8.4708212e-04  1.6907528e-01 -5.6259292e-01
  2.9140669e-03  7.7248965e-03  3.7514956e-03  5.4717045e-03
 -2.7537176e-03 -3.1672046e-03] 5 5
train:	 Loss = 1.3518,	 Acc = 0.5054
13230 0.278
25969 0.559
13400 0.615
1728 0.594
299 0.515
30 0.3
0 0.0
0 0.0
0.5778496596340462
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 2
val:	 Loss = 1.3608,	 Acc = 0.4909
1668 0.389
7898 0.535
3716 0.455
378 0.381
10 0.0
0 0.0
0 0.0
0 0.0
0.5050824862522912
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4292,	 Acc1 = 0.3850,	 Acc2 = 0.4078

 ===== Epoch 48	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084  1.266319    2.987928   -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00197293  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 0 0
train:	 Loss = 1.3488,	 Acc = 0.5057
13230 0.283
25971 0.555
13397 0.616
1729 0.603
299 0.548
30 0.233
0 0.0
0 0.0
0.5766185487375078
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3298,	 Acc = 0.5067
1668 0.389
7898 0.542
3716 0.495
378 0.421
10 0.0
0 0.0
0 0.0
0 0.0
0.523079486752208
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4267,	 Acc1 = 0.3514,	 Acc2 = 0.3673

 ===== Epoch 49	 =====
[-0.3673141  -0.38194367  1.6656221   2.8161938  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00071589  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 0 0
train:	 Loss = 1.3507,	 Acc = 0.5030
13233 0.279
25969 0.554
13398 0.612
1727 0.609
299 0.508
30 0.267
0 0.0
0 0.0
0.574415179972479
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2955,	 Acc = 0.5500
1668 0.385
7898 0.565
3716 0.608
378 0.415
10 0.0
0 0.0
0 0.0
0 0.0
0.5729045159140144
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3985,	 Acc1 = 0.3672,	 Acc2 = 0.3864

 ===== Epoch 50	 =====
[-0.3673141  -0.38194367  1.5788751   1.1278613  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.38515675  0.18952931  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 1 1
train:	 Loss = 1.3547,	 Acc = 0.5063
13232 0.282
25969 0.557
13397 0.616
1729 0.6
299 0.545
30 0.233
0 0.0
0 0.0
0.5778292777134029
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2951,	 Acc = 0.5462
1668 0.393
7898 0.564
3716 0.59
378 0.437
10 0.0
0 0.0
0 0.0
0 0.0
0.5674054324279286
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 1
Testing:	 Loss = 1.3856,	 Acc1 = 0.3773,	 Acc2 = 0.3986

 ===== Epoch 51	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  1.6083446   2.6733778  -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.01664767 -0.08650471  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 5 2
train:	 Loss = 1.3499,	 Acc = 0.5051
13232 0.28
25972 0.555
13395 0.618
1729 0.601
298 0.55
30 0.2
0 0.0
0 0.0
0.5769843568945539
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3037,	 Acc = 0.5453
1668 0.388
7898 0.572
3716 0.57
378 0.447
10 0.0
0 0.0
0 0.0
0 0.0
0.5671554740876521
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 1
Testing:	 Loss = 1.3875,	 Acc1 = 0.3802,	 Acc2 = 0.4021

 ===== Epoch 52	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084  1.3997706   0.8698614  -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.60049415  0.40132537  0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 2 2
train:	 Loss = 1.3498,	 Acc = 0.5049
13233 0.282
25966 0.556
13401 0.613
1727 0.606
299 0.525
30 0.133
0 0.0
0 0.0
0.5762016271153707
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3389,	 Acc = 0.5124
1668 0.388
7898 0.545
3716 0.505
378 0.468
10 0.0
0 0.0
0 0.0
0 0.0
0.5295784035994001
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4249,	 Acc1 = 0.3738,	 Acc2 = 0.3944

 ===== Epoch 53	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084  2.1947453   1.0335526  -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 1.1263542e-03  2.0405303e-03  5.2204342e-03  4.7584870e-03
  2.7000366e-03  8.4708212e-04  6.0168497e-04  9.4006682e-04
  6.2971318e-01  1.1684186e-01  3.7514956e-03  5.4717045e-03
 -2.7537176e-03 -3.1672046e-03] 2 2
train:	 Loss = 1.3485,	 Acc = 0.5070
13231 0.285
25971 0.556
13397 0.617
1728 0.62
299 0.538
30 0.267
0 0.0
0 0.0
0.5778636089318044
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2739,	 Acc = 0.5638
1668 0.388
7898 0.572
3716 0.64
378 0.423
10 0.0
0 0.0
0 0.0
0 0.0
0.5881519746708882
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 1
Testing:	 Loss = 1.3878,	 Acc1 = 0.3544,	 Acc2 = 0.3710

 ===== Epoch 54	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722  0.6724873   3.2503698
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.19812283 -0.17107217
 -0.00275372 -0.0031672 ] 1 2
train:	 Loss = 1.3518,	 Acc = 0.5053
13231 0.282
25967 0.557
13400 0.616
1729 0.591
299 0.502
30 0.2
0 0.0
0 0.0
0.5767048883524442
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3167,	 Acc = 0.5372
1668 0.357
7898 0.572
3716 0.555
378 0.439
10 0.0
0 0.0
0 0.0
0 0.0
0.5621563072821196
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4042,	 Acc1 = 0.3672,	 Acc2 = 0.3864

 ===== Epoch 55	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722  1.6890149   1.8481854
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.5275565   0.10979309
 -0.00275372 -0.0031672 ] 2 2
train:	 Loss = 1.3500,	 Acc = 0.5034
13230 0.278
25973 0.556
13395 0.614
1729 0.593
299 0.482
30 0.2
0 0.0
0 0.0
0.5752908801235939
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2867,	 Acc = 0.5538
1668 0.393
7898 0.577
3716 0.591
378 0.437
10 0.0
0 0.0
0 0.0
0 0.0
0.5761539743376104
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3795,	 Acc1 = 0.3763,	 Acc2 = 0.3974

 ===== Epoch 56	 =====
[-0.3673141  -0.38194367  3.0132487   1.4659817  -0.44182304 -0.36572987
  1.713891    3.2499218  -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.65691906  0.3599024   0.00270004  0.00084708
  0.08684578  0.7199304   0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 6 4
train:	 Loss = 1.3482,	 Acc = 0.5051
13229 0.282
25973 0.554
13398 0.617
1728 0.609
298 0.507
30 0.3
0 0.0
0 0.0
0.5762184082844521
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3273,	 Acc = 0.5200
1668 0.381
7898 0.564
3716 0.507
378 0.362
10 0.0
0 0.0
0 0.0
0 0.0
0.5393267788701883
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4185,	 Acc1 = 0.3782,	 Acc2 = 0.3996

 ===== Epoch 57	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436  2.1911154   2.8796096
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 1.1263542e-03  2.0405303e-03  5.2204342e-03  4.7584870e-03
 -5.2120600e+00 -3.6140180e+00  6.0168497e-04  9.4006682e-04
  2.9140669e-03  7.7248965e-03  3.7514956e-03  5.4717045e-03
 -2.7537176e-03 -3.1672046e-03] 4 6
train:	 Loss = 1.3481,	 Acc = 0.5055
13231 0.284
25970 0.556
13400 0.612
1727 0.613
298 0.527
30 0.2
0 0.0
0 0.0
0.5763186481593241
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3111,	 Acc = 0.5435
1668 0.388
7898 0.574
3716 0.569
378 0.365
10 0.0
0 0.0
0 0.0
0 0.0
0.5650724879186803
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 1
Testing:	 Loss = 1.4152,	 Acc1 = 0.3627,	 Acc2 = 0.3810

 ===== Epoch 58	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084  1.0595579   2.8887212  -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 1.1263542e-03  2.0405303e-03  5.2204342e-03  4.7584870e-03
  2.7000366e-03  8.4708212e-04  6.0168497e-04  9.4006682e-04
 -6.1634719e-01  4.2470756e-01  3.7514956e-03  5.4717045e-03
 -2.7537176e-03 -3.1672046e-03] 1 1
train:	 Loss = 1.3521,	 Acc = 0.5046
13229 0.283
25972 0.555
13398 0.614
1729 0.602
298 0.537
30 0.3
0 0.0
0 0.0
0.5753735486518454
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3487,	 Acc = 0.5271
1668 0.362
7898 0.562
3716 0.544
378 0.362
10 0.0
0 0.0
0 0.0
0 0.0
0.550074987502083
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4190,	 Acc1 = 0.3893,	 Acc2 = 0.4130

 ===== Epoch 59	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 5 1
train:	 Loss = 1.3505,	 Acc = 0.5047
13230 0.282
25972 0.555
13399 0.614
1727 0.605
298 0.513
30 0.233
0 0.0
0 0.0
0.5757253898517839
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2782,	 Acc = 0.5485
1668 0.388
7898 0.574
3716 0.58
378 0.421
10 0.0
0 0.0
0 0.0
0 0.0
0.5707382102982836
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 1
Testing:	 Loss = 1.3777,	 Acc1 = 0.3689,	 Acc2 = 0.3884

 ===== Epoch 60	 =====
[ 1.5377067   1.1831976  -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
  0.9692597   1.4772661 ] [-1.9944427e+00 -1.7535388e+00  5.2204342e-03  4.7584870e-03
  2.7000366e-03  8.4708212e-04  6.0168497e-04  9.4006682e-04
  2.9140669e-03  7.7248965e-03  3.7514956e-03  5.4717045e-03
  1.0300700e-01 -1.8934956e-01] 3 5
train:	 Loss = 1.3511,	 Acc = 0.5052
13232 0.282
25967 0.556
13401 0.615
1727 0.605
299 0.515
30 0.2
0 0.0
0 0.0
0.5765981073773658
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2737,	 Acc = 0.5560
1668 0.385
7898 0.571
3716 0.615
378 0.415
10 0.0
0 0.0
0 0.0
0 0.0
0.5796533911014831
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 1
Testing:	 Loss = 1.3790,	 Acc1 = 0.3631,	 Acc2 = 0.3815

 ===== Epoch 61	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.0406892   2.312338   -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 1.1263542e-03  2.0405303e-03  5.2204342e-03  4.7584870e-03
  2.7000366e-03  8.4708212e-04 -1.6586617e-01 -6.7918593e-01
  2.9140669e-03  7.7248965e-03  5.2268481e+00  5.9798770e+00
 -2.7537176e-03 -3.1672046e-03] 5 5
train:	 Loss = 1.3477,	 Acc = 0.5056
13230 0.283
25969 0.556
13400 0.616
1728 0.606
299 0.535
30 0.233
0 0.0
0 0.0
0.5767875247429151
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3078,	 Acc = 0.5423
1668 0.399
7898 0.573
3716 0.559
378 0.389
10 0.0
0 0.0
0 0.0
0 0.0
0.5622396267288785
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3954,	 Acc1 = 0.3895,	 Acc2 = 0.4133

 ===== Epoch 62	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436  1.2752396   1.6809205
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849 -0.36312383  0.14919996
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 1 1
train:	 Loss = 1.3511,	 Acc = 0.5047
13233 0.285
25968 0.554
13398 0.613
1728 0.605
299 0.502
30 0.233
0 0.0
0 0.0
0.574825579991792
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2927,	 Acc = 0.5538
1668 0.391
7898 0.577
3716 0.592
378 0.421
10 0.0
0 0.0
0 0.0
0 0.0
0.5763206132311282
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3773,	 Acc1 = 0.3868,	 Acc2 = 0.4100

 ===== Epoch 63	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436  0.37733713  1.8451853
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.03451213  0.43107042
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 6 4
train:	 Loss = 1.3495,	 Acc = 0.5068
13231 0.282
25970 0.559
13397 0.615
1729 0.604
299 0.535
30 0.233
0 0.0
0 0.0
0.5785153892576946
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3167,	 Acc = 0.5174
1668 0.385
7898 0.543
3716 0.532
378 0.431
10 0.0
0 0.0
0 0.0
0 0.0
0.5357440426595568
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 1
Testing:	 Loss = 1.4205,	 Acc1 = 0.3433,	 Acc2 = 0.3576

 ===== Epoch 64	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 1 1
train:	 Loss = 1.3464,	 Acc = 0.5067
13226 0.285
25972 0.558
13400 0.614
1729 0.607
299 0.518
30 0.2
0 0.0
0 0.0
0.5775766352884383
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3188,	 Acc = 0.5333
1668 0.393
7898 0.566
3716 0.55
378 0.323
10 0.0
0 0.0
0 0.0
0 0.0
0.5527412097983669
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 1
Testing:	 Loss = 1.4094,	 Acc1 = 0.3765,	 Acc2 = 0.3976

 ===== Epoch 65	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436  1.0278201   2.6842678
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849 -0.34418923  0.17887053
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 1 1
train:	 Loss = 1.3496,	 Acc = 0.5044
13225 0.283
25971 0.555
13403 0.614
1728 0.592
299 0.505
30 0.233
0 0.0
0 0.0
0.5751731795032705
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3463,	 Acc = 0.5034
1668 0.387
7898 0.54
3716 0.483
378 0.476
10 0.0
0 0.0
0 0.0
0 0.0
0.5196633894350942
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4348,	 Acc1 = 0.3534,	 Acc2 = 0.3698

 ===== Epoch 66	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436  1.6308819   2.6332126
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 1.1263542e-03  2.0405303e-03  5.2204342e-03  4.7584870e-03
 -4.1024690e+00 -3.3395650e+00  6.0168497e-04  9.4006682e-04
  1.7175106e+00  2.6148410e+00  3.7514956e-03  5.4717045e-03
 -2.7537176e-03 -3.1672046e-03] 6 6
train:	 Loss = 1.3521,	 Acc = 0.5033
13232 0.282
25969 0.553
13400 0.614
1726 0.594
299 0.518
30 0.233
0 0.0
0 0.0
0.5741116261104674
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3138,	 Acc = 0.5312
1668 0.377
7898 0.565
3716 0.542
378 0.418
10 0.0
0 0.0
0 0.0
0 0.0
0.5525745709048492
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3850,	 Acc1 = 0.3891,	 Acc2 = 0.4128

 ===== Epoch 67	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  3.7012117   1.0389032  -0.40214875 -0.40990722  2.9763658   2.1178362
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.29643875  0.28270656  0.00291407  0.0077249   0.05020154  0.17399088
 -0.00275372 -0.0031672 ] 4 2
train:	 Loss = 1.3508,	 Acc = 0.5047
13231 0.286
25970 0.555
13399 0.608
1727 0.612
299 0.542
30 0.333
0 0.0
0 0.0
0.5746288473144237
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2849,	 Acc = 0.5571
1668 0.371
7898 0.582
3716 0.603
378 0.41
10 0.0
0 0.0
0 0.0
0 0.0
0.5829028495250792
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3793,	 Acc1 = 0.3726,	 Acc2 = 0.3929

 ===== Epoch 68	 =====
[ 1.2109792   2.2830951  -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
  1.1521709   2.597819  ] [-1.6521854e+00 -2.9872661e+00  5.2204342e-03  4.7584870e-03
  2.7000366e-03  8.4708212e-04  6.0168497e-04  9.4006682e-04
  2.9140669e-03  7.7248965e-03  3.7514956e-03  5.4717045e-03
 -1.8822777e-01  2.2532934e-01] 4 1
train:	 Loss = 1.3507,	 Acc = 0.5061
13232 0.28
25967 0.558
13400 0.617
1728 0.605
299 0.518
30 0.267
0 0.0
0 0.0
0.5784327925840093
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3292,	 Acc = 0.5424
1668 0.36
7898 0.574
3716 0.573
378 0.389
10 0.0
0 0.0
0 0.0
0 0.0
0.5676553907682053
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4097,	 Acc1 = 0.3848,	 Acc2 = 0.4076

 ===== Epoch 69	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722  1.7836894   1.218183
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.2781595  -0.03465191
 -0.00275372 -0.0031672 ] 1 3
train:	 Loss = 1.3545,	 Acc = 0.5028
13230 0.274
25966 0.555
13402 0.613
1729 0.607
299 0.532
30 0.167
0 0.0
0 0.0
0.5757495292811278
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3244,	 Acc = 0.5080
1668 0.343
7898 0.552
3716 0.501
378 0.394
10 0.0
0 0.0
0 0.0
0 0.0
0.5309115147475421
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4186,	 Acc1 = 0.3588,	 Acc2 = 0.3762

 ===== Epoch 70	 =====
[ 2.83795     2.4879034  -0.4204066  -0.33581436 -0.44182304 -0.36572987
  1.2155075   2.471867   -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [-1.2636123e+00  4.7419000e-02  5.2204342e-03  4.7584870e-03
  2.7000366e-03  8.4708212e-04  1.5002532e-01 -5.2498404e-02
  2.9140669e-03  7.7248965e-03  3.7514956e-03  5.4717045e-03
 -2.7537176e-03 -3.1672046e-03] 4 2
train:	 Loss = 1.3541,	 Acc = 0.5025
13228 0.275
25973 0.554
13398 0.614
1728 0.615
299 0.518
30 0.2
0 0.0
0 0.0
0.5752389688133629
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3240,	 Acc = 0.5493
1668 0.389
7898 0.556
3716 0.618
378 0.45
10 0.0
0 0.0
0 0.0
0 0.0
0.5715714047658723
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4201,	 Acc1 = 0.3499,	 Acc2 = 0.3656

 ===== Epoch 71	 =====
[ 1.7295998   2.9784324  -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
  1.5786672   3.2759109 ] [-4.7759555e-02 -5.3115642e-01  5.2204342e-03  4.7584870e-03
  2.7000366e-03  8.4708212e-04  6.0168497e-04  9.4006682e-04
  2.9140669e-03  7.7248965e-03  3.7514956e-03  5.4717045e-03
 -8.0098011e-02 -8.2406217e-01] 5 5
train:	 Loss = 1.3516,	 Acc = 0.5044
13226 0.284
25971 0.557
13402 0.608
1728 0.602
299 0.542
30 0.3
0 0.0
0 0.0
0.5746319092445088
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3151,	 Acc = 0.5337
1668 0.4
7898 0.572
3716 0.53
378 0.373
10 0.0
0 0.0
0 0.0
0 0.0
0.5522412931178137
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4064,	 Acc1 = 0.3771,	 Acc2 = 0.3984

 ===== Epoch 72	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
  2.06848     2.705162  ] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.06431491  0.02645272] 3 3
train:	 Loss = 1.3523,	 Acc = 0.5040
13230 0.281
25973 0.553
13395 0.617
1729 0.602
299 0.522
30 0.267
0 0.0
0 0.0
0.5752426012649061
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3244,	 Acc = 0.5436
1668 0.402
7898 0.558
3716 0.592
378 0.407
10 0.0
0 0.0
0 0.0
0 0.0
0.5633227795367439
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4309,	 Acc1 = 0.3464,	 Acc2 = 0.3613

 ===== Epoch 73	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 1
train:	 Loss = 1.3510,	 Acc = 0.5057
13231 0.28
25969 0.558
13399 0.616
1728 0.595
299 0.495
30 0.233
0 0.0
0 0.0
0.5778636089318044
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3285,	 Acc = 0.5020
1668 0.381
7898 0.543
3716 0.486
378 0.357
10 0.0
0 0.0
0 0.0
0 0.0
0.5189135144142643
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 1
Testing:	 Loss = 1.4201,	 Acc1 = 0.3606,	 Acc2 = 0.3785

 ===== Epoch 74	 =====
[ 3.0804343   1.5953431  -0.4204066  -0.33581436 -0.44182304 -0.36572987
  1.5693988   1.6602274  -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [-3.6104984e+00 -2.2158320e+00  5.2204342e-03  4.7584870e-03
  2.7000366e-03  8.4708212e-04 -8.1630908e-02 -6.5489578e-01
  2.9140669e-03  7.7248965e-03  4.5801330e+00  6.0320377e+00
 -2.7537176e-03 -3.1672046e-03] 0 5
train:	 Loss = 1.3533,	 Acc = 0.5027
13229 0.283
25972 0.553
13397 0.611
1729 0.599
299 0.505
30 0.233
0 0.0
0 0.0
0.5730079416805465
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 2
val:	 Loss = 1.3370,	 Acc = 0.4996
1668 0.384
7898 0.538
3716 0.489
378 0.323
10 0.0
0 0.0
0 0.0
0 0.0
0.5157473754374271
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4194,	 Acc1 = 0.3722,	 Acc2 = 0.3924

 ===== Epoch 75	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
  1.9405315   3.5455768 ] [ 1.1263542e-03  2.0405303e-03  5.2204342e-03  4.7584870e-03
  2.7000366e-03  8.4708212e-04  6.0168497e-04  9.4006682e-04
  2.9140669e-03  7.7248965e-03  3.7514956e-03  5.4717045e-03
 -4.1818733e+00 -6.3502932e+00] 2 6
train:	 Loss = 1.3495,	 Acc = 0.5059
13232 0.281
25967 0.557
13401 0.616
1727 0.604
299 0.528
30 0.2
0 0.0
0 0.0
0.5775395905755117
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3162,	 Acc = 0.5505
1668 0.384
7898 0.562
3716 0.615
378 0.423
10 0.0
0 0.0
0 0.0
0 0.0
0.5735710714880853
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4264,	 Acc1 = 0.3493,	 Acc2 = 0.3648

 ===== Epoch 76	 =====
[-0.3673141  -0.38194367  1.7788403   2.8207326  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 1.1263542e-03  2.0405303e-03 -4.0487633e+00 -3.3331144e+00
  2.7000366e-03  8.4708212e-04  6.0168497e-04  9.4006682e-04
  2.9140669e-03  7.7248965e-03  3.7514956e-03  5.4717045e-03
 -2.7537176e-03 -3.1672046e-03] 1 1
train:	 Loss = 1.3486,	 Acc = 0.5044
13231 0.284
25970 0.554
13400 0.614
1727 0.598
298 0.52
30 0.133
0 0.0
0 0.0
0.5746529873264936
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2849,	 Acc = 0.5530
1668 0.363
7898 0.586
3716 0.583
378 0.429
10 0.0
0 0.0
0 0.0
0 0.0
0.5793201133144475
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3733,	 Acc1 = 0.3912,	 Acc2 = 0.4153

 ===== Epoch 77	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084  1.3132846   3.2557867  -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.31490043  0.15581219  0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
train:	 Loss = 1.3490,	 Acc = 0.5056
13230 0.284
25973 0.556
13395 0.615
1729 0.6
299 0.535
30 0.333
0 0.0
0 0.0
0.5764737121614445
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3190,	 Acc = 0.5311
1668 0.385
7898 0.567
3716 0.53
378 0.423
10 0.9
0 0.0
0 0.0
0 0.0
0.5513247792034661
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4024,	 Acc1 = 0.3697,	 Acc2 = 0.3894

 ===== Epoch 78	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436  2.638537    2.9595222
 -0.36378932 -0.37167084  1.8132949   2.3034005  -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.516217    0.00084708
  0.00060168  0.00094007  0.19519503  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 0 0
train:	 Loss = 1.3495,	 Acc = 0.5059
13232 0.283
25970 0.556
13396 0.615
1729 0.604
299 0.545
30 0.3
0 0.0
0 0.0
0.5770567786790266
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2898,	 Acc = 0.5576
1668 0.388
7898 0.575
3716 0.613
378 0.421
10 0.0
0 0.0
0 0.0
0 0.0
0.5810698216963839
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 1
Testing:	 Loss = 1.3732,	 Acc1 = 0.3852,	 Acc2 = 0.4081

 ===== Epoch 79	 =====
[-0.3673141  -0.38194367  0.11925152  1.8631029  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.04200635 -0.30719224  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 5 5
train:	 Loss = 1.3490,	 Acc = 0.5049
13233 0.284
25966 0.555
13399 0.615
1729 0.595
299 0.528
30 0.167
0 0.0
0 0.0
0.5753084035439249
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3182,	 Acc = 0.5549
1668 0.39
7898 0.583
3716 0.582
378 0.439
10 0.0
0 0.0
0 0.0
0 0.0
0.577737043826029
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4134,	 Acc1 = 0.3687,	 Acc2 = 0.3882

 ===== Epoch 80	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 5 1
train:	 Loss = 1.3483,	 Acc = 0.5067
13232 0.284
25969 0.557
13398 0.615
1728 0.616
299 0.518
30 0.167
0 0.0
0 0.0
0.5779016994978756
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3324,	 Acc = 0.5056
1668 0.365
7898 0.525
3716 0.536
378 0.431
10 0.0
0 0.0
0 0.0
0 0.0
0.5251624729211798
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4146,	 Acc1 = 0.3679,	 Acc2 = 0.3872

 ===== Epoch 81	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 1.1263542e-03  2.0405303e-03  5.2204342e-03  4.7584870e-03
  2.7000366e-03  8.4708212e-04  7.4677806e+00  2.5174065e+00
  2.9140669e-03  7.7248965e-03  5.4047837e+00  4.5434523e+00
 -2.7537176e-03 -3.1672046e-03] 2 2
train:	 Loss = 1.3558,	 Acc = 0.5014
13232 0.27
25974 0.554
13392 0.614
1729 0.606
299 0.525
30 0.267
0 0.0
0 0.0
0.5754876400154499
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3301,	 Acc = 0.5260
1668 0.384
7898 0.56
3716 0.526
378 0.458
10 0.0
0 0.0
0 0.0
0 0.0
0.5457423762706216
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4239,	 Acc1 = 0.3625,	 Acc2 = 0.3807

 ===== Epoch 82	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436  1.589964    1.9206582
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.03148241 -0.13019796
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 5 2
train:	 Loss = 1.3493,	 Acc = 0.5076
13228 0.285
25972 0.558
13399 0.618
1728 0.6
299 0.528
30 0.3
0 0.0
0 0.0
0.578811431881819
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3100,	 Acc = 0.5189
1668 0.391
7898 0.552
3716 0.514
378 0.437
10 0.5
0 0.0
0 0.0
0 0.0
0.5366605565739043
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4184,	 Acc1 = 0.3456,	 Acc2 = 0.3603

 ===== Epoch 83	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  1.3379874   1.6098498  -0.40214875 -0.40990722  2.5260012   1.218183
  3.7045598   3.0795524 ] [ 1.1263542e-03  2.0405303e-03  5.2204342e-03  4.7584870e-03
  2.7000366e-03  8.4708212e-04  2.7677743e-02  1.6186683e+00
  2.9140669e-03  7.7248965e-03 -2.6422593e-01  1.2693654e+00
 -7.3483200e+00 -5.5971007e+00] 0 6
train:	 Loss = 1.3480,	 Acc = 0.5048
13230 0.282
25971 0.554
13397 0.616
1729 0.618
299 0.472
30 0.167
0 0.0
0 0.0
0.5758460869985034
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3128,	 Acc = 0.5309
1668 0.394
7898 0.566
3716 0.532
378 0.418
10 0.0
0 0.0
0 0.0
0 0.0
0.5499916680553241
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 6
Testing:	 Loss = 1.4044,	 Acc1 = 0.3755,	 Acc2 = 0.3964

 ===== Epoch 84	 =====
[ 2.198478    2.151613   -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [-3.0798608e-01 -6.0489649e-01  5.2204342e-03  4.7584870e-03
  2.7000366e-03  8.4708212e-04  6.0168497e-04  9.4006682e-04
  2.9140669e-03  7.7248965e-03  3.7514956e-03  5.4717045e-03
 -2.7537176e-03 -3.1672046e-03] 5 5
train:	 Loss = 1.3485,	 Acc = 0.5051
13232 0.283
25970 0.555
13397 0.615
1728 0.608
299 0.518
30 0.267
0 0.0
0 0.0
0.5761152954808807
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3016,	 Acc = 0.5428
1668 0.399
7898 0.564
3716 0.581
378 0.378
10 0.0
0 0.0
0 0.0
0 0.0
0.5628228628561907
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4012,	 Acc1 = 0.3623,	 Acc2 = 0.3805

 ===== Epoch 85	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 5 1
train:	 Loss = 1.3469,	 Acc = 0.5067
13232 0.284
25967 0.558
13400 0.618
1728 0.586
299 0.522
30 0.3
0 0.0
0 0.0
0.5778292777134029
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3135,	 Acc = 0.5442
1668 0.382
7898 0.561
3716 0.596
378 0.421
10 0.0
0 0.0
0 0.0
0 0.0
0.5666555574070988
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 6
Testing:	 Loss = 1.4046,	 Acc1 = 0.3714,	 Acc2 = 0.3914

 ===== Epoch 86	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 0 1
train:	 Loss = 1.3543,	 Acc = 0.5040
13228 0.282
25971 0.552
13400 0.619
1728 0.601
299 0.538
30 0.267
0 0.0
0 0.0
0.5749975861736024
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3215,	 Acc = 0.5301
1668 0.383
7898 0.555
3716 0.557
378 0.402
10 0.0
0 0.0
0 0.0
0 0.0
0.5505749041826362
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3984,	 Acc1 = 0.3776,	 Acc2 = 0.3989

 ===== Epoch 87	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436  1.5631952   1.1570491
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849 -0.04198486  0.08985881
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 0 4
train:	 Loss = 1.3531,	 Acc = 0.5030
13230 0.271
25970 0.554
13399 0.619
1728 0.61
299 0.525
30 0.267
0 0.0
0 0.0
0.5771254767537295
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3250,	 Acc = 0.5117
1668 0.359
7898 0.558
3716 0.492
378 0.423
10 0.0
0 0.0
0 0.0
0 0.0
0.532911181469755
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4017,	 Acc1 = 0.3825,	 Acc2 = 0.4048

 ===== Epoch 88	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722  2.0922315   0.81615824
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.20741417  0.14590435
 -0.00275372 -0.0031672 ] 2 2
train:	 Loss = 1.3571,	 Acc = 0.5036
13231 0.281
25971 0.553
13398 0.615
1727 0.603
299 0.518
30 0.2
0 0.0
0 0.0
0.5745322872661436
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3442,	 Acc = 0.5130
1668 0.384
7898 0.542
3716 0.52
378 0.429
10 0.0
0 0.0
0 0.0
0 0.0
0.5309115147475421
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4235,	 Acc1 = 0.3714,	 Acc2 = 0.3914

 ===== Epoch 89	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722  2.4592376   1.4069387
  3.7085164   3.566522  ] [ 1.1263542e-03  2.0405303e-03  5.2204342e-03  4.7584870e-03
  2.7000366e-03  8.4708212e-04  6.0168497e-04  9.4006682e-04
  2.9140669e-03  7.7248965e-03  4.5180759e-01  3.0639878e-01
 -7.3554220e+00 -6.3841443e+00] 2 2
train:	 Loss = 1.3575,	 Acc = 0.5020
13231 0.282
25969 0.55
13400 0.613
1727 0.599
299 0.548
30 0.333
0 0.0
0 0.0
0.5722872661436331
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2755,	 Acc = 0.5615
1668 0.381
7898 0.581
3716 0.617
378 0.421
10 0.0
0 0.0
0 0.0
0 0.0
0.5866522246292285
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3739,	 Acc1 = 0.3817,	 Acc2 = 0.4038

 ===== Epoch 90	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722  1.7780321   1.2402453
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249  -0.02983515 -0.08280024
 -0.00275372 -0.0031672 ] 1 3
train:	 Loss = 1.3505,	 Acc = 0.5054
13232 0.284
25969 0.555
13397 0.616
1729 0.602
299 0.508
30 0.267
0 0.0
0 0.0
0.5762118578601777
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3072,	 Acc = 0.5519
1668 0.393
7898 0.576
3716 0.588
378 0.41
10 0.0
0 0.0
0 0.0
0 0.0
0.5740709881686386
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3924,	 Acc1 = 0.3831,	 Acc2 = 0.4056

 ===== Epoch 91	 =====
[ 1.4839966   2.4171054  -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [-1.9381799e+00 -3.1375823e+00  5.2204342e-03  4.7584870e-03
  2.7000366e-03  8.4708212e-04  6.0168497e-04  9.4006682e-04
  2.9140669e-03  7.7248965e-03  3.7514956e-03  5.4717045e-03
 -2.7537176e-03 -3.1672046e-03] 6 5
train:	 Loss = 1.3538,	 Acc = 0.5044
13228 0.278
25970 0.557
13400 0.615
1729 0.598
299 0.498
30 0.267
0 0.0
0 0.0
0.5767838177078305
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2856,	 Acc = 0.5522
1668 0.36
7898 0.578
3716 0.599
378 0.413
10 0.0
0 0.0
0 0.0
0 0.0
0.5789868355274121
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3803,	 Acc1 = 0.3681,	 Acc2 = 0.3874

 ===== Epoch 92	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436  1.2622383   2.9018075
 -0.36378932 -0.37167084  2.4255629   0.8624209  -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 1.1263542e-03  2.0405303e-03  5.2204342e-03  4.7584870e-03
 -3.3723388e+00 -3.6387436e+00  6.0168497e-04  9.4006682e-04
  4.2989141e-01  6.8580890e-01  3.7514956e-03  5.4717045e-03
 -2.7537176e-03 -3.1672046e-03] 2 6
train:	 Loss = 1.3468,	 Acc = 0.5078
13232 0.283
25968 0.558
13399 0.62
1729 0.616
298 0.5
30 0.2
0 0.0
0 0.0
0.5795191193511008
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3321,	 Acc = 0.5040
1668 0.389
7898 0.54
3716 0.493
378 0.386
10 0.0
0 0.0
0 0.0
0 0.0
0.5199966672221297
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4358,	 Acc1 = 0.3431,	 Acc2 = 0.3574

 ===== Epoch 93	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 1 1
train:	 Loss = 1.3464,	 Acc = 0.5089
13232 0.284
25968 0.558
13398 0.625
1729 0.6
299 0.505
30 0.133
0 0.0
0 0.0
0.5808709926612592
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3091,	 Acc = 0.5295
1668 0.391
7898 0.555
3716 0.546
378 0.45
10 0.1
0 0.0
0 0.0
0 0.0
0.548741876353941
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3901,	 Acc1 = 0.3829,	 Acc2 = 0.4053

 ===== Epoch 94	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 1 1
train:	 Loss = 1.3480,	 Acc = 0.5060
13230 0.282
25971 0.558
13399 0.615
1727 0.596
299 0.522
30 0.167
0 0.0
0 0.0
0.5773668710471684
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2854,	 Acc = 0.5559
1668 0.396
7898 0.577
3716 0.594
378 0.458
10 0.0
0 0.0
0 0.0
0 0.0
0.5780703216130645
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3900,	 Acc1 = 0.3650,	 Acc2 = 0.3837

 ===== Epoch 95	 =====
[ 1.7501951   1.772338   -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [-0.4569241   0.20057133  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 2 1
train:	 Loss = 1.3526,	 Acc = 0.5055
13233 0.285
25968 0.556
13397 0.613
1729 0.6
299 0.518
30 0.2
0 0.0
0 0.0
0.575839509451271
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3008,	 Acc = 0.5522
1668 0.398
7898 0.572
3716 0.594
378 0.429
10 0.0
0 0.0
0 0.0
0 0.0
0.5736543909348442
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3902,	 Acc1 = 0.3716,	 Acc2 = 0.3917

 ===== Epoch 96	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436  1.5872875   1.9717135
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 0 0
train:	 Loss = 1.3473,	 Acc = 0.5053
13231 0.284
25972 0.555
13395 0.614
1729 0.612
299 0.525
30 0.2
0 0.0
0 0.0
0.576077248038624
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3230,	 Acc = 0.5377
1668 0.371
7898 0.542
3716 0.618
378 0.407
10 0.0
0 0.0
0 0.0
0 0.0
0.5609065155807366
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4263,	 Acc1 = 0.3534,	 Acc2 = 0.3698

 ===== Epoch 97	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 6 1
train:	 Loss = 1.3494,	 Acc = 0.5037
13231 0.284
25970 0.554
13397 0.611
1729 0.6
299 0.532
30 0.233
0 0.0
0 0.0
0.5737839468919734
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2871,	 Acc = 0.5412
1668 0.388
7898 0.56
3716 0.588
378 0.381
10 0.0
0 0.0
0 0.0
0 0.0
0.5624895850691551
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 6
Testing:	 Loss = 1.3762,	 Acc1 = 0.3831,	 Acc2 = 0.4056

 ===== Epoch 98	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436  1.4782999   1.5233152
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849 -0.21846038  0.0379353
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 1 1
train:	 Loss = 1.3502,	 Acc = 0.5053
13228 0.281
25971 0.555
13399 0.617
1729 0.618
299 0.498
30 0.267
0 0.0
0 0.0
0.5769769238196389
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3538,	 Acc = 0.5134
1668 0.347
7898 0.552
3716 0.522
378 0.381
10 0.0
0 0.0
0 0.0
0 0.0
0.5365772371271454
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4537,	 Acc1 = 0.3509,	 Acc2 = 0.3668

 ===== Epoch 99	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 1 1
train:	 Loss = 1.3510,	 Acc = 0.5045
13236 0.281
25967 0.556
13396 0.614
1728 0.606
299 0.518
30 0.233
0 0.0
0 0.0
0.5760743602124577
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 2
val:	 Loss = 1.3681,	 Acc = 0.4786
1668 0.385
7898 0.524
3716 0.433
378 0.407
10 0.0
0 0.0
0 0.0
0 0.0
0.4915847358773538
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4502,	 Acc1 = 0.3648,	 Acc2 = 0.3834

 ===== Epoch 100	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 2 1
train:	 Loss = 1.3568,	 Acc = 0.5017
13231 0.273
25967 0.551
13400 0.618
1729 0.614
299 0.538
30 0.3
0 0.0
0 0.0
0.5748461074230538
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 2
val:	 Loss = 1.3560,	 Acc = 0.4945
1668 0.359
7898 0.539
3716 0.474
378 0.378
10 0.0
0 0.0
0 0.0
0 0.0
0.5134144309281786
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4448,	 Acc1 = 0.3536,	 Acc2 = 0.3700

 ===== Epoch 101	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.1722      1.2152249  -0.40214875 -0.40990722  2.6482105   1.7623875
 -0.38765725 -0.38159567] [ 1.1263542e-03  2.0405303e-03  5.2204342e-03  4.7584870e-03
  2.7000366e-03  8.4708212e-04 -4.5052152e+00 -2.7535701e+00
  4.2906919e+00  5.9117317e+00  3.1174615e-01 -1.5753984e+00
  6.5141048e+00  5.0406823e+00] 5 5
train:	 Loss = 1.3573,	 Acc = 0.5003
13230 0.265
25967 0.555
13401 0.614
1729 0.599
299 0.545
30 0.3
0 0.0
0 0.0
0.5755564138463767
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2974,	 Acc = 0.5518
1668 0.358
7898 0.582
3716 0.589
378 0.426
10 0.0
0 0.0
0 0.0
0 0.0
0.5787368771871355
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3865,	 Acc1 = 0.3854,	 Acc2 = 0.4083

 ===== Epoch 102	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 1 1
train:	 Loss = 1.3566,	 Acc = 0.5027
13231 0.265
25969 0.558
13400 0.618
1727 0.6
299 0.528
30 0.367
0 0.0
0 0.0
0.5787085093542547
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 2
val:	 Loss = 1.3825,	 Acc = 0.4741
1668 0.353
7898 0.52
3716 0.447
378 0.333
10 0.0
0 0.0
0 0.0
0 0.0
0.49100149975004165
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4668,	 Acc1 = 0.3561,	 Acc2 = 0.3730

 ===== Epoch 103	 =====
[ 2.8357277   2.728111   -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.5238333   2.4774647  -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [-3.3541605e+00 -3.4864292e+00  5.7047930e+00  2.1020272e+00
  2.7000366e-03  8.4708212e-04  2.1520747e-01  1.9635894e+00
  2.9140669e-03  7.7248965e-03  3.7514956e-03  5.4717045e-03
 -2.7537176e-03 -3.1672046e-03] 6 6
train:	 Loss = 1.3581,	 Acc = 0.5019
13230 0.265
25972 0.556
13397 0.617
1728 0.601
299 0.532
30 0.3
0 0.0
0 0.0
0.5775841259112635
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3210,	 Acc = 0.5285
1668 0.228
7898 0.565
3716 0.6
378 0.41
10 0.0
0 0.0
0 0.0
0 0.0
0.5703216130644893
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4192,	 Acc1 = 0.3379,	 Acc2 = 0.3912

 ===== Epoch 104	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436  0.3127098   1.8052291
 -0.36378932 -0.37167084 -0.40214875 -0.40990722  4.2697525   0.87744254
 -0.38765725 -0.38159567] [ 1.1263542e-03  2.0405303e-03  5.2204342e-03  4.7584870e-03
  1.3145858e-01 -7.8789568e-01  6.0168497e-04  9.4006682e-04
  2.9140669e-03  7.7248965e-03 -8.8924036e+00 -2.1210797e+00
 -2.7537176e-03 -3.1672046e-03] 5 5
train:	 Loss = 1.3558,	 Acc = 0.5010
13231 0.264
25971 0.556
13396 0.615
1729 0.606
299 0.535
30 0.267
0 0.0
0 0.0
0.5767290283645142
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 2
val:	 Loss = 1.3832,	 Acc = 0.4816
1668 0.36
7898 0.533
3716 0.442
378 0.357
10 0.0
0 0.0
0 0.0
0 0.0
0.4985002499583403
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4684,	 Acc1 = 0.3565,	 Acc2 = 0.3735

 ===== Epoch 105	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722  1.1858436   1.8996643
 -0.38765725 -0.38159567] [ 1.1263542e-03  2.0405303e-03  5.2204342e-03  4.7584870e-03
  2.7000366e-03  8.4708212e-04  6.0168497e-04  9.4006682e-04
  2.9140669e-03  7.7248965e-03  3.5193138e-02  7.3572135e-01
 -2.7537176e-03 -3.1672046e-03] 4 6
train:	 Loss = 1.3541,	 Acc = 0.5025
13231 0.263
25968 0.558
13400 0.619
1728 0.603
299 0.528
30 0.267
0 0.0
0 0.0
0.5790706095353048
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2837,	 Acc = 0.5478
1668 0.357
7898 0.571
3716 0.601
378 0.394
10 0.0
0 0.0
0 0.0
0 0.0
0.5744042659556741
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3760,	 Acc1 = 0.3728,	 Acc2 = 0.3931

 ===== Epoch 106	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  1.7048601   1.0948783  -0.40214875 -0.40990722  2.0096273   3.0640652
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.02947607 -0.00656538
 -0.00275372 -0.0031672 ] 3 0
train:	 Loss = 1.3543,	 Acc = 0.5015
13229 0.264
25971 0.556
13398 0.617
1729 0.607
299 0.528
30 0.267
0 0.0
0 0.0
0.5772322398435803
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3224,	 Acc = 0.4955
1668 0.339
7898 0.532
3716 0.5
378 0.394
10 0.0
0 0.0
0 0.0
0 0.0
0.5173304449258457
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4122,	 Acc1 = 0.3674,	 Acc2 = 0.3867

 ===== Epoch 107	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436  0.79072434  1.7319758
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.03905438 -0.13019796
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 5 5
train:	 Loss = 1.3611,	 Acc = 0.4999
13232 0.263
25971 0.554
13395 0.616
1729 0.597
299 0.522
30 0.333
0 0.0
0 0.0
0.5754634994206257
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3391,	 Acc = 0.4996
1668 0.357
7898 0.545
3716 0.482
378 0.368
10 0.0
0 0.0
0 0.0
0 0.0
0.5194967505415764
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4170,	 Acc1 = 0.3809,	 Acc2 = 0.4028

 ===== Epoch 108	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436  1.707747    1.3923473
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 1.1263542e-03  2.0405303e-03  5.2204342e-03  4.7584870e-03
 -6.7820090e-01  1.7145288e-01  6.0168497e-04  9.4006682e-04
  2.9140669e-03  7.7248965e-03  3.7514956e-03  5.4717045e-03
 -2.7537176e-03 -3.1672046e-03] 1 1
train:	 Loss = 1.3548,	 Acc = 0.5031
13229 0.264
25972 0.556
13398 0.623
1728 0.6
299 0.518
30 0.267
0 0.0
0 0.0
0.5793564583484201
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3087,	 Acc = 0.5447
1668 0.354
7898 0.578
3716 0.572
378 0.434
10 0.0
0 0.0
0 0.0
0 0.0
0.571154807532078
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3885,	 Acc1 = 0.3854,	 Acc2 = 0.4083

 ===== Epoch 109	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 5 1
train:	 Loss = 1.3528,	 Acc = 0.5033
13232 0.264
25969 0.556
13397 0.625
1729 0.602
299 0.505
30 0.233
0 0.0
0 0.0
0.5797363847045192
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 2
val:	 Loss = 1.4066,	 Acc = 0.4865
1668 0.363
7898 0.535
3716 0.446
378 0.431
10 0.0
0 0.0
0 0.0
0 0.0
0.5036660556573904
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 1
Testing:	 Loss = 1.4801,	 Acc1 = 0.3736,	 Acc2 = 0.3941

 ===== Epoch 110	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436  1.2132887   2.6820478
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849 -0.06697891 -0.01398821
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 3 0
train:	 Loss = 1.3551,	 Acc = 0.5033
13231 0.265
25970 0.557
13398 0.622
1729 0.606
298 0.51
30 0.267
0 0.0
0 0.0
0.5795051297525649
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3148,	 Acc = 0.5232
1668 0.362
7898 0.556
3716 0.542
378 0.381
10 0.0
0 0.0
0 0.0
0 0.0
0.5456590568238627
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4123,	 Acc1 = 0.3511,	 Acc2 = 0.3670

 ===== Epoch 111	 =====
[ 1.7279202   1.4183481  -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084  3.4307353   3.1292977  -0.4259259  -0.4217841
  1.4537961   1.4877384 ] [-2.3156850e-01  1.5235670e-01  5.2204342e-03  4.7584870e-03
  2.7000366e-03  8.4708212e-04  6.0168497e-04  9.4006682e-04
 -6.3047628e+00 -5.5533433e+00  3.7514956e-03  5.4717045e-03
  8.4064342e-02  2.0840366e-01] 2 2
train:	 Loss = 1.3568,	 Acc = 0.5033
13230 0.265
25971 0.558
13397 0.622
1729 0.596
299 0.502
30 0.233
0 0.0
0 0.0
0.5792980253946797
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3336,	 Acc = 0.5258
1668 0.36
7898 0.54
3716 0.584
378 0.384
10 0.9
0 0.0
0 0.0
0 0.0
0.5489085152474588
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 6
Testing:	 Loss = 1.4313,	 Acc1 = 0.3534,	 Acc2 = 0.3698

 ===== Epoch 112	 =====
[-0.3673141  -0.38194367  2.6467135   2.8547716   2.3440795   0.82851946
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 1.1263542e-03  2.0405303e-03 -8.9370258e-02 -2.1120742e-01
 -5.5150180e+00 -1.3293837e+00  6.0168497e-04  9.4006682e-04
  2.9140669e-03  7.7248965e-03  3.7514956e-03  5.4717045e-03
 -2.7537176e-03 -3.1672046e-03] 5 3
train:	 Loss = 1.3544,	 Acc = 0.5036
13228 0.266
25970 0.56
13401 0.616
1729 0.613
298 0.5
30 0.267
0 0.0
0 0.0
0.5795838563290528
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3015,	 Acc = 0.5489
1668 0.362
7898 0.581
3716 0.581
378 0.397
10 0.0
0 0.0
0 0.0
0 0.0
0.5749041826362273
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3843,	 Acc1 = 0.3891,	 Acc2 = 0.4128

 ===== Epoch 113	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084  2.5928054   1.0261122  -0.4259259  -0.4217841
  3.877796    0.91437125] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007 -0.3910768   0.33507577  0.0037515   0.0054717
 -0.08088852  0.343809  ] 6 4
train:	 Loss = 1.3529,	 Acc = 0.5048
13230 0.264
25970 0.559
13400 0.624
1727 0.612
299 0.502
30 0.2
0 0.0
0 0.0
0.5816636894703809
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3484,	 Acc = 0.4883
1668 0.301
7898 0.526
3716 0.504
378 0.373
10 0.0
0 0.0
0 0.0
0 0.0
0.5143309448425263
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4247,	 Acc1 = 0.3730,	 Acc2 = 0.3934

 ===== Epoch 114	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084  1.0870492   2.2959602  -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 1.1263542e-03  2.0405303e-03  5.2204342e-03  4.7584870e-03
  2.7000366e-03  8.4708212e-04  6.0168497e-04  9.4006682e-04
  6.8892919e-02  6.2735337e-01  3.7514956e-03  5.4717045e-03
 -2.7537176e-03 -3.1672046e-03] 4 6
train:	 Loss = 1.3550,	 Acc = 0.5027
13231 0.264
25973 0.559
13396 0.617
1727 0.605
299 0.498
30 0.267
0 0.0
0 0.0
0.5791188895594448
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3153,	 Acc = 0.5282
1668 0.355
7898 0.552
3716 0.57
378 0.413
10 0.0
0 0.0
0 0.0
0 0.0
0.5523246125645725
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 6
Testing:	 Loss = 1.4106,	 Acc1 = 0.3652,	 Acc2 = 0.3839

 ===== Epoch 115	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 6 1
train:	 Loss = 1.3543,	 Acc = 0.5017
13232 0.26
25966 0.558
13401 0.619
1728 0.605
299 0.505
30 0.2
0 0.0
0 0.0
0.5788673232908459
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3077,	 Acc = 0.5320
1668 0.353
7898 0.566
3716 0.561
378 0.341
10 0.0
0 0.0
0 0.0
0 0.0
0.5569905015830695
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 6
Testing:	 Loss = 1.3958,	 Acc1 = 0.3771,	 Acc2 = 0.3984

 ===== Epoch 116	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.9894793   1.8869268  -0.40214875 -0.40990722  3.1476107   2.6865544
 -0.38765725 -0.38159567] [ 1.1263542e-03  2.0405303e-03  5.2204342e-03  4.7584870e-03
  2.7000366e-03  8.4708212e-04  1.0295074e+00  7.3810712e-02
  2.9140669e-03  7.7248965e-03  7.0835084e-01 -5.4713707e-02
 -2.7537176e-03 -3.1672046e-03] 2 2
train:	 Loss = 1.3541,	 Acc = 0.5033
13230 0.264
25972 0.559
13397 0.618
1728 0.612
299 0.532
30 0.333
0 0.0
0 0.0
0.5797083956935258
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3122,	 Acc = 0.5412
1668 0.358
7898 0.572
3716 0.574
378 0.402
10 0.0
0 0.0
0 0.0
0 0.0
0.5666555574070988
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4019,	 Acc1 = 0.3778,	 Acc2 = 0.3991

 ===== Epoch 117	 =====
[-0.3673141  -0.38194367  1.1394401   1.5544829  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.02706094  0.31670922  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 6
train:	 Loss = 1.3577,	 Acc = 0.5023
13233 0.264
25972 0.555
13395 0.621
1727 0.616
299 0.518
30 0.233
0 0.0
0 0.0
0.5783260507447553
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2909,	 Acc = 0.5468
1668 0.336
7898 0.572
3716 0.605
378 0.392
10 0.1
0 0.0
0 0.0
0 0.0
0.5760706548908515
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3932,	 Acc1 = 0.3610,	 Acc2 = 0.3790

 ===== Epoch 118	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 3 1
train:	 Loss = 1.3539,	 Acc = 0.5038
13233 0.262
25968 0.56
13397 0.621
1729 0.609
299 0.505
30 0.267
0 0.0
0 0.0
0.5811022861695193
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3063,	 Acc = 0.5491
1668 0.354
7898 0.56
3716 0.626
378 0.444
10 0.0
0 0.0
0 0.0
0 0.0
0.5761539743376104
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4013,	 Acc1 = 0.3650,	 Acc2 = 0.3837

 ===== Epoch 119	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 3 1
train:	 Loss = 1.3543,	 Acc = 0.5046
13233 0.265
25968 0.559
13399 0.623
1727 0.617
299 0.518
30 0.233
0 0.0
0 0.0
0.5811264273471259
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2879,	 Acc = 0.5508
1668 0.355
7898 0.572
3716 0.606
378 0.45
10 0.0
0 0.0
0 0.0
0 0.0
0.5780703216130645
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3837,	 Acc1 = 0.3743,	 Acc2 = 0.3949

 ===== Epoch 120	 =====
[-0.3673141  -0.38194367  3.0898132  -4.218526    2.3494334   1.1659282
 -0.36378932 -0.37167084  0.9507347   1.1798829  -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 1.1263542e-03  2.0405303e-03 -6.4653454e+00  4.1105103e+00
 -7.4553214e-02  3.0497047e-01  6.0168497e-04  9.4006682e-04
 -1.1490615e-01  8.8455760e-01  3.7514956e-03  5.4717045e-03
 -2.7537176e-03 -3.1672046e-03] 4 6
train:	 Loss = 1.3549,	 Acc = 0.5033
13232 0.266
25967 0.557
13400 0.62
1728 0.614
299 0.508
30 0.3
0 0.0
0 0.0
0.579157010428737
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2873,	 Acc = 0.5524
1668 0.33
7898 0.571
3716 0.625
378 0.45
10 0.0
0 0.0
0 0.0
0 0.0
0.5832361273121146
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3905,	 Acc1 = 0.3520,	 Acc2 = 0.3680

 ===== Epoch 121	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084  1.6019496   2.206674   -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007 -0.01593787 -0.07411283  0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 3 3
train:	 Loss = 1.3551,	 Acc = 0.5027
13228 0.262
25974 0.558
13398 0.62
1727 0.62
299 0.508
30 0.167
0 0.0
0 0.0
0.5796079945930289
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 2
val:	 Loss = 1.3968,	 Acc = 0.4671
1668 0.353
7898 0.509
3716 0.441
378 0.354
10 0.0
0 0.0
0 0.0
0 0.0
0.4830028328611898
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4708,	 Acc1 = 0.3586,	 Acc2 = 0.3760

 ===== Epoch 122	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 1 1
train:	 Loss = 1.3609,	 Acc = 0.5015
13230 0.263
25966 0.555
13402 0.62
1729 0.612
299 0.538
30 0.233
0 0.0
0 0.0
0.577680683628639
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2833,	 Acc = 0.5539
1668 0.362
7898 0.573
3716 0.615
378 0.421
10 0.0
0 0.0
0 0.0
0 0.0
0.5805699050158307
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 6
Testing:	 Loss = 1.3790,	 Acc1 = 0.3842,	 Acc2 = 0.4068

 ===== Epoch 123	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436  0.42016786  0.7863434
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 1.1263542e-03  2.0405303e-03  5.2204342e-03  4.7584870e-03
 -1.7045466e+00 -1.2824053e+00  6.0168497e-04  9.4006682e-04
  2.9140669e-03  7.7248965e-03  3.7514956e-03  5.4717045e-03
 -2.7537176e-03 -3.1672046e-03] 1 1
train:	 Loss = 1.3534,	 Acc = 0.5023
13233 0.265
25966 0.557
13400 0.618
1728 0.606
299 0.468
30 0.2
0 0.0
0 0.0
0.5782053448567221
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 2
val:	 Loss = 1.3297,	 Acc = 0.4963
1668 0.356
7898 0.536
3716 0.491
378 0.362
10 0.0
0 0.0
0 0.0
0 0.0
0.5159140143309449
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4139,	 Acc1 = 0.3693,	 Acc2 = 0.3889

 ===== Epoch 124	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084  2.537821    2.0157008  -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.2875666   0.09345965  0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 2 2
train:	 Loss = 1.3579,	 Acc = 0.5026
13231 0.264
25969 0.559
13399 0.617
1728 0.605
299 0.515
30 0.167
0 0.0
0 0.0
0.5789257694628848
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3084,	 Acc = 0.5282
1668 0.359
7898 0.565
3716 0.539
378 0.405
10 0.0
0 0.0
0 0.0
0 0.0
0.5517413764372605
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3978,	 Acc1 = 0.3691,	 Acc2 = 0.3887

 ===== Epoch 125	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 2 1
train:	 Loss = 1.3545,	 Acc = 0.5043
13231 0.265
25970 0.559
13399 0.623
1728 0.609
298 0.493
30 0.2
0 0.0
0 0.0
0.5807121303560652
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2858,	 Acc = 0.5420
1668 0.351
7898 0.575
3716 0.57
378 0.444
10 0.0
0 0.0
0 0.0
0 0.0
0.5685719046825529
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3775,	 Acc1 = 0.3697,	 Acc2 = 0.3894

 ===== Epoch 126	 =====
[ 3.569047    2.546059   -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [-4.1223354e+00 -3.2822261e+00  5.2204342e-03  4.7584870e-03
  2.7000366e-03  8.4708212e-04  6.0168497e-04  9.4006682e-04
  2.9140669e-03  7.7248965e-03  3.7514956e-03  5.4717045e-03
 -2.7537176e-03 -3.1672046e-03] 3 1
train:	 Loss = 1.3551,	 Acc = 0.5035
13231 0.267
25970 0.558
13397 0.616
1729 0.622
299 0.508
30 0.333
0 0.0
0 0.0
0.5790706095353048
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3114,	 Acc = 0.5248
1668 0.356
7898 0.555
3716 0.552
378 0.384
10 0.0
0 0.0
0 0.0
0 0.0
0.5483252791201466
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3933,	 Acc1 = 0.3726,	 Acc2 = 0.3929

 ===== Epoch 127	 =====
[-0.3673141  -0.38194367  2.4324937   1.7382933  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.04958132  0.12713917  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 2 3
train:	 Loss = 1.3539,	 Acc = 0.5029
13234 0.265
25965 0.557
13400 0.62
1729 0.615
298 0.503
30 0.2
0 0.0
0 0.0
0.5790401236058134
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2840,	 Acc = 0.5541
1668 0.356
7898 0.57
3716 0.62
378 0.46
10 0.0
0 0.0
0 0.0
0 0.0
0.581653057823696
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3857,	 Acc1 = 0.3701,	 Acc2 = 0.3899

 ===== Epoch 128	 =====
[-0.3673141  -0.38194367  2.919985    2.609691    2.347904    0.80632156
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 1.1263542e-03  2.0405303e-03  6.3958311e-01  1.2233992e-01
  1.9962502e-01  1.7145288e-01  6.0168497e-04  9.4006682e-04
  2.9140669e-03  7.7248965e-03  3.7514956e-03  5.4717045e-03
 -2.7537176e-03 -3.1672046e-03] 4 2
train:	 Loss = 1.3563,	 Acc = 0.5025
13231 0.262
25968 0.557
13401 0.622
1727 0.601
299 0.505
30 0.233
0 0.0
0 0.0
0.5791913095956548
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3117,	 Acc = 0.5410
1668 0.362
7898 0.571
3716 0.57
378 0.426
10 0.0
0 0.0
0 0.0
0 0.0
0.5659056823862689
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 1
Testing:	 Loss = 1.4081,	 Acc1 = 0.3615,	 Acc2 = 0.3795

 ===== Epoch 129	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084  2.3224673   2.8589592  -0.4259259  -0.4217841
  3.597716    2.1553576 ] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007 -0.10830652 -0.49888954  0.0037515   0.0054717
  0.03512916 -0.5659457 ] 5 5
train:	 Loss = 1.3522,	 Acc = 0.5042
13232 0.266
25966 0.558
13400 0.622
1729 0.612
299 0.515
30 0.2
0 0.0
0 0.0
0.5803157589803013
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3200,	 Acc = 0.5001
1668 0.35
7898 0.533
3716 0.516
378 0.328
10 0.0
0 0.0
0 0.0
0 0.0
0.5209965005832361
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4003,	 Acc1 = 0.3724,	 Acc2 = 0.3926

 ===== Epoch 130	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722  0.618172    3.260175
 -0.38765725 -0.38159567] [ 1.1263542e-03  2.0405303e-03  5.2204342e-03  4.7584870e-03
  2.7000366e-03  8.4708212e-04  6.0168497e-04  9.4006682e-04
  2.9140669e-03  7.7248965e-03 -1.9743348e+00 -6.0210943e+00
 -2.7537176e-03 -3.1672046e-03] 1 6
train:	 Loss = 1.3551,	 Acc = 0.5033
13229 0.264
25969 0.558
13401 0.621
1728 0.609
299 0.482
30 0.2
0 0.0
0 0.0
0.579646124508171
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2948,	 Acc = 0.5547
1668 0.359
7898 0.577
3716 0.607
378 0.452
10 0.0
0 0.0
0 0.0
0 0.0
0.5819030161639727
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3924,	 Acc1 = 0.3668,	 Acc2 = 0.3859

 ===== Epoch 131	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722  0.7814948   2.7257764
 -0.38765725 -0.38159567] [ 1.1263542e-03  2.0405303e-03  5.2204342e-03  4.7584870e-03
  2.7000366e-03  8.4708212e-04  6.0168497e-04  9.4006682e-04
  2.9140669e-03  7.7248965e-03  9.7365096e-02 -1.1019398e+00
 -2.7537176e-03 -3.1672046e-03] 5 5
train:	 Loss = 1.3533,	 Acc = 0.5034
13230 0.265
25972 0.558
13399 0.62
1726 0.616
299 0.485
30 0.2
0 0.0
0 0.0
0.5794911408294308
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2650,	 Acc = 0.5645
1668 0.362
7898 0.582
3716 0.634
378 0.418
10 0.0
0 0.0
0 0.0
0 0.0
0.5926512247958674
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3661,	 Acc1 = 0.3718,	 Acc2 = 0.3919

 ===== Epoch 132	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 1 1
train:	 Loss = 1.3537,	 Acc = 0.5029
13231 0.264
25969 0.558
13398 0.62
1729 0.599
299 0.482
30 0.233
0 0.0
0 0.0
0.5792395896197948
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3145,	 Acc = 0.5467
1668 0.358
7898 0.578
3716 0.579
378 0.426
10 0.0
0 0.0
0 0.0
0 0.0
0.5729045159140144
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3978,	 Acc1 = 0.3868,	 Acc2 = 0.4100

 ===== Epoch 133	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436  1.6136745   1.9051198
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.18144433  0.07996862
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 2 2
train:	 Loss = 1.3545,	 Acc = 0.5040
13233 0.265
25968 0.559
13400 0.621
1728 0.606
297 0.502
30 0.133
0 0.0
0 0.0
0.58045047437414
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2978,	 Acc = 0.5424
1668 0.35
7898 0.572
3716 0.581
378 0.423
10 0.0
0 0.0
0 0.0
0 0.0
0.569238460256624
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3949,	 Acc1 = 0.3621,	 Acc2 = 0.3802

 ===== Epoch 134	 =====
[ 2.9136865   3.4664328  -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [-3.4358246e+00 -4.3145862e+00  5.2204342e-03  4.7584870e-03
  2.7000366e-03  8.4708212e-04  6.0168497e-04  9.4006682e-04
  2.9140669e-03  7.7248965e-03  3.7514956e-03  5.4717045e-03
 -2.7537176e-03 -3.1672046e-03] 3 6
train:	 Loss = 1.3546,	 Acc = 0.5036
13228 0.265
25973 0.558
13398 0.621
1729 0.606
299 0.485
29 0.345
0 0.0
0 0.0
0.5797769624408613
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3094,	 Acc = 0.5258
1668 0.36
7898 0.557
3716 0.551
378 0.376
10 0.0
0 0.0
0 0.0
0 0.0
0.5488251958006999
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 6
Testing:	 Loss = 1.3996,	 Acc1 = 0.3662,	 Acc2 = 0.3852

 ===== Epoch 135	 =====
[ 2.8724012   3.077044   -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722  1.1330371   1.0956146
  3.486914    3.2549663 ] [-3.3925772e+00 -3.8778186e+00  5.2204342e-03  4.7584870e-03
  2.7000366e-03  8.4708212e-04  6.0168497e-04  9.4006682e-04
  2.9140669e-03  7.7248965e-03  8.6644500e-02  1.6184409e+00
 -6.9576435e+00 -5.8806062e+00] 6 6
train:	 Loss = 1.3551,	 Acc = 0.5040
13229 0.263
25972 0.56
13399 0.618
1727 0.62
299 0.505
30 0.167
0 0.0
0 0.0
0.5809737610736959
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 2
val:	 Loss = 1.3372,	 Acc = 0.4906
1668 0.357
7898 0.532
3716 0.476
378 0.376
10 0.0
0 0.0
0 0.0
0 0.0
0.5092484585902349
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4235,	 Acc1 = 0.3646,	 Acc2 = 0.3832

 ===== Epoch 136	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 1 1
train:	 Loss = 1.3569,	 Acc = 0.5032
13232 0.265
25970 0.556
13396 0.622
1729 0.621
299 0.492
30 0.233
0 0.0
0 0.0
0.5794225569718038
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3014,	 Acc = 0.5259
1668 0.357
7898 0.563
3716 0.537
378 0.397
10 0.0
0 0.0
0 0.0
0 0.0
0.5493251124812532
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3950,	 Acc1 = 0.3639,	 Acc2 = 0.3825

 ===== Epoch 137	 =====
[ 1.8225185   1.2034256  -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 1.8652945e+00  2.3744132e-01  5.2204342e-03  4.7584870e-03
  2.7000366e-03  8.4708212e-04  6.0168497e-04  9.4006682e-04
  2.9140669e-03  7.7248965e-03  3.7514956e-03  5.4717045e-03
 -2.7537176e-03 -3.1672046e-03] 6 2
train:	 Loss = 1.3533,	 Acc = 0.5044
13234 0.262
25971 0.56
13396 0.623
1726 0.607
299 0.508
30 0.233
0 0.0
0 0.0
0.5819129930954565
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3254,	 Acc = 0.5356
1668 0.355
7898 0.563
3716 0.568
378 0.458
10 0.0
0 0.0
0 0.0
0 0.0
0.5607398766872188
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4075,	 Acc1 = 0.3714,	 Acc2 = 0.3914

 ===== Epoch 138	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722  1.4853326   2.752741
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249  -0.15417686  0.04158295
 -0.00275372 -0.0031672 ] 1 4
train:	 Loss = 1.3549,	 Acc = 0.5026
13229 0.264
25970 0.554
13400 0.624
1728 0.614
299 0.498
30 0.2
0 0.0
0 0.0
0.578873681415502
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3042,	 Acc = 0.5305
1668 0.36
7898 0.564
3716 0.55
378 0.394
10 0.0
0 0.0
0 0.0
0 0.0
0.5542409598400266
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 6
Testing:	 Loss = 1.3929,	 Acc1 = 0.3792,	 Acc2 = 0.4008

 ===== Epoch 139	 =====
[-0.3673141  -0.38194367  0.08992877  2.3169558  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.03224535 -0.4943627   0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 5 5
train:	 Loss = 1.3518,	 Acc = 0.5043
13235 0.262
25970 0.561
13395 0.622
1727 0.611
299 0.482
30 0.167
0 0.0
0 0.0
0.581613191376355
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3293,	 Acc = 0.5306
1668 0.361
7898 0.567
3716 0.546
378 0.373
10 0.0
0 0.0
0 0.0
0 0.0
0.5541576403932678
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4104,	 Acc1 = 0.3858,	 Acc2 = 0.4088

 ===== Epoch 140	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 1
train:	 Loss = 1.3532,	 Acc = 0.5027
13234 0.263
25967 0.559
13397 0.618
1729 0.62
299 0.472
30 0.2
0 0.0
0 0.0
0.5794746752933224
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3262,	 Acc = 0.5020
1668 0.357
7898 0.535
3716 0.512
378 0.362
10 0.0
0 0.0
0 0.0
0 0.0
0.5220796533911015
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4140,	 Acc1 = 0.3606,	 Acc2 = 0.3785

 ===== Epoch 141	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722  0.9187921   2.4414172
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.08950524 -0.19514634
 -0.00275372 -0.0031672 ] 2 5
train:	 Loss = 1.3530,	 Acc = 0.5039
13229 0.264
25974 0.559
13398 0.619
1727 0.622
298 0.507
30 0.233
0 0.0
0 0.0
0.5803944287541941
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3100,	 Acc = 0.5228
1668 0.355
7898 0.558
3716 0.538
378 0.392
10 0.0
0 0.0
0 0.0
0 0.0
0.546075654057657
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3961,	 Acc1 = 0.3699,	 Acc2 = 0.3897

 ===== Epoch 142	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084  0.97536296  2.7473516  -0.4259259  -0.4217841
  2.5930278   2.0165977 ] [0.00112635 0.00204053 0.00522043 0.00475849 0.00270004 0.00084708
 0.00060168 0.00094007 0.19802429 0.5143394  0.0037515  0.0054717
 0.08090724 0.41151166] 4 4
train:	 Loss = 1.3529,	 Acc = 0.5041
13233 0.266
25971 0.559
13395 0.621
1728 0.604
299 0.488
30 0.233
0 0.0
0 0.0
0.5801607802428602
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3212,	 Acc = 0.5254
1668 0.359
7898 0.551
3716 0.561
378 0.405
10 0.0
0 0.0
0 0.0
0 0.0
0.5485752374604232
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4142,	 Acc1 = 0.3596,	 Acc2 = 0.3772

 ===== Epoch 143	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 1
train:	 Loss = 1.3543,	 Acc = 0.5044
13231 0.265
25968 0.559
13399 0.622
1729 0.61
299 0.468
30 0.267
0 0.0
0 0.0
0.5807362703681351
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3144,	 Acc = 0.5366
1668 0.362
7898 0.569
3716 0.565
378 0.37
10 0.0
0 0.0
0 0.0
0 0.0
0.5609065155807366
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3944,	 Acc1 = 0.3868,	 Acc2 = 0.4100

 ===== Epoch 144	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722  2.0533807   1.4069387
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.3996427   0.10176837
 -0.00275372 -0.0031672 ] 3 2
train:	 Loss = 1.3551,	 Acc = 0.5020
13228 0.263
25972 0.558
13398 0.618
1729 0.599
299 0.498
30 0.267
0 0.0
0 0.0
0.578352804866274
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3173,	 Acc = 0.5350
1668 0.36
7898 0.556
3716 0.584
378 0.394
10 0.0
0 0.0
0 0.0
0 0.0
0.559323446092318
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4047,	 Acc1 = 0.3590,	 Acc2 = 0.3765

 ===== Epoch 145	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 1
train:	 Loss = 1.3537,	 Acc = 0.5028
13231 0.265
25969 0.556
13398 0.62
1729 0.621
299 0.492
30 0.2
0 0.0
0 0.0
0.5789257694628848
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3149,	 Acc = 0.5312
1668 0.356
7898 0.562
3716 0.559
378 0.405
10 0.0
0 0.0
0 0.0
0 0.0
0.5555740709881687
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3948,	 Acc1 = 0.3771,	 Acc2 = 0.3984

 ===== Epoch 146	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722  1.1922556   3.2675292
 -0.38765725 -0.38159567] [ 2.6823905e+00  1.8795747e+00  5.2204342e-03  4.7584870e-03
  2.7000366e-03  8.4708212e-04  6.0168497e-04  9.4006682e-04
  2.9140669e-03  7.7248965e-03 -3.0619597e+00 -6.0331316e+00
 -2.7537176e-03 -3.1672046e-03] 6 6
train:	 Loss = 1.3530,	 Acc = 0.5040
13230 0.265
25970 0.558
13400 0.622
1728 0.614
298 0.52
30 0.167
0 0.0
0 0.0
0.5802636025684352
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 2
val:	 Loss = 1.3667,	 Acc = 0.4904
1668 0.358
7898 0.541
3716 0.457
378 0.354
10 0.0
0 0.0
0 0.0
0 0.0
0.5088318613564405
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 1
Testing:	 Loss = 1.4539,	 Acc1 = 0.3656,	 Acc2 = 0.3844

 ===== Epoch 147	 =====
[-0.3673141  -0.38194367  2.1425242  -4.906113   -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.33929208  0.11514106  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 2 2
train:	 Loss = 1.3539,	 Acc = 0.5035
13232 0.263
25970 0.558
13402 0.622
1724 0.605
298 0.52
30 0.3
0 0.0
0 0.0
0.580388180764774
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2959,	 Acc = 0.5461
1668 0.357
7898 0.576
3716 0.584
378 0.394
10 0.0
0 0.0
0 0.0
0 0.0
0.5724045992334611
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 6
Testing:	 Loss = 1.3904,	 Acc1 = 0.3831,	 Acc2 = 0.4056

 ===== Epoch 148	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084  2.4421725   3.1392186  -0.4259259  -0.4217841
  3.1857276   2.4433503 ] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007 -0.05175654 -0.25727347  0.0037515   0.0054717
  0.56629205 -0.25705224] 5 5
train:	 Loss = 1.3547,	 Acc = 0.5032
13230 0.265
25968 0.558
13400 0.619
1729 0.61
299 0.525
30 0.233
0 0.0
0 0.0
0.5793463042533674
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2827,	 Acc = 0.5518
1668 0.36
7898 0.579
3716 0.594
378 0.426
10 0.0
0 0.0
0 0.0
0 0.0
0.5784869188468589
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 6
Testing:	 Loss = 1.3732,	 Acc1 = 0.3755,	 Acc2 = 0.3964

 ===== Epoch 149	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 1 1
train:	 Loss = 1.3553,	 Acc = 0.5030
13232 0.266
25972 0.558
13397 0.619
1726 0.603
299 0.515
30 0.167
0 0.0
0 0.0
0.5786983391270761
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2943,	 Acc = 0.5485
1668 0.362
7898 0.572
3716 0.598
378 0.405
10 0.0
0 0.0
0 0.0
0 0.0
0.5744042659556741
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3889,	 Acc1 = 0.3687,	 Acc2 = 0.3882

 ===== Epoch 150	 =====
[ 1.5880533   2.6851263  -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
  1.2392286   3.021954  ] [-0.04064987 -0.02348486  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.01222505 -0.01586146] 3 0
train:	 Loss = 1.3552,	 Acc = 0.5023
13231 0.265
25969 0.556
13398 0.62
1729 0.606
299 0.508
30 0.233
0 0.0
0 0.0
0.5782257091128545
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3119,	 Acc = 0.5371
1668 0.36
7898 0.556
3716 0.587
378 0.429
10 0.9
0 0.0
0 0.0
0 0.0
0.5617397100483252
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4072,	 Acc1 = 0.3623,	 Acc2 = 0.3805

 ===== Epoch 151	 =====
[-0.3673141  -0.38194367  1.6326332   2.2261853  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00071589  0.00715811  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 0 0
train:	 Loss = 1.3554,	 Acc = 0.5039
13229 0.262
25969 0.56
13400 0.62
1729 0.614
299 0.505
30 0.2
0 0.0
0 0.0
0.5810703164602795
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3089,	 Acc = 0.5366
1668 0.353
7898 0.572
3716 0.557
378 0.426
10 0.0
0 0.0
0 0.0
0 0.0
0.5621563072821196
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3951,	 Acc1 = 0.3895,	 Acc2 = 0.4133

 ===== Epoch 152	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 6 1
train:	 Loss = 1.3564,	 Acc = 0.5042
13231 0.264
25970 0.559
13397 0.621
1729 0.614
299 0.495
30 0.167
0 0.0
0 0.0
0.5808328304164152
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3196,	 Acc = 0.5200
1668 0.359
7898 0.559
3716 0.521
378 0.418
10 0.0
0 0.0
0 0.0
0 0.0
0.5424929178470255
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4029,	 Acc1 = 0.3743,	 Acc2 = 0.3949

 ===== Epoch 153	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 6 1
train:	 Loss = 1.3544,	 Acc = 0.5044
13228 0.263
25971 0.56
13399 0.622
1729 0.608
299 0.508
30 0.167
0 0.0
0 0.0
0.5815149174471372
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2772,	 Acc = 0.5592
1668 0.351
7898 0.579
3716 0.626
378 0.434
10 0.0
0 0.0
0 0.0
0 0.0
0.5881519746708882
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3844,	 Acc1 = 0.3489,	 Acc2 = 0.3643

 ===== Epoch 154	 =====
[ 2.3321245   2.3412504  -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
  2.035943    2.6030555 ] [-0.03752407  0.00487668  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
  0.29163575 -0.02009287] 0 0
train:	 Loss = 1.3530,	 Acc = 0.5046
13228 0.264
25972 0.559
13398 0.624
1729 0.61
299 0.485
30 0.3
0 0.0
0 0.0
0.581418364391233
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3568,	 Acc = 0.5217
1668 0.358
7898 0.547
3716 0.558
378 0.368
10 0.0
0 0.0
0 0.0
0 0.0
0.5444925845692384
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4619,	 Acc1 = 0.3450,	 Acc2 = 0.3596

 ===== Epoch 155	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  1.5530308   2.900077   -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 3.8170354e+00  3.0310528e+00  5.2204342e-03  4.7584870e-03
  2.7000366e-03  8.4708212e-04  6.3781217e-02 -7.9092097e-01
  2.9140669e-03  7.7248965e-03  3.7514956e-03  5.4717045e-03
 -2.7537176e-03 -3.1672046e-03] 5 5
train:	 Loss = 1.3529,	 Acc = 0.5051
13227 0.266
25970 0.561
13402 0.62
1728 0.605
299 0.478
30 0.233
0 0.0
0 0.0
0.581380192618697
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3260,	 Acc = 0.5207
1668 0.356
7898 0.553
3716 0.539
378 0.407
10 0.0
0 0.0
0 0.0
0 0.0
0.5436593901016498
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4262,	 Acc1 = 0.3522,	 Acc2 = 0.3683

 ===== Epoch 156	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436  0.34444952  2.44231
  4.620087    1.2656026  -0.40214875 -0.40990722  4.611864    1.5932429
 -0.38765725 -0.38159567] [ 1.12635421e-03  2.04053032e-03  5.22043416e-03  4.75848699e-03
  1.55775305e-02 -6.37070239e-01 -8.85449696e+00 -2.84101486e+00
  2.91406689e-03  7.72489654e-03 -7.31573820e-01 -8.93297195e-01
 -2.75371759e-03 -3.16720456e-03] 5 5
train:	 Loss = 1.3512,	 Acc = 0.5058
13225 0.264
25976 0.562
13398 0.622
1728 0.617
299 0.492
30 0.233
0 0.0
0 0.0
0.5829451376988245
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3392,	 Acc = 0.5341
1668 0.359
7898 0.566
3716 0.566
378 0.339
10 0.0
0 0.0
0 0.0
0 0.0
0.5584069321779703
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4218,	 Acc1 = 0.3765,	 Acc2 = 0.3976

 ===== Epoch 157	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084  2.203336    3.245866    3.0642521   0.8651857
  3.6403654   2.7601423 ] [ 1.12635421e-03  2.04053032e-03  5.22043416e-03  4.75848699e-03
  2.70003662e-03  8.47082119e-04  6.01684966e-04  9.40066820e-04
  2.49862731e-01  1.01253718e-01 -6.60853434e+00 -2.10101771e+00
 -2.98721731e-01 -1.30909395e+01] 2 5
train:	 Loss = 1.3507,	 Acc = 0.5036
13231 0.264
25972 0.56
13397 0.617
1727 0.611
299 0.505
30 0.3
0 0.0
0 0.0
0.5802534701267351
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3209,	 Acc = 0.5405
1668 0.359
7898 0.553
3716 0.603
378 0.455
10 0.9
0 0.0
0 0.0
0 0.0
0.56582236293951
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4113,	 Acc1 = 0.3652,	 Acc2 = 0.3839

 ===== Epoch 158	 =====
[-0.3673141  -0.38194367  0.52081025  1.486405   -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 1.1263542e-03  2.0405303e-03  5.4018456e-02 -4.0755320e-05
  2.7000366e-03  8.4708212e-04  6.0168497e-04  9.4006682e-04
  2.9140669e-03  7.7248965e-03  3.7514956e-03  5.4717045e-03
 -2.7537176e-03 -3.1672046e-03] 0 0
train:	 Loss = 1.3528,	 Acc = 0.5050
13228 0.263
25971 0.56
13403 0.623
1725 0.621
299 0.478
30 0.233
0 0.0
0 0.0
0.582311480158347
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3002,	 Acc = 0.5417
1668 0.359
7898 0.569
3716 0.582
378 0.392
10 0.0
0 0.0
0 0.0
0 0.0
0.5670721546408932
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3840,	 Acc1 = 0.3833,	 Acc2 = 0.4058

 ===== Epoch 159	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084  1.9175342   2.033062   -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007 -0.1648594  -0.05852469  0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 2 3
train:	 Loss = 1.3527,	 Acc = 0.5065
13232 0.264
25969 0.561
13397 0.627
1729 0.625
299 0.495
30 0.233
0 0.0
0 0.0
0.5840092699884125
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2827,	 Acc = 0.5554
1668 0.36
7898 0.569
3716 0.629
378 0.434
10 0.0
0 0.0
0 0.0
0 0.0
0.5824862522912848
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3855,	 Acc1 = 0.3683,	 Acc2 = 0.3877

 ===== Epoch 160	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 5 1
train:	 Loss = 1.3534,	 Acc = 0.5036
13229 0.264
25967 0.559
13403 0.618
1728 0.623
299 0.488
30 0.267
0 0.0
0 0.0
0.5800564849011515
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2982,	 Acc = 0.5351
1668 0.354
7898 0.565
3716 0.566
378 0.394
10 0.9
0 0.0
0 0.0
0 0.0
0.5602399600066655
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 1
Testing:	 Loss = 1.3871,	 Acc1 = 0.3749,	 Acc2 = 0.3956

 ===== Epoch 161	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722  1.8168825   2.5370207
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249  -0.38642043  0.09374364
 -0.00275372 -0.0031672 ] 3 1
train:	 Loss = 1.3528,	 Acc = 0.5057
13233 0.262
25973 0.562
13392 0.626
1729 0.612
299 0.478
30 0.2
0 0.0
0 0.0
0.5836129686406103
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2954,	 Acc = 0.5491
1668 0.362
7898 0.571
3716 0.603
378 0.394
10 0.0
0 0.0
0 0.0
0 0.0
0.5750708215297451
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3901,	 Acc1 = 0.3769,	 Acc2 = 0.3981

 ===== Epoch 162	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
  1.9554802   2.9224653 ] [1.1263542e-03 2.0405303e-03 5.2204342e-03 4.7584870e-03 2.7000366e-03
 8.4708212e-04 6.0168497e-04 9.4006682e-04 2.9140669e-03 7.7248965e-03
 5.9107242e+00 2.1881962e+00 4.8578820e-01 5.4268557e-01] 4 6
train:	 Loss = 1.3534,	 Acc = 0.5034
13231 0.263
25969 0.557
13399 0.624
1728 0.602
299 0.508
30 0.233
0 0.0
0 0.0
0.580181050090525
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 2
val:	 Loss = 1.3676,	 Acc = 0.5236
1668 0.363
7898 0.565
3716 0.526
378 0.36
10 0.0
0 0.0
0 0.0
0 0.0
0.5459923346108981
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4542,	 Acc1 = 0.3615,	 Acc2 = 0.3795

 ===== Epoch 163	 =====
[ 2.3335772   1.5574156  -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
  1.3227688   2.0768144 ] [-5.2547604e-01  7.9899979e-01  5.2204342e-03  4.7584870e-03
  2.7000366e-03  8.4708212e-04  6.0168497e-04  9.4006682e-04
  2.9140669e-03  7.7248965e-03  3.7514956e-03  5.4717045e-03
  1.9850345e-01  1.1562412e+00] 6 6
train:	 Loss = 1.3543,	 Acc = 0.5048
13233 0.262
25967 0.562
13399 0.62
1728 0.606
299 0.495
30 0.167
0 0.0
0 0.0
0.5822610626946383
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2979,	 Acc = 0.5480
1668 0.354
7898 0.57
3716 0.598
378 0.45
10 0.9
0 0.0
0 0.0
0 0.0
0.5749041826362273
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3849,	 Acc1 = 0.3858,	 Acc2 = 0.4088

 ===== Epoch 164	 =====
[-0.3673141  -0.38194367  2.386881    1.1187842  -0.44182304 -0.36572987
  2.2015493   3.3394823  -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.08104338 -0.02883621  0.00270004  0.00084708
 -0.01343856 -0.08164667  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 1 3
train:	 Loss = 1.3533,	 Acc = 0.5067
13228 0.266
25968 0.562
13403 0.623
1728 0.619
299 0.525
30 0.3
0 0.0
0 0.0
0.5835425316211258
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3075,	 Acc = 0.5320
1668 0.335
7898 0.563
3716 0.565
378 0.431
10 0.0
0 0.0
0 0.0
0 0.0
0.5594067655390769
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4021,	 Acc1 = 0.3625,	 Acc2 = 0.3807

 ===== Epoch 165	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  1.3498398   2.5418363  -0.40214875 -0.40990722  2.644816    2.0075245
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.0155142   0.00291407  0.0077249   0.03376387  0.0054717
 -0.00275372 -0.0031672 ] 0 0
train:	 Loss = 1.3523,	 Acc = 0.5065
13229 0.266
25972 0.563
13397 0.621
1729 0.614
299 0.472
30 0.267
0 0.0
0 0.0
0.5832669515050571
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3371,	 Acc = 0.5330
1668 0.359
7898 0.576
3716 0.532
378 0.421
10 0.0
0 0.0
0 0.0
0 0.0
0.5572404599233461
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4191,	 Acc1 = 0.3769,	 Acc2 = 0.3981

 ===== Epoch 166	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436  3.1326125   2.775279
 -0.36378932 -0.37167084  2.4954376   2.7448714  -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 1.1263542e-03  2.0405303e-03  5.2204342e-03  4.7584870e-03
  1.1327399e+00 -1.8706657e-01  6.0168497e-04  9.4006682e-04
  1.8388389e-01 -2.9234675e-01  3.7514956e-03  5.4717045e-03
 -2.7537176e-03 -3.1672046e-03] 5 5
train:	 Loss = 1.3515,	 Acc = 0.5048
13231 0.266
25971 0.561
13396 0.62
1729 0.598
299 0.488
30 0.267
0 0.0
0 0.0
0.5809776704888352
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 2
val:	 Loss = 1.3719,	 Acc = 0.4772
1668 0.3
7898 0.526
3716 0.465
378 0.365
10 0.0
0 0.0
0 0.0
0 0.0
0.5019163472754541
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4568,	 Acc1 = 0.3608,	 Acc2 = 0.3787

 ===== Epoch 167	 =====
[ 2.7616177   3.4537904  -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [-0.14166367  0.03323822  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 3 2
train:	 Loss = 1.3517,	 Acc = 0.5052
13233 0.263
25972 0.562
13395 0.62
1727 0.616
299 0.502
30 0.167
0 0.0
0 0.0
0.5825748980035246
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2964,	 Acc = 0.5581
1668 0.357
7898 0.575
3716 0.622
378 0.481
10 0.0
0 0.0
0 0.0
0 0.0
0.5860689885019164
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 1
Testing:	 Loss = 1.3912,	 Acc1 = 0.3687,	 Acc2 = 0.3882

 ===== Epoch 168	 =====
[-0.3673141  -0.38194367  3.5422812   3.0204277   2.516165    1.3901275
 -0.36378932 -0.37167084  1.3012573   1.4923846  -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.1659451   0.00955773  0.01860725  0.07255097
  0.00060168  0.00094007  0.01799561  0.15970923  0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 2 3
train:	 Loss = 1.3522,	 Acc = 0.5055
13233 0.264
25971 0.564
13394 0.62
1729 0.598
299 0.482
30 0.233
0 0.0
0 0.0
0.5828404509571977
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3298,	 Acc = 0.5191
1668 0.351
7898 0.557
3716 0.523
378 0.434
10 0.0
0 0.0
0 0.0
0 0.0
0.5424095984002666
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4182,	 Acc1 = 0.3617,	 Acc2 = 0.3797

 ===== Epoch 169	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436  1.8557401   0.83295906
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 1.1263542e-03  2.0405303e-03  4.0914855e+00  3.6041901e+00
 -4.5478201e+00 -1.3343289e+00  6.0168497e-04  9.4006682e-04
  2.9140669e-03  7.7248965e-03  3.7514956e-03  5.4717045e-03
 -2.7537176e-03 -3.1672046e-03] 2 5
train:	 Loss = 1.3524,	 Acc = 0.5065
13234 0.265
25969 0.564
13396 0.623
1728 0.601
299 0.518
30 0.233
0 0.0
0 0.0
0.5837719086475786
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3138,	 Acc = 0.5456
1668 0.353
7898 0.576
3716 0.585
378 0.397
10 0.0
0 0.0
0 0.0
0 0.0
0.5724045992334611
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 1
Testing:	 Loss = 1.4057,	 Acc1 = 0.3707,	 Acc2 = 0.3907

 ===== Epoch 170	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 5 1
train:	 Loss = 1.3529,	 Acc = 0.5029
13230 0.261
25970 0.559
13399 0.619
1728 0.609
299 0.505
30 0.3
0 0.0
0 0.0
0.5801670448510597
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3504,	 Acc = 0.5216
1668 0.356
7898 0.551
3716 0.538
378 0.463
10 0.9
0 0.0
0 0.0
0 0.0
0.5446592234627562
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4333,	 Acc1 = 0.3672,	 Acc2 = 0.3864

 ===== Epoch 171	 =====
[-0.3673141  -0.38194367  1.7560328   1.4909436  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 1.1263542e-03  2.0405303e-03 -1.3366143e-01  9.9340236e-01
  2.7000366e-03  8.4708212e-04  6.0168497e-04  9.4006682e-04
  2.9140669e-03  7.7248965e-03  3.7514956e-03  5.4717045e-03
 -2.7537176e-03 -3.1672046e-03] 6 6
train:	 Loss = 1.3519,	 Acc = 0.5058
13228 0.264
25971 0.562
13400 0.621
1729 0.621
298 0.51
30 0.233
0 0.0
0 0.0
0.5830356280776287
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2953,	 Acc = 0.5510
1668 0.354
7898 0.574
3716 0.602
378 0.444
10 0.1
0 0.0
0 0.0
0 0.0
0.5783202799533411
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 1
Testing:	 Loss = 1.3916,	 Acc1 = 0.3714,	 Acc2 = 0.3914

 ===== Epoch 172	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722  0.80224055  1.5442154
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249  -0.05984753 -0.1991587
 -0.00275372 -0.0031672 ] 2 5
train:	 Loss = 1.3515,	 Acc = 0.5064
13232 0.264
25973 0.563
13395 0.623
1728 0.61
298 0.5
30 0.267
0 0.0
0 0.0
0.5838885670142913
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2949,	 Acc = 0.5439
1668 0.356
7898 0.568
3716 0.59
378 0.434
10 0.0
0 0.0
0 0.0
0 0.0
0.5700716547242126
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3970,	 Acc1 = 0.3559,	 Acc2 = 0.3728

 ===== Epoch 173	 =====
[-0.3673141  -0.38194367  2.347377    2.95235    -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 1.1263542e-03  2.0405303e-03 -1.7570610e-01 -7.0552939e-01
  2.7000366e-03  8.4708212e-04  6.0168497e-04  9.4006682e-04
  2.9140669e-03  7.7248965e-03  3.7514956e-03  5.4717045e-03
 -2.7537176e-03 -3.1672046e-03] 2 5
train:	 Loss = 1.3510,	 Acc = 0.5054
13230 0.264
25972 0.562
13400 0.623
1726 0.6
299 0.462
29 0.241
0 0.0
0 0.0
0.5823154540626659
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2911,	 Acc = 0.5589
1668 0.357
7898 0.59
3716 0.597
378 0.444
10 0.0
0 0.0
0 0.0
0 0.0
0.5869855024162639
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3896,	 Acc1 = 0.3769,	 Acc2 = 0.3981

 ===== Epoch 174	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.5893054   1.6322399  -0.40214875 -0.40990722  2.6018157   2.8140254
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.06378122  0.18068765  0.00291407  0.0077249  -0.06699385  0.19004032
 -0.00275372 -0.0031672 ] 3 3
train:	 Loss = 1.3504,	 Acc = 0.5062
13232 0.264
25968 0.563
13401 0.623
1726 0.617
299 0.518
30 0.2
0 0.0
0 0.0
0.58376786404017
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2874,	 Acc = 0.5604
1668 0.357
7898 0.59
3716 0.604
378 0.431
10 0.0
0 0.0
0 0.0
0 0.0
0.5886518913514415
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3834,	 Acc1 = 0.3685,	 Acc2 = 0.3879

 ===== Epoch 175	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 1.1263542e-03  2.0405303e-03  5.2204342e-03  4.7584870e-03
  2.7000366e-03  8.4708212e-04  3.1596124e+00  3.7804976e+00
  2.9140669e-03  7.7248965e-03  3.7514956e-03  5.4717045e-03
 -2.7537176e-03 -3.1672046e-03] 4 1
train:	 Loss = 1.3534,	 Acc = 0.5045
13232 0.262
25971 0.562
13397 0.618
1728 0.611
298 0.537
30 0.267
0 0.0
0 0.0
0.5818607570490537
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3056,	 Acc = 0.5349
1668 0.353
7898 0.567
3716 0.562
378 0.402
10 0.0
0 0.0
0 0.0
0 0.0
0.5601566405599067
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4035,	 Acc1 = 0.3668,	 Acc2 = 0.3859

 ===== Epoch 176	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 6 1
train:	 Loss = 1.3499,	 Acc = 0.5049
13230 0.265
25974 0.562
13395 0.616
1728 0.616
299 0.508
30 0.333
0 0.0
0 0.0
0.5815429923236615
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2934,	 Acc = 0.5546
1668 0.354
7898 0.579
3716 0.611
378 0.389
10 0.0
0 0.0
0 0.0
0 0.0
0.5824862522912848
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3830,	 Acc1 = 0.3724,	 Acc2 = 0.3926

 ===== Epoch 177	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722  1.424604    1.5662777
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.01661489  0.01750879
 -0.00275372 -0.0031672 ] 3 3
train:	 Loss = 1.3508,	 Acc = 0.5046
13233 0.264
25968 0.561
13398 0.62
1729 0.609
298 0.497
30 0.2
0 0.0
0 0.0
0.5814402626560123
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3179,	 Acc = 0.5259
1668 0.354
7898 0.562
3716 0.538
378 0.41
10 0.0
0 0.0
0 0.0
0 0.0
0.5497417097150475
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4101,	 Acc1 = 0.3734,	 Acc2 = 0.3939

 ===== Epoch 178	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436  2.8978124   2.9595222
 -0.36378932 -0.37167084  1.9301351   2.3034005  -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 0 0
train:	 Loss = 1.3501,	 Acc = 0.5068
13230 0.267
25970 0.563
13398 0.622
1729 0.613
299 0.502
30 0.3
0 0.0
0 0.0
0.5834500072418288
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3146,	 Acc = 0.5285
1668 0.358
7898 0.567
3716 0.544
378 0.341
10 0.0
0 0.0
0 0.0
0 0.0
0.5521579736710548
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4026,	 Acc1 = 0.3761,	 Acc2 = 0.3971

 ===== Epoch 179	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
  1.8987601   2.5585475 ] [5.2834392e+00 3.7883062e+00 5.2204342e-03 4.7584870e-03 2.7000366e-03
 8.4708212e-04 6.0168497e-04 9.4006682e-04 2.9140669e-03 7.7248965e-03
 3.7514956e-03 5.4717045e-03 1.1721155e-01 1.3085722e+00] 1 6
train:	 Loss = 1.3529,	 Acc = 0.5048
13231 0.265
25971 0.559
13398 0.623
1728 0.613
298 0.517
30 0.2
0 0.0
0 0.0
0.5815811707905854
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 2
val:	 Loss = 1.3525,	 Acc = 0.5073
1668 0.358
7898 0.548
3716 0.504
378 0.37
10 0.0
0 0.0
0 0.0
0 0.0
0.5280786535577404
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 1
Testing:	 Loss = 1.4426,	 Acc1 = 0.3606,	 Acc2 = 0.3785

 ===== Epoch 180	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084  3.3287852   1.4502215  -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 0 0
train:	 Loss = 1.3525,	 Acc = 0.5047
13230 0.265
25973 0.56
13396 0.622
1728 0.607
299 0.492
30 0.167
0 0.0
0 0.0
0.5813981557475981
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3200,	 Acc = 0.5168
1668 0.359
7898 0.548
3716 0.533
378 0.418
10 0.0
0 0.0
0 0.0
0 0.0
0.5387435427428762
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4067,	 Acc1 = 0.3641,	 Acc2 = 0.3827

 ===== Epoch 181	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 0 1
train:	 Loss = 1.3488,	 Acc = 0.5049
13228 0.262
25970 0.562
13400 0.622
1729 0.6
299 0.488
30 0.267
0 0.0
0 0.0
0.5823356184223231
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2902,	 Acc = 0.5522
1668 0.348
7898 0.58
3716 0.601
378 0.407
10 0.0
0 0.0
0 0.0
0 0.0
0.5805699050158307
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3921,	 Acc1 = 0.3728,	 Acc2 = 0.3931

 ===== Epoch 182	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722  1.8051898   2.372779
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.05949066  0.09374364
 -0.00275372 -0.0031672 ] 1 4
train:	 Loss = 1.3519,	 Acc = 0.5046
13226 0.264
25976 0.562
13397 0.619
1728 0.61
299 0.492
30 0.233
0 0.0
0 0.0
0.5814627081824765
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2967,	 Acc = 0.5560
1668 0.354
7898 0.582
3716 0.61
378 0.399
10 0.0
0 0.0
0 0.0
0 0.0
0.5839860023329445
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4009,	 Acc1 = 0.3524,	 Acc2 = 0.3685

 ===== Epoch 183	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 0 1
train:	 Loss = 1.3512,	 Acc = 0.5039
13228 0.262
25972 0.559
13398 0.623
1729 0.607
299 0.515
30 0.267
0 0.0
0 0.0
0.5810321521676161
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3384,	 Acc = 0.5402
1668 0.358
7898 0.581
3716 0.549
378 0.413
10 0.0
0 0.0
0 0.0
0 0.0
0.5655724045992334
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4111,	 Acc1 = 0.3858,	 Acc2 = 0.4088

 ===== Epoch 184	 =====
[ 3.212472    1.4613326  -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [-3.7488120e+00 -2.0655158e+00  5.2204342e-03  4.7584870e-03
  2.7000366e-03  8.4708212e-04  6.0168497e-04  9.4006682e-04
  2.9140669e-03  7.7248965e-03  3.0901861e+00  5.2576523e+00
 -2.7537176e-03 -3.1672046e-03] 5 5
train:	 Loss = 1.3505,	 Acc = 0.5045
13234 0.264
25972 0.56
13392 0.621
1729 0.605
299 0.502
30 0.3
0 0.0
0 0.0
0.5812370238037757
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2770,	 Acc = 0.5525
1668 0.35
7898 0.588
3716 0.583
378 0.423
10 0.0
0 0.0
0 0.0
0 0.0
0.5805699050158307
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 1
Testing:	 Loss = 1.3745,	 Acc1 = 0.3695,	 Acc2 = 0.3892

 ===== Epoch 185	 =====
[ 2.5083613   1.2615812  -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
  2.3103092   1.4484667 ] [-6.4646250e-01 -4.6592492e-01  5.2204342e-03  4.7584870e-03
  2.7000366e-03  8.4708212e-04  6.0168497e-04  9.4006682e-04
  3.9391162e+00  5.3349705e+00  3.7514956e-03  5.4717045e-03
  2.6006466e-01 -7.3943377e-01] 2 5
train:	 Loss = 1.3494,	 Acc = 0.5049
13228 0.263
25973 0.563
13397 0.621
1729 0.604
299 0.482
30 0.233
0 0.0
0 0.0
0.5822390653664189
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3333,	 Acc = 0.5245
1668 0.348
7898 0.544
3716 0.581
378 0.362
10 0.0
0 0.0
0 0.0
0 0.0
0.5490751541409765
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4372,	 Acc1 = 0.3476,	 Acc2 = 0.3628

 ===== Epoch 186	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722  1.7229623   1.2426969
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249  -0.32139215 -0.17909689
 -0.00275372 -0.0031672 ] 1 1
train:	 Loss = 1.3491,	 Acc = 0.5042
13229 0.261
25974 0.558
13396 0.624
1728 0.625
299 0.512
30 0.3
0 0.0
0 0.0
0.5818910372462404
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3358,	 Acc = 0.5286
1668 0.362
7898 0.556
3716 0.557
378 0.429
10 0.0
0 0.0
0 0.0
0 0.0
0.5517413764372605
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4195,	 Acc1 = 0.3712,	 Acc2 = 0.3912

 ===== Epoch 187	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  4.28595     1.1228659  -0.40214875 -0.40990722  2.6836667   2.3311055
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.36863953  0.3798674   0.00291407  0.0077249  -0.3835641   0.24220102
 -0.00275372 -0.0031672 ] 2 4
train:	 Loss = 1.3566,	 Acc = 0.5032
13229 0.265
25973 0.557
13397 0.622
1728 0.593
299 0.538
30 0.267
0 0.0
0 0.0
0.5792840418084824
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3362,	 Acc = 0.5083
1668 0.354
7898 0.539
3716 0.523
378 0.413
10 0.0
0 0.0
0 0.0
0 0.0
0.5297450424929179
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4172,	 Acc1 = 0.3749,	 Acc2 = 0.3956

 ===== Epoch 188	 =====
[-0.3673141  -0.38194367  2.9293532   0.9599357  -0.44182304 -0.36572987
  2.6948519   2.6817741  -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 1.1263542e-03  2.0405303e-03 -4.2870066e-01  8.5422438e-01
  2.7000366e-03  8.4708212e-04  4.3723732e-02  1.7789837e+00
  2.9140669e-03  7.7248965e-03  3.7514956e-03  5.4717045e-03
 -2.7537176e-03 -3.1672046e-03] 6 6
train:	 Loss = 1.3527,	 Acc = 0.5022
13232 0.266
25966 0.558
13401 0.615
1728 0.608
299 0.492
30 0.233
0 0.0
0 0.0
0.577563731170336
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2816,	 Acc = 0.5587
1668 0.332
7898 0.589
3716 0.608
378 0.458
10 0.0
0 0.0
0 0.0
0 0.0
0.5901516413931012
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3719,	 Acc1 = 0.3839,	 Acc2 = 0.4066

 ===== Epoch 189	 =====
[ 0.15457655  1.0517156  -0.4204066  -0.33581436 -0.44182304 -0.36572987
  3.1565473   1.0836834  -0.40214875 -0.40990722  3.5312138   1.4118415
 -0.38765725 -0.38159567] [1.3115002e-01 5.6076288e-01 5.2204342e-03 4.7584870e-03 2.7000366e-03
 8.4708212e-04 6.0771048e-02 6.2276959e-01 2.9140669e-03 7.7248965e-03
 6.7976552e-01 4.3880668e-01 8.3514652e+00 5.9546680e+00] 2 6
train:	 Loss = 1.3518,	 Acc = 0.5042
13231 0.262
25971 0.562
13397 0.618
1728 0.61
299 0.498
30 0.267
0 0.0
0 0.0
0.5815087507543754
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2835,	 Acc = 0.5577
1668 0.349
7898 0.581
3716 0.619
378 0.402
10 0.0
0 0.0
0 0.0
0 0.0
0.5867355440759874
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 1
Testing:	 Loss = 1.3894,	 Acc1 = 0.3633,	 Acc2 = 0.3817

 ===== Epoch 190	 =====
[-0.3673141  -0.38194367  4.0725346  -4.5316844   3.6274538   8.100566
  3.0555162   3.848856   -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 1.1263542e-03  2.0405303e-03  7.2783887e-02  1.0074333e-01
 -9.9549606e-02 -7.6269636e+00 -6.0746465e+00 -7.3249888e+00
  5.7639074e+00  2.0224917e+00  3.7514956e-03  5.4717045e-03
 -2.7537176e-03 -3.1672046e-03] 2 2
train:	 Loss = 1.3520,	 Acc = 0.5048
13233 0.262
25966 0.563
13399 0.619
1729 0.617
299 0.485
30 0.3
0 0.0
0 0.0
0.5823817685826714
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2868,	 Acc = 0.5442
1668 0.217
7898 0.584
3716 0.62
378 0.426
10 0.0
0 0.0
0 0.0
0 0.0
0.5896517247125479
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3900,	 Acc1 = 0.3439,	 Acc2 = 0.3907

 ===== Epoch 191	 =====
[ 1.8621118   1.5801722  -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
  1.6208776   1.8280933 ] [-9.3504265e-02 -4.8010567e-01  5.2204342e-03  4.7584870e-03
  2.7000366e-03  8.4708212e-04  6.0168497e-04  9.4006682e-04
  2.9140669e-03  7.7248965e-03  3.7514956e-03  5.4717045e-03
 -1.7748766e-02 -7.0558244e-01] 5 5
train:	 Loss = 1.3537,	 Acc = 0.5041
13233 0.262
25967 0.56
13399 0.623
1728 0.61
299 0.482
30 0.3
0 0.0
0 0.0
0.5816333920768655
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2805,	 Acc = 0.5534
1668 0.359
7898 0.584
3716 0.592
378 0.41
10 0.0
0 0.0
0 0.0
0 0.0
0.580403266122313
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3662,	 Acc1 = 0.3893,	 Acc2 = 0.4130

 ===== Epoch 192	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722  1.6686469   2.2771754
 -0.38765725 -0.38159567] [ 1.1263542e-03  2.0405303e-03  5.2204342e-03  4.7584870e-03
  2.7000366e-03  8.4708212e-04  6.0168497e-04  9.4006682e-04
  2.9140669e-03  7.7248965e-03  6.2349189e-02 -1.1621253e+00
 -2.7537176e-03 -3.1672046e-03] 5 5
train:	 Loss = 1.3510,	 Acc = 0.5038
13228 0.261
25975 0.562
13396 0.618
1728 0.606
299 0.492
30 0.267
0 0.0
0 0.0
0.5811769817514725
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3069,	 Acc = 0.5319
1668 0.355
7898 0.567
3716 0.545
378 0.463
10 0.0
0 0.0
0 0.0
0 0.0
0.5564905849025162
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3984,	 Acc1 = 0.3693,	 Acc2 = 0.3889

 ===== Epoch 193	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436  2.3819394   2.8796096
 -0.36378932 -0.37167084  1.340776    2.1223483  -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 1.1263542e-03  2.0405303e-03  5.2204342e-03  4.7584870e-03
 -3.3661491e-01 -7.7654262e+00  6.0168497e-04  9.4006682e-04
  4.3366471e-01  5.4489311e-02  3.7514956e-03  5.4717045e-03
 -2.7537176e-03 -3.1672046e-03] 2 2
train:	 Loss = 1.3511,	 Acc = 0.5046
13229 0.263
25973 0.562
13397 0.62
1728 0.603
299 0.508
30 0.3
0 0.0
0 0.0
0.5818427595529485
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2903,	 Acc = 0.5531
1668 0.351
7898 0.579
3716 0.605
378 0.407
10 0.0
0 0.0
0 0.0
0 0.0
0.5811531411431428
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3832,	 Acc1 = 0.3837,	 Acc2 = 0.4063

 ===== Epoch 194	 =====
[ 2.7363682   3.2767954  -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
  2.5987444   3.52725   ] [ 0.02534464  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.34765446 -0.0031672 ] 0 0
train:	 Loss = 1.3517,	 Acc = 0.5033
13230 0.263
25971 0.559
13399 0.619
1728 0.616
298 0.477
30 0.233
0 0.0
0 0.0
0.5800222082749964
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2901,	 Acc = 0.5442
1668 0.355
7898 0.574
3716 0.581
378 0.415
10 0.0
0 0.0
0 0.0
0 0.0
0.570488251958007
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3798,	 Acc1 = 0.3734,	 Acc2 = 0.3939

 ===== Epoch 195	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436  1.0259073   0.9128717
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.15720887  0.02310001
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 2 3
train:	 Loss = 1.3493,	 Acc = 0.5053
13231 0.264
25972 0.561
13396 0.624
1728 0.602
299 0.498
30 0.267
0 0.0
0 0.0
0.5822812311406156
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2888,	 Acc = 0.5481
1668 0.299
7898 0.577
3716 0.611
378 0.444
10 0.0
0 0.0
0 0.0
0 0.0
0.5827362106315614
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3888,	 Acc1 = 0.3674,	 Acc2 = 0.3867

 ===== Epoch 196	 =====
[ 3.067297    2.591572   -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
  2.3045921   2.8517764 ] [-6.2374061e-01 -5.5100948e-01  5.2204342e-03  4.7584870e-03
  2.7000366e-03  8.4708212e-04  6.0168497e-04  9.4006682e-04
  2.9140669e-03  7.7248965e-03  3.7514956e-03  5.4717045e-03
 -2.1505833e-01 -8.3252496e-01] 0 5
train:	 Loss = 1.3512,	 Acc = 0.5038
13231 0.263
25970 0.561
13398 0.619
1728 0.605
299 0.478
30 0.267
0 0.0
0 0.0
0.5807362703681351
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3182,	 Acc = 0.5277
1668 0.359
7898 0.568
3716 0.532
378 0.407
10 0.0
0 0.0
0 0.0
0 0.0
0.5511581403099484
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4074,	 Acc1 = 0.3802,	 Acc2 = 0.4021

 ===== Epoch 197	 =====
[ 1.8052601   2.333665   -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.38544554  0.047419    0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 2 2
train:	 Loss = 1.3493,	 Acc = 0.5050
13229 0.264
25971 0.562
13399 0.621
1728 0.606
299 0.498
30 0.3
0 0.0
0 0.0
0.5820117314794699
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2931,	 Acc = 0.5522
1668 0.356
7898 0.576
3716 0.605
378 0.41
10 0.0
0 0.0
0 0.0
0 0.0
0.5794867522079653
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4006,	 Acc1 = 0.3445,	 Acc2 = 0.3591

 ===== Epoch 198	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722  2.3287299   1.2745647
  3.110101    3.571758  ] [ 1.1263542e-03  2.0405303e-03  5.2204342e-03  4.7584870e-03
  2.7000366e-03  8.4708212e-04  6.0168497e-04  9.4006682e-04
  2.9140669e-03  7.7248965e-03 -1.9133556e-01  2.0207739e-01
 -6.2812610e+00 -6.3926072e+00] 1 2
train:	 Loss = 1.3510,	 Acc = 0.5027
13230 0.26
25971 0.558
13398 0.621
1728 0.611
299 0.525
30 0.267
0 0.0
0 0.0
0.580094626563028
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3181,	 Acc = 0.5236
1668 0.35
7898 0.558
3716 0.543
378 0.402
10 0.0
0 0.0
0 0.0
0 0.0
0.5477420429928346
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 1
Testing:	 Loss = 1.4277,	 Acc1 = 0.3404,	 Acc2 = 0.3541

 ===== Epoch 199	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 0 1
train:	 Loss = 1.3491,	 Acc = 0.5031
13229 0.261
25970 0.561
13399 0.618
1729 0.606
299 0.488
30 0.267
0 0.0
0 0.0
0.5803944287541941
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3055,	 Acc = 0.5203
1668 0.364
7898 0.557
3716 0.529
378 0.373
10 0.0
0 0.0
0 0.0
0 0.0
0.5420763206132311
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 6
Testing:	 Loss = 1.4023,	 Acc1 = 0.3701,	 Acc2 = 0.3899

 ===== Epoch 200	 =====
[-0.3673141  -0.38194367  2.164516    2.0582597  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.10731869  0.22552364  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 2 2
train:	 Loss = 1.3528,	 Acc = 0.5029
13230 0.264
25967 0.56
13401 0.616
1729 0.61
299 0.498
30 0.267
0 0.0
0 0.0
0.5793463042533674
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3329,	 Acc = 0.4964
1668 0.348
7898 0.534
3716 0.491
378 0.423
10 0.0
0 0.0
0 0.0
0 0.0
0.5169971671388102
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4183,	 Acc1 = 0.3662,	 Acc2 = 0.3852

 ===== Epoch 201	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436  2.729933    1.7452946
 -0.36378932 -0.37167084  1.9106607   1.9313749  -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849 -0.539597    0.05524314
  0.00060168  0.00094007 -0.17428537  0.09345965  0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 2 1
train:	 Loss = 1.3518,	 Acc = 0.5027
13231 0.263
25971 0.56
13398 0.617
1727 0.605
299 0.478
30 0.233
0 0.0
0 0.0
0.5790947495473747
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3156,	 Acc = 0.5162
1668 0.358
7898 0.545
3716 0.537
378 0.431
10 0.0
0 0.0
0 0.0
0 0.0
0.538160306615564
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4035,	 Acc1 = 0.3728,	 Acc2 = 0.3931

 ===== Epoch 202	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 2 1
train:	 Loss = 1.3512,	 Acc = 0.5033
13232 0.265
25967 0.559
13400 0.617
1728 0.612
299 0.482
30 0.333
0 0.0
0 0.0
0.579446697566628
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3270,	 Acc = 0.5393
1668 0.357
7898 0.572
3716 0.561
378 0.46
10 0.0
0 0.0
0 0.0
0 0.0
0.5646558906848859
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4132,	 Acc1 = 0.3852,	 Acc2 = 0.4081

 ===== Epoch 203	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084  0.9507347   2.5861402  -0.4259259  -0.4217841
  2.5820355   1.1264386 ] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007 -0.13092884  0.07787151  0.0037515   0.0054717
 -0.35475674 -0.05394421] 1 1
train:	 Loss = 1.3506,	 Acc = 0.5024
13230 0.265
25969 0.557
13400 0.617
1728 0.609
299 0.505
30 0.167
0 0.0
0 0.0
0.5782117510742046
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3172,	 Acc = 0.5418
1668 0.36
7898 0.575
3716 0.565
378 0.431
10 0.0
0 0.0
0 0.0
0 0.0
0.5669888351941343
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3999,	 Acc1 = 0.3821,	 Acc2 = 0.4043

 ===== Epoch 204	 =====
[-0.3673141  -0.38194367  1.1707999   0.894127   -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.07578692  0.00955773  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 0
train:	 Loss = 1.3505,	 Acc = 0.5044
13231 0.264
25971 0.561
13399 0.619
1726 0.608
299 0.515
30 0.233
0 0.0
0 0.0
0.5810983705491852
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2786,	 Acc = 0.5571
1668 0.359
7898 0.581
3716 0.61
378 0.413
10 0.0
0 0.0
0 0.0
0 0.0
0.5845692384602567
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3799,	 Acc1 = 0.3730,	 Acc2 = 0.3934

 ===== Epoch 205	 =====
[ 0.6519166   2.5435302  -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [-1.0665499e+00 -3.2793899e+00  5.2204342e-03  4.7584870e-03
  2.7000366e-03  8.4708212e-04  6.0168497e-04  9.4006682e-04
  2.9140669e-03  7.7248965e-03  3.7514956e-03  5.4717045e-03
 -2.7537176e-03 -3.1672046e-03] 6 5
train:	 Loss = 1.3503,	 Acc = 0.5039
13231 0.266
25968 0.558
13400 0.621
1728 0.61
299 0.495
30 0.267
0 0.0
0 0.0
0.580084490042245
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3074,	 Acc = 0.5282
1668 0.354
7898 0.557
3716 0.555
378 0.437
10 0.0
0 0.0
0 0.0
0 0.0
0.5524079320113314
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3973,	 Acc1 = 0.3639,	 Acc2 = 0.3825

 ===== Epoch 206	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.0220633   1.4419243  -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
 -0.2902195   0.44788     0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 6 6
train:	 Loss = 1.3494,	 Acc = 0.5032
13231 0.263
25972 0.56
13396 0.617
1728 0.609
299 0.505
30 0.267
0 0.0
0 0.0
0.5797465298732649
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2984,	 Acc = 0.5464
1668 0.355
7898 0.571
3716 0.594
378 0.429
10 0.0
0 0.0
0 0.0
0 0.0
0.5729878353607732
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 1
Testing:	 Loss = 1.3940,	 Acc1 = 0.3697,	 Acc2 = 0.3894

 ===== Epoch 207	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436  0.31768078  2.8707302
  4.6861243   1.8561405  -0.40214875 -0.40990722  4.5134172   2.1153848
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.05571785 -0.4763547
 -0.11672996 -1.024107    0.00291407  0.0077249   0.19026297 -0.8491612
 -0.00275372 -0.0031672 ] 5 5
train:	 Loss = 1.3532,	 Acc = 0.5019
13233 0.265
25973 0.557
13392 0.615
1729 0.614
299 0.485
30 0.233
0 0.0
0 0.0
0.577601815416556
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2906,	 Acc = 0.5294
1668 0.362
7898 0.564
3716 0.547
378 0.384
10 0.0
0 0.0
0 0.0
0 0.0
0.5526578903516081
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3782,	 Acc1 = 0.3895,	 Acc2 = 0.4133

 ===== Epoch 208	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  1.326135    2.9672475  -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
 -0.03951329  0.50131845  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 6
train:	 Loss = 1.3503,	 Acc = 0.5047
13233 0.263
25969 0.562
13399 0.619
1726 0.618
299 0.505
30 0.267
0 0.0
0 0.0
0.5818506626753253
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3231,	 Acc = 0.5051
1668 0.353
7898 0.544
3716 0.505
378 0.386
10 0.0
0 0.0
0 0.0
0 0.0
0.5263289451758041
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 1
Testing:	 Loss = 1.4130,	 Acc1 = 0.3730,	 Acc2 = 0.3934

 ===== Epoch 209	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  1.2612265   3.7984786  -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 1.1263542e-03  2.0405303e-03  5.2204342e-03  4.7584870e-03
  2.7000366e-03  8.4708212e-04 -2.8866441e+00 -7.2375441e+00
  2.9140669e-03  7.7248965e-03  3.7514956e-03  5.4717045e-03
 -2.7537176e-03 -3.1672046e-03] 4 6
train:	 Loss = 1.3490,	 Acc = 0.5034
13232 0.266
25965 0.559
13402 0.616
1728 0.62
299 0.498
30 0.333
0 0.0
0 0.0
0.5792294322132098
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2836,	 Acc = 0.5554
1668 0.353
7898 0.584
3716 0.6
378 0.444
10 0.0
0 0.0
0 0.0
0 0.0
0.583652724545909
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3841,	 Acc1 = 0.3730,	 Acc2 = 0.3934

 ===== Epoch 210	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 1 1
train:	 Loss = 1.3498,	 Acc = 0.5031
13230 0.264
25972 0.562
13398 0.614
1727 0.6
299 0.495
30 0.2
0 0.0
0 0.0
0.5794670014000869
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2967,	 Acc = 0.5360
1668 0.357
7898 0.565
3716 0.569
378 0.421
10 0.0
0 0.0
0 0.0
0 0.0
0.5609065155807366
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 1
Testing:	 Loss = 1.3819,	 Acc1 = 0.3769,	 Acc2 = 0.3981

 ===== Epoch 211	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
  1.128867    1.3856319 ] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.37527797 -0.15549822] 2 1
train:	 Loss = 1.3520,	 Acc = 0.5033
13228 0.264
25972 0.56
13400 0.615
1727 0.616
299 0.508
30 0.3
0 0.0
0 0.0
0.5798735154967655
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2954,	 Acc = 0.5467
1668 0.354
7898 0.577
3716 0.584
378 0.405
10 0.0
0 0.0
0 0.0
0 0.0
0.5734044325945675
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 1
Testing:	 Loss = 1.3923,	 Acc1 = 0.3658,	 Acc2 = 0.3847

 ===== Epoch 212	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 5 1
train:	 Loss = 1.3486,	 Acc = 0.5024
13230 0.263
25972 0.558
13396 0.615
1729 0.623
299 0.502
30 0.2
0 0.0
0 0.0
0.5787428185197702
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3208,	 Acc = 0.5219
1668 0.359
7898 0.558
3716 0.533
378 0.384
10 0.0
0 0.0
0 0.0
0 0.0
0.5444925845692384
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 1
Testing:	 Loss = 1.4066,	 Acc1 = 0.3710,	 Acc2 = 0.3909

 ===== Epoch 213	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722  0.8128023   2.8802125
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249  -0.00553984 -0.5482341
 -0.00275372 -0.0031672 ] 5 5
train:	 Loss = 1.3490,	 Acc = 0.5030
13230 0.261
25970 0.561
13398 0.617
1729 0.607
299 0.505
30 0.267
0 0.0
0 0.0
0.580336020856467
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3171,	 Acc = 0.5373
1668 0.362
7898 0.559
3716 0.583
378 0.431
10 0.0
0 0.0
0 0.0
0 0.0
0.5616563906015664
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4012,	 Acc1 = 0.3753,	 Acc2 = 0.3961

 ===== Epoch 214	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 1
train:	 Loss = 1.3510,	 Acc = 0.5057
13229 0.264
25974 0.563
13396 0.62
1728 0.611
299 0.508
30 0.267
0 0.0
0 0.0
0.5827600357254931
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3437,	 Acc = 0.4932
1668 0.343
7898 0.533
3716 0.487
378 0.37
10 0.9
0 0.0
0 0.0
0 0.0
0.5140809865022496
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 1
Testing:	 Loss = 1.4280,	 Acc1 = 0.3668,	 Acc2 = 0.3859

 ===== Epoch 215	 =====
[ 1.818193    2.2830951  -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
  2.0100012   2.5114214 ] [7.4329686e-01 1.2045698e+00 5.2204342e-03 4.7584870e-03 2.7000366e-03
 8.4708212e-04 6.0168497e-04 9.4006682e-04 2.9140669e-03 7.7248965e-03
 3.7514956e-03 5.4717045e-03 1.2036867e-01 1.7105569e+00] 6 6
train:	 Loss = 1.3491,	 Acc = 0.5053
13231 0.263
25968 0.56
13400 0.621
1728 0.634
299 0.525
30 0.267
0 0.0
0 0.0
0.5826191913095956
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3018,	 Acc = 0.5427
1668 0.333
7898 0.572
3716 0.59
378 0.415
10 0.0
0 0.0
0 0.0
0 0.0
0.571821363106149
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3944,	 Acc1 = 0.3732,	 Acc2 = 0.3936

 ===== Epoch 216	 =====
[-0.3673141  -0.38194367  2.57137     2.5302668  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.39409614  0.00715811  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 2 0
train:	 Loss = 1.3497,	 Acc = 0.5048
13232 0.263
25968 0.562
13399 0.618
1728 0.628
299 0.515
30 0.267
0 0.0
0 0.0
0.5821263035921205
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3158,	 Acc = 0.5201
1668 0.349
7898 0.557
3716 0.531
378 0.41
10 0.0
0 0.0
0 0.0
0 0.0
0.5439093484419264
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 1
Testing:	 Loss = 1.4074,	 Acc1 = 0.3681,	 Acc2 = 0.3874

 ===== Epoch 217	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436  0.37733713  1.6454039
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 1.1263542e-03  2.0405303e-03  5.2204342e-03  4.7584870e-03
  3.4512129e-02  6.5359980e-01  6.0168497e-04  9.4006682e-04
  2.9140669e-03  7.7248965e-03  3.7514956e-03  5.4717045e-03
 -2.7537176e-03 -3.1672046e-03] 6 6
train:	 Loss = 1.3510,	 Acc = 0.5049
13234 0.264
25965 0.563
13399 0.618
1729 0.608
299 0.495
30 0.333
0 0.0
0 0.0
0.5819129930954565
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3496,	 Acc = 0.5026
1668 0.353
7898 0.532
3716 0.52
378 0.397
10 0.0
0 0.0
0 0.0
0 0.0
0.5234127645392435
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4303,	 Acc1 = 0.3683,	 Acc2 = 0.3877

 ===== Epoch 218	 =====
[ 0.8440264   3.0644014  -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00059582  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 0 0
train:	 Loss = 1.3481,	 Acc = 0.5050
13228 0.266
25971 0.56
13399 0.621
1729 0.614
299 0.508
30 0.267
0 0.0
0 0.0
0.5813700878632809
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2960,	 Acc = 0.5310
1668 0.35
7898 0.556
3716 0.573
378 0.397
10 0.0
0 0.0
0 0.0
0 0.0
0.5561573071154807
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 1
Testing:	 Loss = 1.3868,	 Acc1 = 0.3753,	 Acc2 = 0.3961

 ===== Epoch 219	 =====
[-0.3673141  -0.38194367  1.434297    2.3033402  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 9.7825241e-01  2.4014270e+00  7.2036617e-02  5.4227364e-01
  2.7000366e-03  8.4708212e-04  6.0168497e-04  9.4006682e-04
  2.9140669e-03  7.7248965e-03  3.7514956e-03  5.4717045e-03
 -2.7537176e-03 -3.1672046e-03] 6 6
train:	 Loss = 1.3490,	 Acc = 0.5040
13232 0.265
25969 0.563
13399 0.613
1728 0.613
298 0.513
30 0.267
0 0.0
0 0.0
0.5803640401699498
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2817,	 Acc = 0.5560
1668 0.348
7898 0.574
3716 0.622
378 0.46
10 0.0
0 0.0
0 0.0
0 0.0
0.5849858356940509
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3789,	 Acc1 = 0.3666,	 Acc2 = 0.3857

 ===== Epoch 220	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722  1.7712424   1.8677964
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249  -0.22777854  0.21411449
 -0.00275372 -0.0031672 ] 1 4
train:	 Loss = 1.3512,	 Acc = 0.5041
13227 0.265
25974 0.56
13399 0.619
1727 0.605
299 0.508
30 0.3
0 0.0
0 0.0
0.5804629607279924
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3044,	 Acc = 0.5111
1668 0.349
7898 0.54
3716 0.537
378 0.381
10 0.0
0 0.0
0 0.0
0 0.0
0.5336610564905849
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4063,	 Acc1 = 0.3633,	 Acc2 = 0.3817

 ===== Epoch 221	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 5.4949880e-01  4.2449269e+00  5.2204342e-03  4.7584870e-03
  2.7000366e-03  8.4708212e-04  6.0168497e-04  9.4006682e-04
  2.9140669e-03  7.7248965e-03  3.7514956e-03  5.4717045e-03
 -2.7537176e-03 -3.1672046e-03] 2 1
train:	 Loss = 1.3498,	 Acc = 0.5048
13229 0.265
25969 0.562
13400 0.617
1729 0.611
299 0.498
30 0.333
0 0.0
0 0.0
0.5815048156999059
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3226,	 Acc = 0.4952
1668 0.311
7898 0.53
3716 0.516
378 0.384
10 0.0
0 0.0
0 0.0
0 0.0
0.5207465422429595
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4195,	 Acc1 = 0.3573,	 Acc2 = 0.3745

 ===== Epoch 222	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 3 1
train:	 Loss = 1.3503,	 Acc = 0.5036
13232 0.264
25969 0.56
13398 0.618
1729 0.614
298 0.507
30 0.2
0 0.0
0 0.0
0.5800502124372344
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2888,	 Acc = 0.5426
1668 0.354
7898 0.566
3716 0.592
378 0.418
10 0.0
0 0.0
0 0.0
0 0.0
0.5687385435760707
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3869,	 Acc1 = 0.3670,	 Acc2 = 0.3862

 ===== Epoch 223	 =====
[-0.3673141  -0.38194367  1.9600711   2.5257282  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 1.1263542e-03  2.0405303e-03 -4.3828349e+00 -3.0211637e+00
  4.6395645e+00  1.2593740e+00  6.0168497e-04  9.4006682e-04
  2.9140669e-03  7.7248965e-03  3.7514956e-03  5.4717045e-03
 -2.7537176e-03 -3.1672046e-03] 6 6
train:	 Loss = 1.3478,	 Acc = 0.5043
13232 0.265
25969 0.559
13398 0.619
1728 0.62
299 0.518
30 0.3
0 0.0
0 0.0
0.5807502896871379
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3155,	 Acc = 0.5037
1668 0.35
7898 0.533
3716 0.517
378 0.458
10 0.0
0 0.0
0 0.0
0 0.0
0.5250791534744209
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4099,	 Acc1 = 0.3627,	 Acc2 = 0.3810

 ===== Epoch 224	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084  1.8820238   3.218584   -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.48361796  0.19478254  0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
train:	 Loss = 1.3481,	 Acc = 0.5055
13230 0.264
25970 0.562
13399 0.62
1728 0.618
299 0.512
30 0.333
0 0.0
0 0.0
0.5826534060734804
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3061,	 Acc = 0.5347
1668 0.359
7898 0.559
3716 0.575
378 0.426
10 0.0
0 0.0
0 0.0
0 0.0
0.5591568071988002
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3889,	 Acc1 = 0.3738,	 Acc2 = 0.3944

 ===== Epoch 225	 =====
[ 4.520224    1.9771464   0.6324007   2.6686919  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [-1.5538762e+00 -2.4754103e-01 -1.5800752e-02 -2.9039490e-01
  2.7000366e-03  8.4708212e-04  6.0168497e-04  9.4006682e-04
  2.9140669e-03  7.7248965e-03  3.7514956e-03  5.4717045e-03
 -2.7537176e-03 -3.1672046e-03] 5 5
train:	 Loss = 1.3457,	 Acc = 0.5046
13228 0.258
25967 0.561
13403 0.623
1729 0.617
299 0.535
30 0.367
0 0.0
0 0.0
0.583204595925461
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2912,	 Acc = 0.5576
1668 0.36
7898 0.587
3716 0.598
378 0.429
10 0.0
0 0.0
0 0.0
0 0.0
0.5850691551408098
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3786,	 Acc1 = 0.3846,	 Acc2 = 0.4073

 ===== Epoch 226	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 1 1
train:	 Loss = 1.3487,	 Acc = 0.5042
13231 0.264
25974 0.561
13395 0.618
1727 0.62
299 0.482
30 0.267
0 0.0
0 0.0
0.5809535304767652
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2989,	 Acc = 0.5284
1668 0.352
7898 0.567
3716 0.535
378 0.452
10 0.0
0 0.0
0 0.0
0 0.0
0.5529078486918847
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3896,	 Acc1 = 0.3765,	 Acc2 = 0.3976

 ===== Epoch 227	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722  1.4189469   1.5589236
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.02733328  0.02954587
 -0.00275372 -0.0031672 ] 3 3
train:	 Loss = 1.3482,	 Acc = 0.5049
13226 0.261
25971 0.563
13402 0.619
1729 0.614
298 0.493
30 0.267
0 0.0
0 0.0
0.5827419744146753
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3039,	 Acc = 0.5482
1668 0.371
7898 0.586
3716 0.562
378 0.421
10 0.0
0 0.0
0 0.0
0 0.0
0.5728211964672555
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3880,	 Acc1 = 0.3891,	 Acc2 = 0.4128

 ===== Epoch 228	 =====
[-0.3673141  -0.38194367  2.8096185  -4.381913    2.366642    7.8253117
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 1.1263542e-03  2.0405303e-03  6.5609747e-01  7.4843774e+00
  6.3058329e-01 -7.6813598e+00  6.0168497e-04  9.4006682e-04
  2.9140669e-03  7.7248965e-03  3.7514956e-03  5.4717045e-03
 -2.7537176e-03 -3.1672046e-03] 2 5
train:	 Loss = 1.3491,	 Acc = 0.5052
13231 0.263
25972 0.561
13395 0.623
1729 0.619
299 0.508
30 0.3
0 0.0
0 0.0
0.5826674713337356
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3007,	 Acc = 0.5170
1668 0.347
7898 0.554
3716 0.527
378 0.413
10 0.0
0 0.0
0 0.0
0 0.0
0.5407432094650891
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 1
Testing:	 Loss = 1.3945,	 Acc1 = 0.3646,	 Acc2 = 0.3832

 ===== Epoch 229	 =====
[ 1.7180462   3.458847   -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722  1.4653412   1.058844
 -0.38765725 -0.38159567] [-2.1833539e+00 -4.3060780e+00  5.2204342e-03  4.7584870e-03
  2.7000366e-03  8.4708212e-04  6.0168497e-04  9.4006682e-04
  2.9140669e-03  7.7248965e-03  1.4024086e-01  9.4840657e-03
 -2.7537176e-03 -3.1672046e-03] 3 2
train:	 Loss = 1.3484,	 Acc = 0.5051
13231 0.264
25970 0.562
13398 0.619
1728 0.623
299 0.525
30 0.3
0 0.0
0 0.0
0.5822570911285455
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2933,	 Acc = 0.5309
1668 0.359
7898 0.562
3716 0.555
378 0.421
10 0.0
0 0.0
0 0.0
0 0.0
0.5548241959673388
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3834,	 Acc1 = 0.3736,	 Acc2 = 0.3941

 ===== Epoch 230	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722  1.6841117   1.3848763
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.37248665 -0.17107217
 -0.00275372 -0.0031672 ] 2 2
train:	 Loss = 1.3470,	 Acc = 0.5051
13232 0.262
25964 0.564
13402 0.618
1729 0.607
299 0.495
30 0.267
0 0.0
0 0.0
0.582826380842024
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2924,	 Acc = 0.5529
1668 0.359
7898 0.576
3716 0.608
378 0.397
10 0.0
0 0.0
0 0.0
0 0.0
0.5798200299950008
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3970,	 Acc1 = 0.3677,	 Acc2 = 0.3869

 ===== Epoch 231	 =====
[-0.3673141  -0.38194367  0.5428033   2.4962277  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.12608646  0.05994977  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 2 4
train:	 Loss = 1.3505,	 Acc = 0.5036
13232 0.263
25972 0.559
13395 0.621
1729 0.608
298 0.48
30 0.3
0 0.0
0 0.0
0.5803640401699498
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2986,	 Acc = 0.5446
1668 0.358
7898 0.57
3716 0.589
378 0.421
10 0.0
0 0.0
0 0.0
0 0.0
0.570488251958007
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3894,	 Acc1 = 0.3705,	 Acc2 = 0.3904

 ===== Epoch 232	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436  2.8289778   8.726548
 -0.36378932 -0.37167084  2.6867347   1.0137113  -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 1.1263542e-03  2.0405303e-03  5.2204342e-03  4.7584870e-03
  5.2151781e-01 -7.7060852e+00  6.0168497e-04  9.4006682e-04
  4.9587321e-01  7.7871509e-02  3.7514956e-03  5.4717045e-03
 -2.7537176e-03 -3.1672046e-03] 3 3
train:	 Loss = 1.3512,	 Acc = 0.5031
13227 0.262
25969 0.559
13402 0.62
1729 0.611
299 0.525
30 0.3
0 0.0
0 0.0
0.5801008955079775
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3337,	 Acc = 0.5198
1668 0.353
7898 0.554
3716 0.534
378 0.423
10 0.0
0 0.0
0 0.0
0 0.0
0.5429928345275787
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4285,	 Acc1 = 0.3563,	 Acc2 = 0.3733

 ===== Epoch 233	 =====
[-0.3673141  -0.38194367  1.1716143   1.5022899  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.1200804  -0.04803317  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 3 2
train:	 Loss = 1.3483,	 Acc = 0.5048
13229 0.264
25968 0.561
13401 0.62
1729 0.612
299 0.522
30 0.3
0 0.0
0 0.0
0.5815289545465517
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2707,	 Acc = 0.5564
1668 0.353
7898 0.579
3716 0.614
378 0.442
10 0.0
0 0.0
0 0.0
0 0.0
0.5847358773537744
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3729,	 Acc1 = 0.3697,	 Acc2 = 0.3894

 ===== Epoch 234	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084  1.3029747   3.059853   -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.13487177  0.3038995   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
train:	 Loss = 1.3477,	 Acc = 0.5029
13236 0.262
25969 0.558
13395 0.619
1727 0.618
299 0.505
30 0.3
0 0.0
0 0.0
0.5797923708353453
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3419,	 Acc = 0.4909
1668 0.358
7898 0.527
3716 0.486
378 0.378
10 0.0
0 0.0
0 0.0
0 0.0
0.5093317780369938
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4406,	 Acc1 = 0.3456,	 Acc2 = 0.3603

 ===== Epoch 235	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722  1.4593059   3.2724319
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.00232223  0.0054717
 -0.00275372 -0.0031672 ] 0 0
train:	 Loss = 1.3483,	 Acc = 0.5072
13227 0.263
25974 0.564
13398 0.625
1728 0.606
299 0.508
30 0.3
0 0.0
0 0.0
0.5850491201815153
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3015,	 Acc = 0.5241
1668 0.354
7898 0.554
3716 0.548
378 0.431
10 0.0
0 0.0
0 0.0
0 0.0
0.5477420429928346
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4073,	 Acc1 = 0.3625,	 Acc2 = 0.3807

 ===== Epoch 236	 =====
[ 0.09620149  1.4891461  -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [-1.9845746e-02  1.1024684e+00  5.2204342e-03  4.7584870e-03
  2.7000366e-03  8.4708212e-04  6.0168497e-04  9.4006682e-04
  2.9140669e-03  7.7248965e-03  3.7514956e-03  5.4717045e-03
 -2.7537176e-03 -3.1672046e-03] 6 6
train:	 Loss = 1.3481,	 Acc = 0.5055
13234 0.264
25968 0.563
13398 0.618
1728 0.615
298 0.52
30 0.367
0 0.0
0 0.0
0.5827096711892231
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3027,	 Acc = 0.4985
1668 0.35
7898 0.537
3716 0.5
378 0.341
10 0.0
0 0.0
0 0.0
0 0.0
0.519080153307782
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3923,	 Acc1 = 0.3726,	 Acc2 = 0.3929

 ===== Epoch 237	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436  1.9046897   0.7708048
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849 -0.03214177  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 3 0
train:	 Loss = 1.3460,	 Acc = 0.5059
13231 0.262
25974 0.562
13394 0.623
1728 0.622
299 0.502
30 0.367
0 0.0
0 0.0
0.5836330718165359
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2808,	 Acc = 0.5553
1668 0.354
7898 0.583
3716 0.602
378 0.426
10 0.0
0 0.0
0 0.0
0 0.0
0.5832361273121146
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3682,	 Acc1 = 0.3854,	 Acc2 = 0.4083

 ===== Epoch 238	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084  1.5028642   3.350033   -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 1.1263542e-03  2.0405303e-03  5.2204342e-03  4.7584870e-03
  2.7000366e-03  8.4708212e-04  6.0168497e-04  9.4006682e-04
 -3.1321156e+00 -5.9001794e+00  3.7514956e-03  5.4717045e-03
 -2.7537176e-03 -3.1672046e-03] 4 6
train:	 Loss = 1.3471,	 Acc = 0.5039
13232 0.264
25971 0.559
13397 0.62
1727 0.615
299 0.485
30 0.233
0 0.0
0 0.0
0.5806054461181923
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3260,	 Acc = 0.5361
1668 0.355
7898 0.566
3716 0.564
378 0.444
10 0.0
0 0.0
0 0.0
0 0.0
0.5612397933677721
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4146,	 Acc1 = 0.3707,	 Acc2 = 0.3907

 ===== Epoch 239	 =====
[ 0.21049283  1.5321305  -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00111376  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 0 0
train:	 Loss = 1.3479,	 Acc = 0.5065
13228 0.264
25974 0.565
13397 0.618
1728 0.613
299 0.528
30 0.4
0 0.0
0 0.0
0.5839287438447427
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3322,	 Acc = 0.5164
1668 0.36
7898 0.558
3716 0.512
378 0.397
10 0.0
0 0.0
0 0.0
0 0.0
0.538160306615564
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4294,	 Acc1 = 0.3561,	 Acc2 = 0.3730

 ===== Epoch 240	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  1.7522715   2.9924362  -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 1.1263542e-03  2.0405303e-03  5.5704136e+00  1.1925709e+00
  2.7000366e-03  8.4708212e-04  2.2222605e-01  1.7582960e-01
  2.9140669e-03  7.7248965e-03  3.7514956e-03  5.4717045e-03
 -2.7537176e-03 -3.1672046e-03] 2 4
train:	 Loss = 1.3498,	 Acc = 0.5023
13231 0.258
25972 0.56
13396 0.619
1728 0.605
299 0.522
30 0.267
0 0.0
0 0.0
0.5803258901629451
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 2
val:	 Loss = 1.3279,	 Acc = 0.4959
1668 0.356
7898 0.535
3716 0.489
378 0.384
10 0.0
0 0.0
0 0.0
0 0.0
0.5154140976503916
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4132,	 Acc1 = 0.3662,	 Acc2 = 0.3852

 ===== Epoch 241	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 1 1
train:	 Loss = 1.3475,	 Acc = 0.5053
13229 0.265
25969 0.562
13401 0.62
1728 0.611
299 0.522
30 0.333
0 0.0
0 0.0
0.5821082868660535
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3013,	 Acc = 0.5347
1668 0.354
7898 0.556
3716 0.579
378 0.452
10 0.0
0 0.0
0 0.0
0 0.0
0.5598233627728711
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3993,	 Acc1 = 0.3615,	 Acc2 = 0.3795

 ===== Epoch 242	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722  2.3890805   1.8530881
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.50897604 -0.06675079
 -0.00275372 -0.0031672 ] 2 2
train:	 Loss = 1.3491,	 Acc = 0.5051
13229 0.264
25971 0.561
13398 0.621
1729 0.617
299 0.505
30 0.333
0 0.0
0 0.0
0.5822289810992831
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3134,	 Acc = 0.4991
1668 0.335
7898 0.535
3716 0.512
378 0.37
10 0.0
0 0.0
0 0.0
0 0.0
0.5219130144975838
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4069,	 Acc1 = 0.3615,	 Acc2 = 0.3795

 ===== Epoch 243	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436  0.9555442   1.9495157
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849 -0.0344129  -0.03376859
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 1 2
train:	 Loss = 1.3497,	 Acc = 0.5046
13231 0.263
25968 0.562
13400 0.619
1728 0.601
299 0.512
30 0.3
0 0.0
0 0.0
0.5816294508147254
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3212,	 Acc = 0.5331
1668 0.359
7898 0.573
3716 0.536
378 0.45
10 0.0
0 0.0
0 0.0
0 0.0
0.5572404599233461
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4060,	 Acc1 = 0.3837,	 Acc2 = 0.4063

 ===== Epoch 244	 =====
[ 2.5954015   1.2059541  -0.4204066  -0.33581436  4.3788915   2.8107958
 -0.36378932 -0.37167084  5.3219504   2.367885    0.4209006   1.4363551
  3.4165635   1.2939979 ] [-3.1024113e+00 -1.7790643e+00  5.2204342e-03  4.7584870e-03
  6.6845477e-01 -7.6417990e+00  6.0168497e-04  9.4006682e-04
 -6.3991064e-01  1.3632703e-01 -5.5376277e-03  6.1644759e-02
 -2.0322040e-01  1.4493240e-01] 2 4
train:	 Loss = 1.3491,	 Acc = 0.5046
13230 0.266
25970 0.561
13398 0.618
1729 0.605
299 0.502
30 0.267
0 0.0
0 0.0
0.5807946700140009
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3156,	 Acc = 0.5137
1668 0.348
7898 0.549
3716 0.521
378 0.439
10 0.0
0 0.0
0 0.0
0 0.0
0.5366605565739043
0.596233961006499
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4088,	 Acc1 = 0.3573,	 Acc2 = 0.3745

 ===== Epoch 245	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084  0.6626434   1.9437758  -0.4259259  -0.4217841
  2.5033314   1.956381  ] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007 -0.0065119   0.40132537  0.0037515   0.0054717
  0.12036867  0.36073467] 6 4
train:	 Loss = 1.3469,	 Acc = 0.5035
13228 0.263
25972 0.56
13400 0.618
1727 0.611
299 0.502
30 0.4
0 0.0
0 0.0
0.5801873129284542
0.596233961006499
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2684,	 Acc = 0.5684
1668 0.345
7898 0.593
3716 0.63
378 0.455
10 0.0
0 0.0
0 0.0
0 0.0
0.5994000999833361
0.5994000999833361
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3698,	 Acc1 = 0.3765,	 Acc2 = 0.3976

 ===== Epoch 246	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 6 1
train:	 Loss = 1.3472,	 Acc = 0.5043
13228 0.265
25970 0.559
13400 0.622
1729 0.604
299 0.498
30 0.4
0 0.0
0 0.0
0.5806459399439993
0.5994000999833361
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 2
val:	 Loss = 1.3461,	 Acc = 0.4830
1668 0.358
7898 0.527
3716 0.455
378 0.376
10 0.9
0 0.0
0 0.0
0 0.0
0.5004165972337944
0.5994000999833361
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4245,	 Acc1 = 0.3759,	 Acc2 = 0.3969

 ===== Epoch 247	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722  1.3435091   2.6546865
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249  -0.05984753 -0.41181386
 -0.00275372 -0.0031672 ] 5 5
train:	 Loss = 1.3489,	 Acc = 0.5050
13232 0.264
25967 0.561
13399 0.623
1729 0.602
299 0.485
30 0.3
0 0.0
0 0.0
0.5820056006179992
0.5994000999833361
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3143,	 Acc = 0.5185
1668 0.323
7898 0.549
3716 0.555
378 0.399
10 0.0
0 0.0
0 0.0
0 0.0
0.5456590568238627
0.5994000999833361
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4174,	 Acc1 = 0.3557,	 Acc2 = 0.3725

 ===== Epoch 248	 =====
[ 4.2869678   2.8772924  -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [-4.8743801e+00 -3.6537623e+00  5.2204342e-03  4.7584870e-03
  2.7000366e-03  8.4708212e-04  6.0168497e-04  9.4006682e-04
  2.9140669e-03  7.7248965e-03  3.7514956e-03  5.4717045e-03
 -2.7537176e-03 -3.1672046e-03] 4 1
train:	 Loss = 1.3479,	 Acc = 0.5058
13231 0.266
25971 0.563
13397 0.621
1728 0.604
299 0.505
30 0.333
0 0.0
0 0.0
0.5824502112251057
0.5994000999833361
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2803,	 Acc = 0.5495
1668 0.356
7898 0.576
3716 0.589
378 0.489
10 0.0
0 0.0
0 0.0
0 0.0
0.5764872521246459
0.5994000999833361
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3730,	 Acc1 = 0.3714,	 Acc2 = 0.3914

 ===== Epoch 249	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 3.3488209e+00  3.7145662e+00  5.2204342e-03  4.7584870e-03
  2.7000366e-03  8.4708212e-04  6.0168497e-04  9.4006682e-04
  2.9140669e-03  7.7248965e-03  3.7514956e-03  5.4717045e-03
 -2.7537176e-03 -3.1672046e-03] 3 1
train:	 Loss = 1.3480,	 Acc = 0.5054
13229 0.265
25973 0.564
13396 0.616
1729 0.611
299 0.492
30 0.367
0 0.0
0 0.0
0.5821324257126994
0.5994000999833361
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2899,	 Acc = 0.5484
1668 0.353
7898 0.56
3716 0.625
378 0.399
10 0.9
0 0.0
0 0.0
0 0.0
0.5755707382102982
0.5994000999833361
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4057,	 Acc1 = 0.3522,	 Acc2 = 0.3683

 ===== Epoch 250	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436  1.7134829   2.8174553
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 1.1263542e-03  2.0405303e-03  5.2204342e-03  4.7584870e-03
  5.0637156e-01  8.4708212e-04  6.0168497e-04  9.4006682e-04
  1.6882914e+00  3.5267470e+00  3.7514956e-03  5.4717045e-03
 -2.7537176e-03 -3.1672046e-03] 2 0
train:	 Loss = 1.3483,	 Acc = 0.5053
13232 0.263
25967 0.563
13400 0.621
1728 0.606
299 0.508
30 0.3
0 0.0
0 0.0
0.5826332560834299
0.5994000999833361
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3518,	 Acc = 0.5078
1668 0.35
7898 0.546
3716 0.51
378 0.394
10 0.0
0 0.0
0 0.0
0 0.0
0.5298283619396768
0.5994000999833361
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4411,	 Acc1 = 0.3563,	 Acc2 = 0.3733

 ===== Epoch 251	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084  3.3276415   1.4502215  -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00479635  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 0 0
train:	 Loss = 1.3482,	 Acc = 0.5044
13229 0.263
25972 0.561
13398 0.62
1728 0.622
299 0.478
30 0.4
0 0.0
0 0.0
0.5816737876264272
0.5994000999833361
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3065,	 Acc = 0.5361
1668 0.358
7898 0.568
3716 0.562
378 0.413
10 0.0
0 0.0
0 0.0
0 0.0
0.5609065155807366
0.5994000999833361
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3951,	 Acc1 = 0.3747,	 Acc2 = 0.3954

 ===== Epoch 252	 =====
[-0.3673141  -0.38194367  0.5134805   3.0022736  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 1.1263542e-03  2.0405303e-03 -1.7162611e+00 -3.5250840e+00
  2.7000366e-03  8.4708212e-04  6.0168497e-04  9.4006682e-04
  2.9140669e-03  7.7248965e-03  3.7514956e-03  5.4717045e-03
 -2.7537176e-03 -3.1672046e-03] 6 6
train:	 Loss = 1.3484,	 Acc = 0.5067
13229 0.264
25968 0.564
13401 0.624
1729 0.602
299 0.485
30 0.4
0 0.0
0 0.0
0.5841359499843097
0.5994000999833361
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3223,	 Acc = 0.5378
1668 0.362
7898 0.565
3716 0.573
378 0.421
10 0.0
0 0.0
0 0.0
0 0.0
0.5623229461756374
0.5994000999833361
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4139,	 Acc1 = 0.3594,	 Acc2 = 0.3770

 ===== Epoch 253	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722  2.2781866   1.191218
  0.6874206   3.5848484 ] [ 1.1263542e-03  2.0405303e-03  5.2204342e-03  4.7584870e-03
  2.7000366e-03  8.4708212e-04  6.0168497e-04  9.4006682e-04
  2.9140669e-03  7.7248965e-03 -3.2496640e-01  1.7508788e-02
 -1.9325281e+00 -6.4137645e+00] 4 1
train:	 Loss = 1.3464,	 Acc = 0.5046
13235 0.262
25964 0.561
13399 0.623
1729 0.608
299 0.508
30 0.367
0 0.0
0 0.0
0.5821201805847276
0.5994000999833361
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2841,	 Acc = 0.5546
1668 0.359
7898 0.582
3716 0.597
378 0.442
10 0.0
0 0.0
0 0.0
0 0.0
0.5818196967172138
0.5994000999833361
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 1
Testing:	 Loss = 1.3942,	 Acc1 = 0.3650,	 Acc2 = 0.3837

 ===== Epoch 254	 =====
[ 0.9408348   2.2249396  -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
  0.80965364  2.6135278 ] [ 2.2734284e-01  4.5866135e-01  5.2204342e-03  4.7584870e-03
  2.7000366e-03  8.4708212e-04  6.0168497e-04  9.4006682e-04
  2.9140669e-03  7.7248965e-03  3.7514956e-03  5.4717045e-03
 -7.1417183e-02  6.9501668e-01] 6 6
train:	 Loss = 1.3467,	 Acc = 0.5047
13233 0.26
25965 0.561
13401 0.624
1728 0.616
299 0.492
30 0.3
0 0.0
0 0.0
0.5829852980228376
0.5994000999833361
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3019,	 Acc = 0.5400
1668 0.356
7898 0.562
3716 0.591
378 0.405
10 0.0
0 0.0
0 0.0
0 0.0
0.5656557240459923
0.5994000999833361
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 1
Testing:	 Loss = 1.4086,	 Acc1 = 0.3646,	 Acc2 = 0.3832

 ===== Epoch 255	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084  1.4387174   2.8093557  -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.13298658  0.18309145  0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
train:	 Loss = 1.3475,	 Acc = 0.5072
13231 0.266
25971 0.563
13399 0.624
1727 0.615
298 0.51
30 0.267
0 0.0
0 0.0
0.584381412190706
0.5994000999833361
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2884,	 Acc = 0.5590
1668 0.347
7898 0.586
3716 0.608
378 0.458
10 0.0
0 0.0
0 0.0
0 0.0
0.5884852524579237
0.5994000999833361
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3837,	 Acc1 = 0.3771,	 Acc2 = 0.3984

 ===== Epoch 256	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436  2.8978124   2.9595222
 -0.36378932 -0.37167084  1.8132949   2.3034005  -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.19519503  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 0 0
train:	 Loss = 1.3460,	 Acc = 0.5059
13231 0.263
25971 0.563
13396 0.622
1729 0.615
299 0.492
30 0.367
0 0.0
0 0.0
0.5836330718165359
0.5994000999833361
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 2
val:	 Loss = 1.3551,	 Acc = 0.4942
1668 0.357
7898 0.549
3716 0.45
378 0.399
10 0.0
0 0.0
0 0.0
0 0.0
0.5132477920346609
0.5994000999833361
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4364,	 Acc1 = 0.3716,	 Acc2 = 0.3917

 ===== Epoch 257	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084  1.1357323   1.9462559  -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.20745026  0.04669524  0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 2 2
train:	 Loss = 1.3450,	 Acc = 0.5053
13233 0.264
25966 0.561
13399 0.623
1729 0.615
299 0.495
30 0.333
0 0.0
0 0.0
0.5824541921154914
0.5994000999833361
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2836,	 Acc = 0.5444
1668 0.358
7898 0.565
3716 0.598
378 0.421
10 0.0
0 0.0
0 0.0
0 0.0
0.5703216130644893
0.5994000999833361
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3838,	 Acc1 = 0.3699,	 Acc2 = 0.3897

 ===== Epoch 258	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 1.1263542e-03  2.0405303e-03  5.2204342e-03  4.7584870e-03
  2.7000366e-03  8.4708212e-04  6.0168497e-04  9.4006682e-04
  2.9140669e-03  7.7248965e-03  4.0856266e+00  2.8221490e+00
 -2.7537176e-03 -3.1672046e-03] 6 6
train:	 Loss = 1.3460,	 Acc = 0.5046
13230 0.265
25965 0.561
13403 0.618
1729 0.613
299 0.538
30 0.3
0 0.0
0 0.0
0.5812774586008786
0.5994000999833361
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3008,	 Acc = 0.5375
1668 0.359
7898 0.57
3716 0.563
378 0.413
10 0.0
0 0.0
0 0.0
0 0.0
0.5622396267288785
0.5994000999833361
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3974,	 Acc1 = 0.3730,	 Acc2 = 0.3934

 ===== Epoch 259	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722  0.9648101   2.2894323
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249  -0.00768263 -0.2513194
 -0.00275372 -0.0031672 ] 5 5
train:	 Loss = 1.3490,	 Acc = 0.5051
13231 0.263
25971 0.562
13398 0.62
1728 0.614
298 0.523
30 0.367
0 0.0
0 0.0
0.5823053711526855
0.5994000999833361
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3391,	 Acc = 0.5252
1668 0.359
7898 0.563
3716 0.533
378 0.418
10 0.0
0 0.0
0 0.0
0 0.0
0.5484085985669055
0.5994000999833361
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4301,	 Acc1 = 0.3689,	 Acc2 = 0.3884

 ===== Epoch 260	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084  1.9650731   3.350033   -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 1.1263542e-03  2.0405303e-03  5.2204342e-03  4.7584870e-03
  2.7000366e-03  8.4708212e-04  6.0168497e-04  9.4006682e-04
 -3.8927608e+00 -5.9001794e+00  3.7514956e-03  5.4717045e-03
 -2.7537176e-03 -3.1672046e-03] 4 1
train:	 Loss = 1.3486,	 Acc = 0.5052
13233 0.262
25966 0.562
13399 0.624
1729 0.599
299 0.502
30 0.367
0 0.0
0 0.0
0.5827680274243777
0.5994000999833361
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2990,	 Acc = 0.5339
1668 0.294
7898 0.573
3716 0.572
378 0.418
10 0.0
0 0.0
0 0.0
0 0.0
0.567238793534411
0.5994000999833361
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3998,	 Acc1 = 0.3604,	 Acc2 = 0.3782

 ===== Epoch 261	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 2 1
train:	 Loss = 1.3454,	 Acc = 0.5053
13230 0.264
25968 0.561
13400 0.622
1729 0.614
299 0.492
30 0.267
0 0.0
0 0.0
0.5822671752039782
0.5994000999833361
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2987,	 Acc = 0.5318
1668 0.353
7898 0.567
3716 0.556
378 0.37
10 0.0
0 0.0
0 0.0
0 0.0
0.5567405432427929
0.5994000999833361
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3937,	 Acc1 = 0.3763,	 Acc2 = 0.3974

 ===== Epoch 262	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  1.2770293   2.740548   -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 4.3183560e+00  3.2551091e+00  5.2204342e-03  4.7584870e-03
  2.7000366e-03  8.4708212e-04  1.5646350e-02  7.2478849e-01
  2.9140669e-03  7.7248965e-03  3.7514956e-03  5.4717045e-03
 -2.7537176e-03 -3.1672046e-03] 4 6
train:	 Loss = 1.3457,	 Acc = 0.5055
13231 0.264
25969 0.562
13399 0.621
1728 0.606
299 0.515
30 0.3
0 0.0
0 0.0
0.5824984912492456
0.5994000999833361
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3079,	 Acc = 0.5508
1668 0.355
7898 0.583
3716 0.587
378 0.399
10 0.0
0 0.0
0 0.0
0 0.0
0.5780703216130645
0.5994000999833361
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4040,	 Acc1 = 0.3728,	 Acc2 = 0.3931

 ===== Epoch 263	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 3 1
train:	 Loss = 1.3460,	 Acc = 0.5058
13234 0.264
25967 0.564
13399 0.618
1727 0.622
299 0.512
30 0.3
0 0.0
0 0.0
0.583120081116315
0.5994000999833361
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3002,	 Acc = 0.5340
1668 0.355
7898 0.565
3716 0.56
378 0.434
10 0.0
0 0.0
0 0.0
0 0.0
0.5589068488585236
0.5994000999833361
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 1
Testing:	 Loss = 1.3968,	 Acc1 = 0.3672,	 Acc2 = 0.3864

 ===== Epoch 264	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722  1.8338563   2.056552
 -0.38765725 -0.38159567] [ 1.1263542e-03  2.0405303e-03  5.2204342e-03  4.7584870e-03
  2.7000366e-03  8.4708212e-04  6.0168497e-04  9.4006682e-04
  2.9140669e-03  7.7248965e-03  9.0932302e-02 -9.4947022e-01
 -2.7537176e-03 -3.1672046e-03] 5 5
train:	 Loss = 1.3446,	 Acc = 0.5061
13229 0.265
25968 0.563
13401 0.62
1729 0.61
299 0.532
30 0.367
0 0.0
0 0.0
0.5830014241919521
0.5994000999833361
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2984,	 Acc = 0.5434
1668 0.354
7898 0.57
3716 0.589
378 0.399
10 0.0
0 0.0
0 0.0
0 0.0
0.5696550574904182
0.5994000999833361
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 6
Testing:	 Loss = 1.3981,	 Acc1 = 0.3743,	 Acc2 = 0.3949

 ===== Epoch 265	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722  1.3118248   1.164253
 -0.38765725 -0.38159567] [ 1.1263542e-03  2.0405303e-03  5.2204342e-03  4.7584870e-03
  2.7000366e-03  8.4708212e-04  6.0168497e-04  9.4006682e-04
  2.9140669e-03  7.7248965e-03 -3.2884886e+00 -2.5905259e+00
 -2.7537176e-03 -3.1672046e-03] 5 5
train:	 Loss = 1.3452,	 Acc = 0.5068
13235 0.266
25970 0.563
13395 0.622
1727 0.605
299 0.528
30 0.4
0 0.0
0 0.0
0.5838825716424036
0.5994000999833361
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3764,	 Acc = 0.4832
1668 0.358
7898 0.513
3716 0.484
378 0.407
10 0.9
0 0.0
0 0.0
0 0.0
0.5005832361273121
0.5994000999833361
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 1
Testing:	 Loss = 1.4608,	 Acc1 = 0.3652,	 Acc2 = 0.3839

 ===== Epoch 266	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722  1.665629    1.6030483
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249  -0.47431698  0.02954587
 -0.00275372 -0.0031672 ] 1 1
train:	 Loss = 1.3451,	 Acc = 0.5050
13232 0.264
25968 0.561
13398 0.621
1729 0.616
299 0.515
30 0.3
0 0.0
0 0.0
0.5821987253765932
0.5994000999833361
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 2
val:	 Loss = 1.3233,	 Acc = 0.5065
1668 0.356
7898 0.546
3716 0.501
378 0.418
10 0.0
0 0.0
0 0.0
0 0.0
0.5274954174304283
0.5994000999833361
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4170,	 Acc1 = 0.3553,	 Acc2 = 0.3720

 ===== Epoch 267	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436  1.7322211   1.8296468
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.36549413 -0.26866063
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 5 5
train:	 Loss = 1.3455,	 Acc = 0.5060
13231 0.265
25972 0.561
13395 0.623
1729 0.614
299 0.502
30 0.367
0 0.0
0 0.0
0.5829088714544357
0.5994000999833361
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 2
val:	 Loss = 1.3563,	 Acc = 0.4865
1668 0.356
7898 0.533
3716 0.46
378 0.357
10 0.0
0 0.0
0 0.0
0 0.0
0.5047492084652558
0.5994000999833361
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4410,	 Acc1 = 0.3701,	 Acc2 = 0.3899

 ===== Epoch 268	 =====
[ 3.1207318   3.4563189  -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [-2.5353447e-01 -6.0773259e-01  5.2204342e-03  4.7584870e-03
  2.7000366e-03  8.4708212e-04  6.0168497e-04  9.4006682e-04
  2.9140669e-03  7.7248965e-03  3.7514956e-03  5.4717045e-03
  4.6862173e+00  5.7304029e+00] 5 5
train:	 Loss = 1.3479,	 Acc = 0.5055
13231 0.264
25973 0.562
13395 0.622
1728 0.604
299 0.515
30 0.4
0 0.0
0 0.0
0.5826674713337356
0.5994000999833361
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3035,	 Acc = 0.5443
1668 0.356
7898 0.569
3716 0.591
378 0.41
10 0.0
0 0.0
0 0.0
0 0.0
0.570488251958007
0.5994000999833361
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4094,	 Acc1 = 0.3635,	 Acc2 = 0.3820

 ===== Epoch 269	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 1 1
train:	 Loss = 1.3477,	 Acc = 0.5052
13234 0.266
25966 0.562
13398 0.619
1729 0.604
299 0.525
30 0.267
0 0.0
0 0.0
0.5817440007725363
0.5994000999833361
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3191,	 Acc = 0.5143
1668 0.354
7898 0.54
3716 0.543
378 0.431
10 0.0
0 0.0
0 0.0
0 0.0
0.5365772371271454
0.5994000999833361
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4064,	 Acc1 = 0.3691,	 Acc2 = 0.3887

 ===== Epoch 270	 =====
[ 1.3237205   2.4069915  -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [-0.50953215  0.2828198   0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 1 1
train:	 Loss = 1.3475,	 Acc = 0.5059
13229 0.265
25971 0.563
13398 0.62
1729 0.611
299 0.505
30 0.333
0 0.0
0 0.0
0.5827841745721389
0.5994000999833361
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2847,	 Acc = 0.5555
1668 0.354
7898 0.58
3716 0.605
378 0.455
10 0.0
0 0.0
0 0.0
0 0.0
0.5834860856523912
0.5994000999833361
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3815,	 Acc1 = 0.3701,	 Acc2 = 0.3899

 ===== Epoch 271	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722  2.9013047   1.0269761
  3.7423732   3.0350442 ] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249  -0.25564814  0.44281903
 -0.53944033  0.41151166] 6 4
train:	 Loss = 1.3465,	 Acc = 0.5059
13229 0.262
25971 0.563
13398 0.621
1729 0.61
299 0.532
30 0.4
0 0.0
0 0.0
0.5836531730513916
0.5994000999833361
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3027,	 Acc = 0.5213
1668 0.354
7898 0.55
3716 0.55
378 0.386
10 0.0
0 0.0
0 0.0
0 0.0
0.5445759040159973
0.5994000999833361
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3979,	 Acc1 = 0.3681,	 Acc2 = 0.3874

 ===== Epoch 272	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.2754886   3.756497   -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 1.1263542e-03  2.0405303e-03  5.2204342e-03  4.7584870e-03
  2.7000366e-03  8.4708212e-04 -7.6618090e-02 -1.4176085e+00
  2.9140669e-03  7.7248965e-03  3.7514956e-03  5.4717045e-03
 -2.7537176e-03 -3.1672046e-03] 5 5
train:	 Loss = 1.3453,	 Acc = 0.5060
13229 0.263
25967 0.563
13402 0.623
1729 0.6
299 0.522
30 0.4
0 0.0
0 0.0
0.5835807565114539
0.5994000999833361
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2837,	 Acc = 0.5604
1668 0.356
7898 0.588
3716 0.609
378 0.415
10 0.0
0 0.0
0 0.0
0 0.0
0.5888185302449592
0.5994000999833361
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3812,	 Acc1 = 0.3730,	 Acc2 = 0.3934

 ===== Epoch 273	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 3 1
train:	 Loss = 1.3455,	 Acc = 0.5063
13230 0.264
25971 0.563
13397 0.621
1729 0.612
299 0.525
30 0.367
0 0.0
0 0.0
0.5835707043885483
0.5994000999833361
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3068,	 Acc = 0.5465
1668 0.354
7898 0.565
3716 0.605
378 0.439
10 0.0
0 0.0
0 0.0
0 0.0
0.573154474254291
0.5994000999833361
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3957,	 Acc1 = 0.3662,	 Acc2 = 0.3852

 ===== Epoch 274	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722  2.702149    1.5785346
 -0.38765725 -0.38159567] [ 3.5188417e+00  4.2846332e+00  5.2204342e-03  4.7584870e-03
  2.7000366e-03  8.4708212e-04  6.0168497e-04  9.4006682e-04
  2.9140669e-03  7.7248965e-03 -5.8420464e-02 -2.2614824e-02
 -2.7537176e-03 -3.1672046e-03] 2 3
train:	 Loss = 1.3461,	 Acc = 0.5072
13229 0.265
25969 0.563
13401 0.625
1729 0.614
298 0.517
30 0.333
0 0.0
0 0.0
0.5843531996041229
0.5994000999833361
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3061,	 Acc = 0.5391
1668 0.357
7898 0.564
3716 0.583
378 0.415
10 0.0
0 0.0
0 0.0
0 0.0
0.5644059323446092
0.5994000999833361
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3962,	 Acc1 = 0.3753,	 Acc2 = 0.3961

 ===== Epoch 275	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722  1.4510086   3.2307587
 -0.38765725 -0.38159567] [ 1.1263542e-03  2.0405303e-03  5.2204342e-03  4.7584870e-03
  2.7000366e-03  8.4708212e-04  6.0168497e-04  9.4006682e-04
  2.9140669e-03  7.7248965e-03  2.1312901e-01 -1.0658286e+00
 -2.7537176e-03 -3.1672046e-03] 5 5
train:	 Loss = 1.3446,	 Acc = 0.5072
13231 0.265
25970 0.563
13399 0.625
1727 0.609
299 0.495
30 0.3
0 0.0
0 0.0
0.584381412190706
0.5994000999833361
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3202,	 Acc = 0.5152
1668 0.354
7898 0.549
3716 0.53
378 0.381
10 0.0
0 0.0
0 0.0
0 0.0
0.5376603899350109
0.5994000999833361
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4151,	 Acc1 = 0.3561,	 Acc2 = 0.3730

 ===== Epoch 276	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
  2.157737    2.390988  ] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.23321293  0.2718749 ] 1 4
train:	 Loss = 1.3445,	 Acc = 0.5047
13232 0.264
25967 0.561
13399 0.621
1729 0.602
299 0.492
30 0.333
0 0.0
0 0.0
0.5816434916956353
0.5994000999833361
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2893,	 Acc = 0.5580
1668 0.353
7898 0.591
3716 0.596
378 0.421
10 0.0
0 0.0
0 0.0
0 0.0
0.5865689051824696
0.5994000999833361
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 1
Testing:	 Loss = 1.3860,	 Acc1 = 0.3732,	 Acc2 = 0.3936

 ===== Epoch 277	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436  0.39339906  2.2314296
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00875948  0.43601552
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 6 4
train:	 Loss = 1.3462,	 Acc = 0.5069
13227 0.266
25973 0.562
13400 0.625
1727 0.609
299 0.505
30 0.367
0 0.0
0 0.0
0.5837215477081271
0.5994000999833361
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 2
val:	 Loss = 1.3709,	 Acc = 0.4894
1668 0.351
7898 0.536
3716 0.462
378 0.397
10 0.0
0 0.0
0 0.0
0 0.0
0.508581903016164
0.5994000999833361
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4505,	 Acc1 = 0.3674,	 Acc2 = 0.3867

 ===== Epoch 278	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722  1.5430424   1.5760833
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.01875768 -0.01057774
 -0.00275372 -0.0031672 ] 3 0
train:	 Loss = 1.3475,	 Acc = 0.5052
13229 0.264
25970 0.562
13399 0.62
1729 0.605
299 0.508
30 0.333
0 0.0
0 0.0
0.5820841480194077
0.5994000999833361
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3124,	 Acc = 0.5224
1668 0.354
7898 0.552
3716 0.544
378 0.447
10 0.0
0 0.0
0 0.0
0 0.0
0.5457423762706216
0.5994000999833361
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4020,	 Acc1 = 0.3703,	 Acc2 = 0.3902

 ===== Epoch 279	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 1
train:	 Loss = 1.3436,	 Acc = 0.5076
13230 0.265
25972 0.564
13397 0.623
1728 0.616
299 0.532
30 0.333
0 0.0
0 0.0
0.5851156278665572
0.5994000999833361
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2934,	 Acc = 0.5453
1668 0.357
7898 0.572
3716 0.593
378 0.362
10 0.0
0 0.0
0 0.0
0 0.0
0.5714880853191134
0.5994000999833361
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4038,	 Acc1 = 0.3619,	 Acc2 = 0.3800

 ===== Epoch 280	 =====
[ 1.7247037   1.7622241  -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
  1.7286011   2.0035071 ] [ 1.2764066e-01 -4.2905489e-01  5.2204342e-03  4.7584870e-03
  2.7000366e-03  8.4708212e-04  6.0168497e-04  9.4006682e-04
  2.9140669e-03  7.7248965e-03  3.7514956e-03  5.4717045e-03
 -1.2745465e-01 -6.1672270e-01] 5 5
train:	 Loss = 1.3465,	 Acc = 0.5052
13230 0.263
25971 0.561
13399 0.622
1727 0.617
299 0.485
30 0.367
0 0.0
0 0.0
0.5825568483561049
0.5994000999833361
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2976,	 Acc = 0.5394
1668 0.357
7898 0.556
3716 0.601
378 0.399
10 0.0
0 0.0
0 0.0
0 0.0
0.5646558906848859
0.5994000999833361
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 6
Testing:	 Loss = 1.4052,	 Acc1 = 0.3528,	 Acc2 = 0.3690

 ===== Epoch 281	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 1.1263542e-03  2.0405303e-03  5.2204342e-03  4.7584870e-03
  2.7000366e-03  8.4708212e-04  6.0168497e-04  9.4006682e-04
  2.9140669e-03  7.7248965e-03  4.1435108e+00  2.7940624e+00
 -2.7537176e-03 -3.1672046e-03] 6 6
train:	 Loss = 1.3453,	 Acc = 0.5054
13234 0.263
25969 0.564
13395 0.619
1729 0.601
299 0.512
30 0.367
0 0.0
0 0.0
0.5827579547100574
0.5994000999833361
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3162,	 Acc = 0.5225
1668 0.356
7898 0.554
3716 0.545
378 0.394
10 0.0
0 0.0
0 0.0
0 0.0
0.5456590568238627
0.5994000999833361
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4276,	 Acc1 = 0.3458,	 Acc2 = 0.3606

 ===== Epoch 282	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  0.9536181   1.4699118  -0.40214875 -0.40990722  2.160503    1.7770957
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.3937098   0.3798674   0.00291407  0.0077249   0.37463164  0.1499167
 -0.00275372 -0.0031672 ] 2 2
train:	 Loss = 1.3482,	 Acc = 0.5050
13229 0.266
25970 0.562
13399 0.618
1729 0.611
299 0.508
30 0.3
0 0.0
0 0.0
0.58148067685326
0.5994000999833361
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3119,	 Acc = 0.5339
1668 0.353
7898 0.564
3716 0.56
378 0.452
10 0.0
0 0.0
0 0.0
0 0.0
0.5590734877520414
0.5994000999833361
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4097,	 Acc1 = 0.3565,	 Acc2 = 0.3735

 ===== Epoch 283	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  3.4974573   1.9484994  -0.40214875 -0.40990722  3.1257339   2.7086165
 -0.38765725 -0.38159567] [ 1.1263542e-03  2.0405303e-03  5.2204342e-03  4.7584870e-03
  2.7000366e-03  8.4708212e-04  7.7779371e-01  1.5514196e-02
  2.9140669e-03  7.7248965e-03  1.4667146e-01 -1.2649515e+01
 -2.7537176e-03 -3.1672046e-03] 2 0
train:	 Loss = 1.3448,	 Acc = 0.5054
13229 0.261
25969 0.562
13401 0.625
1729 0.602
298 0.487
30 0.4
0 0.0
0 0.0
0.5833876457382866
0.5994000999833361
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2951,	 Acc = 0.5443
1668 0.353
7898 0.574
3716 0.582
378 0.418
10 0.0
0 0.0
0 0.0
0 0.0
0.5709881686385603
0.5994000999833361
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3940,	 Acc1 = 0.3648,	 Acc2 = 0.3834

 ===== Epoch 284	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722  3.3871267   1.8628936
 -0.38765725 -0.38159567] [ 1.1263542e-03  2.0405303e-03  5.2204342e-03  4.7584870e-03
  2.7000366e-03  8.4708212e-04  6.0168497e-04  9.4006682e-04
  2.9140669e-03  7.7248965e-03 -1.4354596e+00  1.0578073e-01
 -2.7537176e-03 -3.1672046e-03] 2 1
train:	 Loss = 1.3459,	 Acc = 0.5027
13231 0.26
25969 0.561
13399 0.618
1728 0.602
299 0.518
30 0.333
0 0.0
0 0.0
0.5803741701870851
0.5994000999833361
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3268,	 Acc = 0.5228
1668 0.354
7898 0.552
3716 0.552
378 0.384
10 0.0
0 0.0
0 0.0
0 0.0
0.5462422929511748
0.5994000999833361
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4290,	 Acc1 = 0.3604,	 Acc2 = 0.3782

 ===== Epoch 285	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.4024825   1.3887479  -0.40214875 -0.40990722  2.992963    2.1987314
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
 -0.18492232 -0.28082642  0.00291407  0.0077249  -0.644393   -0.24329467
 -0.00275372 -0.0031672 ] 0 5
train:	 Loss = 1.3463,	 Acc = 0.5048
13232 0.265
25974 0.562
13392 0.617
1729 0.61
299 0.525
30 0.433
0 0.0
0 0.0
0.5815227887215141
0.5994000999833361
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3041,	 Acc = 0.5519
1668 0.353
7898 0.583
3716 0.586
378 0.455
10 0.0
0 0.0
0 0.0
0 0.0
0.5795700716547242
0.5994000999833361
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3912,	 Acc1 = 0.3821,	 Acc2 = 0.4043

 ===== Epoch 286	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 5 1
train:	 Loss = 1.3462,	 Acc = 0.5051
13230 0.267
25974 0.561
13395 0.619
1728 0.606
299 0.502
30 0.333
0 0.0
0 0.0
0.5813257374595665
0.5994000999833361
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2925,	 Acc = 0.5536
1668 0.356
7898 0.578
3716 0.604
378 0.442
10 0.0
0 0.0
0 0.0
0 0.0
0.5811531411431428
0.5994000999833361
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3849,	 Acc1 = 0.3710,	 Acc2 = 0.3909

 ===== Epoch 287	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436  2.2117653   1.5765902
 -0.36378932 -0.37167084  0.9329796   1.8544896  -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 1.12635421e-03  2.04053032e-03  5.22043416e-03  4.75848699e-03
  2.12500170e-01  7.84624195e+00  6.01684966e-04  9.40066820e-04
 -1.24329194e-01  3.15590620e-01  3.75149562e-03  5.47170453e-03
 -2.75371759e-03 -3.16720456e-03] 4 6
train:	 Loss = 1.3466,	 Acc = 0.5041
13233 0.265
25968 0.56
13398 0.62
1728 0.593
299 0.488
30 0.367
0 0.0
0 0.0
0.5805228979069599
0.5994000999833361
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2868,	 Acc = 0.5533
1668 0.354
7898 0.578
3716 0.604
378 0.421
10 0.0
0 0.0
0 0.0
0 0.0
0.5809865022496251
0.5994000999833361
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3830,	 Acc1 = 0.3712,	 Acc2 = 0.3912

 ===== Epoch 288	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722  0.8493896   2.4414172
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249  -0.13416713  0.5672022
 -0.00275372 -0.0031672 ] 6 6
train:	 Loss = 1.3467,	 Acc = 0.5063
13225 0.264
25972 0.562
13401 0.623
1729 0.618
299 0.515
30 0.4
0 0.0
0 0.0
0.5835485506022061
0.5994000999833361
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3062,	 Acc = 0.5247
1668 0.356
7898 0.553
3716 0.553
378 0.405
10 0.0
0 0.0
0 0.0
0 0.0
0.5481586402266289
0.5994000999833361
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3944,	 Acc1 = 0.3798,	 Acc2 = 0.4016

 ===== Epoch 289	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 1
train:	 Loss = 1.3451,	 Acc = 0.5053
13229 0.261
25969 0.563
13400 0.622
1729 0.599
299 0.508
30 0.533
0 0.0
0 0.0
0.5833393680449948
0.5994000999833361
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2942,	 Acc = 0.5464
1668 0.355
7898 0.57
3716 0.597
378 0.418
10 0.0
0 0.0
0 0.0
0 0.0
0.5729878353607732
0.5994000999833361
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3880,	 Acc1 = 0.3732,	 Acc2 = 0.3936

 ===== Epoch 290	 =====
[-0.3673141  -0.38194367  1.82649     2.462189   -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.11482626 -0.48476422  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 5 5
train:	 Loss = 1.3460,	 Acc = 0.5054
13231 0.265
25971 0.563
13397 0.619
1728 0.604
299 0.508
30 0.3
0 0.0
0 0.0
0.5822329511164756
0.5994000999833361
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2840,	 Acc = 0.5498
1668 0.353
7898 0.584
3716 0.58
378 0.418
10 0.0
0 0.0
0 0.0
0 0.0
0.5772371271454757
0.5994000999833361
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3862,	 Acc1 = 0.3627,	 Acc2 = 0.3810

 ===== Epoch 291	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 1 1
train:	 Loss = 1.3476,	 Acc = 0.5064
13230 0.264
25969 0.564
13401 0.62
1727 0.618
299 0.505
30 0.367
0 0.0
0 0.0
0.583618983247236
0.5994000999833361
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2941,	 Acc = 0.5459
1668 0.354
7898 0.578
3716 0.582
378 0.392
10 0.0
0 0.0
0 0.0
0 0.0
0.5725712381269789
0.5994000999833361
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3906,	 Acc1 = 0.3718,	 Acc2 = 0.3919

 ===== Epoch 292	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  0.90507764  2.6873715  -0.40214875 -0.40990722  2.192565    2.4438686
 -0.38765725 -0.38159567] [ 1.1263542e-03  2.0405303e-03  5.2204342e-03  4.7584870e-03
  2.7000366e-03  8.4708212e-04  9.5870100e-02  1.8372803e+00
  2.9140669e-03  7.7248965e-03 -4.9570870e+00 -4.6849780e+00
 -2.7537176e-03 -3.1672046e-03] 6 6
train:	 Loss = 1.3462,	 Acc = 0.5049
13230 0.265
25971 0.562
13399 0.618
1727 0.611
299 0.502
30 0.333
0 0.0
0 0.0
0.5815671317530053
0.5994000999833361
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3074,	 Acc = 0.4994
1668 0.354
7898 0.536
3716 0.503
378 0.349
10 0.0
0 0.0
0 0.0
0 0.0
0.5195800699883353
0.5994000999833361
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4170,	 Acc1 = 0.3586,	 Acc2 = 0.3760

 ===== Epoch 293	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436  2.8978124   2.9595222
 -0.36378932 -0.37167084  1.9301351   2.3034005  -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 0 0
train:	 Loss = 1.3477,	 Acc = 0.5058
13231 0.264
25971 0.562
13397 0.622
1728 0.605
299 0.528
30 0.367
0 0.0
0 0.0
0.5829088714544357
0.5994000999833361
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3253,	 Acc = 0.5058
1668 0.354
7898 0.539
3716 0.517
378 0.384
10 0.0
0 0.0
0 0.0
0 0.0
0.5268288618563572
0.5994000999833361
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4154,	 Acc1 = 0.3635,	 Acc2 = 0.3820

 ===== Epoch 294	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  1.2426009   2.8916807  -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.06678829 -0.805495    0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 5 5
train:	 Loss = 1.3447,	 Acc = 0.5066
13230 0.265
25968 0.563
13401 0.622
1728 0.615
299 0.498
30 0.367
0 0.0
0 0.0
0.5835948438178922
0.5994000999833361
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3165,	 Acc = 0.5451
1668 0.354
7898 0.58
3716 0.572
378 0.418
10 0.0
0 0.0
0 0.0
0 0.0
0.5715714047658723
0.5994000999833361
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4106,	 Acc1 = 0.3687,	 Acc2 = 0.3882

 ===== Epoch 295	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722  2.7025256   1.5760833
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.00948407
 -0.00275372 -0.0031672 ] 0 0
train:	 Loss = 1.3460,	 Acc = 0.5058
13230 0.264
25971 0.563
13398 0.621
1728 0.605
299 0.508
30 0.333
0 0.0
0 0.0
0.5829913580842949
0.5994000999833361
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3192,	 Acc = 0.5366
1668 0.348
7898 0.562
3716 0.579
378 0.442
10 0.0
0 0.0
0 0.0
0 0.0
0.5628228628561907
0.5994000999833361
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4098,	 Acc1 = 0.3619,	 Acc2 = 0.3800

 ===== Epoch 296	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 2 1
train:	 Loss = 1.3461,	 Acc = 0.5042
13230 0.264
25972 0.561
13398 0.618
1727 0.607
299 0.515
30 0.3
0 0.0
0 0.0
0.5807946700140009
0.5994000999833361
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3003,	 Acc = 0.5482
1668 0.354
7898 0.579
3716 0.584
378 0.423
10 0.0
0 0.0
0 0.0
0 0.0
0.575154140976504
0.5994000999833361
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3882,	 Acc1 = 0.3722,	 Acc2 = 0.3924

 ===== Epoch 297	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436  3.1310828   1.9894719
 -0.36378932 -0.37167084  3.166125    1.1997242  -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 1.1263542e-03  2.0405303e-03  5.2204342e-03  4.7584870e-03
  7.8440771e-02 -7.6269636e+00  6.0168497e-04  9.4006682e-04
 -3.3923253e-01  1.6360626e-01  3.7514956e-03  5.4717045e-03
 -2.7537176e-03 -3.1672046e-03] 3 2
train:	 Loss = 1.3450,	 Acc = 0.5041
13230 0.263
25973 0.56
13397 0.62
1728 0.609
298 0.497
30 0.467
0 0.0
0 0.0
0.580987785448752
0.5994000999833361
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 2
val:	 Loss = 1.3100,	 Acc = 0.5093
1668 0.354
7898 0.551
3716 0.507
378 0.349
10 0.0
0 0.0
0 0.0
0 0.0
0.5308281953007832
0.5994000999833361
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4006,	 Acc1 = 0.3679,	 Acc2 = 0.3872

 ===== Epoch 298	 =====
[ 2.720815    3.0416448  -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
  2.2720551   3.2942379 ] [ 7.6407269e-02 -4.4323570e-01  5.2204342e-03  4.7584870e-03
  2.7000366e-03  8.4708212e-04  6.0168497e-04  9.4006682e-04
  2.9140669e-03  7.7248965e-03  3.7514956e-03  5.4717045e-03
  7.8538187e-02 -6.5057403e-01] 5 5
train:	 Loss = 1.3455,	 Acc = 0.5040
13231 0.263
25968 0.56
13401 0.62
1727 0.61
299 0.525
30 0.4
0 0.0
0 0.0
0.5810018105009053
0.5994000999833361
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.2952,	 Acc = 0.5442
1668 0.353
7898 0.57
3716 0.589
378 0.418
10 0.0
0 0.0
0 0.0
0 0.0
0.5708215297450425
0.5994000999833361
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3888,	 Acc1 = 0.3701,	 Acc2 = 0.3899

 ===== Epoch 299	 =====
[-0.3673141  -0.38194367 -0.4204066  -0.33581436  4.102026    2.3934746
 -0.36378932 -0.37167084  4.278975    1.7478422  -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 1.1263542e-03  2.0405303e-03  5.2204342e-03  4.7584870e-03
 -1.0924979e+00  8.2647260e-03  6.0168497e-04  9.4006682e-04
 -6.5404814e-01 -3.9662067e-03  3.7514956e-03  5.4717045e-03
 -2.7537176e-03 -3.1672046e-03] 0 0
train:	 Loss = 1.3462,	 Acc = 0.5042
13232 0.266
25973 0.561
13394 0.618
1728 0.598
299 0.502
30 0.4
0 0.0
0 0.0
0.5802191966010043
0.5994000999833361
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3058,	 Acc = 0.5510
1668 0.354
7898 0.58
3716 0.592
378 0.426
10 0.0
0 0.0
0 0.0
0 0.0
0.5783202799533411
0.5994000999833361
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.3971,	 Acc1 = 0.3788,	 Acc2 = 0.4003

 ===== Epoch 300	 =====
[ 2.9907858   3.4360907  -0.4204066  -0.33581436 -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722  2.6693327   1.561375
 -0.38765725 -0.38159567] [-3.5165889e+00 -4.2805524e+00  5.2204342e-03  4.7584870e-03
  2.7000366e-03  8.4708212e-04  6.0168497e-04  9.4006682e-04
  2.9140669e-03  7.7248965e-03 -5.8603435e+00 -3.2405283e+00
 -2.7537176e-03 -3.1672046e-03] 2 5
train:	 Loss = 1.3465,	 Acc = 0.5045
13229 0.264
25974 0.56
13396 0.624
1728 0.6
299 0.492
30 0.233
0 0.0
0 0.0
0.5814082603133223
0.5994000999833361
[-0.3673141  -0.38194367 -0.4204066  -0.33581436 -0.44182304 -0.36572987
  2.6158333   1.7525865  -0.40214875 -0.40990722  2.3721068   3.0174894
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053  0.00522043  0.00475849  0.00270004  0.00084708
  0.39571556 -0.04764036  0.00291407  0.0077249   0.00232223  0.02954587
 -0.00275372 -0.0031672 ] 3 3
val:	 Loss = 1.3172,	 Acc = 0.5351
1668 0.353
7898 0.573
3716 0.547
378 0.437
10 0.0
0 0.0
0 0.0
0 0.0
0.5604899183469422
0.5994000999833361
[-0.3673141  -0.38194367  1.0787587   2.3396485  -0.44182304 -0.36572987
 -0.36378932 -0.37167084 -0.40214875 -0.40990722 -0.4259259  -0.4217841
 -0.38765725 -0.38159567] [ 0.00112635  0.00204053 -0.03156548  0.223124    0.00270004  0.00084708
  0.00060168  0.00094007  0.00291407  0.0077249   0.0037515   0.0054717
 -0.00275372 -0.0031672 ] 4 4
Testing:	 Loss = 1.4092,	 Acc1 = 0.3652,	 Acc2 = 0.3839
