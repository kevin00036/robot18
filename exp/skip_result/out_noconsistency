(0.24300000000005184, array([-1.000000e+00, -1.000000e+00,  1.189783e+03,  1.790000e-01,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 4, array([-1.000000e+00, -1.000000e+00,  1.173932e+03,  2.700000e-01,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 1)
(0.24, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.543573e+03, -3.290000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.370508e+03,  2.799000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]))
(0.238, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.370508e+03,  2.799000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.356274e+03, -3.410000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]))
(0.236, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.356274e+03, -3.410000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.427118e+03, -3.330000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]))
(0.2350000000000001, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.427118e+03, -3.330000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.245642e+03, -3.340000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]))
(0.248, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.245642e+03, -3.340000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.415149e+03, -3.430000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]))
(0.256, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.415149e+03, -3.430000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.211029e+03, -3.400000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]))
(0.256, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.211029e+03, -3.400000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([-1.000000e+00, -1.000000e+00,  2.398328e+03,  4.980000e-01,
        2.350128e+03, -3.360000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]))
(0.2670000000000001, array([-1.000000e+00, -1.000000e+00,  2.398328e+03,  4.980000e-01,
        2.350128e+03, -3.360000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.383771e+03, -3.390000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]))
(0.2609999999999997, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.383771e+03, -3.390000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.435205e+03, -3.500000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]))
(0.28500000000000014, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.435205e+03, -3.500000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.406091e+03, -3.470000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]))
14 1 14

 ===== Epoch 1	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   3.294144    1.7522359  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   2.8032398   1.565598   -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 0
train:	 Loss = 1.9263,	 Acc = 0.1753,	 Loss Con1 = 1.3516,	 Loss Con2 = 1.3531
2922 0.127
5682 0.183
2913 0.194
377 0.252
68 0.368
6 0.333
0 0.0
0 0.0
0.19102365686491266
0.0
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 0
val:	 Loss = 1.9350,	 Acc = 0.1963,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.3900
380 0.158
1718 0.22
815 0.167
85 0.176
2 0.0
0 0.0
0 0.0
0 0.0
0.20190839694656487
0.20190839694656487

 ===== Epoch 2	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.955623    2.955684  ] [ 2.3454592   2.704906   -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.955623    2.955684  ] 4 2
train:	 Loss = 1.8704,	 Acc = 0.2445,	 Loss Con1 = 1.3507,	 Loss Con2 = 1.3733
2929 0.13
5679 0.276
2909 0.291
377 0.286
68 0.324
6 0.333
0 0.0
0 0.0
0.2814470627281779
0.20190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.7998,	 Acc = 0.2700,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.4153
380 0.108
1718 0.333
815 0.234
85 0.071
2 0.0
0 0.0
0 0.0
0 0.0
0.29351145038167936
0.29351145038167936

 ===== Epoch 3	 =====
[ 2.2587368   2.6973116  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.955623    2.955684  ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.955623    2.955684  ] 0 6
train:	 Loss = 1.7460,	 Acc = 0.3140,	 Loss Con1 = 1.3522,	 Loss Con2 = 1.4042
2920 0.161
5685 0.362
2913 0.374
376 0.311
68 0.294
6 0.167
0 0.0
0 0.0
0.36328470380194516
0.29351145038167936
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6976,	 Acc = 0.3483,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.4475
380 0.189
1718 0.423
815 0.291
85 0.118
2 0.0
0 0.0
0 0.0
0 0.0
0.3713740458015267
0.3713740458015267

 ===== Epoch 4	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 0
train:	 Loss = 1.7010,	 Acc = 0.3374,	 Loss Con1 = 1.3534,	 Loss Con2 = 1.4299
2922 0.171
5678 0.389
2917 0.401
376 0.348
69 0.348
6 0.333
0 0.0
0 0.0
0.3910015476453681
0.3713740458015267
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.6857,	 Acc = 0.3590,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.4436
380 0.189
1718 0.425
815 0.323
85 0.141
2 0.0
0 0.0
0 0.0
0 0.0
0.383587786259542
0.383587786259542

 ===== Epoch 5	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 0
train:	 Loss = 1.6814,	 Acc = 0.3471,	 Loss Con1 = 1.3522,	 Loss Con2 = 1.4366
2923 0.158
5682 0.405
2911 0.42
377 0.371
69 0.362
6 0.333
0 0.0
0 0.0
0.40829187396351574
0.383587786259542
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 0
val:	 Loss = 1.6700,	 Acc = 0.3607,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.4615
380 0.187
1718 0.419
815 0.344
85 0.129
2 0.0
0 0.0
0 0.0
0 0.0
0.38587786259541984
0.38587786259541984

 ===== Epoch 6	 =====
[ 1.6706833   2.5226455  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.432725    2.8113446 ] [ 1.8945701   2.6517465  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.3939261   2.9635565 ] 1 6
train:	 Loss = 1.6739,	 Acc = 0.3504,	 Loss Con1 = 1.3522,	 Loss Con2 = 1.4425
2926 0.167
5679 0.403
2911 0.427
377 0.395
69 0.362
6 0.333
0 0.0
0 0.0
0.4097544790975448
0.38587786259541984
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.6457,	 Acc = 0.3783,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.4823
380 0.166
1718 0.444
815 0.367
85 0.118
2 0.0
0 0.0
0 0.0
0 0.0
0.40916030534351144
0.40916030534351144

 ===== Epoch 7	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 3 0
train:	 Loss = 1.6687,	 Acc = 0.3571,	 Loss Con1 = 1.3518,	 Loss Con2 = 1.4531
2925 0.161
5676 0.409
2917 0.447
377 0.403
67 0.343
6 0.333
0 0.0
0 0.0
0.42043569611854475
0.40916030534351144
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 0
val:	 Loss = 1.6401,	 Acc = 0.3760,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.4819
380 0.179
1718 0.442
815 0.357
85 0.106
2 0.0
0 0.0
0 0.0
0 0.0
0.40458015267175573
0.40916030534351144

 ===== Epoch 8	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 0
train:	 Loss = 1.6678,	 Acc = 0.3594,	 Loss Con1 = 1.3519,	 Loss Con2 = 1.4503
2923 0.159
5681 0.417
2916 0.444
373 0.391
69 0.362
6 0.333
0 0.0
0 0.0
0.42410171365395244
0.40916030534351144
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.6673,	 Acc = 0.3597,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.4711
380 0.179
1718 0.416
815 0.348
85 0.141
2 0.0
0 0.0
0 0.0
0 0.0
0.38587786259541984
0.40916030534351144

 ===== Epoch 9	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   2.6464717   3.35732    -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   2.5944066   3.2851534  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 5
train:	 Loss = 1.6633,	 Acc = 0.3610,	 Loss Con1 = 1.3518,	 Loss Con2 = 1.4619
2927 0.161
5679 0.416
2912 0.449
375 0.405
69 0.391
6 0.167
0 0.0
0 0.0
0.42583784979537664
0.40916030534351144
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.6507,	 Acc = 0.3680,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.4923
380 0.195
1718 0.435
815 0.336
85 0.106
2 0.0
0 0.0
0 0.0
0 0.0
0.3931297709923664
0.40916030534351144

 ===== Epoch 10	 =====
[ 2.5928674   3.193465   -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 2.3438845   3.0643642  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 5
train:	 Loss = 1.6625,	 Acc = 0.3579,	 Loss Con1 = 1.3523,	 Loss Con2 = 1.4645
2925 0.165
5679 0.412
2913 0.441
377 0.395
68 0.412
6 0.333
0 0.0
0 0.0
0.4202145305761362
0.40916030534351144
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 0
val:	 Loss = 1.6388,	 Acc = 0.3690,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.4803
380 0.195
1718 0.427
815 0.356
85 0.118
2 0.0
0 0.0
0 0.0
0 0.0
0.39427480916030533
0.40916030534351144

 ===== Epoch 11	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 0
train:	 Loss = 1.6559,	 Acc = 0.3628,	 Loss Con1 = 1.3522,	 Loss Con2 = 1.4667
2925 0.16
5677 0.419
2915 0.451
376 0.402
69 0.391
6 0.333
0 0.0
0 0.0
0.42839765564525045
0.40916030534351144
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6315,	 Acc = 0.3943,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.4869
380 0.2
1718 0.454
815 0.389
85 0.118
2 0.0
0 0.0
0 0.0
0 0.0
0.42251908396946564
0.42251908396946564

 ===== Epoch 12	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 0
train:	 Loss = 1.6554,	 Acc = 0.3659,	 Loss Con1 = 1.3525,	 Loss Con2 = 1.4749
2919 0.159
5684 0.425
2915 0.451
375 0.408
69 0.377
6 0.333
0 0.0
0 0.0
0.43253398165543155
0.42251908396946564
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6376,	 Acc = 0.3940,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.4946
380 0.195
1718 0.453
815 0.385
85 0.176
2 0.0
0 0.0
0 0.0
0 0.0
0.42290076335877863
0.42290076335877863

 ===== Epoch 13	 =====
[-0.36602148 -0.3783333   1.6152153   0.91405404 -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.6069332   0.87521094 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 3 3
train:	 Loss = 1.6511,	 Acc = 0.3654,	 Loss Con1 = 1.3511,	 Loss Con2 = 1.4736
2926 0.165
5681 0.422
2911 0.452
375 0.384
69 0.406
6 0.333
0 0.0
0 0.0
0.4301039593010396
0.42290076335877863
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6497,	 Acc = 0.3787,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.5045
380 0.192
1718 0.447
815 0.35
85 0.118
2 0.0
0 0.0
0 0.0
0 0.0
0.40572519083969466
0.42290076335877863

 ===== Epoch 14	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   0.31759694  2.8770957
  4.661485    1.8486012  -0.40141198 -0.40778467  4.5374603   2.1307
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  0.3625627   2.8128455
  4.8732452   1.670017   -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 2
train:	 Loss = 1.6571,	 Acc = 0.3623,	 Loss Con1 = 1.3522,	 Loss Con2 = 1.4809
2924 0.165
5677 0.418
2917 0.447
375 0.395
69 0.435
6 0.333
0 0.0
0 0.0
0.42624944714727997
0.42290076335877863
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 0
val:	 Loss = 1.6443,	 Acc = 0.3573,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.5038
380 0.192
1718 0.408
815 0.352
85 0.129
2 0.0
0 0.0
0 0.0
0 0.0
0.38129770992366413
0.42290076335877863

 ===== Epoch 15	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 0
train:	 Loss = 1.6529,	 Acc = 0.3649,	 Loss Con1 = 1.3525,	 Loss Con2 = 1.4853
2922 0.166
5682 0.419
2915 0.455
375 0.395
68 0.338
6 0.333
0 0.0
0 0.0
0.42902940526199423
0.42290076335877863
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.6396,	 Acc = 0.3773,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.4972
380 0.187
1718 0.432
815 0.377
85 0.129
2 0.0
0 0.0
0 0.0
0 0.0
0.4049618320610687
0.42290076335877863

 ===== Epoch 16	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   2.3709676   1.0989803
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  2.3858078   1.0946761
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.6473,	 Acc = 0.3677,	 Loss Con1 = 1.3526,	 Loss Con2 = 1.4853
2923 0.16
5681 0.427
2912 0.455
377 0.414
69 0.362
6 0.333
0 0.0
0 0.0
0.4349364289662797
0.42290076335877863
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6484,	 Acc = 0.3740,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.5160
380 0.195
1718 0.438
815 0.347
85 0.153
2 0.0
0 0.0
0 0.0
0 0.0
0.4
0.42290076335877863

 ===== Epoch 17	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.097257    2.6827774
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.1021814   2.9686744
 -0.38555372 -0.3798593 ] 5 6
train:	 Loss = 1.6513,	 Acc = 0.3672,	 Loss Con1 = 1.3523,	 Loss Con2 = 1.4848
2921 0.16
5681 0.426
2915 0.456
376 0.391
69 0.42
6 0.5
0 0.0
0 0.0
0.43406654139493756
0.42290076335877863
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6303,	 Acc = 0.3800,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.5255
380 0.187
1718 0.444
815 0.367
85 0.094
2 0.0
0 0.0
0 0.0
0 0.0
0.4080152671755725
0.42290076335877863

 ===== Epoch 18	 =====
[ 2.7271242   2.666935   -0.4208693  -0.3341335  -0.4409929  -0.3635196
  1.4008247   3.3916812  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 3.058152    2.7200944  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  1.4210448   3.4223754  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 4
train:	 Loss = 1.6465,	 Acc = 0.3669,	 Loss Con1 = 1.3516,	 Loss Con2 = 1.4870
2926 0.159
5680 0.427
2911 0.452
377 0.406
68 0.397
6 0.333
0 0.0
0 0.0
0.4340853793408538
0.42290076335877863
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 3
val:	 Loss = 1.6200,	 Acc = 0.3993,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.5139
380 0.192
1718 0.45
815 0.416
85 0.153
2 0.0
0 0.0
0 0.0
0 0.0
0.42938931297709926
0.42938931297709926

 ===== Epoch 19	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.6483,	 Acc = 0.3670,	 Loss Con1 = 1.3525,	 Loss Con2 = 1.4897
2925 0.16
5679 0.423
2913 0.46
376 0.394
69 0.406
6 0.333
0 0.0
0 0.0
0.4338162114342585
0.42938931297709926
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.6627,	 Acc = 0.3720,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.5199
380 0.184
1718 0.432
815 0.356
85 0.153
2 0.0
0 0.0
0 0.0
0 0.0
0.39923664122137403
0.42938931297709926

 ===== Epoch 20	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  0.86389387  2.4732838
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  0.91693085  2.357446
 -0.38555372 -0.3798593 ] 2 5
train:	 Loss = 1.6483,	 Acc = 0.3720,	 Loss Con1 = 1.3515,	 Loss Con2 = 1.4964
2926 0.159
5681 0.435
2911 0.46
375 0.395
69 0.42
6 0.333
0 0.0
0 0.0
0.4409422694094227
0.42938931297709926
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6534,	 Acc = 0.3880,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.5123
380 0.189
1718 0.457
815 0.36
85 0.165
2 0.0
0 0.0
0 0.0
0 0.0
0.416793893129771
0.42938931297709926

 ===== Epoch 21	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 0
train:	 Loss = 1.6469,	 Acc = 0.3647,	 Loss Con1 = 1.3520,	 Loss Con2 = 1.4952
2923 0.16
5682 0.421
2911 0.456
377 0.393
69 0.377
6 0.333
0 0.0
0 0.0
0.43073521282476507
0.42938931297709926
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6435,	 Acc = 0.3817,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.5184
380 0.182
1718 0.441
815 0.372
85 0.176
2 0.0
0 0.0
0 0.0
0 0.0
0.41068702290076337
0.42938931297709926

 ===== Epoch 22	 =====
[ 2.5863137   2.5277083  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.9265243   2.9399376 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.9688504   2.9478106 ] 4 4
train:	 Loss = 1.6460,	 Acc = 0.3702,	 Loss Con1 = 1.3517,	 Loss Con2 = 1.4966
2925 0.158
5681 0.428
2912 0.466
376 0.404
68 0.412
6 0.167
0 0.0
0 0.0
0.43879243613844965
0.42938931297709926
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.6366,	 Acc = 0.3770,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.5264
380 0.187
1718 0.442
815 0.351
85 0.165
2 0.0
0 0.0
0 0.0
0 0.0
0.40458015267175573
0.42938931297709926

 ===== Epoch 23	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  3.913863    1.949055   -0.40141198 -0.40778467  3.2194922  -5.046304
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  3.8060176   2.029976   -0.40141198 -0.40778467  4.063538    2.7419283
 -0.38555372 -0.3798593 ] 2 2
train:	 Loss = 1.6410,	 Acc = 0.3697,	 Loss Con1 = 1.3532,	 Loss Con2 = 1.5025
2922 0.158
5680 0.426
2916 0.467
375 0.4
69 0.42
6 0.333
0 0.0
0 0.0
0.43787309307981426
0.42938931297709926
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.6590,	 Acc = 0.3717,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.5203
380 0.195
1718 0.44
815 0.33
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.3973282442748092
0.42938931297709926

 ===== Epoch 24	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   2.7189255   1.6146338
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  2.5940154   1.4191946
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 5
train:	 Loss = 1.6397,	 Acc = 0.3695,	 Loss Con1 = 1.3527,	 Loss Con2 = 1.5035
2923 0.159
5683 0.426
2910 0.463
377 0.419
69 0.42
6 0.333
0 0.0
0 0.0
0.43758982863460477
0.42938931297709926
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6431,	 Acc = 0.3903,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.5086
380 0.187
1718 0.446
815 0.389
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.4198473282442748
0.42938931297709926

 ===== Epoch 25	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  1.3912746   3.3805199  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  1.5249591   3.5172482  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
train:	 Loss = 1.6396,	 Acc = 0.3722,	 Loss Con1 = 1.3515,	 Loss Con2 = 1.5058
2928 0.16
5676 0.428
2915 0.471
375 0.403
68 0.426
6 0.333
0 0.0
0 0.0
0.440929203539823
0.42938931297709926
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6363,	 Acc = 0.3993,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.5397
380 0.192
1718 0.459
815 0.399
85 0.141
2 0.0
0 0.0
0 0.0
0 0.0
0.42938931297709926
0.42938931297709926

 ===== Epoch 26	 =====
[ 1.7637073   1.96827    -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 1.9450791   1.8442316  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 5
train:	 Loss = 1.6386,	 Acc = 0.3708,	 Loss Con1 = 1.3514,	 Loss Con2 = 1.5029
2926 0.159
5679 0.428
2912 0.467
376 0.391
69 0.449
6 0.333
0 0.0
0 0.0
0.43928334439283345
0.42938931297709926
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6389,	 Acc = 0.3877,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.5403
380 0.192
1718 0.448
815 0.377
85 0.153
2 0.0
0 0.0
0 0.0
0 0.0
0.41603053435114506
0.42938931297709926

 ===== Epoch 27	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   1.1575085   2.7126203
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  1.0960656   2.6394727
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 5
train:	 Loss = 1.6386,	 Acc = 0.3717,	 Loss Con1 = 1.3519,	 Loss Con2 = 1.5168
2923 0.161
5683 0.425
2911 0.472
376 0.412
69 0.42
6 0.333
0 0.0
0 0.0
0.43969043670536206
0.42938931297709926
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6482,	 Acc = 0.3883,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.5428
380 0.195
1718 0.459
815 0.357
85 0.141
2 0.0
0 0.0
0 0.0
0 0.0
0.416412213740458
0.42938931297709926

 ===== Epoch 28	 =====
[-0.36602148 -0.3783333   1.0819353   3.004938   -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0817951   3.001963   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.6355,	 Acc = 0.3742,	 Loss Con1 = 1.3528,	 Loss Con2 = 1.5179
2925 0.159
5677 0.434
2915 0.47
376 0.396
69 0.406
6 0.667
0 0.0
0 0.0
0.44387924361384495
0.42938931297709926
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6460,	 Acc = 0.4060,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.5305
380 0.192
1718 0.465
815 0.41
85 0.141
2 0.0
0 0.0
0 0.0
0 0.0
0.43702290076335876
0.43702290076335876

 ===== Epoch 29	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   0.73392373  2.608156
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  0.7640788   2.719491
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
train:	 Loss = 1.6346,	 Acc = 0.3749,	 Loss Con1 = 1.3524,	 Loss Con2 = 1.5285
2925 0.158
5679 0.434
2913 0.471
376 0.402
69 0.493
6 0.5
0 0.0
0 0.0
0.4450956540970917
0.43702290076335876
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6442,	 Acc = 0.3800,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.5304
380 0.192
1718 0.444
815 0.361
85 0.118
2 0.5
0 0.0
0 0.0
0 0.0
0.4072519083969466
0.43702290076335876

 ===== Epoch 30	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   1.2658799   2.9964874  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   1.3339657   2.9964874  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.6337,	 Acc = 0.3702,	 Loss Con1 = 1.3533,	 Loss Con2 = 1.5221
2925 0.164
5675 0.424
2916 0.462
377 0.416
69 0.493
6 0.667
0 0.0
0 0.0
0.4368019462567732
0.43702290076335876
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6259,	 Acc = 0.4050,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.5305
380 0.189
1718 0.451
815 0.433
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.4362595419847328
0.43702290076335876

 ===== Epoch 31	 =====
[ 1.197561    3.0593011  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  0.83267075  3.4621823 ] [ 0.910895    3.2213106  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  0.8630929   3.5382886 ] 6 6
train:	 Loss = 1.6322,	 Acc = 0.3738,	 Loss Con1 = 1.3522,	 Loss Con2 = 1.5287
2922 0.161
5683 0.428
2912 0.475
376 0.407
69 0.464
6 0.333
0 0.0
0 0.0
0.4425160291841698
0.43702290076335876
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6451,	 Acc = 0.3987,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.5244
380 0.208
1718 0.455
815 0.388
85 0.212
2 0.5
0 0.0
0 0.0
0 0.0
0.4263358778625954
0.43702290076335876

 ===== Epoch 32	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.5428917   1.3444828
  3.1932065   3.603897  ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.8444457   1.2927257
  3.1548493   3.559283  ] 2 2
train:	 Loss = 1.6291,	 Acc = 0.3748,	 Loss Con1 = 1.3524,	 Loss Con2 = 1.5351
2924 0.162
5680 0.427
2914 0.479
375 0.411
69 0.464
6 0.5
0 0.0
0 0.0
0.4434984520123839
0.43702290076335876
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6500,	 Acc = 0.3890,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.5318
380 0.189
1718 0.456
815 0.363
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.4179389312977099
0.43702290076335876

 ===== Epoch 33	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.1063483   2.6679895
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.1518093   2.505324
 -0.38555372 -0.3798593 ] 5 5
train:	 Loss = 1.6355,	 Acc = 0.3765,	 Loss Con1 = 1.3521,	 Loss Con2 = 1.5344
2924 0.16
5678 0.432
2915 0.478
376 0.431
69 0.464
6 0.333
0 0.0
0 0.0
0.44659442724458204
0.43702290076335876
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.6517,	 Acc = 0.3957,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.5275
380 0.192
1718 0.455
815 0.389
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.4251908396946565
0.43702290076335876

 ===== Epoch 34	 =====
[ 2.727076    2.846664   -0.4208693  -0.3341335  -0.4409929  -0.3635196
  1.4935044   2.3118043  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  1.5052999   2.1053162  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 5
train:	 Loss = 1.6301,	 Acc = 0.3779,	 Loss Con1 = 1.3531,	 Loss Con2 = 1.5329
2924 0.163
5677 0.437
2916 0.476
376 0.396
69 0.406
6 0.5
0 0.0
0 0.0
0.4474789915966387
0.43702290076335876
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6404,	 Acc = 0.4093,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.5581
380 0.189
1718 0.46
815 0.425
85 0.224
2 0.5
0 0.0
0 0.0
0 0.0
0.44122137404580153
0.44122137404580153

 ===== Epoch 35	 =====
[-0.36602148 -0.3783333   1.4840267   1.9900779  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.5017431   1.998607   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 0
train:	 Loss = 1.6257,	 Acc = 0.3765,	 Loss Con1 = 1.3533,	 Loss Con2 = 1.5469
2924 0.161
5676 0.432
2916 0.474
377 0.43
69 0.493
6 0.667
0 0.0
0 0.0
0.4461521450685537
0.44122137404580153
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6339,	 Acc = 0.4083,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.5360
380 0.192
1718 0.463
815 0.422
85 0.153
2 0.0
0 0.0
0 0.0
0 0.0
0.4396946564885496
0.44122137404580153

 ===== Epoch 36	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   1.2647355   3.0014641  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   1.2647355   3.0014641  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.6296,	 Acc = 0.3777,	 Loss Con1 = 1.3516,	 Loss Con2 = 1.5436
2925 0.163
5679 0.434
2914 0.476
376 0.418
68 0.485
6 0.333
0 0.0
0 0.0
0.4470861439787681
0.44122137404580153
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6434,	 Acc = 0.3767,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.5465
380 0.2
1718 0.436
815 0.358
85 0.141
2 0.5
0 0.0
0 0.0
0 0.0
0.4022900763358779
0.44122137404580153

 ===== Epoch 37	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   3.6516511  -4.0486636
 -0.3640846  -0.3725409   3.2626753   2.3295684  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  3.628941    2.9684365
 -0.3640846  -0.3725409   3.2735457   2.406712   -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 2
train:	 Loss = 1.6307,	 Acc = 0.3728,	 Loss Con1 = 1.3519,	 Loss Con2 = 1.5389
2926 0.16
5677 0.426
2914 0.476
376 0.428
69 0.377
6 0.5
0 0.0
0 0.0
0.4418270294182703
0.44122137404580153
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.6354,	 Acc = 0.3930,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.5562
380 0.195
1718 0.451
815 0.385
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.4217557251908397
0.44122137404580153

 ===== Epoch 38	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 0
train:	 Loss = 1.6272,	 Acc = 0.3747,	 Loss Con1 = 1.3523,	 Loss Con2 = 1.5639
2924 0.16
5680 0.43
2912 0.475
377 0.424
69 0.406
6 0.5
0 0.0
0 0.0
0.44416187527642637
0.44122137404580153
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6474,	 Acc = 0.3933,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.5557
380 0.189
1718 0.445
815 0.405
85 0.153
2 0.5
0 0.0
0 0.0
0 0.0
0.42290076335877863
0.44122137404580153

 ===== Epoch 39	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   1.5367863   1.2434522
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  1.7879269   1.2435989
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.6264,	 Acc = 0.3758,	 Loss Con1 = 1.3525,	 Loss Con2 = 1.5505
2925 0.16
5679 0.429
2914 0.482
376 0.415
68 0.426
6 0.667
0 0.0
0 0.0
0.44575915072431715
0.44122137404580153
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6274,	 Acc = 0.4130,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.5429
380 0.192
1718 0.467
815 0.425
85 0.188
2 0.5
0 0.0
0 0.0
0 0.0
0.4450381679389313
0.4450381679389313

 ===== Epoch 40	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   1.4541163   3.203033   -0.42342317 -0.42019257
  2.1509376   1.0477846 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   1.3825976   3.123401   -0.42342317 -0.42019257
  2.1870909   0.95593256] 1 5
train:	 Loss = 1.6255,	 Acc = 0.3754,	 Loss Con1 = 1.3522,	 Loss Con2 = 1.5491
2924 0.162
5677 0.429
2916 0.476
377 0.424
69 0.449
5 0.8
0 0.0
0 0.0
0.4443830163644405
0.4450381679389313
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6335,	 Acc = 0.4047,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.5651
380 0.189
1718 0.453
815 0.421
85 0.224
2 0.5
0 0.0
0 0.0
0 0.0
0.43587786259541983
0.4450381679389313

 ===== Epoch 41	 =====
[ 2.5341654   1.5480583  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 2.0934966   1.3177015  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 5
train:	 Loss = 1.6260,	 Acc = 0.3762,	 Loss Con1 = 1.3515,	 Loss Con2 = 1.5444
2924 0.16
5683 0.431
2911 0.481
375 0.416
69 0.391
6 0.5
0 0.0
0 0.0
0.4460415745245467
0.4450381679389313
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6233,	 Acc = 0.4187,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.5642
380 0.189
1718 0.469
815 0.44
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.45190839694656487
0.45190839694656487

 ===== Epoch 42	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  0.9058592   1.1737533 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  0.8339942   1.0084195 ] 5 1
train:	 Loss = 1.6236,	 Acc = 0.3788,	 Loss Con1 = 1.3519,	 Loss Con2 = 1.5499
2928 0.162
5676 0.433
2913 0.483
376 0.434
69 0.42
6 0.333
0 0.0
0 0.0
0.4491150442477876
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6569,	 Acc = 0.3910,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.5561
380 0.182
1718 0.453
815 0.375
85 0.224
2 0.5
0 0.0
0 0.0
0 0.0
0.4213740458015267
0.45190839694656487

 ===== Epoch 43	 =====
[-0.36602148 -0.3783333   2.5249984   1.1972182  -0.4409929  -0.3635196
  2.3203251   3.3247118  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   2.2693458   1.2466564  -0.44088638 -0.36343393
  2.2074232   3.4223754  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 6
train:	 Loss = 1.6241,	 Acc = 0.3777,	 Loss Con1 = 1.3516,	 Loss Con2 = 1.5599
2924 0.161
5681 0.432
2911 0.479
377 0.432
69 0.522
6 0.333
0 0.0
0 0.0
0.4478107032286599
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6377,	 Acc = 0.4123,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.5610
380 0.192
1718 0.47
815 0.412
85 0.247
2 0.0
0 0.0
0 0.0
0 0.0
0.4442748091603053
0.45190839694656487

 ===== Epoch 44	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   1.2435659   1.5830176  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   1.2052314   1.8492874  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.6249,	 Acc = 0.3768,	 Loss Con1 = 1.3530,	 Loss Con2 = 1.5571
2920 0.159
5681 0.43
2915 0.481
377 0.435
69 0.522
6 0.5
0 0.0
0 0.0
0.4472811671087533
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6307,	 Acc = 0.4053,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.5382
380 0.195
1718 0.466
815 0.394
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.43587786259541983
0.45190839694656487

 ===== Epoch 45	 =====
[ 3.027559    1.0696247  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   1.7802404   2.7551024  -0.42342317 -0.42019257
  2.891636    1.2341132 ] [ 2.025567    1.2696048  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   1.7338971   2.9019241  -0.42342317 -0.42019257
  2.9463077   1.438812  ] 1 6
train:	 Loss = 1.6245,	 Acc = 0.3777,	 Loss Con1 = 1.3520,	 Loss Con2 = 1.5727
2923 0.161
5682 0.431
2913 0.481
375 0.429
69 0.464
6 0.333
0 0.0
0 0.0
0.44754007739082363
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6384,	 Acc = 0.4020,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.5820
380 0.195
1718 0.458
815 0.398
85 0.235
2 0.5
0 0.0
0 0.0
0 0.0
0.43206106870229005
0.45190839694656487

 ===== Epoch 46	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.6254,	 Acc = 0.3762,	 Loss Con1 = 1.3521,	 Loss Con2 = 1.5644
2927 0.16
5678 0.431
2914 0.476
374 0.422
69 0.522
6 0.333
0 0.0
0 0.0
0.44607897356487114
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6396,	 Acc = 0.3887,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.5672
380 0.179
1718 0.45
815 0.38
85 0.176
2 0.0
0 0.0
0 0.0
0 0.0
0.41908396946564885
0.45190839694656487

 ===== Epoch 47	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   3.1694157   1.2346271  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   2.8049572   1.0828285  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 5
train:	 Loss = 1.6253,	 Acc = 0.3756,	 Loss Con1 = 1.3524,	 Loss Con2 = 1.5792
2921 0.157
5685 0.432
2911 0.475
376 0.42
69 0.507
6 0.5
0 0.0
0 0.0
0.4461147341660219
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6245,	 Acc = 0.4193,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.5666
380 0.197
1718 0.468
815 0.438
85 0.259
2 0.0
0 0.0
0 0.0
0 0.0
0.45152671755725193
0.45190839694656487

 ===== Epoch 48	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   1.4346627   3.0661654  -0.42342317 -0.42019257
  2.2959912   0.9795517 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   1.4541163   3.203033   -0.42342317 -0.42019257
  2.1509376   1.0477846 ] 1 6
train:	 Loss = 1.6227,	 Acc = 0.3812,	 Loss Con1 = 1.3521,	 Loss Con2 = 1.5833
2926 0.162
5677 0.434
2913 0.488
377 0.44
69 0.449
6 0.667
0 0.0
0 0.0
0.4522229595222296
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6420,	 Acc = 0.3900,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.5565
380 0.179
1718 0.453
815 0.373
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.4206106870229008
0.45190839694656487

 ===== Epoch 49	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   1.5771273   2.150396   -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   1.4552609   2.1727927  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 0
train:	 Loss = 1.6241,	 Acc = 0.3776,	 Loss Con1 = 1.3523,	 Loss Con2 = 1.5735
2924 0.16
5679 0.433
2915 0.479
375 0.429
69 0.464
6 0.5
0 0.0
0 0.0
0.44792127377266694
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6519,	 Acc = 0.3770,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.5579
380 0.2
1718 0.434
815 0.36
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.4026717557251908
0.45190839694656487

 ===== Epoch 50	 =====
[ 0.62092185  2.0821826  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.6208,	 Acc = 0.3786,	 Loss Con1 = 1.3517,	 Loss Con2 = 1.5831
2926 0.162
5679 0.432
2913 0.483
376 0.434
68 0.441
6 0.5
0 0.0
0 0.0
0.44879451448794516
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6170,	 Acc = 0.4153,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.5692
380 0.195
1718 0.471
815 0.418
85 0.259
2 0.0
0 0.0
0 0.0
0 0.0
0.44732824427480916
0.45190839694656487

 ===== Epoch 51	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  3.0092382   1.837409
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.7023816   1.6328447
 -0.38555372 -0.3798593 ] 5 5
train:	 Loss = 1.6240,	 Acc = 0.3807,	 Loss Con1 = 1.3531,	 Loss Con2 = 1.5758
2920 0.16
5682 0.439
2914 0.479
377 0.438
69 0.449
6 0.333
0 0.0
0 0.0
0.45181255526083114
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6282,	 Acc = 0.3937,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.5779
380 0.189
1718 0.449
815 0.398
85 0.153
2 0.0
0 0.0
0 0.0
0 0.0
0.4232824427480916
0.45190839694656487

 ===== Epoch 52	 =====
[-0.36602148 -0.3783333   1.7870173   0.83476806 -0.4409929  -0.3635196
  3.5903249   3.0177705  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.8047231   0.8797408  -0.44088638 -0.36343393
  3.5633638   3.0345125  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 3 1
train:	 Loss = 1.6232,	 Acc = 0.3763,	 Loss Con1 = 1.3528,	 Loss Con2 = 1.5916
2920 0.161
5682 0.429
2915 0.482
376 0.41
69 0.478
6 0.5
0 0.0
0 0.0
0.4459549071618037
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6159,	 Acc = 0.4090,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.5677
380 0.2
1718 0.456
815 0.432
85 0.176
2 0.0
0 0.0
0 0.0
0 0.0
0.43931297709923667
0.45190839694656487

 ===== Epoch 53	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 0
train:	 Loss = 1.6200,	 Acc = 0.3793,	 Loss Con1 = 1.3524,	 Loss Con2 = 1.5926
2921 0.159
5683 0.434
2913 0.486
377 0.43
68 0.471
6 0.5
0 0.0
0 0.0
0.45064662319000776
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6342,	 Acc = 0.4047,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.6002
380 0.205
1718 0.471
815 0.38
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.433587786259542
0.45190839694656487

 ===== Epoch 54	 =====
[-0.36602148 -0.3783333   1.7244695   0.88913554 -0.4409929  -0.3635196
  1.3042119   3.8604653  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.715778    0.89559513 -0.44088638 -0.36343393
  1.4378967   3.8604653  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.6185,	 Acc = 0.3816,	 Loss Con1 = 1.3518,	 Loss Con2 = 1.6095
2926 0.162
5677 0.436
2914 0.491
377 0.416
68 0.382
6 0.667
0 0.0
0 0.0
0.4526653395266534
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6121,	 Acc = 0.4160,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.5679
380 0.195
1718 0.468
815 0.428
85 0.247
2 0.0
0 0.0
0 0.0
0 0.0
0.4480916030534351
0.45190839694656487

 ===== Epoch 55	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 0
train:	 Loss = 1.6190,	 Acc = 0.3788,	 Loss Con1 = 1.3526,	 Loss Con2 = 1.6152
2924 0.16
5680 0.434
2912 0.484
377 0.406
69 0.493
6 0.5
0 0.0
0 0.0
0.44946926138876603
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6358,	 Acc = 0.3993,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.5946
380 0.197
1718 0.459
815 0.389
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.42862595419847327
0.45190839694656487

 ===== Epoch 56	 =====
[-0.36602148 -0.3783333   2.1980443   1.1632385  -0.4409929  -0.3635196
  4.7861824   2.7610555  -0.40141198 -0.40778467  4.7840824   2.9021297
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   2.3903751   1.1515301  -0.44088638 -0.36343393
  5.4546022   2.7722168  -0.40141198 -0.40778467  4.350694   -4.807235
 -0.38555372 -0.3798593 ] 3 2
train:	 Loss = 1.6208,	 Acc = 0.3824,	 Loss Con1 = 1.3532,	 Loss Con2 = 1.6195
2926 0.162
5677 0.437
2913 0.485
377 0.44
69 0.522
6 0.333
0 0.0
0 0.0
0.453550099535501
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6289,	 Acc = 0.4127,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.6297
380 0.197
1718 0.469
815 0.42
85 0.176
2 0.0
0 0.0
0 0.0
0 0.0
0.4438931297709924
0.45190839694656487

 ===== Epoch 57	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   2.2082567  -4.0953393
 -0.3640846  -0.3725409   1.6011586   2.1628387  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   1.6949893   2.2175858  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 6
train:	 Loss = 1.6195,	 Acc = 0.3823,	 Loss Con1 = 1.3524,	 Loss Con2 = 1.6237
2922 0.161
5682 0.44
2912 0.486
377 0.419
69 0.464
6 0.167
0 0.0
0 0.0
0.4539022772496131
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6270,	 Acc = 0.4110,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.5813
380 0.182
1718 0.471
815 0.412
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.4442748091603053
0.45190839694656487

 ===== Epoch 58	 =====
[ 3.3185494   2.013835   -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.593594    2.2418618 ] [ 1.8172349   1.9100479  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.3497808   2.121142  ] 1 2
train:	 Loss = 1.6183,	 Acc = 0.3798,	 Loss Con1 = 1.3528,	 Loss Con2 = 1.6354
2922 0.163
5681 0.433
2915 0.485
375 0.437
69 0.449
6 0.333
0 0.0
0 0.0
0.4499226177315941
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6237,	 Acc = 0.4123,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.5767
380 0.192
1718 0.466
815 0.42
85 0.235
2 0.5
0 0.0
0 0.0
0 0.0
0.4442748091603053
0.45190839694656487

 ===== Epoch 59	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.9071242   2.5672808 ] [ 2.1408272   2.4542978  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.9221152   2.79035   ] 1 5
train:	 Loss = 1.6186,	 Acc = 0.3792,	 Loss Con1 = 1.3530,	 Loss Con2 = 1.6321
2924 0.157
5678 0.437
2914 0.48
377 0.435
69 0.449
6 0.167
0 0.0
0 0.0
0.450906678460858
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6267,	 Acc = 0.4127,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.5619
380 0.195
1718 0.462
815 0.432
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.4442748091603053
0.45190839694656487

 ===== Epoch 60	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   2.7853856   1.701317
 -0.3640846  -0.3725409   2.193331    1.8667068  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   4.327659    3.0857646   3.0555103   1.6881447
 -0.3640846  -0.3725409   1.9318597   1.8517759  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 3 2
train:	 Loss = 1.6178,	 Acc = 0.3796,	 Loss Con1 = 1.3523,	 Loss Con2 = 1.6378
2922 0.164
5681 0.433
2914 0.481
376 0.436
69 0.507
6 0.5
0 0.0
0 0.0
0.44925934114525756
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6182,	 Acc = 0.4163,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.6140
380 0.195
1718 0.469
815 0.426
85 0.271
2 0.0
0 0.0
0 0.0
0 0.0
0.4484732824427481
0.45190839694656487

 ===== Epoch 61	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 0
train:	 Loss = 1.6207,	 Acc = 0.3799,	 Loss Con1 = 1.3514,	 Loss Con2 = 1.6388
2925 0.16
5680 0.435
2913 0.486
375 0.419
69 0.435
6 0.833
0 0.0
0 0.0
0.45095654097091675
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6271,	 Acc = 0.4020,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.5778
380 0.192
1718 0.449
815 0.418
85 0.235
2 0.5
0 0.0
0 0.0
0 0.0
0.43244274809160305
0.45190839694656487

 ===== Epoch 62	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   1.3190743   1.0811992
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  1.6763734   1.170249
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 2
train:	 Loss = 1.6181,	 Acc = 0.3833,	 Loss Con1 = 1.3521,	 Loss Con2 = 1.6320
2924 0.162
5680 0.438
2914 0.488
375 0.427
69 0.551
6 0.667
0 0.0
0 0.0
0.4547766475011057
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6251,	 Acc = 0.3900,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.5759
380 0.189
1718 0.449
815 0.38
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.41908396946564885
0.45190839694656487

 ===== Epoch 63	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   2.0822122   2.063608
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  2.2608833   1.9593177
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 5
train:	 Loss = 1.6171,	 Acc = 0.3819,	 Loss Con1 = 1.3521,	 Loss Con2 = 1.6223
2925 0.161
5682 0.436
2912 0.488
374 0.449
69 0.435
6 0.333
0 0.0
0 0.0
0.4532787791662059
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6238,	 Acc = 0.4100,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.6133
380 0.189
1718 0.464
815 0.418
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.44198473282442746
0.45190839694656487

 ===== Epoch 64	 =====
[-0.36602148 -0.3783333   2.7800624   2.345732   -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   2.735593    2.4561193  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 6
train:	 Loss = 1.6176,	 Acc = 0.3806,	 Loss Con1 = 1.3526,	 Loss Con2 = 1.6272
2922 0.163
5683 0.431
2913 0.49
375 0.443
69 0.478
6 0.5
0 0.0
0 0.0
0.45102807870882156
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6231,	 Acc = 0.4177,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.6024
380 0.192
1718 0.467
815 0.438
85 0.247
2 0.0
0 0.0
0 0.0
0 0.0
0.45038167938931295
0.45190839694656487

 ===== Epoch 65	 =====
[ 0.3022398   1.3227642  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 0.3415716   1.4189571  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 4
train:	 Loss = 1.6143,	 Acc = 0.3825,	 Loss Con1 = 1.3520,	 Loss Con2 = 1.6582
2927 0.16
5678 0.436
2913 0.492
375 0.435
69 0.493
6 0.333
0 0.0
0 0.0
0.4544851233270656
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6454,	 Acc = 0.4120,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.6321
380 0.2
1718 0.466
815 0.418
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.44274809160305345
0.45190839694656487

 ===== Epoch 66	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   4.9693823   2.6770577
 -0.3640846  -0.3725409   5.3138247   2.1628387  -0.42342317 -0.42019257
  2.994805    0.97430295] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  4.323095    2.643918
 -0.3640846  -0.3725409   4.7445393   2.1354651  -0.42342317 -0.42019257
  2.9648244   0.97167856] 2 0
train:	 Loss = 1.6150,	 Acc = 0.3815,	 Loss Con1 = 1.3524,	 Loss Con2 = 1.6537
2928 0.16
5679 0.433
2909 0.495
377 0.432
69 0.478
6 0.667
0 0.0
0 0.0
0.45331858407079645
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6285,	 Acc = 0.4107,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.5906
380 0.189
1718 0.471
815 0.406
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.44274809160305345
0.45190839694656487

 ===== Epoch 67	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.1196074   1.9507821
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.1366552   2.005004
 -0.38555372 -0.3798593 ] 2 6
train:	 Loss = 1.6156,	 Acc = 0.3799,	 Loss Con1 = 1.3520,	 Loss Con2 = 1.6722
2923 0.16
5684 0.434
2909 0.487
377 0.411
69 0.478
6 0.667
0 0.0
0 0.0
0.4509673852957435
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6376,	 Acc = 0.4020,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.6459
380 0.2
1718 0.455
815 0.401
85 0.247
2 0.0
0 0.0
0 0.0
0 0.0
0.4312977099236641
0.45190839694656487

 ===== Epoch 68	 =====
[-0.36602148 -0.3783333   0.62541854  1.9085265  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   0.6313873   1.8038248  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 5
train:	 Loss = 1.6148,	 Acc = 0.3790,	 Loss Con1 = 1.3524,	 Loss Con2 = 1.6927
2920 0.162
5681 0.43
2915 0.49
377 0.419
69 0.478
6 0.333
0 0.0
0 0.0
0.44916003536693194
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6240,	 Acc = 0.4017,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.6619
380 0.189
1718 0.454
815 0.41
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.43244274809160305
0.45190839694656487

 ===== Epoch 69	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.0299492   1.7462177
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.2148206   1.6648849
 -0.38555372 -0.3798593 ] 3 2
train:	 Loss = 1.6155,	 Acc = 0.3779,	 Loss Con1 = 1.3525,	 Loss Con2 = 1.7034
2920 0.157
5683 0.435
2913 0.481
377 0.411
69 0.449
6 0.5
0 0.0
0 0.0
0.44916003536693194
0.45190839694656487
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6371,	 Acc = 0.4200,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.6061
380 0.195
1718 0.477
815 0.426
85 0.212
2 0.5
0 0.0
0 0.0
0 0.0
0.45267175572519086
0.45267175572519086

 ===== Epoch 70	 =====
[-0.36602148 -0.3783333   2.2248504  -4.1194715   2.1815202   0.81892717
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   2.375348    3.0314069   2.4977427   0.9301942
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 2
train:	 Loss = 1.6174,	 Acc = 0.3807,	 Loss Con1 = 1.3519,	 Loss Con2 = 1.6945
2925 0.159
5681 0.436
2911 0.486
376 0.434
69 0.464
6 0.667
0 0.0
0 0.0
0.4522835342253677
0.45267175572519086
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6341,	 Acc = 0.4197,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.6444
380 0.2
1718 0.477
815 0.423
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.45152671755725193
0.45267175572519086

 ===== Epoch 71	 =====
[ 2.2909071   2.1505303  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.1932635   2.3442113 ] [ 2.842128    2.0239606  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.2364714   2.244486  ] 5 5
train:	 Loss = 1.6141,	 Acc = 0.3824,	 Loss Con1 = 1.3515,	 Loss Con2 = 1.7214
2922 0.159
5683 0.435
2912 0.492
376 0.449
69 0.507
6 0.5
0 0.0
0 0.0
0.45445500773822683
0.45267175572519086
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6343,	 Acc = 0.4190,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.6626
380 0.205
1718 0.473
815 0.423
85 0.259
2 0.0
0 0.0
0 0.0
0 0.0
0.45
0.45267175572519086

 ===== Epoch 72	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   1.9573141   1.3723656
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  1.7455213   1.4169718
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 4
train:	 Loss = 1.6145,	 Acc = 0.3823,	 Loss Con1 = 1.3529,	 Loss Con2 = 1.7409
2921 0.162
5679 0.436
2918 0.49
375 0.419
69 0.493
6 0.333
0 0.0
0 0.0
0.45329943627721897
0.45267175572519086
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6306,	 Acc = 0.4030,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.6324
380 0.184
1718 0.456
815 0.411
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.4347328244274809
0.45267175572519086

 ===== Epoch 73	 =====
[-0.36602148 -0.3783333   1.5697255   2.8576927  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.4676273   3.0336719  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.6140,	 Acc = 0.3829,	 Loss Con1 = 1.3522,	 Loss Con2 = 1.7376
2923 0.162
5682 0.434
2911 0.496
377 0.443
69 0.464
6 0.333
0 0.0
0 0.0
0.4542841348811498
0.45267175572519086
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6401,	 Acc = 0.4157,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.5949
380 0.189
1718 0.474
815 0.42
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.4484732824427481
0.45267175572519086

 ===== Epoch 74	 =====
[-0.36602148 -0.3783333   3.0594962   2.3774464  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   2.7831113  -4.7848034  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 2
train:	 Loss = 1.6109,	 Acc = 0.3812,	 Loss Con1 = 1.3519,	 Loss Con2 = 1.7434
2923 0.161
5682 0.436
2912 0.486
376 0.42
69 0.522
6 0.333
0 0.0
0 0.0
0.4521835268103925
0.45267175572519086
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6371,	 Acc = 0.4067,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.6230
380 0.187
1718 0.467
815 0.404
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.4385496183206107
0.45267175572519086

 ===== Epoch 75	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.6219,	 Acc = 0.3794,	 Loss Con1 = 1.3526,	 Loss Con2 = 1.7209
2925 0.164
5680 0.431
2913 0.487
375 0.424
69 0.478
6 0.333
0 0.0
0 0.0
0.44907663386044455
0.45267175572519086
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6417,	 Acc = 0.4010,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.7418
380 0.195
1718 0.458
815 0.399
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.4309160305343511
0.45267175572519086

 ===== Epoch 76	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 0
train:	 Loss = 1.6145,	 Acc = 0.3803,	 Loss Con1 = 1.3509,	 Loss Con2 = 1.7977
2922 0.16
5682 0.435
2915 0.488
375 0.413
68 0.471
6 0.333
0 0.0
0 0.0
0.45147026309971255
0.45267175572519086
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6290,	 Acc = 0.4203,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.6997
380 0.192
1718 0.477
815 0.431
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.4534351145038168
0.4534351145038168

 ===== Epoch 77	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   0.84649444  2.67547    -0.42342317 -0.42019257
  2.495716    1.1160176 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   0.8688083   2.6431196  -0.42342317 -0.42019257
  2.3956327   1.1002716 ] 1 3
train:	 Loss = 1.6151,	 Acc = 0.3806,	 Loss Con1 = 1.3519,	 Loss Con2 = 1.7632
2923 0.163
5680 0.435
2913 0.489
377 0.395
69 0.449
6 0.333
0 0.0
0 0.0
0.45107794361525705
0.4534351145038168
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6271,	 Acc = 0.4027,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.6842
380 0.195
1718 0.451
815 0.42
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.43282442748091604
0.4534351145038168

 ===== Epoch 78	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 3 0
train:	 Loss = 1.6143,	 Acc = 0.3828,	 Loss Con1 = 1.3518,	 Loss Con2 = 1.8281
2927 0.161
5676 0.44
2915 0.487
375 0.416
69 0.478
6 0.333
0 0.0
0 0.0
0.45470633779449177
0.4534351145038168
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6582,	 Acc = 0.4150,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.7192
380 0.189
1718 0.469
815 0.427
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.44770992366412216
0.4534351145038168

 ===== Epoch 79	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  1.9013901   0.7568214
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 3 6
train:	 Loss = 1.6135,	 Acc = 0.3821,	 Loss Con1 = 1.3519,	 Loss Con2 = 1.8215
2927 0.167
5679 0.431
2911 0.493
376 0.444
69 0.493
6 0.333
0 0.0
0 0.0
0.45171994248423847
0.4534351145038168
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6538,	 Acc = 0.4007,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.6291
380 0.197
1718 0.46
815 0.389
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.4301526717557252
0.4534351145038168

 ===== Epoch 80	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.2369688   0.9769273 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 5
train:	 Loss = 1.6119,	 Acc = 0.3835,	 Loss Con1 = 1.3529,	 Loss Con2 = 1.7990
2919 0.161
5682 0.436
2916 0.495
377 0.427
68 0.515
6 0.333
0 0.0
0 0.0
0.455298928058349
0.4534351145038168
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6697,	 Acc = 0.3990,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.7251
380 0.182
1718 0.456
815 0.4
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.4305343511450382
0.4534351145038168

 ===== Epoch 81	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   2.2468338   2.074721
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  2.2242076   2.0259995
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 2
train:	 Loss = 1.6146,	 Acc = 0.3855,	 Loss Con1 = 1.3521,	 Loss Con2 = 1.8202
2926 0.161
5681 0.44
2913 0.499
374 0.439
68 0.412
6 0.333
0 0.0
0 0.0
0.45830568458305687
0.4534351145038168
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6291,	 Acc = 0.3837,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.6160
380 0.184
1718 0.44
815 0.38
85 0.176
2 0.0
0 0.0
0 0.0
0 0.0
0.4125954198473282
0.4534351145038168

 ===== Epoch 82	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.235657    1.5317948
 -0.38555372 -0.3798593 ] [ 2.3669856   3.4466047  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.2223976   1.5761582
 -0.38555372 -0.3798593 ] 2 6
train:	 Loss = 1.6089,	 Acc = 0.3829,	 Loss Con1 = 1.3518,	 Loss Con2 = 1.8015
2926 0.161
5677 0.437
2913 0.494
377 0.419
69 0.464
6 0.167
0 0.0
0 0.0
0.45476664454766647
0.4534351145038168
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6257,	 Acc = 0.4210,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.6754
380 0.189
1718 0.471
815 0.443
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.4545801526717557
0.4545801526717557

 ===== Epoch 83	 =====
[-0.36602148 -0.3783333   1.7394972   2.9188561  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.8262476   2.9091017  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 0
train:	 Loss = 1.6098,	 Acc = 0.3874,	 Loss Con1 = 1.3521,	 Loss Con2 = 1.8391
2924 0.16
5680 0.443
2912 0.501
377 0.422
69 0.478
6 0.333
0 0.0
0 0.0
0.460968597965502
0.4545801526717557
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6394,	 Acc = 0.4063,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.7566
380 0.195
1718 0.465
815 0.405
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.43702290076335876
0.4545801526717557

 ===== Epoch 84	 =====
[ 2.2361588   2.3555734  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.2272131   2.5987728 ] [ 2.4065223   2.2315352  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.2272131   2.6013973 ] 0 5
train:	 Loss = 1.6129,	 Acc = 0.3831,	 Loss Con1 = 1.3514,	 Loss Con2 = 1.8579
2924 0.161
5683 0.437
2911 0.493
375 0.421
69 0.507
6 0.167
0 0.0
0 0.0
0.4547766475011057
0.4545801526717557
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6542,	 Acc = 0.3937,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.8784
380 0.187
1718 0.447
815 0.4
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.42366412213740456
0.4545801526717557

 ===== Epoch 85	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 0
train:	 Loss = 1.6100,	 Acc = 0.3835,	 Loss Con1 = 1.3512,	 Loss Con2 = 1.8623
2925 0.163
5684 0.436
2909 0.491
375 0.453
69 0.522
6 0.5
0 0.0
0 0.0
0.4549375207342696
0.4545801526717557
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6391,	 Acc = 0.3843,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.6671
380 0.187
1718 0.442
815 0.369
85 0.247
2 0.5
0 0.0
0 0.0
0 0.0
0.4129770992366412
0.4545801526717557

 ===== Epoch 86	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   2.605277    1.3615408  -0.42342317 -0.42019257
  3.601472    1.2656053 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   2.5709486   1.6302992  -0.42342317 -0.42019257
  4.132747    1.5280398 ] 6 6
train:	 Loss = 1.6115,	 Acc = 0.3857,	 Loss Con1 = 1.3519,	 Loss Con2 = 1.8384
2924 0.161
5683 0.442
2912 0.493
374 0.42
69 0.507
6 0.5
0 0.0
0 0.0
0.4582043343653251
0.4545801526717557
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6320,	 Acc = 0.4103,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.6521
380 0.189
1718 0.467
815 0.416
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.44236641221374046
0.4545801526717557

 ===== Epoch 87	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.4056292   2.1627402
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.5245826   2.2144976
 -0.38555372 -0.3798593 ] 4 4
train:	 Loss = 1.6098,	 Acc = 0.3842,	 Loss Con1 = 1.3522,	 Loss Con2 = 1.8970
2921 0.165
5680 0.434
2916 0.498
377 0.424
68 0.544
6 0.333
0 0.0
0 0.0
0.454957444456726
0.4545801526717557
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6253,	 Acc = 0.3913,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.6326
380 0.197
1718 0.446
815 0.391
85 0.165
2 0.0
0 0.0
0 0.0
0 0.0
0.41946564885496185
0.4545801526717557

 ===== Epoch 88	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  3.8245633   3.2181184 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  3.7319765  -4.744146  ] 4 6
train:	 Loss = 1.6130,	 Acc = 0.3794,	 Loss Con1 = 1.3524,	 Loss Con2 = 1.9318
2925 0.161
5679 0.434
2913 0.483
376 0.434
69 0.478
6 0.333
0 0.0
0 0.0
0.4499612960300785
0.4545801526717557
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6679,	 Acc = 0.3977,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.8894
380 0.187
1718 0.46
815 0.38
85 0.247
2 0.0
0 0.0
0 0.0
0 0.0
0.42824427480916033
0.4545801526717557

 ===== Epoch 89	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.1780736   1.8349445
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.162163    1.9557112
 -0.38555372 -0.3798593 ] 2 6
train:	 Loss = 1.6096,	 Acc = 0.3857,	 Loss Con1 = 1.3517,	 Loss Con2 = 1.8926
2924 0.161
5684 0.441
2910 0.496
375 0.424
69 0.478
6 0.333
0 0.0
0 0.0
0.4582043343653251
0.4545801526717557
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6346,	 Acc = 0.4183,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.8005
380 0.187
1718 0.473
815 0.429
85 0.247
2 0.0
0 0.0
0 0.0
0 0.0
0.45190839694656487
0.4545801526717557

 ===== Epoch 90	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.3364265   0.80226415
  2.031897    3.4569337 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.5000844   0.8195166
  2.0266063   3.498923  ] 4 4
train:	 Loss = 1.6151,	 Acc = 0.3800,	 Loss Con1 = 1.3517,	 Loss Con2 = 1.9785
2922 0.16
5684 0.433
2912 0.486
375 0.443
69 0.464
6 0.333
0 0.0
0 0.0
0.45102807870882156
0.4545801526717557
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6363,	 Acc = 0.4163,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.8042
380 0.189
1718 0.474
815 0.421
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.449236641221374
0.4545801526717557

 ===== Epoch 91	 =====
[-0.36602148 -0.3783333   2.4392996   2.0263228  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   2.634059    2.1005282  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 4
train:	 Loss = 1.6090,	 Acc = 0.3802,	 Loss Con1 = 1.3508,	 Loss Con2 = 1.8710
2928 0.162
5678 0.433
2911 0.489
377 0.422
68 0.471
6 0.333
0 0.0
0 0.0
0.4508849557522124
0.4545801526717557
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6504,	 Acc = 0.4160,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.8838
380 0.192
1718 0.479
815 0.407
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.4484732824427481
0.4545801526717557

 ===== Epoch 92	 =====
[ 0.6690908   1.2392281  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 0.8562903   1.3506097  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 6
train:	 Loss = 1.6107,	 Acc = 0.3835,	 Loss Con1 = 1.3520,	 Loss Con2 = 1.8860
2924 0.161
5681 0.438
2912 0.492
376 0.431
69 0.493
6 0.333
0 0.0
0 0.0
0.45544007076514814
0.4545801526717557
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6591,	 Acc = 0.3990,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.9780
380 0.195
1718 0.453
815 0.402
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.42862595419847327
0.4545801526717557

 ===== Epoch 93	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   1.9286684   1.007852
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  2.0603166   0.956867
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 3
train:	 Loss = 1.6090,	 Acc = 0.3868,	 Loss Con1 = 1.3525,	 Loss Con2 = 1.9825
2927 0.161
5678 0.442
2911 0.498
377 0.43
69 0.493
6 0.5
0 0.0
0 0.0
0.4600154850127198
0.4545801526717557
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6486,	 Acc = 0.4197,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.9231
380 0.189
1718 0.471
815 0.439
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.4530534351145038
0.4545801526717557

 ===== Epoch 94	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.321906    1.1744235
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.3438781   1.1202015
 -0.38555372 -0.3798593 ] 5 3
train:	 Loss = 1.6085,	 Acc = 0.3835,	 Loss Con1 = 1.3520,	 Loss Con2 = 2.0763
2925 0.162
5679 0.434
2913 0.497
376 0.434
69 0.551
6 0.333
0 0.0
0 0.0
0.45526926904788234
0.4545801526717557
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6667,	 Acc = 0.3820,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.7858
380 0.187
1718 0.442
815 0.367
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.41030534351145037
0.4545801526717557

 ===== Epoch 95	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 0
train:	 Loss = 1.6125,	 Acc = 0.3836,	 Loss Con1 = 1.3519,	 Loss Con2 = 2.0168
2925 0.161
5681 0.439
2912 0.49
375 0.432
69 0.522
6 0.5
0 0.0
0 0.0
0.45571160013269935
0.4545801526717557
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6458,	 Acc = 0.3957,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.8008
380 0.187
1718 0.453
815 0.388
85 0.247
2 0.0
0 0.0
0 0.0
0 0.0
0.4259541984732824
0.4545801526717557

 ===== Epoch 96	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   1.9240849   1.2879052
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  1.9537289   1.383631
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
train:	 Loss = 1.6091,	 Acc = 0.3849,	 Loss Con1 = 1.3526,	 Loss Con2 = 2.0015
2920 0.161
5681 0.437
2917 0.496
375 0.437
69 0.58
6 0.333
0 0.0
0 0.0
0.45711759504862953
0.4545801526717557
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6423,	 Acc = 0.3857,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.7471
380 0.189
1718 0.447
815 0.373
85 0.153
2 0.0
0 0.0
0 0.0
0 0.0
0.41412213740458015
0.4545801526717557

 ===== Epoch 97	 =====
[-0.36602148 -0.3783333   1.4860581   1.9357102  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.5821583   2.0303159  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 6
train:	 Loss = 1.6050,	 Acc = 0.3855,	 Loss Con1 = 1.3519,	 Loss Con2 = 1.9999
2925 0.164
5680 0.44
2914 0.493
375 0.424
68 0.515
6 0.333
0 0.0
0 0.0
0.45714917615835454
0.4545801526717557
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6359,	 Acc = 0.4147,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.8909
380 0.192
1718 0.469
815 0.425
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.44694656488549617
0.4545801526717557

 ===== Epoch 98	 =====
[-0.36602148 -0.3783333   2.086353    1.392035   -0.4409929  -0.3635196
  1.4704748   3.7600114  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   2.4537332   1.4663529  -0.44088638 -0.36343393
  1.554167    3.8018668  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 2
train:	 Loss = 1.6113,	 Acc = 0.3832,	 Loss Con1 = 1.3511,	 Loss Con2 = 1.9734
2923 0.16
5685 0.437
2911 0.495
374 0.414
69 0.507
6 0.333
0 0.0
0 0.0
0.4551686014372582
0.4545801526717557
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6644,	 Acc = 0.3910,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.0240
380 0.187
1718 0.455
815 0.375
85 0.176
2 0.0
0 0.0
0 0.0
0 0.0
0.4206106870229008
0.4545801526717557

 ===== Epoch 99	 =====
[ 2.324227    2.1252165  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.1879728   2.357333  ] [ 2.2738273   2.1252165  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.1884143   2.357333  ] 0 0
train:	 Loss = 1.6107,	 Acc = 0.3820,	 Loss Con1 = 1.3515,	 Loss Con2 = 1.9985
2925 0.161
5684 0.437
2909 0.49
375 0.421
69 0.478
6 0.167
0 0.0
0 0.0
0.45338936193741014
0.4545801526717557
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6215,	 Acc = 0.4037,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.6633
380 0.187
1718 0.463
815 0.4
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.4351145038167939
0.4545801526717557

 ===== Epoch 100	 =====
[ 2.287219    2.1252165  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.1884143   2.357333  ] [ 2.3229847   2.1252165  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.1879728   2.357333  ] 0 0
train:	 Loss = 1.6060,	 Acc = 0.3880,	 Loss Con1 = 1.3532,	 Loss Con2 = 1.9838
2919 0.161
5681 0.442
2917 0.503
376 0.415
69 0.551
6 0.333
0 0.0
0 0.0
0.4613769477290308
0.4545801526717557
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6695,	 Acc = 0.3740,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.8759
380 0.189
1718 0.434
815 0.36
85 0.129
2 0.0
0 0.0
0 0.0
0 0.0
0.40076335877862596
0.4545801526717557

 ===== Epoch 101	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   1.7502973   1.732434
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  1.717251    1.8970813
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.6105,	 Acc = 0.3834,	 Loss Con1 = 1.3519,	 Loss Con2 = 2.0966
2928 0.161
5677 0.438
2912 0.491
376 0.426
69 0.565
6 0.5
0 0.0
0 0.0
0.45530973451327433
0.4545801526717557
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6528,	 Acc = 0.3913,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.9028
380 0.192
1718 0.449
815 0.388
85 0.165
2 0.0
0 0.0
0 0.0
0 0.0
0.4202290076335878
0.4545801526717557

 ===== Epoch 102	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 3.0505517   3.469387   -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 3 6
train:	 Loss = 1.6105,	 Acc = 0.3842,	 Loss Con1 = 1.3525,	 Loss Con2 = 1.9513
2921 0.161
5682 0.439
2915 0.494
375 0.413
69 0.507
6 0.333
0 0.0
0 0.0
0.4563943848789654
0.4545801526717557
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6551,	 Acc = 0.3813,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.8116
380 0.189
1718 0.439
815 0.369
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.40916030534351144
0.4545801526717557

 ===== Epoch 103	 =====
[ 2.1668005   2.6770606  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.9692906   2.9451864 ] [ 2.482594    2.6264327  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.8872854   2.8690805 ] 0 5
train:	 Loss = 1.6101,	 Acc = 0.3853,	 Loss Con1 = 1.3527,	 Loss Con2 = 2.1439
2924 0.161
5677 0.435
2917 0.505
375 0.427
69 0.493
6 0.333
0 0.0
0 0.0
0.4576514816452897
0.4545801526717557
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6598,	 Acc = 0.3863,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.7952
380 0.195
1718 0.449
815 0.368
85 0.165
2 0.0
0 0.0
0 0.0
0 0.0
0.41412213740458015
0.4545801526717557

 ===== Epoch 104	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.1913328   1.7412884
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.4299998   1.5687642
 -0.38555372 -0.3798593 ] 4 5
train:	 Loss = 1.6081,	 Acc = 0.3839,	 Loss Con1 = 1.3516,	 Loss Con2 = 2.0646
2926 0.161
5679 0.437
2913 0.498
375 0.427
69 0.478
6 0.5
0 0.0
0 0.0
0.4562043795620438
0.4545801526717557
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6433,	 Acc = 0.4047,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.8358
380 0.192
1718 0.46
815 0.411
85 0.176
2 0.0
0 0.0
0 0.0
0 0.0
0.4354961832061069
0.4545801526717557

 ===== Epoch 105	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 0
train:	 Loss = 1.6143,	 Acc = 0.3807,	 Loss Con1 = 1.3511,	 Loss Con2 = 1.9967
2926 0.16
5684 0.432
2907 0.495
376 0.423
69 0.449
6 0.333
0 0.0
0 0.0
0.4520017695200177
0.4545801526717557
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6426,	 Acc = 0.4070,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.8739
380 0.189
1718 0.467
815 0.406
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.4385496183206107
0.4545801526717557

 ===== Epoch 106	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 0
train:	 Loss = 1.6094,	 Acc = 0.3795,	 Loss Con1 = 1.3517,	 Loss Con2 = 2.0941
2924 0.169
5683 0.43
2911 0.486
375 0.421
69 0.478
6 0.333
0 0.0
0 0.0
0.4475895621406457
0.4545801526717557
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6278,	 Acc = 0.4063,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.7205
380 0.189
1718 0.467
815 0.409
85 0.141
2 0.0
0 0.0
0 0.0
0 0.0
0.43778625954198475
0.4545801526717557

 ===== Epoch 107	 =====
[-0.36602148 -0.3783333   1.4499109   1.1179322  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.649171    1.2715704  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 6
train:	 Loss = 1.6067,	 Acc = 0.3899,	 Loss Con1 = 1.3524,	 Loss Con2 = 2.0333
2923 0.161
5679 0.448
2915 0.498
376 0.418
69 0.551
6 0.333
0 0.0
0 0.0
0.46379215035931454
0.4545801526717557
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6420,	 Acc = 0.3970,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.8620
380 0.187
1718 0.454
815 0.405
85 0.118
2 0.0
0 0.0
0 0.0
0 0.0
0.42748091603053434
0.4545801526717557

 ===== Epoch 108	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 0
train:	 Loss = 1.6117,	 Acc = 0.3842,	 Loss Con1 = 1.3521,	 Loss Con2 = 2.4905
2927 0.163
5678 0.438
2911 0.495
377 0.422
69 0.507
6 0.5
0 0.0
0 0.0
0.4559230173653357
0.4545801526717557
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6477,	 Acc = 0.4093,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.0295
380 0.192
1718 0.471
815 0.404
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.44083969465648853
0.4545801526717557

 ===== Epoch 109	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 0
train:	 Loss = 1.6080,	 Acc = 0.3821,	 Loss Con1 = 1.3515,	 Loss Con2 = 2.4043
2926 0.163
5680 0.432
2913 0.496
374 0.422
69 0.536
6 0.167
0 0.0
0 0.0
0.45299712452997126
0.4545801526717557
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6835,	 Acc = 0.4027,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.1992
380 0.182
1718 0.47
815 0.384
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.4347328244274809
0.4545801526717557

 ===== Epoch 110	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 0
train:	 Loss = 1.6108,	 Acc = 0.3852,	 Loss Con1 = 1.3524,	 Loss Con2 = 2.3529
2925 0.163
5680 0.44
2911 0.494
377 0.422
69 0.507
6 0.167
0 0.0
0 0.0
0.45714917615835454
0.4545801526717557
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6633,	 Acc = 0.3850,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.1263
380 0.184
1718 0.444
815 0.379
85 0.165
2 0.0
0 0.0
0 0.0
0 0.0
0.41412213740458015
0.4545801526717557

 ===== Epoch 111	 =====
[-0.36602148 -0.3783333   3.2946584   2.1395884  -0.4409929  -0.3635196
  2.2602236   3.7767537  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   3.3033755   2.283986   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.6113,	 Acc = 0.3838,	 Loss Con1 = 1.3526,	 Loss Con2 = 2.2243
2924 0.16
5681 0.436
2911 0.497
377 0.438
69 0.493
6 0.5
0 0.0
0 0.0
0.45599292348518355
0.4545801526717557
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6596,	 Acc = 0.4087,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.0823
380 0.195
1718 0.464
815 0.417
85 0.165
2 0.0
0 0.0
0 0.0
0 0.0
0.4396946564885496
0.4545801526717557

 ===== Epoch 112	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.5450406   2.9637454
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.5829234   2.9883916
 -0.38555372 -0.3798593 ] 4 0
train:	 Loss = 1.6059,	 Acc = 0.3831,	 Loss Con1 = 1.3518,	 Loss Con2 = 2.1282
2924 0.157
5679 0.439
2914 0.496
376 0.418
69 0.435
6 0.167
0 0.0
0 0.0
0.4562140645731977
0.4545801526717557
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6426,	 Acc = 0.3983,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.7691
380 0.179
1718 0.461
815 0.388
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.4301526717557252
0.4545801526717557

 ===== Epoch 113	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   2.4721851   1.9102455
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  2.263557    1.9993268
 -0.3640846  -0.3725409   2.4330614   0.8339783  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 6
train:	 Loss = 1.6086,	 Acc = 0.3844,	 Loss Con1 = 1.3518,	 Loss Con2 = 2.0873
2922 0.159
5681 0.435
2915 0.498
376 0.457
68 0.544
6 0.333
0 0.0
0 0.0
0.45732920627901835
0.4545801526717557
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6483,	 Acc = 0.4017,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.9652
380 0.189
1718 0.458
815 0.4
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.43244274809160305
0.4545801526717557

 ===== Epoch 114	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 0
train:	 Loss = 1.6086,	 Acc = 0.3827,	 Loss Con1 = 1.3528,	 Loss Con2 = 2.1120
2922 0.163
5679 0.435
2916 0.496
376 0.402
69 0.551
6 0.333
0 0.0
0 0.0
0.45379173115189037
0.4545801526717557
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6345,	 Acc = 0.3980,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.7342
380 0.192
1718 0.465
815 0.374
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.42786259541984734
0.4545801526717557

 ===== Epoch 115	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 0
train:	 Loss = 1.6073,	 Acc = 0.3863,	 Loss Con1 = 1.3522,	 Loss Con2 = 2.0919
2920 0.162
5684 0.439
2913 0.499
376 0.428
69 0.551
6 0.333
0 0.0
0 0.0
0.45866489832007074
0.4545801526717557
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6636,	 Acc = 0.4057,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.1372
380 0.182
1718 0.469
815 0.401
85 0.176
2 0.0
0 0.0
0 0.0
0 0.0
0.4381679389312977
0.4545801526717557

 ===== Epoch 116	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.4529825   2.5077884
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.6037596   2.6285553
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.6073,	 Acc = 0.3831,	 Loss Con1 = 1.3512,	 Loss Con2 = 2.2064
2927 0.161
5678 0.439
2914 0.49
374 0.422
69 0.493
6 0.167
0 0.0
0 0.0
0.4551487667293441
0.4545801526717557
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6407,	 Acc = 0.4050,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.8264
380 0.187
1718 0.467
815 0.4
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.4366412213740458
0.4545801526717557

 ===== Epoch 117	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   2.0210998   2.912658
 -0.3640846  -0.3725409   1.0461746   1.6302992  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  2.0759788   2.9684365
 -0.3640846  -0.3725409   1.0839361   1.6750922  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
train:	 Loss = 1.6069,	 Acc = 0.3851,	 Loss Con1 = 1.3519,	 Loss Con2 = 2.1069
2924 0.167
5684 0.436
2909 0.497
376 0.418
69 0.536
6 0.333
0 0.0
0 0.0
0.45566121185316233
0.4545801526717557
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6615,	 Acc = 0.4040,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.0113
380 0.179
1718 0.463
815 0.406
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.4366412213740458
0.4545801526717557

 ===== Epoch 118	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 0
train:	 Loss = 1.6077,	 Acc = 0.3869,	 Loss Con1 = 1.3514,	 Loss Con2 = 2.1801
2928 0.161
5678 0.441
2911 0.498
376 0.428
69 0.594
6 0.167
0 0.0
0 0.0
0.46017699115044247
0.4545801526717557
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.6633,	 Acc = 0.3907,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.8977
380 0.189
1718 0.446
815 0.385
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.4198473282442748
0.4545801526717557

 ===== Epoch 119	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 0
train:	 Loss = 1.6092,	 Acc = 0.3866,	 Loss Con1 = 1.3518,	 Loss Con2 = 2.1019
2927 0.161
5680 0.443
2913 0.493
373 0.442
69 0.536
6 0.167
0 0.0
0 0.0
0.45979427054529365
0.4545801526717557
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6223,	 Acc = 0.4140,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.7692
380 0.189
1718 0.472
815 0.417
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.44656488549618323
0.4545801526717557

 ===== Epoch 120	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  1.6378613   2.9759147  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  1.6249423   2.8224437  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 5
train:	 Loss = 1.6081,	 Acc = 0.3835,	 Loss Con1 = 1.3528,	 Loss Con2 = 2.4674
2919 0.161
5686 0.44
2911 0.49
377 0.419
69 0.493
6 0.333
0 0.0
0 0.0
0.455298928058349
0.4545801526717557
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6526,	 Acc = 0.4157,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.1684
380 0.184
1718 0.474
815 0.423
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.449236641221374
0.4545801526717557

 ===== Epoch 121	 =====
[ 2.4000733   1.7176619  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.876646    1.9033213 ] [ 3.8633132   1.7353816  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.6861813   1.9295647 ] 3 3
train:	 Loss = 1.6081,	 Acc = 0.3871,	 Loss Con1 = 1.3506,	 Loss Con2 = 2.5274
2926 0.16
5680 0.443
2913 0.498
376 0.428
67 0.478
6 0.5
0 0.0
0 0.0
0.4606281796062818
0.4545801526717557
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6633,	 Acc = 0.3913,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.0912
380 0.189
1718 0.456
815 0.371
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.4206106870229008
0.4545801526717557

 ===== Epoch 122	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 0
train:	 Loss = 1.6073,	 Acc = 0.3840,	 Loss Con1 = 1.3515,	 Loss Con2 = 2.5507
2925 0.161
5679 0.437
2913 0.499
376 0.418
69 0.493
6 0.333
0 0.0
0 0.0
0.45626451398872053
0.4545801526717557
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6537,	 Acc = 0.4030,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.1222
380 0.189
1718 0.46
815 0.401
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.43396946564885497
0.4545801526717557

 ===== Epoch 123	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   1.4277965   2.3743615  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   1.3602839   2.4291086  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 1
train:	 Loss = 1.6089,	 Acc = 0.3861,	 Loss Con1 = 1.3520,	 Loss Con2 = 2.4850
2924 0.162
5681 0.444
2912 0.492
376 0.41
69 0.536
6 0.167
0 0.0
0 0.0
0.4584254754533392
0.4545801526717557
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6622,	 Acc = 0.3823,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.9898
380 0.192
1718 0.442
815 0.368
85 0.176
2 0.0
0 0.0
0 0.0
0 0.0
0.40992366412213743
0.4545801526717557

 ===== Epoch 124	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 0
train:	 Loss = 1.6058,	 Acc = 0.3888,	 Loss Con1 = 1.3518,	 Loss Con2 = 2.3444
2923 0.159
5680 0.446
2915 0.499
375 0.424
69 0.565
6 0.167
0 0.0
0 0.0
0.46301824212271975
0.4545801526717557
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6317,	 Acc = 0.3987,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.7916
380 0.187
1718 0.459
815 0.391
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.42938931297709926
0.4545801526717557

 ===== Epoch 125	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 3 0
train:	 Loss = 1.6097,	 Acc = 0.3864,	 Loss Con1 = 1.3522,	 Loss Con2 = 2.0418
2926 0.164
5676 0.439
2915 0.499
376 0.42
69 0.536
6 0.333
0 0.0
0 0.0
0.4584162795841628
0.4545801526717557
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6532,	 Acc = 0.3987,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.9493
380 0.192
1718 0.458
815 0.395
85 0.165
2 0.0
0 0.0
0 0.0
0 0.0
0.42862595419847327
0.4545801526717557

 ===== Epoch 126	 =====
[-0.36602148 -0.3783333   0.13722287  1.5664642  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   0.11152866  1.5252405  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 3
train:	 Loss = 1.6044,	 Acc = 0.3872,	 Loss Con1 = 1.3520,	 Loss Con2 = 2.2209
2922 0.161
5682 0.443
2913 0.498
377 0.419
68 0.588
6 0.167
0 0.0
0 0.0
0.46031395091753263
0.4545801526717557
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6490,	 Acc = 0.4057,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.9198
380 0.176
1718 0.468
815 0.404
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.4389312977099237
0.4545801526717557

 ===== Epoch 127	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 0
train:	 Loss = 1.6053,	 Acc = 0.3860,	 Loss Con1 = 1.3518,	 Loss Con2 = 2.1761
2925 0.16
5678 0.44
2916 0.499
374 0.422
69 0.522
6 0.5
0 0.0
0 0.0
0.459139666040031
0.4545801526717557
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6651,	 Acc = 0.3963,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.9757
380 0.184
1718 0.468
815 0.367
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.42709923664122135
0.4545801526717557

 ===== Epoch 128	 =====
[ 4.011371    2.6745293  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  1.2806213   2.8140726  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  1.2474805   2.987076   -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
train:	 Loss = 1.6117,	 Acc = 0.3837,	 Loss Con1 = 1.3531,	 Loss Con2 = 2.2669
2920 0.16
5684 0.437
2912 0.493
377 0.424
69 0.565
6 0.5
0 0.0
0 0.0
0.45590185676392575
0.4545801526717557
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6619,	 Acc = 0.3947,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.9098
380 0.184
1718 0.455
815 0.389
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.4251908396946565
0.4545801526717557

 ===== Epoch 129	 =====
[-0.36602148 -0.3783333   1.0957439   1.527954   -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.134593    1.5524195  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 0
train:	 Loss = 1.6059,	 Acc = 0.3863,	 Loss Con1 = 1.3523,	 Loss Con2 = 2.3198
2928 0.16
5680 0.441
2908 0.499
377 0.438
69 0.42
6 0.5
0 0.0
0 0.0
0.4595132743362832
0.4545801526717557
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6498,	 Acc = 0.4163,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.9762
380 0.189
1718 0.471
815 0.421
85 0.294
2 0.0
0 0.0
0 0.0
0 0.0
0.449236641221374
0.4545801526717557

 ===== Epoch 130	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 0
train:	 Loss = 1.6057,	 Acc = 0.3867,	 Loss Con1 = 1.3529,	 Loss Con2 = 2.3085
2922 0.163
5681 0.441
2913 0.498
377 0.416
69 0.522
6 0.167
0 0.0
0 0.0
0.4588768516471369
0.4545801526717557
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6442,	 Acc = 0.3903,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.8173
380 0.192
1718 0.459
815 0.362
85 0.165
2 0.0
0 0.0
0 0.0
0 0.0
0.41908396946564885
0.4545801526717557

 ===== Epoch 131	 =====
[-0.36602148 -0.3783333   2.2305367   2.0965476  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   2.2303562   2.0959983  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.6084,	 Acc = 0.3877,	 Loss Con1 = 1.3520,	 Loss Con2 = 2.2008
2927 0.162
5679 0.446
2911 0.493
376 0.423
69 0.507
6 0.167
0 0.0
0 0.0
0.46078973564871145
0.4545801526717557
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6663,	 Acc = 0.3970,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.0130
380 0.189
1718 0.464
815 0.374
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.42709923664122135
0.4545801526717557

 ===== Epoch 132	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   2.63566     2.9660015
 -0.3640846  -0.3725409   1.8117089   2.3146374  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  2.8954394   2.9662137
 -0.3640846  -0.3725409   1.9284266   2.3146374  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 2
train:	 Loss = 1.6077,	 Acc = 0.3849,	 Loss Con1 = 1.3517,	 Loss Con2 = 2.4097
2924 0.162
5679 0.44
2914 0.493
377 0.424
68 0.485
6 0.333
0 0.0
0 0.0
0.4567669172932331
0.4545801526717557
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6726,	 Acc = 0.4047,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.1931
380 0.192
1718 0.465
815 0.398
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.4354961832061069
0.4545801526717557

 ===== Epoch 133	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   2.4536576   1.4113109  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   2.3975885   1.4785005  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 1
train:	 Loss = 1.6073,	 Acc = 0.3859,	 Loss Con1 = 1.3527,	 Loss Con2 = 2.1981
2925 0.16
5675 0.436
2918 0.506
375 0.448
69 0.478
6 0.5
0 0.0
0 0.0
0.459139666040031
0.4545801526717557
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6738,	 Acc = 0.4137,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.1959
380 0.184
1718 0.483
815 0.4
85 0.176
2 0.0
0 0.0
0 0.0
0 0.0
0.44694656488549617
0.4545801526717557

 ===== Epoch 134	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.0945039   2.7483606 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.0548232   2.737863  ] 3 1
train:	 Loss = 1.6067,	 Acc = 0.3881,	 Loss Con1 = 1.3519,	 Loss Con2 = 2.4477
2924 0.169
5681 0.441
2914 0.496
375 0.437
68 0.544
6 0.333
0 0.0
0 0.0
0.45897832817337464
0.4545801526717557
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6483,	 Acc = 0.3953,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.8553
380 0.189
1718 0.458
815 0.385
85 0.165
2 0.0
0 0.0
0 0.0
0 0.0
0.4251908396946565
0.4545801526717557

 ===== Epoch 135	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.6062,	 Acc = 0.3921,	 Loss Con1 = 1.3513,	 Loss Con2 = 2.2035
2925 0.163
5681 0.448
2911 0.503
376 0.441
69 0.551
6 0.333
0 0.0
0 0.0
0.4662169633971027
0.4545801526717557
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6300,	 Acc = 0.4150,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.0054
380 0.192
1718 0.471
815 0.421
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.44732824427480916
0.4545801526717557

 ===== Epoch 136	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.2360299   1.1291393 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.2170715   1.2052454 ] 1 1
train:	 Loss = 1.6047,	 Acc = 0.3872,	 Loss Con1 = 1.3519,	 Loss Con2 = 2.2963
2923 0.161
5682 0.442
2911 0.499
377 0.427
69 0.522
6 0.333
0 0.0
0 0.0
0.4603648424543947
0.4545801526717557
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6342,	 Acc = 0.4030,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.8295
380 0.187
1718 0.464
815 0.396
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.43435114503816796
0.4545801526717557

 ===== Epoch 137	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.7186716   1.590946
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.7186716   1.5884813
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.6070,	 Acc = 0.3866,	 Loss Con1 = 1.3521,	 Loss Con2 = 2.2540
2926 0.161
5679 0.441
2913 0.498
376 0.418
68 0.559
6 0.5
0 0.0
0 0.0
0.4595222295952223
0.4545801526717557
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6981,	 Acc = 0.3770,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.1503
380 0.189
1718 0.445
815 0.341
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.40419847328244274
0.4545801526717557

 ===== Epoch 138	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 3 0
train:	 Loss = 1.6092,	 Acc = 0.3831,	 Loss Con1 = 1.3513,	 Loss Con2 = 2.3630
2923 0.161
5685 0.439
2910 0.49
376 0.418
68 0.5
6 0.167
0 0.0
0 0.0
0.454726368159204
0.4545801526717557
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6639,	 Acc = 0.3943,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.9371
380 0.192
1718 0.459
815 0.379
85 0.153
2 0.0
0 0.0
0 0.0
0 0.0
0.42366412213740456
0.4545801526717557

 ===== Epoch 139	 =====
[-0.36602148 -0.3783333   2.741478    2.273242   -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   2.7412794   2.4198806  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.6079,	 Acc = 0.3812,	 Loss Con1 = 1.3517,	 Loss Con2 = 2.1320
2928 0.163
5674 0.434
2916 0.49
376 0.415
68 0.529
6 0.333
0 0.0
0 0.0
0.4519911504424779
0.4545801526717557
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6663,	 Acc = 0.3787,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.8684
380 0.189
1718 0.436
815 0.368
85 0.176
2 0.0
0 0.0
0 0.0
0 0.0
0.40610687022900765
0.4545801526717557

 ===== Epoch 140	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   1.921793    2.1747403
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  1.8987163   2.1927042
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 0
train:	 Loss = 1.6078,	 Acc = 0.3852,	 Loss Con1 = 1.3516,	 Loss Con2 = 2.2910
2926 0.16
5676 0.439
2915 0.498
376 0.428
69 0.493
6 0.5
0 0.0
0 0.0
0.457973899579739
0.4545801526717557
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6615,	 Acc = 0.4147,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.0690
380 0.192
1718 0.476
815 0.413
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.44694656488549617
0.4545801526717557

 ===== Epoch 141	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 3 0
train:	 Loss = 1.6071,	 Acc = 0.3833,	 Loss Con1 = 1.3523,	 Loss Con2 = 2.2292
2924 0.162
5679 0.439
2915 0.492
375 0.397
69 0.522
6 0.167
0 0.0
0 0.0
0.4548872180451128
0.4545801526717557
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6329,	 Acc = 0.4000,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.7553
380 0.187
1718 0.462
815 0.391
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.4309160305343511
0.4545801526717557

 ===== Epoch 142	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.6677825   1.9286004
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.9867626   1.8595908
 -0.38555372 -0.3798593 ] 2 2
train:	 Loss = 1.6053,	 Acc = 0.3876,	 Loss Con1 = 1.3515,	 Loss Con2 = 2.1202
2927 0.161
5679 0.441
2911 0.506
376 0.412
69 0.536
6 0.167
0 0.0
0 0.0
0.46112155734985066
0.4545801526717557
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6593,	 Acc = 0.3970,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.9552
380 0.192
1718 0.454
815 0.396
85 0.176
2 0.0
0 0.0
0 0.0
0 0.0
0.4267175572519084
0.4545801526717557

 ===== Epoch 143	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   2.9016497   1.2943513  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   3.1133447   1.3167477  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 0
train:	 Loss = 1.6059,	 Acc = 0.3855,	 Loss Con1 = 1.3516,	 Loss Con2 = 2.1641
2925 0.161
5682 0.438
2910 0.505
376 0.396
69 0.493
6 0.333
0 0.0
0 0.0
0.45814442109919273
0.4545801526717557
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6342,	 Acc = 0.4037,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.8132
380 0.189
1718 0.464
815 0.4
85 0.176
2 0.0
0 0.0
0 0.0
0 0.0
0.4347328244274809
0.4545801526717557

 ===== Epoch 144	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 3 0
train:	 Loss = 1.6061,	 Acc = 0.3838,	 Loss Con1 = 1.3507,	 Loss Con2 = 2.1771
2927 0.161
5684 0.433
2908 0.5
374 0.43
69 0.594
6 0.333
0 0.0
0 0.0
0.4559230173653357
0.4545801526717557
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6397,	 Acc = 0.4063,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.0128
380 0.189
1718 0.468
815 0.396
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.43778625954198475
0.4545801526717557

 ===== Epoch 145	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.5211736   2.044438
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.5211736   2.044438
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.6065,	 Acc = 0.3872,	 Loss Con1 = 1.3508,	 Loss Con2 = 2.1966
2926 0.161
5681 0.444
2912 0.495
374 0.433
69 0.522
6 0.5
0 0.0
0 0.0
0.4604069896040699
0.4545801526717557
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6855,	 Acc = 0.3977,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.0167
380 0.182
1718 0.467
815 0.374
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.42900763358778626
0.4545801526717557

 ===== Epoch 146	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.9780483   0.8564861
  2.7117527   3.3152192 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.2258067   0.9600006
 -0.38555372 -0.3798593 ] 2 6
train:	 Loss = 1.6097,	 Acc = 0.3819,	 Loss Con1 = 1.3529,	 Loss Con2 = 2.2316
2920 0.162
5681 0.433
2915 0.493
377 0.411
69 0.594
6 0.5
0 0.0
0 0.0
0.45291777188328913
0.4545801526717557
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6450,	 Acc = 0.4027,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.8753
380 0.189
1718 0.46
815 0.4
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.433587786259542
0.4545801526717557

 ===== Epoch 147	 =====
[-0.36602148 -0.3783333   2.7244198   2.9211216   2.6868415   1.05675
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   2.5544548   2.9158967   2.9252381   1.0546671
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 2
train:	 Loss = 1.6064,	 Acc = 0.3832,	 Loss Con1 = 1.3518,	 Loss Con2 = 2.1925
2921 0.158
5685 0.439
2911 0.494
377 0.401
68 0.544
6 0.5
0 0.0
0 0.0
0.4559522493644302
0.4545801526717557
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6312,	 Acc = 0.4150,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.8284
380 0.187
1718 0.479
815 0.412
85 0.176
2 0.0
0 0.0
0 0.0
0 0.0
0.4480916030534351
0.4545801526717557

 ===== Epoch 148	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   3.137947    3.1657054  -0.42342317 -0.42019257
  3.6468842   2.0712793 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   3.1877236   3.282665   -0.42342317 -0.42019257
  3.0984159   2.220867  ] 6 2
train:	 Loss = 1.6035,	 Acc = 0.3858,	 Loss Con1 = 1.3509,	 Loss Con2 = 2.2226
2927 0.16
5683 0.439
2908 0.5
375 0.429
69 0.536
6 0.333
0 0.0
0 0.0
0.4590200199093021
0.4545801526717557
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6240,	 Acc = 0.4080,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.7285
380 0.184
1718 0.476
815 0.393
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.4404580152671756
0.4545801526717557

 ===== Epoch 149	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   1.9198837   2.2592008
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  1.7543077   2.0437813
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 5
train:	 Loss = 1.6052,	 Acc = 0.3859,	 Loss Con1 = 1.3518,	 Loss Con2 = 2.1285
2922 0.159
5685 0.441
2912 0.5
374 0.42
69 0.493
6 0.333
0 0.0
0 0.0
0.45909794384258235
0.4545801526717557
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6392,	 Acc = 0.4087,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.8408
380 0.184
1718 0.471
815 0.409
85 0.165
2 0.0
0 0.0
0 0.0
0 0.0
0.44122137404580153
0.4545801526717557

 ===== Epoch 150	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.1862829   1.1522417
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.121123    1.2804025
 -0.38555372 -0.3798593 ] 4 6
train:	 Loss = 1.6068,	 Acc = 0.3858,	 Loss Con1 = 1.3520,	 Loss Con2 = 2.3046
2929 0.16
5676 0.438
2914 0.504
374 0.42
69 0.522
6 0.333
0 0.0
0 0.0
0.4589003208319504
0.4545801526717557
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6723,	 Acc = 0.4173,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.2584
380 0.195
1718 0.477
815 0.422
85 0.176
2 0.0
0 0.0
0 0.0
0 0.0
0.449618320610687
0.4545801526717557

 ===== Epoch 151	 =====
[-0.36602148 -0.3783333   2.7353861   1.5528723  -0.4409929  -0.3635196
  2.6084757   3.464231   -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   2.6848254   1.7607914  -0.44088638 -0.36343393
  2.7039652   3.6930423  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.6053,	 Acc = 0.3871,	 Loss Con1 = 1.3525,	 Loss Con2 = 2.3390
2922 0.16
5683 0.44
2911 0.505
377 0.435
69 0.478
6 0.167
0 0.0
0 0.0
0.4605350431129781
0.4545801526717557
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6474,	 Acc = 0.4153,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.1028
380 0.189
1718 0.468
815 0.434
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.4480916030534351
0.4545801526717557

 ===== Epoch 152	 =====
[ 1.6441531   2.6618724  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.4834275   2.942562  ] [ 1.6777627   2.889698   -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.5275172   3.1525095 ] 6 6
train:	 Loss = 1.6051,	 Acc = 0.3881,	 Loss Con1 = 1.3515,	 Loss Con2 = 2.4098
2924 0.16
5683 0.444
2913 0.499
373 0.434
69 0.507
6 0.5
0 0.0
0 0.0
0.46185316231755863
0.4545801526717557
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6351,	 Acc = 0.4230,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.8956
380 0.189
1718 0.475
815 0.439
85 0.271
2 0.0
0 0.0
0 0.0
0 0.0
0.4568702290076336
0.4568702290076336

 ===== Epoch 153	 =====
[-0.36602148 -0.3783333   1.0624404   1.4645252  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.1016963   1.4572933  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 3 0
train:	 Loss = 1.6088,	 Acc = 0.3854,	 Loss Con1 = 1.3526,	 Loss Con2 = 2.1529
2924 0.159
5678 0.441
2914 0.5
377 0.411
69 0.449
6 0.333
0 0.0
0 0.0
0.4585360459973463
0.4568702290076336
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6473,	 Acc = 0.3983,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.8726
380 0.192
1718 0.46
815 0.393
85 0.141
2 0.0
0 0.0
0 0.0
0 0.0
0.42824427480916033
0.4568702290076336

 ===== Epoch 154	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 0
train:	 Loss = 1.6058,	 Acc = 0.3845,	 Loss Con1 = 1.3517,	 Loss Con2 = 2.2316
2925 0.16
5680 0.437
2912 0.499
376 0.436
69 0.522
6 0.167
0 0.0
0 0.0
0.45714917615835454
0.4568702290076336
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6623,	 Acc = 0.3987,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.0957
380 0.189
1718 0.467
815 0.373
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.42900763358778626
0.4568702290076336

 ===== Epoch 155	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 3 0
train:	 Loss = 1.6072,	 Acc = 0.3830,	 Loss Con1 = 1.3526,	 Loss Con2 = 2.1953
2924 0.164
5680 0.432
2913 0.499
376 0.423
69 0.507
6 0.333
0 0.0
0 0.0
0.453781512605042
0.4568702290076336
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6179,	 Acc = 0.4203,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.8214
380 0.189
1718 0.476
815 0.436
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.4538167938931298
0.4568702290076336

 ===== Epoch 156	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.0288026   2.3034332  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 4.2535176   1.6974107  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  1.8209739   2.186237   -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 6
train:	 Loss = 1.6083,	 Acc = 0.3853,	 Loss Con1 = 1.3507,	 Loss Con2 = 2.1271
2926 0.161
5681 0.439
2910 0.5
377 0.414
68 0.485
6 0.5
0 0.0
0 0.0
0.457973899579739
0.4568702290076336
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6473,	 Acc = 0.4033,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.8962
380 0.187
1718 0.464
815 0.398
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.4347328244274809
0.4568702290076336

 ===== Epoch 157	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 0
train:	 Loss = 1.6034,	 Acc = 0.3875,	 Loss Con1 = 1.3517,	 Loss Con2 = 2.2734
2926 0.159
5678 0.443
2912 0.503
377 0.398
69 0.594
6 0.333
0 0.0
0 0.0
0.46140234461402346
0.4568702290076336
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6410,	 Acc = 0.4150,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.0964
380 0.192
1718 0.469
815 0.423
85 0.259
2 0.0
0 0.0
0 0.0
0 0.0
0.44732824427480916
0.4568702290076336

 ===== Epoch 158	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 0
train:	 Loss = 1.6028,	 Acc = 0.3858,	 Loss Con1 = 1.3533,	 Loss Con2 = 2.3404
2917 0.161
5683 0.441
2916 0.493
377 0.432
69 0.522
6 0.333
0 0.0
0 0.0
0.4582919014473539
0.4568702290076336
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6586,	 Acc = 0.4130,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.8986
380 0.189
1718 0.48
815 0.4
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.4454198473282443
0.4568702290076336

 ===== Epoch 159	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.1602404   3.559104   -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.2360704   3.3944716  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 5
train:	 Loss = 1.6040,	 Acc = 0.3864,	 Loss Con1 = 1.3519,	 Loss Con2 = 2.1348
2924 0.161
5680 0.439
2914 0.504
376 0.407
68 0.544
6 0.5
0 0.0
0 0.0
0.4594206103494029
0.4568702290076336
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6348,	 Acc = 0.3973,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.8049
380 0.189
1718 0.458
815 0.384
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.42748091603053434
0.4568702290076336

 ===== Epoch 160	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   0.16554558  2.360993   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 6
train:	 Loss = 1.6072,	 Acc = 0.3825,	 Loss Con1 = 1.3523,	 Loss Con2 = 2.2190
2921 0.159
5685 0.439
2910 0.491
377 0.406
69 0.507
6 0.333
0 0.0
0 0.0
0.4546258428208246
0.4568702290076336
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6525,	 Acc = 0.4080,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.0292
380 0.187
1718 0.469
815 0.405
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.4400763358778626
0.4568702290076336

 ===== Epoch 161	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   3.5011618   2.1814082
 -0.3640846  -0.3725409   2.9342628   1.8044944  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  3.2339206   2.2616088
 -0.3640846  -0.3725409   2.7929423   1.8642184  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 3 6
train:	 Loss = 1.6068,	 Acc = 0.3820,	 Loss Con1 = 1.3505,	 Loss Con2 = 2.2470
2927 0.161
5681 0.432
2909 0.501
376 0.418
69 0.478
6 0.167
0 0.0
0 0.0
0.45360026545736093
0.4568702290076336
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6514,	 Acc = 0.3923,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.8870
380 0.189
1718 0.451
815 0.385
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.4217557251908397
0.4568702290076336

 ===== Epoch 162	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   1.328244    2.9964874  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   1.33511     2.9865332  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.6066,	 Acc = 0.3839,	 Loss Con1 = 1.3521,	 Loss Con2 = 2.2797
2924 0.16
5677 0.437
2918 0.499
375 0.405
69 0.507
5 0.4
0 0.0
0 0.0
0.4562140645731977
0.4568702290076336
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6517,	 Acc = 0.4067,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.0775
380 0.187
1718 0.462
815 0.422
85 0.141
2 0.0
0 0.0
0 0.0
0 0.0
0.4385496183206107
0.4568702290076336

 ===== Epoch 163	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.060259    2.551777   -0.40141198 -0.40778467  2.7819371   2.5595458
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.1636107   2.5406156  -0.40141198 -0.40778467  2.8277762   2.584192
 -0.38555372 -0.3798593 ] 3 0
train:	 Loss = 1.6063,	 Acc = 0.3854,	 Loss Con1 = 1.3530,	 Loss Con2 = 2.4507
2921 0.161
5681 0.439
2914 0.496
377 0.432
69 0.551
6 0.333
0 0.0
0 0.0
0.4578313253012048
0.4568702290076336
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6595,	 Acc = 0.4117,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.0471
380 0.184
1718 0.481
815 0.401
85 0.141
2 0.0
0 0.0
0 0.0
0 0.0
0.4446564885496183
0.4568702290076336

 ===== Epoch 164	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.1305935   2.8577662
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.1347616   3.1362693
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.6018,	 Acc = 0.3908,	 Loss Con1 = 1.3521,	 Loss Con2 = 2.4078
2924 0.162
5682 0.449
2910 0.498
377 0.424
69 0.594
6 0.333
0 0.0
0 0.0
0.46483856700574966
0.4568702290076336
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6570,	 Acc = 0.4123,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.1031
380 0.189
1718 0.473
815 0.412
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.4446564885496183
0.4568702290076336

 ===== Epoch 165	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.6005,	 Acc = 0.3865,	 Loss Con1 = 1.3522,	 Loss Con2 = 2.2620
2926 0.159
5678 0.442
2913 0.497
376 0.434
69 0.551
6 0.167
0 0.0
0 0.0
0.460185799601858
0.4568702290076336
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6924,	 Acc = 0.3810,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.2792
380 0.187
1718 0.446
815 0.353
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.40916030534351144
0.4568702290076336

 ===== Epoch 166	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.9185879   0.9244403 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 5
train:	 Loss = 1.6057,	 Acc = 0.3879,	 Loss Con1 = 1.3517,	 Loss Con2 = 2.3170
2925 0.162
5679 0.443
2916 0.498
374 0.428
68 0.544
6 0.333
0 0.0
0 0.0
0.4609089903792989
0.4568702290076336
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6453,	 Acc = 0.4053,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.0152
380 0.182
1718 0.462
815 0.413
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.43778625954198475
0.4568702290076336

 ===== Epoch 167	 =====
[-0.36602148 -0.3783333   3.4242225   3.0547748   2.2697506   1.3857014
 -0.3640846  -0.3725409   0.9569193   1.5929717  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  2.3086374   1.4436446
 -0.3640846  -0.3725409   0.8785344   1.7472588  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
train:	 Loss = 1.6096,	 Acc = 0.3856,	 Loss Con1 = 1.3519,	 Loss Con2 = 2.1732
2923 0.165
5685 0.439
2912 0.495
373 0.426
69 0.522
6 0.167
0 0.0
0 0.0
0.4568269762299613
0.4568702290076336
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6820,	 Acc = 0.3870,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.1329
380 0.187
1718 0.455
815 0.362
85 0.165
2 0.0
0 0.0
0 0.0
0 0.0
0.41603053435114506
0.4568702290076336

 ===== Epoch 168	 =====
[ 2.5931208   3.3250978  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.1068492   3.5697806 ] [ 2.495833    3.2390301  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.2210402   3.5304155 ] 5 5
train:	 Loss = 1.6059,	 Acc = 0.3854,	 Loss Con1 = 1.3522,	 Loss Con2 = 2.1908
2924 0.16
5683 0.439
2909 0.499
377 0.435
69 0.478
6 0.333
0 0.0
0 0.0
0.458093763821318
0.4568702290076336
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6582,	 Acc = 0.4063,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.0065
380 0.187
1718 0.475
815 0.388
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.4381679389312977
0.4568702290076336

 ===== Epoch 169	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   2.4342055   3.312527    3.1895645   1.1399186
  3.6296904   2.9399376 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   2.5183105   3.2851534   3.24639     1.1275954
  3.6759844   2.9241917 ] 2 2
train:	 Loss = 1.6043,	 Acc = 0.3867,	 Loss Con1 = 1.3523,	 Loss Con2 = 2.2662
2920 0.161
5684 0.441
2913 0.501
376 0.407
69 0.522
6 0.333
0 0.0
0 0.0
0.45965959328028294
0.4568702290076336
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6549,	 Acc = 0.4103,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.9454
380 0.187
1718 0.471
815 0.413
85 0.153
2 0.0
0 0.0
0 0.0
0 0.0
0.44274809160305345
0.4568702290076336

 ===== Epoch 170	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 0
train:	 Loss = 1.6036,	 Acc = 0.3867,	 Loss Con1 = 1.3530,	 Loss Con2 = 2.2247
2920 0.159
5681 0.441
2915 0.501
377 0.419
69 0.58
6 0.167
0 0.0
0 0.0
0.46021220159151194
0.4568702290076336
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6486,	 Acc = 0.4033,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.0205
380 0.187
1718 0.462
815 0.409
85 0.141
2 0.0
0 0.0
0 0.0
0 0.0
0.4347328244274809
0.4568702290076336

 ===== Epoch 171	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   1.4089158   1.1525066  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   1.5347884   1.2371157  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 2
train:	 Loss = 1.6045,	 Acc = 0.3877,	 Loss Con1 = 1.3516,	 Loss Con2 = 2.2902
2924 0.16
5679 0.441
2914 0.501
376 0.431
69 0.594
6 0.167
0 0.0
0 0.0
0.4613003095975232
0.4568702290076336
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6648,	 Acc = 0.3790,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.9669
380 0.187
1718 0.437
815 0.366
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.4068702290076336
0.4568702290076336

 ===== Epoch 172	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   2.8946233   2.9660015
 -0.3640846  -0.3725409   1.9284266   2.3146374  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  2.8954394   2.9662137
 -0.3640846  -0.3725409   1.9284266   2.3146374  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.6016,	 Acc = 0.3873,	 Loss Con1 = 1.3516,	 Loss Con2 = 2.3164
2927 0.161
5679 0.443
2911 0.499
376 0.42
69 0.551
6 0.167
0 0.0
0 0.0
0.46067912841499836
0.4568702290076336
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6648,	 Acc = 0.4160,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.2032
380 0.187
1718 0.483
815 0.409
85 0.176
2 0.0
0 0.0
0 0.0
0 0.0
0.449236641221374
0.4568702290076336

 ===== Epoch 173	 =====
[-0.36602148 -0.3783333   0.60226876  1.8654855  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   0.5558447   2.0031369  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.6067,	 Acc = 0.3888,	 Loss Con1 = 1.3519,	 Loss Con2 = 2.3188
2927 0.16
5681 0.447
2909 0.498
376 0.42
69 0.551
6 0.333
0 0.0
0 0.0
0.4630018803229731
0.4568702290076336
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6616,	 Acc = 0.4090,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.1117
380 0.189
1718 0.471
815 0.401
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.44083969465648853
0.4568702290076336

 ===== Epoch 174	 =====
[ 2.4001565   1.7303188  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.6650176   1.924316  ] [ 2.3837566   1.7201933  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.8312347   1.8954482 ] 3 3
train:	 Loss = 1.6045,	 Acc = 0.3878,	 Loss Con1 = 1.3520,	 Loss Con2 = 2.2322
2924 0.161
5683 0.442
2911 0.502
375 0.416
69 0.565
6 0.167
0 0.0
0 0.0
0.4610791685095091
0.4568702290076336
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6486,	 Acc = 0.4133,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.9997
380 0.195
1718 0.48
815 0.391
85 0.259
2 0.0
0 0.0
0 0.0
0 0.0
0.4450381679389313
0.4568702290076336

 ===== Epoch 175	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 1.8529845   1.3911119  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 6
train:	 Loss = 1.6020,	 Acc = 0.3862,	 Loss Con1 = 1.3524,	 Loss Con2 = 2.2481
2924 0.163
5680 0.44
2912 0.495
377 0.435
69 0.536
6 0.333
0 0.0
0 0.0
0.45831490490933213
0.4568702290076336
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6499,	 Acc = 0.4150,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.9302
380 0.187
1718 0.478
815 0.412
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.4480916030534351
0.4568702290076336

 ===== Epoch 176	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.4144676   1.837409
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.412952    1.6648849
 -0.38555372 -0.3798593 ] 5 5
train:	 Loss = 1.6070,	 Acc = 0.3849,	 Loss Con1 = 1.3525,	 Loss Con2 = 2.2494
2921 0.16
5679 0.44
2916 0.497
377 0.419
69 0.507
6 0.167
0 0.0
0 0.0
0.4574997236653034
0.4568702290076336
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6691,	 Acc = 0.3823,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.0684
380 0.187
1718 0.452
815 0.347
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.41068702290076337
0.4568702290076336

 ===== Epoch 177	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.2267716   2.6013973 ] [ 2.2405376   2.3555734  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.2267716   2.5987728 ] 0 6
train:	 Loss = 1.6050,	 Acc = 0.3906,	 Loss Con1 = 1.3523,	 Loss Con2 = 3.1424
2925 0.16
5678 0.449
2915 0.502
375 0.424
69 0.449
6 0.167
0 0.0
0 0.0
0.4651111356850603
0.4568702290076336
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6748,	 Acc = 0.3893,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.4414
380 0.189
1718 0.451
815 0.379
85 0.141
2 0.0
0 0.0
0 0.0
0 0.0
0.4183206106870229
0.4568702290076336

 ===== Epoch 178	 =====
[-0.36602148 -0.3783333   1.7833624   2.8169172  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.6613556   2.8094456  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.6038,	 Acc = 0.3894,	 Loss Con1 = 1.3525,	 Loss Con2 = 2.7942
2927 0.161
5675 0.444
2914 0.505
377 0.43
69 0.478
6 0.167
0 0.0
0 0.0
0.4632230947903993
0.4568702290076336
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6645,	 Acc = 0.3887,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.2537
380 0.189
1718 0.453
815 0.371
85 0.165
2 0.0
0 0.0
0 0.0
0 0.0
0.41755725190839693
0.4568702290076336

 ===== Epoch 179	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.365652    3.1078959 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.4128277   3.126266  ] 2 0
train:	 Loss = 1.6070,	 Acc = 0.3875,	 Loss Con1 = 1.3523,	 Loss Con2 = 2.7333
2925 0.161
5680 0.443
2912 0.499
376 0.42
69 0.551
6 0.333
0 0.0
0 0.0
0.4606878248368904
0.4568702290076336
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6497,	 Acc = 0.4163,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.2379
380 0.189
1718 0.477
815 0.416
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.449236641221374
0.4568702290076336

 ===== Epoch 180	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.9752657   1.8625531  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.9348235   1.9434742  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 4
train:	 Loss = 1.6054,	 Acc = 0.3871,	 Loss Con1 = 1.3510,	 Loss Con2 = 2.5884
2925 0.16
5684 0.442
2908 0.502
376 0.404
69 0.522
6 0.333
0 0.0
0 0.0
0.46046665929448194
0.4568702290076336
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6757,	 Acc = 0.3960,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.3434
380 0.192
1718 0.458
815 0.385
85 0.165
2 0.0
0 0.0
0 0.0
0 0.0
0.4255725190839695
0.4568702290076336

 ===== Epoch 181	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   2.258674    2.0236003
 -0.3640846  -0.3725409   0.9895322   2.837223   -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  2.2585907   2.030445
 -0.3640846  -0.3725409   1.008413    2.8745506  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 1
train:	 Loss = 1.6051,	 Acc = 0.3895,	 Loss Con1 = 1.3516,	 Loss Con2 = 2.7434
2929 0.16
5679 0.444
2909 0.503
376 0.444
69 0.551
6 0.0
0 0.0
0 0.0
0.46365748423498176
0.4568702290076336
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6412,	 Acc = 0.4063,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.0427
380 0.189
1718 0.463
815 0.413
85 0.165
2 0.0
0 0.0
0 0.0
0 0.0
0.43778625954198475
0.4568702290076336

 ===== Epoch 182	 =====
[ 1.4396603   1.0443108  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.6183407   1.270854  ] [ 1.3537037   1.0341852  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.5103225   1.270854  ] 1 0
train:	 Loss = 1.6048,	 Acc = 0.3887,	 Loss Con1 = 1.3526,	 Loss Con2 = 2.3622
2925 0.16
5677 0.443
2915 0.502
376 0.444
69 0.522
6 0.167
0 0.0
0 0.0
0.46267831471856685
0.4568702290076336
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6620,	 Acc = 0.3993,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.9449
380 0.189
1718 0.466
815 0.378
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.4297709923664122
0.4568702290076336

 ===== Epoch 183	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 0.62092185  2.0821826  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.6038,	 Acc = 0.3904,	 Loss Con1 = 1.3524,	 Loss Con2 = 2.2494
2920 0.159
5684 0.446
2912 0.504
377 0.43
69 0.551
6 0.167
0 0.0
0 0.0
0.46496463306808133
0.4568702290076336
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6480,	 Acc = 0.4047,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.1130
380 0.187
1718 0.467
815 0.394
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.4362595419847328
0.4568702290076336

 ===== Epoch 184	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  3.0482862   1.2235563  -0.40141198 -0.40778467  3.395272    1.8029042
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  3.140966    1.3268002  -0.40141198 -0.40778467  3.426337    1.8891662
 -0.38555372 -0.3798593 ] 2 6
train:	 Loss = 1.6034,	 Acc = 0.3849,	 Loss Con1 = 1.3513,	 Loss Con2 = 2.4290
2926 0.16
5678 0.438
2914 0.5
375 0.413
69 0.536
6 0.333
0 0.0
0 0.0
0.4576421145764211
0.4568702290076336
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6523,	 Acc = 0.3983,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.9910
380 0.192
1718 0.457
815 0.396
85 0.165
2 0.0
0 0.0
0 0.0
0 0.0
0.42824427480916033
0.4568702290076336

 ===== Epoch 185	 =====
[ 0.21495906  1.5404643  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 0.21533065  1.5354013  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.6034,	 Acc = 0.3859,	 Loss Con1 = 1.3525,	 Loss Con2 = 2.4097
2921 0.161
5680 0.437
2915 0.504
377 0.419
69 0.551
6 0.333
0 0.0
0 0.0
0.45860506245164145
0.4568702290076336
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6674,	 Acc = 0.3917,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.0198
380 0.189
1718 0.463
815 0.352
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.4209923664122137
0.4568702290076336

 ===== Epoch 186	 =====
[ 0.13615988  2.1935642  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 0.07923291  2.4770803  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.6053,	 Acc = 0.3851,	 Loss Con1 = 1.3521,	 Loss Con2 = 2.4796
2926 0.164
5679 0.44
2913 0.495
376 0.41
68 0.485
6 0.167
0 0.0
0 0.0
0.4566467595664676
0.4568702290076336
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6336,	 Acc = 0.4117,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.0312
380 0.189
1718 0.466
815 0.423
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.4438931297709924
0.4568702290076336

 ===== Epoch 187	 =====
[ 2.2937658   2.140405   -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.5103225   2.2549837 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
train:	 Loss = 1.6071,	 Acc = 0.3820,	 Loss Con1 = 1.3520,	 Loss Con2 = 2.5630
2924 0.16
5682 0.436
2913 0.491
375 0.427
68 0.471
6 0.167
0 0.0
0 0.0
0.4536709420610349
0.4568702290076336
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6666,	 Acc = 0.4060,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.2413
380 0.166
1718 0.473
815 0.401
85 0.176
2 0.0
0 0.0
0 0.0
0 0.0
0.44083969465648853
0.4568702290076336

 ===== Epoch 188	 =====
[-0.36602148 -0.3783333   0.98770756  1.441872   -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0590516   1.5524195  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.6041,	 Acc = 0.3864,	 Loss Con1 = 1.3504,	 Loss Con2 = 2.5037
2925 0.159
5680 0.445
2914 0.497
375 0.4
68 0.529
6 0.167
0 0.0
0 0.0
0.46002432820966493
0.4568702290076336
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6572,	 Acc = 0.3947,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.0155
380 0.192
1718 0.459
815 0.378
85 0.165
2 0.0
0 0.0
0 0.0
0 0.0
0.42404580152671756
0.4568702290076336

 ===== Epoch 189	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 0
train:	 Loss = 1.6029,	 Acc = 0.3874,	 Loss Con1 = 1.3534,	 Loss Con2 = 2.6150
2917 0.16
5684 0.446
2915 0.496
377 0.401
69 0.507
6 0.167
0 0.0
0 0.0
0.4607225720914816
0.4568702290076336
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6573,	 Acc = 0.4170,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.2200
380 0.192
1718 0.468
815 0.44
85 0.176
2 0.0
0 0.0
0 0.0
0 0.0
0.449618320610687
0.4568702290076336

 ===== Epoch 190	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 0
train:	 Loss = 1.6026,	 Acc = 0.3869,	 Loss Con1 = 1.3525,	 Loss Con2 = 2.6551
2923 0.161
5678 0.442
2915 0.498
377 0.424
69 0.536
6 0.167
0 0.0
0 0.0
0.46003316749585405
0.4568702290076336
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6810,	 Acc = 0.3897,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.4326
380 0.189
1718 0.446
815 0.382
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.41870229007633586
0.4568702290076336

 ===== Epoch 191	 =====
[-0.36602148 -0.3783333   2.2488136   2.7602844  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   2.1962402   2.9657245  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.6041,	 Acc = 0.3842,	 Loss Con1 = 1.3519,	 Loss Con2 = 2.6304
2926 0.162
5678 0.438
2913 0.495
376 0.434
69 0.449
6 0.167
0 0.0
0 0.0
0.45609378456093785
0.4568702290076336
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6482,	 Acc = 0.4087,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.9355
380 0.192
1718 0.473
815 0.399
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.4400763358778626
0.4568702290076336

 ===== Epoch 192	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   3.0057707   1.4723846
 -0.3640846  -0.3725409   2.454802    0.8140702  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  3.0115764   1.5281084
 -0.3640846  -0.3725409   2.4467916   0.8265127  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 6
train:	 Loss = 1.6031,	 Acc = 0.3864,	 Loss Con1 = 1.3522,	 Loss Con2 = 2.3495
2926 0.16
5677 0.443
2915 0.491
375 0.456
69 0.522
6 0.167
0 0.0
0 0.0
0.4595222295952223
0.4568702290076336
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6395,	 Acc = 0.4113,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.1680
380 0.187
1718 0.466
815 0.421
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.4438931297709924
0.4568702290076336

 ===== Epoch 193	 =====
[-0.36602148 -0.3783333   1.5136765   2.7104473  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.4932145   2.8615386  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 6
train:	 Loss = 1.6068,	 Acc = 0.3839,	 Loss Con1 = 1.3517,	 Loss Con2 = 2.7114
2926 0.162
5683 0.439
2907 0.493
377 0.419
69 0.536
6 0.167
0 0.0
0 0.0
0.45576199955762
0.4568702290076336
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6557,	 Acc = 0.4150,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.3404
380 0.187
1718 0.482
815 0.405
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.4480916030534351
0.4568702290076336

 ===== Epoch 194	 =====
[ 1.6910409   2.6112442  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.1822987   2.9031966 ] [ 1.4624715   2.725157   -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.2506363   3.0921495 ] 4 6
train:	 Loss = 1.6057,	 Acc = 0.3846,	 Loss Con1 = 1.3524,	 Loss Con2 = 2.3118
2928 0.159
5674 0.439
2914 0.499
377 0.419
69 0.493
6 0.167
0 0.0
0 0.0
0.4575221238938053
0.4568702290076336
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6674,	 Acc = 0.3970,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.0283
380 0.189
1718 0.465
815 0.373
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.42709923664122135
0.4568702290076336

 ===== Epoch 195	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.687229    1.923671
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.3996928   1.9138126
 -0.38555372 -0.3798593 ] 3 2
train:	 Loss = 1.6027,	 Acc = 0.3895,	 Loss Con1 = 1.3526,	 Loss Con2 = 2.3505
2922 0.159
5685 0.444
2909 0.505
377 0.443
69 0.493
6 0.333
0 0.0
0 0.0
0.4638514260446606
0.4568702290076336
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6759,	 Acc = 0.3803,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.2065
380 0.192
1718 0.435
815 0.373
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.4076335877862595
0.4568702290076336

 ===== Epoch 196	 =====
[-0.36602148 -0.3783333   1.9218609  -3.9948795  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.900571    3.099354   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 2
train:	 Loss = 1.6096,	 Acc = 0.3819,	 Loss Con1 = 1.3514,	 Loss Con2 = 2.7658
2925 0.159
5683 0.436
2909 0.496
376 0.394
69 0.507
6 0.167
0 0.0
0 0.0
0.4539422757934314
0.4568702290076336
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6611,	 Acc = 0.3993,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.1123
380 0.187
1718 0.457
815 0.398
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.4301526717557252
0.4568702290076336

 ===== Epoch 197	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.2370467   1.4652498
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.1749175   1.3543415
 -0.38555372 -0.3798593 ] 1 5
train:	 Loss = 1.6041,	 Acc = 0.3854,	 Loss Con1 = 1.3516,	 Loss Con2 = 2.4929
2924 0.16
5678 0.44
2914 0.499
377 0.424
69 0.493
6 0.167
0 0.0
0 0.0
0.45831490490933213
0.4568702290076336
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6342,	 Acc = 0.4177,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.9064
380 0.192
1718 0.476
815 0.421
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.45038167938931295
0.4568702290076336

 ===== Epoch 198	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 3 0
train:	 Loss = 1.6068,	 Acc = 0.3875,	 Loss Con1 = 1.3521,	 Loss Con2 = 2.4396
2927 0.162
5680 0.442
2909 0.5
377 0.432
69 0.507
6 0.5
0 0.0
0 0.0
0.46067912841499836
0.4568702290076336
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6614,	 Acc = 0.4017,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.0925
380 0.192
1718 0.466
815 0.393
85 0.141
2 0.0
0 0.0
0 0.0
0 0.0
0.43206106870229005
0.4568702290076336

 ===== Epoch 199	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 0
train:	 Loss = 1.6035,	 Acc = 0.3857,	 Loss Con1 = 1.3527,	 Loss Con2 = 2.3908
2923 0.159
5680 0.44
2914 0.499
376 0.428
69 0.493
6 0.333
0 0.0
0 0.0
0.4588170259812051
0.4568702290076336
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6696,	 Acc = 0.4097,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.1205
380 0.192
1718 0.474
815 0.399
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.44122137404580153
0.4568702290076336

 ===== Epoch 200	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   1.3946123   3.1507745  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   1.3957566   3.3150158  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.6056,	 Acc = 0.3874,	 Loss Con1 = 1.3528,	 Loss Con2 = 2.2985
2923 0.159
5679 0.442
2915 0.502
376 0.418
69 0.565
6 0.333
0 0.0
0 0.0
0.4611387506909895
0.4568702290076336
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6807,	 Acc = 0.3953,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.0985
380 0.187
1718 0.465
815 0.369
85 0.176
2 0.0
0 0.0
0 0.0
0 0.0
0.4255725190839695
0.4568702290076336

 ===== Epoch 201	 =====
[ 2.4783263   2.5099885  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.087449    2.7536092 ] [ 2.651886    2.5606163  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.1064076   2.842837  ] 3 6
train:	 Loss = 1.6072,	 Acc = 0.3843,	 Loss Con1 = 1.3526,	 Loss Con2 = 2.3150
2919 0.164
5681 0.439
2918 0.495
376 0.404
68 0.456
6 0.167
0 0.0
0 0.0
0.4554094375069068
0.4568702290076336
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6390,	 Acc = 0.4080,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.9367
380 0.192
1718 0.463
815 0.411
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.43931297709923667
0.4568702290076336

 ===== Epoch 202	 =====
[-0.36602148 -0.3783333   1.5392647   2.8350396  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.5569779   3.0087576  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 3 6
train:	 Loss = 1.6055,	 Acc = 0.3834,	 Loss Con1 = 1.3531,	 Loss Con2 = 2.3973
2922 0.167
5679 0.439
2916 0.486
376 0.407
69 0.507
6 0.333
0 0.0
0 0.0
0.4533495467609993
0.4568702290076336
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 2
val:	 Loss = 1.6572,	 Acc = 0.3973,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.0011
380 0.179
1718 0.466
815 0.378
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.42900763358778626
0.4568702290076336

 ===== Epoch 203	 =====
[ 2.3047001   2.1252165  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.1879728   2.357333  ] [ 2.3047001   2.1252165  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.1879728   2.357333  ] 0 0
train:	 Loss = 1.6056,	 Acc = 0.3834,	 Loss Con1 = 1.3523,	 Loss Con2 = 2.1826
2922 0.161
5681 0.436
2915 0.498
376 0.404
68 0.559
6 0.333
0 0.0
0 0.0
0.4554499226177316
0.4568702290076336
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6583,	 Acc = 0.4123,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.1425
380 0.182
1718 0.477
815 0.404
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.44580152671755724
0.4568702290076336

 ===== Epoch 204	 =====
[-0.36602148 -0.3783333   1.770365    1.738628   -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.8757973   1.541095   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 5
train:	 Loss = 1.6055,	 Acc = 0.3879,	 Loss Con1 = 1.3510,	 Loss Con2 = 2.2395
2927 0.161
5681 0.446
2909 0.497
376 0.42
69 0.449
6 0.333
0 0.0
0 0.0
0.46134277181727684
0.4568702290076336
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6539,	 Acc = 0.3860,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.9461
380 0.176
1718 0.454
815 0.362
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.416412213740458
0.4568702290076336

 ===== Epoch 205	 =====
[ 2.1974173   2.051806   -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.1906183   2.2733538 ] [ 2.6216717   1.8923281  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.2016401   2.1316392 ] 4 5
train:	 Loss = 1.6047,	 Acc = 0.3868,	 Loss Con1 = 1.3521,	 Loss Con2 = 2.5909
2921 0.159
5680 0.446
2917 0.495
375 0.405
69 0.507
6 0.333
0 0.0
0 0.0
0.46048413838841606
0.4568702290076336
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6557,	 Acc = 0.4070,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.0853
380 0.187
1718 0.467
815 0.402
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.4389312977099237
0.4568702290076336

 ===== Epoch 206	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.3775953   1.411028
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.3419844   1.4406035
 -0.38555372 -0.3798593 ] 3 1
train:	 Loss = 1.6052,	 Acc = 0.3868,	 Loss Con1 = 1.3520,	 Loss Con2 = 2.6217
2922 0.163
5684 0.441
2911 0.497
376 0.431
69 0.522
6 0.333
0 0.0
0 0.0
0.4592084899403051
0.4568702290076336
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6638,	 Acc = 0.3890,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.1243
380 0.192
1718 0.456
815 0.361
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.41755725190839693
0.4568702290076336

 ===== Epoch 207	 =====
[ 0.62092185  2.0821826  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.6029,	 Acc = 0.3882,	 Loss Con1 = 1.3526,	 Loss Con2 = 2.2839
2924 0.16
5678 0.443
2914 0.503
377 0.43
69 0.493
6 0.333
0 0.0
0 0.0
0.46207430340557276
0.4568702290076336
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6681,	 Acc = 0.4063,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.0944
380 0.189
1718 0.464
815 0.411
85 0.165
2 0.0
0 0.0
0 0.0
0 0.0
0.43778625954198475
0.4568702290076336

 ===== Epoch 208	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.5725272   1.0059067  -0.40141198 -0.40778467  1.6897548   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.6431578   2.8503723
 -0.38555372 -0.3798593 ] 2 5
train:	 Loss = 1.6026,	 Acc = 0.3866,	 Loss Con1 = 1.3524,	 Loss Con2 = 2.5807
2924 0.161
5678 0.443
2914 0.495
377 0.427
69 0.565
6 0.167
0 0.0
0 0.0
0.4594206103494029
0.4568702290076336
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6830,	 Acc = 0.4117,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.5602
380 0.195
1718 0.471
815 0.407
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.4431297709923664
0.4568702290076336

 ===== Epoch 209	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.200046    1.7289653
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.225049    1.8349445
 -0.38555372 -0.3798593 ] 3 6
train:	 Loss = 1.6056,	 Acc = 0.3871,	 Loss Con1 = 1.3506,	 Loss Con2 = 2.5600
2927 0.16
5684 0.445
2907 0.497
375 0.405
69 0.507
6 0.167
0 0.0
0 0.0
0.46067912841499836
0.4568702290076336
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6563,	 Acc = 0.3993,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.1203
380 0.189
1718 0.469
815 0.373
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.4297709923664122
0.4568702290076336

 ===== Epoch 210	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   1.5474803   0.8789385
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  1.7195431   0.9168579
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 2
train:	 Loss = 1.6019,	 Acc = 0.3873,	 Loss Con1 = 1.3522,	 Loss Con2 = 2.5656
2922 0.162
5679 0.438
2916 0.505
377 0.438
68 0.485
6 0.167
0 0.0
0 0.0
0.45998231262436434
0.4568702290076336
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6511,	 Acc = 0.3897,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.9885
380 0.189
1718 0.46
815 0.355
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.41870229007633586
0.4568702290076336

 ===== Epoch 211	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   1.3030684   1.2993282  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  1.8016795   2.9906638
 -0.3640846  -0.3725409   1.4449618   1.045501   -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 6
train:	 Loss = 1.6055,	 Acc = 0.3856,	 Loss Con1 = 1.3525,	 Loss Con2 = 2.4735
2924 0.161
5681 0.44
2911 0.497
377 0.44
69 0.478
6 0.167
0 0.0
0 0.0
0.45831490490933213
0.4568702290076336
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6594,	 Acc = 0.4080,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.2774
380 0.187
1718 0.462
815 0.415
85 0.247
2 0.0
0 0.0
0 0.0
0 0.0
0.4400763358778626
0.4568702290076336

 ===== Epoch 212	 =====
[ 3.2688763   2.8542583  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  3.0314002   3.063282  ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  3.0208182   2.918943  ] 5 5
train:	 Loss = 1.6041,	 Acc = 0.3845,	 Loss Con1 = 1.3521,	 Loss Con2 = 2.6069
2921 0.16
5683 0.439
2912 0.498
377 0.403
69 0.507
6 0.333
0 0.0
0 0.0
0.457168122029402
0.4568702290076336
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6754,	 Acc = 0.4063,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.3562
380 0.187
1718 0.47
815 0.398
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.4381679389312977
0.4568702290076336

 ===== Epoch 213	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 0
train:	 Loss = 1.6032,	 Acc = 0.3858,	 Loss Con1 = 1.3523,	 Loss Con2 = 2.7471
2926 0.159
5679 0.44
2913 0.502
375 0.416
69 0.478
6 0.167
0 0.0
0 0.0
0.4590798495907985
0.4568702290076336
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6523,	 Acc = 0.4060,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.0660
380 0.192
1718 0.467
815 0.399
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.43702290076335876
0.4568702290076336

 ===== Epoch 214	 =====
[-0.36602148 -0.3783333   1.4316338   1.4599944  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.474938    1.5909231  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.6052,	 Acc = 0.3825,	 Loss Con1 = 1.3513,	 Loss Con2 = 2.2930
2925 0.159
5682 0.436
2910 0.496
376 0.423
69 0.464
6 0.333
0 0.0
0 0.0
0.45482693796306534
0.4568702290076336
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6633,	 Acc = 0.3993,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.1406
380 0.187
1718 0.47
815 0.371
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.4301526717557252
0.4568702290076336

 ===== Epoch 215	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   1.3091433   2.2391968
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  1.3012168   2.3527405
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.6074,	 Acc = 0.3872,	 Loss Con1 = 1.3521,	 Loss Con2 = 2.6997
2922 0.167
5681 0.441
2914 0.498
376 0.415
69 0.478
6 0.167
0 0.0
0 0.0
0.4584346672562459
0.4568702290076336
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6406,	 Acc = 0.4123,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.0818
380 0.189
1718 0.464
815 0.434
85 0.153
2 0.0
0 0.0
0 0.0
0 0.0
0.4446564885496183
0.4568702290076336

 ===== Epoch 216	 =====
[-0.36602148 -0.3783333   1.6554244   2.8123865  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.6552639   2.8117106  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.6026,	 Acc = 0.3880,	 Loss Con1 = 1.3526,	 Loss Con2 = 2.5196
2924 0.159
5678 0.444
2915 0.506
377 0.398
68 0.485
6 0.167
0 0.0
0 0.0
0.46207430340557276
0.4568702290076336
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6682,	 Acc = 0.3893,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.1250
380 0.187
1718 0.448
815 0.383
85 0.176
2 0.0
0 0.0
0 0.0
0 0.0
0.41870229007633586
0.4568702290076336

 ===== Epoch 217	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   1.2182386   2.2680912
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  1.2752388   2.1326907
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 5
train:	 Loss = 1.6027,	 Acc = 0.3878,	 Loss Con1 = 1.3524,	 Loss Con2 = 2.5212
2921 0.159
5680 0.443
2915 0.501
377 0.419
69 0.522
6 0.333
0 0.0
0 0.0
0.46158947717475407
0.4568702290076336
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6636,	 Acc = 0.4047,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.0725
380 0.184
1718 0.459
815 0.415
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.4366412213740458
0.4568702290076336

 ===== Epoch 218	 =====
[-0.36602148 -0.3783333   1.6871043   1.8111181  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.58825     1.9714283  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
train:	 Loss = 1.6049,	 Acc = 0.3855,	 Loss Con1 = 1.3521,	 Loss Con2 = 2.4540
2925 0.161
5678 0.443
2915 0.494
375 0.4
69 0.478
6 0.333
0 0.0
0 0.0
0.4580338383279885
0.4568702290076336
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6866,	 Acc = 0.3893,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.2096
380 0.187
1718 0.458
815 0.362
85 0.176
2 0.0
0 0.0
0 0.0
0 0.0
0.41870229007633586
0.4568702290076336

 ===== Epoch 219	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.4094166   2.0148623
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.3995675   1.9433881
 -0.38555372 -0.3798593 ] 3 5
train:	 Loss = 1.6025,	 Acc = 0.3868,	 Loss Con1 = 1.3530,	 Loss Con2 = 2.4203
2923 0.158
5676 0.441
2917 0.505
377 0.406
69 0.493
6 0.333
0 0.0
0 0.0
0.46058595909342176
0.4568702290076336
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.7011,	 Acc = 0.3940,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.5460
380 0.187
1718 0.462
815 0.368
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.42404580152671756
0.4568702290076336

 ===== Epoch 220	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 0.37899297  1.5404643  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 6
train:	 Loss = 1.6046,	 Acc = 0.3827,	 Loss Con1 = 1.3514,	 Loss Con2 = 2.6150
2925 0.156
5679 0.44
2914 0.493
375 0.413
69 0.464
6 0.5
0 0.0
0 0.0
0.45604334844631206
0.4568702290076336
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6762,	 Acc = 0.4063,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.3514
380 0.189
1718 0.471
815 0.394
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.43778625954198475
0.4568702290076336

 ===== Epoch 221	 =====
[-0.36602148 -0.3783333   2.5786107   3.0570402   2.6035762   1.1501012
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  2.539767    1.2124808
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 6
train:	 Loss = 1.6014,	 Acc = 0.3876,	 Loss Con1 = 1.3523,	 Loss Con2 = 2.4666
2926 0.16
5679 0.445
2912 0.501
377 0.401
68 0.471
6 0.0
0 0.0
0 0.0
0.46118115461181153
0.4568702290076336
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6499,	 Acc = 0.4167,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.9029
380 0.187
1718 0.465
815 0.444
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.45
0.4568702290076336

 ===== Epoch 222	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   1.3602839   0.95093787 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   1.4449618   1.010662   -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 6
train:	 Loss = 1.6055,	 Acc = 0.3860,	 Loss Con1 = 1.3520,	 Loss Con2 = 2.5052
2923 0.161
5684 0.44
2911 0.5
376 0.415
68 0.485
6 0.333
0 0.0
0 0.0
0.4588170259812051
0.4568702290076336
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6600,	 Acc = 0.4003,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.0170
380 0.184
1718 0.464
815 0.388
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.4316793893129771
0.4568702290076336

 ===== Epoch 223	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   2.4958658   1.4212637
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  2.2987041   1.5525583
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 3 6
train:	 Loss = 1.6027,	 Acc = 0.3873,	 Loss Con1 = 1.3516,	 Loss Con2 = 2.5363
2924 0.161
5683 0.443
2912 0.498
375 0.416
68 0.574
6 0.167
0 0.0
0 0.0
0.4605263157894737
0.4568702290076336
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6767,	 Acc = 0.3803,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.0036
380 0.187
1718 0.444
815 0.358
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.4083969465648855
0.4568702290076336

 ===== Epoch 224	 =====
[-0.36602148 -0.3783333   1.2123104   0.85742116 -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 5.0929637   3.178277   -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.6043,	 Acc = 0.3880,	 Loss Con1 = 1.3516,	 Loss Con2 = 2.2991
2922 0.159
5682 0.444
2913 0.499
377 0.438
69 0.507
5 0.4
0 0.0
0 0.0
0.4618615962856511
0.4568702290076336
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6524,	 Acc = 0.4190,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.9986
380 0.184
1718 0.477
815 0.426
85 0.224
2 0.5
0 0.0
0 0.0
0 0.0
0.4530534351145038
0.4568702290076336

 ===== Epoch 225	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.6029,	 Acc = 0.3834,	 Loss Con1 = 1.3514,	 Loss Con2 = 2.3877
2926 0.161
5680 0.435
2915 0.499
373 0.416
68 0.515
6 0.333
0 0.0
0 0.0
0.4553196195531962
0.4568702290076336
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6484,	 Acc = 0.4150,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.0050
380 0.189
1718 0.484
815 0.398
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.44770992366412216
0.4568702290076336

 ===== Epoch 226	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 0
train:	 Loss = 1.6027,	 Acc = 0.3859,	 Loss Con1 = 1.3527,	 Loss Con2 = 2.6027
2924 0.16
5679 0.443
2914 0.494
376 0.412
69 0.522
6 0.333
0 0.0
0 0.0
0.45897832817337464
0.4568702290076336
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6550,	 Acc = 0.4033,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.1351
380 0.192
1718 0.46
815 0.407
85 0.176
2 0.0
0 0.0
0 0.0
0 0.0
0.43396946564885497
0.4568702290076336

 ===== Epoch 227	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   1.2647355   3.0014641  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   1.2647355   3.0014641  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.6034,	 Acc = 0.3888,	 Loss Con1 = 1.3522,	 Loss Con2 = 2.3123
2922 0.165
5678 0.443
2917 0.501
376 0.426
69 0.507
6 0.333
0 0.0
0 0.0
0.46119831969931463
0.4568702290076336
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6534,	 Acc = 0.3950,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.1331
380 0.189
1718 0.459
815 0.378
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.4248091603053435
0.4568702290076336

 ===== Epoch 228	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.4961367   1.3965598  -0.40141198 -0.40778467  2.5277388   2.7148178
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.1782155   1.4802712  -0.40141198 -0.40778467  2.4796264   2.739464
 -0.38555372 -0.3798593 ] 2 4
train:	 Loss = 1.6011,	 Acc = 0.3895,	 Loss Con1 = 1.3529,	 Loss Con2 = 2.5891
2918 0.162
5685 0.444
2915 0.506
376 0.407
68 0.544
6 0.167
0 0.0
0 0.0
0.46298342541436466
0.4568702290076336
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6695,	 Acc = 0.3940,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.0894
380 0.192
1718 0.459
815 0.373
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.4232824427480916
0.4568702290076336

 ===== Epoch 229	 =====
[ 2.7723243   3.2845953  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.407978    3.5304155 ] [ 2.7499032   3.2845953  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.6024108   3.5304155 ] 0 0
train:	 Loss = 1.6053,	 Acc = 0.3839,	 Loss Con1 = 1.3519,	 Loss Con2 = 2.5262
2926 0.16
5677 0.439
2914 0.497
376 0.394
69 0.565
6 0.333
0 0.0
0 0.0
0.4564255695642557
0.4568702290076336
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6441,	 Acc = 0.4210,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.9621
380 0.189
1718 0.483
815 0.422
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.4545801526717557
0.4568702290076336

 ===== Epoch 230	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  3.411143    2.236464   -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  3.468998    2.3257563  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 6
train:	 Loss = 1.6034,	 Acc = 0.3859,	 Loss Con1 = 1.3525,	 Loss Con2 = 2.7483
2922 0.16
5679 0.44
2915 0.5
377 0.424
69 0.522
6 0.0
0 0.0
0 0.0
0.4588768516471369
0.4568702290076336
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6896,	 Acc = 0.3757,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.5832
380 0.192
1718 0.427
815 0.374
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.4022900763358779
0.4568702290076336

 ===== Epoch 231	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 0
train:	 Loss = 1.6041,	 Acc = 0.3883,	 Loss Con1 = 1.3510,	 Loss Con2 = 2.3063
2927 0.16
5678 0.446
2912 0.499
377 0.416
68 0.485
6 0.333
0 0.0
0 0.0
0.46222762968698156
0.4568702290076336
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6259,	 Acc = 0.3927,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.8638
380 0.187
1718 0.455
815 0.38
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.42251908396946564
0.4568702290076336

 ===== Epoch 232	 =====
[-0.36602148 -0.3783333   0.88007677  1.7726077  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   0.89497083  1.640751   -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 5
train:	 Loss = 1.6042,	 Acc = 0.3890,	 Loss Con1 = 1.3525,	 Loss Con2 = 2.5144
2923 0.159
5681 0.446
2912 0.5
377 0.432
69 0.507
6 0.167
0 0.0
0 0.0
0.4633499170812604
0.4568702290076336
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6679,	 Acc = 0.4247,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.3456
380 0.189
1718 0.478
815 0.443
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.4587786259541985
0.4587786259541985

 ===== Epoch 233	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.6009,	 Acc = 0.3858,	 Loss Con1 = 1.3524,	 Loss Con2 = 2.4714
2920 0.159
5686 0.438
2911 0.503
376 0.428
69 0.507
6 0.167
0 0.0
0 0.0
0.45910698496905394
0.4587786259541985
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6524,	 Acc = 0.3870,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.9709
380 0.184
1718 0.456
815 0.356
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.416412213740458
0.4587786259541985

 ===== Epoch 234	 =====
[-0.36602148 -0.3783333   2.3641613   0.8211762  -0.4409929  -0.3635196
  4.6474433   2.3257563  -0.40141198 -0.40778467  4.6927824   2.5620105
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  4.3474956   2.0997353  -0.40141198 -0.40778467  4.626108    2.3599107
 -0.38555372 -0.3798593 ] 5 5
train:	 Loss = 1.6061,	 Acc = 0.3832,	 Loss Con1 = 1.3528,	 Loss Con2 = 2.5633
2917 0.162
5685 0.438
2914 0.493
377 0.398
69 0.507
6 0.167
0 0.0
0 0.0
0.4545354104518838
0.4587786259541985
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6435,	 Acc = 0.3947,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.9739
380 0.184
1718 0.449
815 0.402
85 0.165
2 0.0
0 0.0
0 0.0
0 0.0
0.4251908396946565
0.4587786259541985

 ===== Epoch 235	 =====
[-0.36602148 -0.3783333   1.2841996   2.9573665  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.1841427   3.0631156  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.6022,	 Acc = 0.3859,	 Loss Con1 = 1.3516,	 Loss Con2 = 2.4415
2928 0.159
5680 0.444
2908 0.496
377 0.408
69 0.493
6 0.167
0 0.0
0 0.0
0.4594026548672566
0.4587786259541985
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6649,	 Acc = 0.4127,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.2645
380 0.189
1718 0.469
815 0.417
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.4450381679389313
0.4587786259541985

 ===== Epoch 236	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 0
train:	 Loss = 1.6020,	 Acc = 0.3867,	 Loss Con1 = 1.3515,	 Loss Con2 = 2.4430
2926 0.163
5681 0.44
2911 0.499
375 0.427
69 0.536
6 0.167
0 0.0
0 0.0
0.4590798495907985
0.4587786259541985
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6622,	 Acc = 0.3997,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.9399
380 0.192
1718 0.469
815 0.373
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.4297709923664122
0.4587786259541985

 ===== Epoch 237	 =====
[-0.36602148 -0.3783333   2.26831     1.2244018  -0.4409929  -0.3635196
  1.8220973   3.6149116  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   2.1535954   1.2081529  -0.44088638 -0.36343393
  1.8602928   3.6874616  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 1
train:	 Loss = 1.6023,	 Acc = 0.3880,	 Loss Con1 = 1.3519,	 Loss Con2 = 2.2884
2928 0.162
5677 0.443
2911 0.504
377 0.416
69 0.435
6 0.333
0 0.0
0 0.0
0.461283185840708
0.4587786259541985
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6561,	 Acc = 0.4063,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.0559
380 0.182
1718 0.475
815 0.385
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.4389312977099237
0.4587786259541985

 ===== Epoch 238	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   1.7113378   2.599265
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  1.6007301   2.6172454
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 0
train:	 Loss = 1.6033,	 Acc = 0.3872,	 Loss Con1 = 1.3532,	 Loss Con2 = 2.3212
2922 0.16
5677 0.447
2917 0.494
377 0.414
69 0.478
6 0.333
0 0.0
0 0.0
0.46064558921070087
0.4587786259541985
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6638,	 Acc = 0.4143,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.1995
380 0.189
1718 0.476
815 0.411
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.44694656488549617
0.4587786259541985

 ===== Epoch 239	 =====
[-0.36602148 -0.3783333   1.8710915   0.9299112  -0.4409929  -0.3635196
  3.3493571   3.1405468  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.9038203   0.91824424 -0.44088638 -0.36343393
  3.293187    3.109853   -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 3 3
train:	 Loss = 1.6012,	 Acc = 0.3857,	 Loss Con1 = 1.3526,	 Loss Con2 = 2.3182
2924 0.159
5681 0.441
2911 0.502
377 0.401
69 0.522
6 0.167
0 0.0
0 0.0
0.4590888987173817
0.4587786259541985
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6644,	 Acc = 0.4130,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.2188
380 0.189
1718 0.474
815 0.409
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.4454198473282443
0.4587786259541985

 ===== Epoch 240	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 3 0
train:	 Loss = 1.6022,	 Acc = 0.3846,	 Loss Con1 = 1.3515,	 Loss Con2 = 2.5022
2929 0.164
5680 0.439
2907 0.495
377 0.408
69 0.507
6 0.167
0 0.0
0 0.0
0.4560238964487222
0.4587786259541985
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6519,	 Acc = 0.4147,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.0977
380 0.192
1718 0.471
815 0.416
85 0.271
2 0.0
0 0.0
0 0.0
0 0.0
0.44694656488549617
0.4587786259541985

 ===== Epoch 241	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   3.2341778  -4.673227
 -0.3640846  -0.3725409   3.016079    1.7920518  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  3.5918841   2.350518
 -0.3640846  -0.3725409   2.7465973   1.8467989  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 3 2
train:	 Loss = 1.6027,	 Acc = 0.3868,	 Loss Con1 = 1.3525,	 Loss Con2 = 2.4898
2917 0.159
5684 0.439
2917 0.506
375 0.419
69 0.507
6 0.333
0 0.0
0 0.0
0.46017014694508895
0.4587786259541985
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6633,	 Acc = 0.4023,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.3027
380 0.189
1718 0.466
815 0.383
85 0.259
2 0.0
0 0.0
0 0.0
0 0.0
0.43320610687022904
0.4587786259541985

 ===== Epoch 242	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   1.4483947   2.9566712  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   1.3082187   2.9591594  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.6071,	 Acc = 0.3824,	 Loss Con1 = 1.3524,	 Loss Con2 = 2.5279
2920 0.16
5683 0.435
2913 0.493
377 0.419
69 0.536
6 0.333
0 0.0
0 0.0
0.4540229885057471
0.4587786259541985
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6445,	 Acc = 0.4080,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.9510
380 0.189
1718 0.475
815 0.391
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.4396946564885496
0.4587786259541985

 ===== Epoch 243	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   1.5021772   2.1429307  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   1.5090415   2.1976779  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 1
train:	 Loss = 1.6032,	 Acc = 0.3854,	 Loss Con1 = 1.3527,	 Loss Con2 = 2.4810
2925 0.161
5677 0.442
2914 0.495
377 0.427
69 0.435
6 0.167
0 0.0
0 0.0
0.4580338383279885
0.4587786259541985
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6688,	 Acc = 0.4160,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.3478
380 0.189
1718 0.473
815 0.425
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.4488549618320611
0.4587786259541985

 ===== Epoch 244	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.5734527   2.739464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.7337005   2.744393
 -0.38555372 -0.3798593 ] 2 0
train:	 Loss = 1.6049,	 Acc = 0.3838,	 Loss Con1 = 1.3520,	 Loss Con2 = 2.5994
2925 0.172
5679 0.435
2912 0.492
377 0.411
69 0.464
6 0.0
0 0.0
0 0.0
0.4522835342253677
0.4587786259541985
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6594,	 Acc = 0.4130,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.0908
380 0.195
1718 0.464
815 0.426
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.4446564885496183
0.4587786259541985

 ===== Epoch 245	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   1.7163029   1.9769248
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  1.7115207   1.9837676
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.6022,	 Acc = 0.3871,	 Loss Con1 = 1.3525,	 Loss Con2 = 2.5141
2923 0.159
5678 0.444
2916 0.5
377 0.419
68 0.471
6 0.333
0 0.0
0 0.0
0.4609176340519624
0.4587786259541985
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6502,	 Acc = 0.3983,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.1449
380 0.189
1718 0.459
815 0.395
85 0.153
2 0.0
0 0.0
0 0.0
0 0.0
0.42862595419847327
0.4587786259541985

 ===== Epoch 246	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  1.2660165   2.5350347  -0.40141198 -0.40778467  2.4034815   2.0148623
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  1.2755649   2.5434058  -0.40141198 -0.40778467  2.6262355   2.0222564
 -0.38555372 -0.3798593 ] 1 2
train:	 Loss = 1.6052,	 Acc = 0.3839,	 Loss Con1 = 1.3513,	 Loss Con2 = 2.4500
2925 0.16
5680 0.438
2914 0.496
376 0.426
67 0.493
6 0.167
0 0.0
0 0.0
0.45626451398872053
0.4587786259541985
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6781,	 Acc = 0.3720,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.1020
380 0.189
1718 0.431
815 0.358
85 0.141
2 0.0
0 0.0
0 0.0
0 0.0
0.3984732824427481
0.4587786259541985

 ===== Epoch 247	 =====
[ 2.058167    2.206221   -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 1.8272215   2.160656   -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 3 0
train:	 Loss = 1.6037,	 Acc = 0.3864,	 Loss Con1 = 1.3525,	 Loss Con2 = 2.6604
2921 0.161
5685 0.442
2912 0.498
375 0.429
69 0.449
6 0.333
0 0.0
0 0.0
0.45937879960207806
0.4587786259541985
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6661,	 Acc = 0.4077,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.1964
380 0.192
1718 0.463
815 0.409
85 0.247
2 0.0
0 0.0
0 0.0
0 0.0
0.4389312977099237
0.4587786259541985

 ===== Epoch 248	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.5889851   2.6556664
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 6
train:	 Loss = 1.6020,	 Acc = 0.3890,	 Loss Con1 = 1.3520,	 Loss Con2 = 2.4599
2923 0.16
5683 0.441
2912 0.51
376 0.404
68 0.559
6 0.333
0 0.0
0 0.0
0.4629076838032062
0.4587786259541985
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6785,	 Acc = 0.3943,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.3365
380 0.189
1718 0.455
815 0.384
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.42404580152671756
0.4587786259541985

 ===== Epoch 249	 =====
[-0.36602148 -0.3783333   1.8966796   1.9674246  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   2.014291    2.1231773  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.6030,	 Acc = 0.3855,	 Loss Con1 = 1.3529,	 Loss Con2 = 2.6023
2921 0.159
5681 0.44
2916 0.5
375 0.419
69 0.493
6 0.333
0 0.0
0 0.0
0.45871559633027525
0.4587786259541985
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6535,	 Acc = 0.4200,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.0306
380 0.189
1718 0.473
815 0.44
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.4534351145038168
0.4587786259541985

 ===== Epoch 250	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   3.5178533   2.0533447  -0.42342317 -0.42019257
  3.5516515   1.0294142 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   3.2008824   2.1155572  -0.42342317 -0.42019257
  3.6204307   1.0714037 ] 2 4
train:	 Loss = 1.6029,	 Acc = 0.3889,	 Loss Con1 = 1.3518,	 Loss Con2 = 2.4987
2926 0.16
5684 0.449
2907 0.497
376 0.402
69 0.536
6 0.333
0 0.0
0 0.0
0.46306126963061267
0.4587786259541985
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6523,	 Acc = 0.4160,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.1403
380 0.192
1718 0.473
815 0.42
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.4484732824427481
0.4587786259541985

 ===== Epoch 251	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 0
train:	 Loss = 1.6031,	 Acc = 0.3885,	 Loss Con1 = 1.3526,	 Loss Con2 = 2.6603
2924 0.162
5680 0.446
2912 0.499
377 0.414
69 0.522
6 0.167
0 0.0
0 0.0
0.46185316231755863
0.4587786259541985
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6513,	 Acc = 0.3913,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.1263
380 0.195
1718 0.451
815 0.377
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.4198473282442748
0.4587786259541985

 ===== Epoch 252	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 3 0
train:	 Loss = 1.5996,	 Acc = 0.3869,	 Loss Con1 = 1.3520,	 Loss Con2 = 2.6323
2924 0.164
5682 0.441
2913 0.501
375 0.405
68 0.471
6 0.333
0 0.0
0 0.0
0.45897832817337464
0.4587786259541985
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6597,	 Acc = 0.3997,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.1844
380 0.187
1718 0.463
815 0.385
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.4305343511450382
0.4587786259541985

 ===== Epoch 253	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.6036,	 Acc = 0.3892,	 Loss Con1 = 1.3521,	 Loss Con2 = 2.3732
2924 0.161
5680 0.445
2913 0.504
376 0.428
69 0.435
6 0.333
0 0.0
0 0.0
0.46306943830163644
0.4587786259541985
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6573,	 Acc = 0.4040,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.1080
380 0.192
1718 0.462
815 0.402
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.4347328244274809
0.4587786259541985

 ===== Epoch 254	 =====
[ 2.762147    3.2845953  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.6041749   3.5304155 ] [ 2.807496    3.2845953  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.600207    3.5304155 ] 0 0
train:	 Loss = 1.5992,	 Acc = 0.3862,	 Loss Con1 = 1.3527,	 Loss Con2 = 2.5709
2921 0.161
5681 0.439
2914 0.501
377 0.422
69 0.536
6 0.167
0 0.0
0 0.0
0.45904719796617666
0.4587786259541985
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6665,	 Acc = 0.4177,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.3913
380 0.189
1718 0.474
815 0.423
85 0.247
2 0.0
0 0.0
0 0.0
0 0.0
0.45076335877862594
0.4587786259541985

 ===== Epoch 255	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   1.63342     1.2812372
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  1.557943    1.1746943
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 5
train:	 Loss = 1.6008,	 Acc = 0.3874,	 Loss Con1 = 1.3526,	 Loss Con2 = 2.5123
2922 0.16
5683 0.442
2911 0.501
377 0.419
69 0.536
6 0.333
0 0.0
0 0.0
0.46075613530842363
0.4587786259541985
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6530,	 Acc = 0.4090,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.2972
380 0.189
1718 0.464
815 0.415
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.44083969465648853
0.4587786259541985

 ===== Epoch 256	 =====
[ 2.9780552   2.6821234  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  0.7503761   2.2922716  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  0.76273406  2.5908422  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.6013,	 Acc = 0.3839,	 Loss Con1 = 1.3519,	 Loss Con2 = 2.5775
2925 0.164
5676 0.437
2916 0.493
377 0.424
68 0.485
6 0.333
0 0.0
0 0.0
0.4549375207342696
0.4587786259541985
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6399,	 Acc = 0.4213,	 Loss Con1 = 1.3848,	 Loss Con2 = 1.9758
380 0.195
1718 0.484
815 0.416
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.4541984732824427
0.4587786259541985

 ===== Epoch 257	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   2.2384303   1.9213587
 -0.3640846  -0.3725409   0.910576    2.6331658  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  2.2593548   2.0237768
 -0.3640846  -0.3725409   0.9895322   2.837223   -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 6
train:	 Loss = 1.6018,	 Acc = 0.3871,	 Loss Con1 = 1.3519,	 Loss Con2 = 2.4551
2925 0.16
5681 0.442
2910 0.501
377 0.419
69 0.507
6 0.333
0 0.0
0 0.0
0.46046665929448194
0.4587786259541985
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6468,	 Acc = 0.4130,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.2399
380 0.187
1718 0.469
815 0.422
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.44580152671755724
0.4587786259541985

 ===== Epoch 258	 =====
[-0.36602148 -0.3783333   0.7915353   0.8710131  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 5
train:	 Loss = 1.6021,	 Acc = 0.3869,	 Loss Con1 = 1.3528,	 Loss Con2 = 2.6418
2923 0.161
5680 0.444
2913 0.499
377 0.395
69 0.507
6 0.333
0 0.0
0 0.0
0.45992260917634054
0.4587786259541985
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6394,	 Acc = 0.4207,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.0174
380 0.189
1718 0.473
815 0.436
85 0.271
2 0.0
0 0.0
0 0.0
0 0.0
0.4541984732824427
0.4587786259541985

 ===== Epoch 259	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   1.0759257   3.25778    -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 2
train:	 Loss = 1.6032,	 Acc = 0.3902,	 Loss Con1 = 1.3521,	 Loss Con2 = 2.6258
2919 0.16
5685 0.447
2913 0.504
376 0.407
69 0.507
6 0.167
0 0.0
0 0.0
0.46436070284009284
0.4587786259541985
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6786,	 Acc = 0.4087,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.3473
380 0.192
1718 0.469
815 0.41
85 0.165
2 0.0
0 0.0
0 0.0
0 0.0
0.4400763358778626
0.4587786259541985

 ===== Epoch 260	 =====
[ 0.2402805   2.8643837  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 0.24024826  2.6365583  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 5
train:	 Loss = 1.6009,	 Acc = 0.3867,	 Loss Con1 = 1.3524,	 Loss Con2 = 2.5690
2923 0.16
5678 0.444
2916 0.496
376 0.423
69 0.464
6 0.5
0 0.0
0 0.0
0.45981205085682697
0.4587786259541985
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6759,	 Acc = 0.3860,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.2903
380 0.184
1718 0.444
815 0.382
85 0.165
2 0.0
0 0.0
0 0.0
0 0.0
0.4152671755725191
0.4587786259541985

 ===== Epoch 261	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.4057534   1.9014895
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.439091    1.9483174
 -0.38555372 -0.3798593 ] 2 6
train:	 Loss = 1.6009,	 Acc = 0.3869,	 Loss Con1 = 1.3515,	 Loss Con2 = 2.6376
2928 0.159
5680 0.442
2909 0.503
376 0.42
69 0.522
6 0.0
0 0.0
0 0.0
0.46084070796460175
0.4587786259541985
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6670,	 Acc = 0.4183,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.1493
380 0.187
1718 0.478
815 0.421
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.45190839694656487
0.4587786259541985

 ===== Epoch 262	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   1.2365717   2.6592767
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  1.2125851   2.6883726
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 3 1
train:	 Loss = 1.6023,	 Acc = 0.3885,	 Loss Con1 = 1.3532,	 Loss Con2 = 2.7901
2921 0.158
5679 0.444
2917 0.504
376 0.418
69 0.507
6 0.333
0 0.0
0 0.0
0.4629158837183597
0.4587786259541985
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6823,	 Acc = 0.4040,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.3764
380 0.189
1718 0.466
815 0.393
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.4351145038167939
0.4587786259541985

 ===== Epoch 263	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 3 0
train:	 Loss = 1.6015,	 Acc = 0.3869,	 Loss Con1 = 1.3521,	 Loss Con2 = 2.7281
2924 0.16
5680 0.442
2912 0.501
377 0.411
69 0.565
6 0.167
0 0.0
0 0.0
0.46030517470145954
0.4587786259541985
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6926,	 Acc = 0.4023,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.5345
380 0.184
1718 0.464
815 0.396
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.43396946564885497
0.4587786259541985

 ===== Epoch 264	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   4.0389495   2.4058952
 -0.3640846  -0.3725409   4.2502017   1.7572128  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  4.040009    2.4060862
 -0.3640846  -0.3725409   4.2748046   1.7572128  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.6049,	 Acc = 0.3854,	 Loss Con1 = 1.3519,	 Loss Con2 = 2.8911
2922 0.159
5683 0.444
2915 0.492
374 0.398
69 0.551
5 0.2
0 0.0
0 0.0
0.4584346672562459
0.4587786259541985
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6567,	 Acc = 0.4093,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.2715
380 0.192
1718 0.476
815 0.394
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.44083969465648853
0.4587786259541985

 ===== Epoch 265	 =====
[-0.36602148 -0.3783333   1.8723093   2.0874863  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.9452469   2.05523    -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 3 2
train:	 Loss = 1.6005,	 Acc = 0.3875,	 Loss Con1 = 1.3522,	 Loss Con2 = 3.0121
2922 0.16
5684 0.445
2911 0.497
376 0.434
69 0.449
6 0.5
0 0.0
0 0.0
0.46108777360159187
0.4587786259541985
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6740,	 Acc = 0.4040,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.1700
380 0.187
1718 0.467
815 0.398
85 0.176
2 0.0
0 0.0
0 0.0
0 0.0
0.4354961832061069
0.4587786259541985

 ===== Epoch 266	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 0
train:	 Loss = 1.6012,	 Acc = 0.3866,	 Loss Con1 = 1.3525,	 Loss Con2 = 2.8505
2920 0.159
5681 0.443
2915 0.502
377 0.401
69 0.493
6 0.333
0 0.0
0 0.0
0.46021220159151194
0.4587786259541985
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6626,	 Acc = 0.4147,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.2186
380 0.189
1718 0.481
815 0.398
85 0.247
2 0.0
0 0.0
0 0.0
0 0.0
0.44732824427480916
0.4587786259541985

 ===== Epoch 267	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  0.817297    1.1990697
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  0.8953368   1.2927257
 -0.38555372 -0.3798593 ] 2 6
train:	 Loss = 1.6031,	 Acc = 0.3851,	 Loss Con1 = 1.3526,	 Loss Con2 = 2.8008
2923 0.16
5681 0.443
2912 0.497
377 0.39
69 0.449
6 0.333
0 0.0
0 0.0
0.45793255942509675
0.4587786259541985
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6647,	 Acc = 0.4093,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.3125
380 0.187
1718 0.469
815 0.409
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.4416030534351145
0.4587786259541985

 ===== Epoch 268	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 0
train:	 Loss = 1.6020,	 Acc = 0.3868,	 Loss Con1 = 1.3527,	 Loss Con2 = 2.7477
2921 0.157
5681 0.441
2915 0.503
376 0.428
69 0.493
6 0.333
0 0.0
0 0.0
0.46081574002431747
0.4587786259541985
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6531,	 Acc = 0.4147,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.3827
380 0.192
1718 0.467
815 0.428
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.44694656488549617
0.4587786259541985

 ===== Epoch 269	 =====
[-0.36602148 -0.3783333   3.1508803   1.1677691  -0.4409929  -0.3635196
  2.1815858   2.560148   -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   2.9232297   1.0699934  -0.44088638 -0.36343393
  2.3427925   2.4513233  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 5
train:	 Loss = 1.6017,	 Acc = 0.3897,	 Loss Con1 = 1.3525,	 Loss Con2 = 2.7575
2917 0.158
5684 0.447
2915 0.503
377 0.432
69 0.464
6 0.167
0 0.0
0 0.0
0.4643685780576732
0.4587786259541985
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6673,	 Acc = 0.3883,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.1962
380 0.192
1718 0.453
815 0.364
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.416793893129771
0.4587786259541985

 ===== Epoch 270	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   0.77554864  1.6128798  -0.42342317 -0.42019257
  2.6808903   1.6855006 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   0.7538062   1.4038453  -0.42342317 -0.42019257
  2.478521    1.4860502 ] 5 5
train:	 Loss = 1.6032,	 Acc = 0.3863,	 Loss Con1 = 1.3523,	 Loss Con2 = 2.8722
2924 0.16
5683 0.441
2910 0.499
376 0.434
69 0.493
6 0.333
0 0.0
0 0.0
0.45953118089341
0.4587786259541985
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6735,	 Acc = 0.4107,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.7380
380 0.189
1718 0.468
815 0.417
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.44274809160305345
0.4587786259541985

 ===== Epoch 271	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   3.0665004   1.9947059
 -0.3640846  -0.3725409   2.382712    1.7621897  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  3.5510068   2.0926814
 -0.3640846  -0.3725409   2.5457747   1.8741724  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 3 2
train:	 Loss = 1.6010,	 Acc = 0.3856,	 Loss Con1 = 1.3518,	 Loss Con2 = 2.9724
2923 0.16
5682 0.438
2912 0.503
376 0.412
69 0.507
6 0.333
0 0.0
0 0.0
0.45848535102266447
0.4587786259541985
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6539,	 Acc = 0.3997,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.0375
380 0.184
1718 0.458
815 0.398
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.4309160305343511
0.4587786259541985

 ===== Epoch 272	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   1.148589    2.9566712  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409   1.0782144   3.1408203  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 6
train:	 Loss = 1.6031,	 Acc = 0.3860,	 Loss Con1 = 1.3510,	 Loss Con2 = 2.7396
2925 0.161
5679 0.44
2912 0.501
377 0.419
69 0.449
6 0.167
0 0.0
0 0.0
0.458697334955214
0.4587786259541985
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6692,	 Acc = 0.4077,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.5541
380 0.192
1718 0.468
815 0.399
85 0.247
2 0.0
0 0.0
0 0.0
0 0.0
0.4389312977099237
0.4587786259541985

 ===== Epoch 273	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  3.4375434   2.3648214  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  3.4015946   2.4903886  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 6
train:	 Loss = 1.6056,	 Acc = 0.3824,	 Loss Con1 = 1.3518,	 Loss Con2 = 2.8131
2927 0.16
5681 0.436
2908 0.497
377 0.414
69 0.478
6 0.333
0 0.0
0 0.0
0.4545957305607787
0.4587786259541985
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6309,	 Acc = 0.4170,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.2561
380 0.192
1718 0.474
815 0.417
85 0.271
2 0.0
0 0.0
0 0.0
0 0.0
0.449618320610687
0.4587786259541985

 ===== Epoch 274	 =====
[-0.36602148 -0.3783333   2.7386353   2.461263   -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   3.1547287  -4.59908    -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 2
train:	 Loss = 1.6053,	 Acc = 0.3859,	 Loss Con1 = 1.3530,	 Loss Con2 = 2.9240
2923 0.16
5680 0.439
2914 0.505
377 0.398
68 0.471
6 0.333
0 0.0
0 0.0
0.4589275843007186
0.4587786259541985
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6514,	 Acc = 0.4203,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.3042
380 0.192
1718 0.477
815 0.426
85 0.247
2 0.0
0 0.0
0 0.0
0 0.0
0.4534351145038168
0.4587786259541985

 ===== Epoch 275	 =====
[ 1.5731994   2.537834   -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.0535581   3.126266  ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.0451815   3.0737793 ] 5 5
train:	 Loss = 1.6012,	 Acc = 0.3859,	 Loss Con1 = 1.3516,	 Loss Con2 = 2.9278
2922 0.161
5684 0.439
2913 0.502
374 0.406
69 0.551
6 0.333
0 0.0
0 0.0
0.4585452133539686
0.4587786259541985
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6719,	 Acc = 0.3947,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.3348
380 0.192
1718 0.458
815 0.377
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.42404580152671756
0.4587786259541985

 ===== Epoch 276	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 0
train:	 Loss = 1.6020,	 Acc = 0.3876,	 Loss Con1 = 1.3513,	 Loss Con2 = 2.9576
2926 0.161
5682 0.443
2910 0.502
375 0.403
69 0.536
6 0.333
0 0.0
0 0.0
0.46095996460959965
0.4587786259541985
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6958,	 Acc = 0.3990,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.5191
380 0.195
1718 0.464
815 0.377
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.42862595419847327
0.4587786259541985

 ===== Epoch 277	 =====
[-0.36602148 -0.3783333   1.0766557   1.5483416  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.0765158   1.5478896  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.6014,	 Acc = 0.3845,	 Loss Con1 = 1.3506,	 Loss Con2 = 2.7647
2926 0.161
5684 0.439
2909 0.495
375 0.424
68 0.515
6 0.333
0 0.0
0 0.0
0.4569785445697854
0.4587786259541985
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6572,	 Acc = 0.4197,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.1283
380 0.189
1718 0.473
815 0.439
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.4530534351145038
0.4587786259541985

 ===== Epoch 278	 =====
[ 2.6003602   1.6113433  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 4
train:	 Loss = 1.6018,	 Acc = 0.3887,	 Loss Con1 = 1.3513,	 Loss Con2 = 2.4044
2925 0.161
5681 0.44
2913 0.506
375 0.445
68 0.559
6 0.333
0 0.0
0 0.0
0.4623465664049541
0.4587786259541985
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6861,	 Acc = 0.4070,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.4785
380 0.192
1718 0.463
815 0.411
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.4381679389312977
0.4587786259541985

 ===== Epoch 279	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.6037,	 Acc = 0.3865,	 Loss Con1 = 1.3518,	 Loss Con2 = 2.7189
2923 0.161
5680 0.444
2915 0.495
376 0.407
68 0.515
6 0.333
0 0.0
0 0.0
0.4593698175787728
0.4587786259541985
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6563,	 Acc = 0.4047,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.2815
380 0.189
1718 0.47
815 0.384
85 0.247
2 0.0
0 0.0
0 0.0
0 0.0
0.43587786259541983
0.4587786259541985

 ===== Epoch 280	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.9617958   3.5619075 ] 2 2
train:	 Loss = 1.6038,	 Acc = 0.3891,	 Loss Con1 = 1.3516,	 Loss Con2 = 2.7856
2924 0.161
5684 0.446
2908 0.503
377 0.39
69 0.507
6 0.333
0 0.0
0 0.0
0.4627377266696152
0.4587786259541985
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6401,	 Acc = 0.4007,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.2566
380 0.195
1718 0.459
815 0.39
85 0.247
2 0.0
0 0.0
0 0.0
0 0.0
0.4305343511450382
0.4587786259541985

 ===== Epoch 281	 =====
[-0.36602148 -0.3783333   2.0774174   1.6253624  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   2.4155557   1.7426721  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 2
train:	 Loss = 1.6039,	 Acc = 0.3852,	 Loss Con1 = 1.3522,	 Loss Con2 = 2.8248
2924 0.159
5682 0.44
2910 0.5
377 0.406
69 0.522
6 0.167
0 0.0
0 0.0
0.45831490490933213
0.4587786259541985
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6428,	 Acc = 0.4110,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.2334
380 0.187
1718 0.469
815 0.422
85 0.153
2 0.0
0 0.0
0 0.0
0 0.0
0.4435114503816794
0.4587786259541985

 ===== Epoch 282	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   1.6758165   1.170105
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  1.6675868   1.1569124
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 1
train:	 Loss = 1.6040,	 Acc = 0.3846,	 Loss Con1 = 1.3532,	 Loss Con2 = 2.6261
2922 0.163
5681 0.442
2913 0.489
377 0.411
69 0.507
6 0.333
0 0.0
0 0.0
0.4563342913995136
0.4587786259541985
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6604,	 Acc = 0.4183,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.3542
380 0.192
1718 0.468
815 0.434
85 0.282
2 0.0
0 0.0
0 0.0
0 0.0
0.45114503816793894
0.4587786259541985

 ===== Epoch 283	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   0.77135545  1.4412676
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  0.75109035  1.2169263
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 5
train:	 Loss = 1.6037,	 Acc = 0.3869,	 Loss Con1 = 1.3518,	 Loss Con2 = 2.8529
2924 0.159
5682 0.446
2912 0.495
375 0.408
69 0.536
6 0.167
0 0.0
0 0.0
0.46041574524546663
0.4587786259541985
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6525,	 Acc = 0.4230,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.3185
380 0.189
1718 0.469
815 0.45
85 0.282
2 0.0
0 0.0
0 0.0
0 0.0
0.4568702290076336
0.4587786259541985

 ===== Epoch 284	 =====
[-0.36602148 -0.3783333   1.9125184   3.01853    -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.78604     3.0178175  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 0 0
train:	 Loss = 1.6023,	 Acc = 0.3860,	 Loss Con1 = 1.3519,	 Loss Con2 = 2.8692
2922 0.16
5683 0.443
2911 0.497
377 0.414
69 0.493
6 0.167
0 0.0
0 0.0
0.45909794384258235
0.4587786259541985
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6729,	 Acc = 0.4023,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.4631
380 0.189
1718 0.462
815 0.396
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.43320610687022904
0.4587786259541985

 ===== Epoch 285	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 0
train:	 Loss = 1.6019,	 Acc = 0.3851,	 Loss Con1 = 1.3524,	 Loss Con2 = 3.1528
2922 0.159
5680 0.441
2916 0.493
375 0.429
69 0.551
6 0.333
0 0.0
0 0.0
0.4581030289630776
0.4587786259541985
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.6795,	 Acc = 0.3973,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.5016
380 0.187
1718 0.464
815 0.373
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.42786259541984734
0.4587786259541985

 ===== Epoch 286	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 0
train:	 Loss = 1.6016,	 Acc = 0.3891,	 Loss Con1 = 1.3514,	 Loss Con2 = 3.2179
2921 0.159
5683 0.447
2914 0.501
376 0.412
68 0.529
6 0.333
0 0.0
0 0.0
0.4633580192328949
0.4587786259541985
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6888,	 Acc = 0.3863,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.5202
380 0.187
1718 0.451
815 0.363
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.4152671755725191
0.4587786259541985

 ===== Epoch 287	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.1765103   2.000422  ] [ 2.66811     1.6619711  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.24573     1.8639561 ] 5 6
train:	 Loss = 1.6003,	 Acc = 0.3873,	 Loss Con1 = 1.3507,	 Loss Con2 = 2.8795
2927 0.161
5680 0.441
2912 0.503
376 0.431
68 0.5
5 0.0
0 0.0
0 0.0
0.46067912841499836
0.4587786259541985
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6979,	 Acc = 0.3897,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.6403
380 0.189
1718 0.449
815 0.382
85 0.176
2 0.0
0 0.0
0 0.0
0 0.0
0.41870229007633586
0.4587786259541985

 ===== Epoch 288	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   1.730435    1.8346757
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  2.0587883   1.6548038
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 5 5
train:	 Loss = 1.6015,	 Acc = 0.3879,	 Loss Con1 = 1.3525,	 Loss Con2 = 2.8113
2924 0.161
5681 0.442
2912 0.504
376 0.402
69 0.551
6 0.5
0 0.0
0 0.0
0.4610791685095091
0.4587786259541985
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6540,	 Acc = 0.3833,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.1134
380 0.189
1718 0.441
815 0.372
85 0.2
2 0.0
0 0.0
0 0.0
0 0.0
0.4114503816793893
0.4587786259541985

 ===== Epoch 289	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   2.2414863   2.019155
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  2.2200058   1.9415358
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 2 5
train:	 Loss = 1.6040,	 Acc = 0.3856,	 Loss Con1 = 1.3524,	 Loss Con2 = 3.2599
2925 0.159
5678 0.441
2913 0.496
377 0.427
69 0.507
6 0.333
0 0.0
0 0.0
0.45891850049762245
0.4587786259541985
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6730,	 Acc = 0.4140,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.4295
380 0.182
1718 0.476
815 0.42
85 0.153
2 0.0
0 0.0
0 0.0
0 0.0
0.44770992366412216
0.4587786259541985

 ===== Epoch 290	 =====
[-0.36602148 -0.3783333   0.6591301   1.6842606  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   0.6021454   1.8649774  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.6025,	 Acc = 0.3869,	 Loss Con1 = 1.3533,	 Loss Con2 = 2.9346
2919 0.164
5682 0.445
2916 0.494
376 0.396
69 0.507
6 0.167
0 0.0
0 0.0
0.4587247209636424
0.4587786259541985
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6698,	 Acc = 0.4163,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.4493
380 0.195
1718 0.474
815 0.42
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.4484732824427481
0.4587786259541985

 ===== Epoch 291	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 0
train:	 Loss = 1.5999,	 Acc = 0.3892,	 Loss Con1 = 1.3530,	 Loss Con2 = 3.1565
2921 0.16
5682 0.447
2913 0.502
377 0.398
69 0.522
6 0.333
0 0.0
0 0.0
0.4631369514756273
0.4587786259541985
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6860,	 Acc = 0.3983,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.5538
380 0.189
1718 0.465
815 0.378
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.42862595419847327
0.4587786259541985

 ===== Epoch 292	 =====
[ 1.7183571   2.1505303  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.2907585   2.4308145 ] [ 1.3011383   2.340385   -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.2541635   2.6328893 ] 3 6
train:	 Loss = 1.5981,	 Acc = 0.3877,	 Loss Con1 = 1.3525,	 Loss Con2 = 3.4982
2926 0.16
5677 0.443
2913 0.503
377 0.406
69 0.536
6 0.5
0 0.0
0 0.0
0.4615129396151294
0.4587786259541985
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6614,	 Acc = 0.4003,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.2798
380 0.189
1718 0.464
815 0.384
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.4309160305343511
0.4587786259541985

 ===== Epoch 293	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   2.5909717   2.383669
 -0.3640846  -0.3725409   1.5513804   1.6128798  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  2.3621218   2.3971953
 -0.3640846  -0.3725409   1.6211835   1.6203451  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 0
train:	 Loss = 1.5990,	 Acc = 0.3874,	 Loss Con1 = 1.3530,	 Loss Con2 = 3.2149
2922 0.159
5681 0.444
2913 0.498
377 0.424
69 0.493
6 0.5
0 0.0
0 0.0
0.46108777360159187
0.4587786259541985
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6699,	 Acc = 0.4110,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.6609
380 0.192
1718 0.472
815 0.409
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.44274809160305345
0.4587786259541985

 ===== Epoch 294	 =====
[-0.36602148 -0.3783333   1.4917445   2.2687113  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333   1.6292711   2.3972316  -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 6
train:	 Loss = 1.6033,	 Acc = 0.3885,	 Loss Con1 = 1.3518,	 Loss Con2 = 3.3175
2923 0.16
5680 0.443
2914 0.504
376 0.423
69 0.551
6 0.333
0 0.0
0 0.0
0.46246545052515203
0.4587786259541985
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6856,	 Acc = 0.3850,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.6033
380 0.189
1718 0.443
815 0.373
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.4133587786259542
0.4587786259541985

 ===== Epoch 295	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   1.7235608   1.8235624
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  1.7871629   2.0326676
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.6018,	 Acc = 0.3881,	 Loss Con1 = 1.3510,	 Loss Con2 = 3.4042
2925 0.159
5682 0.445
2910 0.501
376 0.42
69 0.507
6 0.5
0 0.0
0 0.0
0.46223598363374985
0.4587786259541985
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6638,	 Acc = 0.4000,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.3062
380 0.187
1718 0.461
815 0.391
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.4309160305343511
0.4587786259541985

 ===== Epoch 296	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 3 0
train:	 Loss = 1.6024,	 Acc = 0.3862,	 Loss Con1 = 1.3526,	 Loss Con2 = 3.3882
2922 0.161
5682 0.441
2913 0.499
376 0.423
69 0.449
6 0.333
0 0.0
0 0.0
0.4588768516471369
0.4587786259541985
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6778,	 Acc = 0.4037,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.4625
380 0.187
1718 0.473
815 0.377
85 0.247
2 0.0
0 0.0
0 0.0
0 0.0
0.4351145038167939
0.4587786259541985

 ===== Epoch 297	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   0.960803    1.8502342
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  0.90734184  1.8726312
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 1 0
train:	 Loss = 1.6028,	 Acc = 0.3859,	 Loss Con1 = 1.3521,	 Loss Con2 = 3.0851
2923 0.161
5684 0.444
2911 0.493
375 0.403
69 0.507
6 0.333
0 0.0
0 0.0
0.458595909342178
0.4587786259541985
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 4
val:	 Loss = 1.7266,	 Acc = 0.4067,	 Loss Con1 = 1.3848,	 Loss Con2 = 3.3532
380 0.184
1718 0.469
815 0.399
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.4389312977099237
0.4587786259541985

 ===== Epoch 298	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   1.8797777   2.074721
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197  1.8861094   1.972654
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 3 5
train:	 Loss = 1.6038,	 Acc = 0.3849,	 Loss Con1 = 1.3511,	 Loss Con2 = 2.9164
2926 0.164
5682 0.435
2909 0.501
376 0.426
69 0.478
6 0.5
0 0.0
0 0.0
0.4562043795620438
0.4587786259541985
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 6
val:	 Loss = 1.6388,	 Acc = 0.4107,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.3012
380 0.195
1718 0.469
815 0.41
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.44198473282442746
0.4587786259541985

 ===== Epoch 299	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  1.3367901   3.3972619  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  1.3575729   3.4698117  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 6 6
train:	 Loss = 1.6071,	 Acc = 0.3858,	 Loss Con1 = 1.3517,	 Loss Con2 = 2.8687
2922 0.157
5681 0.441
2915 0.501
375 0.411
69 0.478
6 0.333
0 0.0
0 0.0
0.45954012823347334
0.4587786259541985
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6572,	 Acc = 0.4123,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.1639
380 0.187
1718 0.47
815 0.415
85 0.247
2 0.0
0 0.0
0 0.0
0 0.0
0.4450381679389313
0.4587786259541985

 ===== Epoch 300	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] 4 0
train:	 Loss = 1.6058,	 Acc = 0.3839,	 Loss Con1 = 1.3530,	 Loss Con2 = 3.2948
2922 0.16
5682 0.435
2912 0.504
377 0.401
69 0.493
6 0.333
0 0.0
0 0.0
0.4563342913995136
0.4587786259541985
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [-0.36602148 -0.3783333  -0.4209566  -0.33425197 -0.44088638 -0.36343393
  2.6101618   1.728615   -0.40141198 -0.40778467  2.5800178   2.9464927
 -0.38555372 -0.3798593 ] 3 1
val:	 Loss = 1.6720,	 Acc = 0.3943,	 Loss Con1 = 1.3848,	 Loss Con2 = 2.4475
380 0.187
1718 0.453
815 0.39
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.42442748091603055
0.4587786259541985
