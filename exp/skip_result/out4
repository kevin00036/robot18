(0.24300000000005184, array([-1.000000e+00, -1.000000e+00,  1.189783e+03,  1.790000e-01,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 4, array([  0.   ,   0.   , -15.851,   0.091,   0.   ,   0.   ,   0.   ,
         0.   ,   0.   ,   0.   ,   0.   ,   0.   ,   0.   ,   0.   ]), 1)
((0.236, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.364685e+03, -3.300000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), array([0., 0., 0.])), (0.24, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.543573e+03, -3.290000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), array([0., 0., 0.])))
(0.236, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.364685e+03, -3.300000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([0.   , 0.   , 0.   , 0.   , 5.823, 3.129, 0.   , 0.   , 0.   ,
       0.   , 0.   , 0.   , 0.   , 0.   ]))
(0.236, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.364685e+03, -3.300000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([ 0.   ,  0.   ,  0.   ,  0.   , -8.411, -0.011,  0.   ,  0.   ,
        0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ]))
(0.236, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.364685e+03, -3.300000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  6.2433e+01,
       -3.0000e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
        0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]))
(0.24, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.543573e+03, -3.290000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([   0.   ,    0.   ,    0.   ,    0.   , -173.065,    3.128,
          0.   ,    0.   ,    0.   ,    0.   ,    0.   ,    0.   ,
          0.   ,    0.   ]))
(0.24, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.543573e+03, -3.290000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([ 0.00000e+00,  0.00000e+00,  0.00000e+00,  0.00000e+00,
       -1.87299e+02, -1.20000e-02,  0.00000e+00,  0.00000e+00,
        0.00000e+00,  0.00000e+00,  0.00000e+00,  0.00000e+00,
        0.00000e+00,  0.00000e+00]))
(0.24, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.543573e+03, -3.290000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([ 0.00000e+00,  0.00000e+00,  0.00000e+00,  0.00000e+00,
       -1.16455e+02, -4.00000e-03,  0.00000e+00,  0.00000e+00,
        0.00000e+00,  0.00000e+00,  0.00000e+00,  0.00000e+00,
        0.00000e+00,  0.00000e+00]))
(0.24, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.543573e+03, -3.290000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([ 0.00000e+00,  0.00000e+00,  0.00000e+00,  0.00000e+00,
       -2.97931e+02, -5.00000e-03,  0.00000e+00,  0.00000e+00,
        0.00000e+00,  0.00000e+00,  0.00000e+00,  0.00000e+00,
        0.00000e+00,  0.00000e+00]))
(0.238, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.370508e+03,  2.799000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([  0.   ,   0.   ,   0.   ,   0.   , -14.234,  -3.14 ,   0.   ,
         0.   ,   0.   ,   0.   ,   0.   ,   0.   ,   0.   ,   0.   ]))
(0.238, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.370508e+03,  2.799000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([ 0.   ,  0.   ,  0.   ,  0.   , 56.61 , -3.132,  0.   ,  0.   ,
        0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ]))
(0.238, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.370508e+03,  2.799000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([   0.   ,    0.   ,    0.   ,    0.   , -124.866,   -3.133,
          0.   ,    0.   ,    0.   ,    0.   ,    0.   ,    0.   ,
          0.   ,    0.   ]))
14 1 14

 ===== Epoch 1	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038  0.9078604   1.5513011
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451 -0.20300324  0.12233283
 -0.00180886 -0.00223772] 1 3
train:	 Loss = 1.6322,	 Acc = 0.3776
10765 0.254
21098 0.402
10886 0.445
1400 0.449
243 0.379
24 0.125
0 0.0
0 0.0
0.41710498945053637
0.0
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 0
val:	 Loss = 1.4418,	 Acc = 0.4484
1367 0.356
6407 0.481
3023 0.433
308 0.338
8 0.0
0 0.0
0 0.0
0 0.0
0.4613174635747999
0.4613174635747999
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.5119,	 Acc1 = 0.3441,	 Acc2 = 0.3586

 ===== Epoch 2	 =====
[ 0.71786404  2.6840699  -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
  0.4850053   3.1184196 ] [-0.16195942  0.34940755  0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
  0.18525122  0.32205316] 6 6
train:	 Loss = 1.4893,	 Acc = 0.4563
10767 0.273
21092 0.501
10889 0.54
1401 0.531
243 0.473
24 0.292
0 0.0
0 0.0
0.5148147047460548
0.4613174635747999
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 1
val:	 Loss = 1.4377,	 Acc = 0.4728
1367 0.376
6407 0.517
3023 0.436
308 0.351
8 0.0
0 0.0
0 0.0
0 0.0
0.4863533757438949
0.4863533757438949
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.5031,	 Acc1 = 0.3652,	 Acc2 = 0.3839

 ===== Epoch 3	 =====
[ 2.2088628   1.708514   -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966  1.8216083   3.2991996  -0.4254193  -0.4215889
  2.8443394   1.8950212 ] [ 1.6532382e-02  1.5858607e-01  4.6474081e-03  3.7761789e-03
  2.1561917e-03  6.3325174e-04  1.1600148e-03  1.7411708e-03
 -3.8076878e+00 -6.0924611e+00  3.4734835e-03  4.9582324e-03
 -4.7939184e-01  2.4653338e-01] 1 4
train:	 Loss = 1.4685,	 Acc = 0.4649
10761 0.273
21100 0.512
10886 0.55
1402 0.566
243 0.49
24 0.125
0 0.0
0 0.0
0.5261922448373199
0.4863533757438949
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 1
val:	 Loss = 1.3978,	 Acc = 0.4957
1367 0.389
6407 0.524
3023 0.5
308 0.351
8 0.0
0 0.0
0 0.0
0 0.0
0.5106710445310897
0.5106710445310897
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
Testing:	 Loss = 1.4814,	 Acc1 = 0.3561,	 Acc2 = 0.3730

 ===== Epoch 4	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038  1.5116508   2.0322545
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 0 0
train:	 Loss = 1.4534,	 Acc = 0.4719
10763 0.272
21097 0.521
10886 0.562
1403 0.574
243 0.469
24 0.292
0 0.0
0 0.0
0.535910617181232
0.5106710445310897
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 0
val:	 Loss = 1.3698,	 Acc = 0.5099
1367 0.375
6407 0.536
3023 0.536
308 0.328
8 0.0
0 0.0
0 0.0
0 0.0
0.5289349476708394
0.5289349476708394
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 6
Testing:	 Loss = 1.4619,	 Acc1 = 0.3699,	 Acc2 = 0.3897

 ===== Epoch 5	 =====
[-0.36718738 -0.38160205  1.8869426   0.9914419  -0.44155708 -0.3651737
  0.8047052   3.718078   -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 1.4681835e-03  2.4594050e-03 -1.6732471e-03  3.9928827e-01
  2.1561917e-03  6.3325174e-04 -2.1761010e+00 -7.5810270e+00
  1.5405245e-03  6.3845129e-03  3.4734835e-03  4.9582324e-03
 -1.8088582e-03 -2.2377169e-03] 6 6
train:	 Loss = 1.4400,	 Acc = 0.4758
10768 0.272
21096 0.524
10884 0.569
1401 0.592
243 0.519
24 0.208
0 0.0
0 0.0
0.5408939610080837
0.5289349476708394
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3752,	 Acc = 0.5204
1367 0.393
6407 0.525
3023 0.587
308 0.351
8 0.0
0 0.0
0 0.0
0 0.0
0.5382721116355428
0.5382721116355428
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
Testing:	 Loss = 1.4535,	 Acc1 = 0.3757,	 Acc2 = 0.3966

 ===== Epoch 6	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 2 1
train:	 Loss = 1.4333,	 Acc = 0.4779
10765 0.274
21093 0.527
10890 0.57
1402 0.586
242 0.517
24 0.208
0 0.0
0 0.0
0.5431042168137649
0.5382721116355428
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3558,	 Acc = 0.5158
1367 0.388
6407 0.553
3023 0.514
308 0.328
8 0.0
0 0.0
0 0.0
0 0.0
0.5336548327518982
0.5382721116355428
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4364,	 Acc1 = 0.3829,	 Acc2 = 0.4053

 ===== Epoch 7	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966  1.5110937   2.1895056  -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.14189407 -0.15690453  0.00347348  0.00495823
 -0.00180886 -0.00223772] 2 5
train:	 Loss = 1.4238,	 Acc = 0.4805
10764 0.274
21098 0.53
10885 0.575
1402 0.59
243 0.543
24 0.208
0 0.0
0 0.0
0.5466837037917509
0.5382721116355428
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.4120,	 Acc = 0.5031
1367 0.383
6407 0.518
3023 0.544
308 0.334
8 0.0
0 0.0
0 0.0
0 0.0
0.5199056022983788
0.5382721116355428
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
Testing:	 Loss = 1.4985,	 Acc1 = 0.3627,	 Acc2 = 0.3810

 ===== Epoch 8	 =====
[ 0.21467297  0.87448937 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [-0.05658953  0.2597793   0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 0 6
train:	 Loss = 1.4271,	 Acc = 0.4773
10764 0.269
21094 0.525
10890 0.576
1401 0.585
243 0.539
24 0.25
0 0.0
0 0.0
0.5440687031974325
0.5382721116355428
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3613,	 Acc = 0.5192
1367 0.385
6407 0.545
3023 0.544
308 0.341
8 0.0
0 0.0
0 0.0
0 0.0
0.5380668992407142
0.5382721116355428
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
Testing:	 Loss = 1.4441,	 Acc1 = 0.3854,	 Acc2 = 0.4083

 ===== Epoch 9	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664   1.7516701   2.4681408
  6.3824897   1.3278606   5.843966    2.4948578  -0.4254193  -0.4215889
  6.0860023   2.5735228 ] [ 1.4681835e-03  2.4594050e-03  4.6474081e-03  3.7761789e-03
 -4.5790358e+00 -3.1829052e+00 -1.2567253e+01 -3.1495390e+00
 -1.0698684e+01 -4.7698197e+00  3.4734835e-03  4.9582324e-03
 -1.2183123e+01 -5.0131989e+00] 1 2
train:	 Loss = 1.4269,	 Acc = 0.4749
10764 0.268
21092 0.524
10890 0.571
1403 0.582
243 0.494
24 0.25
0 0.0
0 0.0
0.5410079638654464
0.5382721116355428
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 2
val:	 Loss = 1.4186,	 Acc = 0.4669
1367 0.394
6407 0.505
3023 0.436
308 0.308
8 0.0
0 0.0
0 0.0
0 0.0
0.47711881797660577
0.5382721116355428
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4908,	 Acc1 = 0.3782,	 Acc2 = 0.3996

 ===== Epoch 10	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664   0.38627747  1.6117122
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 1.4681835e-03  2.4594050e-03  4.6474081e-03  3.7761789e-03
 -5.0555192e-02  8.3578235e-01  9.8693857e+00  2.6761827e+00
  1.5405245e-03  6.3845129e-03  1.0181524e+01  3.1363444e+00
 -1.8088582e-03 -2.2377169e-03] 6 6
train:	 Loss = 1.4291,	 Acc = 0.4734
10765 0.27
21100 0.523
10882 0.565
1403 0.572
242 0.521
24 0.333
0 0.0
0 0.0
0.5383792457876437
0.5382721116355428
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 2
val:	 Loss = 1.3872,	 Acc = 0.4848
1367 0.38
6407 0.526
3023 0.459
308 0.341
8 0.875
0 0.0
0 0.0
0 0.0
0.4995895752103427
0.5382721116355428
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4563,	 Acc1 = 0.3765,	 Acc2 = 0.3976

 ===== Epoch 11	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966  1.6354147   3.3091297  -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 1.4681835e-03  2.4594050e-03  4.6474081e-03  3.7761789e-03
  2.1561917e-03  6.3325174e-04  1.1600148e-03  1.7411708e-03
 -3.4887059e+00 -6.1087904e+00  3.4734835e-03  4.9582324e-03
 -1.8088582e-03 -2.2377169e-03] 6 1
train:	 Loss = 1.4282,	 Acc = 0.4733
10762 0.271
21095 0.523
10890 0.565
1402 0.568
243 0.502
24 0.292
0 0.0
0 0.0
0.5379449693944256
0.5382721116355428
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 2
val:	 Loss = 1.4217,	 Acc = 0.4764
1367 0.375
6407 0.521
3023 0.449
308 0.286
8 0.0
0 0.0
0 0.0
0 0.0
0.49066283603529653
0.5382721116355428
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4870,	 Acc1 = 0.3798,	 Acc2 = 0.4016

 ===== Epoch 12	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664   1.6522582   1.4186829
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.08441805  0.20755078
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 2
train:	 Loss = 1.4216,	 Acc = 0.4776
10758 0.273
21100 0.528
10888 0.572
1403 0.561
243 0.477
24 0.208
0 0.0
0 0.0
0.5431398181710143
0.5382721116355428
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3711,	 Acc = 0.4828
1367 0.207
6407 0.518
3023 0.546
308 0.36
8 0.0
0 0.0
0 0.0
0 0.0
0.5214446952595937
0.5382721116355428
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4578,	 Acc1 = 0.3419,	 Acc2 = 0.3897

 ===== Epoch 13	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664   2.176085    1.0614679
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.42943937  0.11531045
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 2 2
train:	 Loss = 1.4196,	 Acc = 0.4771
10766 0.27
21093 0.528
10887 0.572
1403 0.569
243 0.465
24 0.292
0 0.0
0 0.0
0.5433580980683507
0.5382721116355428
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.4306,	 Acc = 0.4735
1367 0.388
6407 0.512
3023 0.445
308 0.321
8 0.875
0 0.0
0 0.0
0 0.0
0.485429919967166
0.5382721116355428
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
Testing:	 Loss = 1.4976,	 Acc1 = 0.3738,	 Acc2 = 0.3944

 ===== Epoch 14	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 3 1
train:	 Loss = 1.4185,	 Acc = 0.4784
10763 0.269
21096 0.529
10890 0.577
1400 0.558
243 0.473
24 0.25
0 0.0
0 0.0
0.545241137491457
0.5382721116355428
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3557,	 Acc = 0.5138
1367 0.383
6407 0.534
3023 0.55
308 0.331
8 0.0
0 0.0
0 0.0
0 0.0
0.5322183459880977
0.5382721116355428
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
Testing:	 Loss = 1.4444,	 Acc1 = 0.3800,	 Acc2 = 0.4018

 ===== Epoch 15	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038  2.1916695   2.1083236
 -0.38724938 -0.38149557] [ 1.4681835e-03  2.4594050e-03  4.6474081e-03  3.7761789e-03
  2.1561917e-03  6.3325174e-04  1.1600148e-03  1.7411708e-03
  1.5405245e-03  6.3845129e-03 -5.1847095e+00 -4.3169422e+00
 -1.8088582e-03 -2.2377169e-03] 4 1
train:	 Loss = 1.4168,	 Acc = 0.4778
10768 0.271
21091 0.526
10888 0.579
1402 0.565
243 0.469
24 0.292
0 0.0
0 0.0
0.5441036614360437
0.5382721116355428
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3649,	 Acc = 0.5099
1367 0.386
6407 0.528
3023 0.543
308 0.367
8 0.0
0 0.0
0 0.0
0 0.0
0.5272932485122102
0.5382721116355428
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4531,	 Acc1 = 0.3699,	 Acc2 = 0.3897

 ===== Epoch 16	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038  2.4237514   1.9071084
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451 -0.4566115   0.34869814
 -0.00180886 -0.00223772] 4 4
train:	 Loss = 1.4126,	 Acc = 0.4808
10762 0.275
21098 0.527
10886 0.581
1403 0.583
243 0.465
24 0.375
0 0.0
0 0.0
0.546472930409461
0.5382721116355428
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3570,	 Acc = 0.5050
1367 0.377
6407 0.538
3023 0.513
308 0.331
8 0.0
0 0.0
0 0.0
0 0.0
0.5228811820233942
0.5382721116355428
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
Testing:	 Loss = 1.4389,	 Acc1 = 0.3780,	 Acc2 = 0.3994

 ===== Epoch 17	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
  1.1979951   0.9781272 ] [1.4813714e+00 2.1101694e+00 4.6474081e-03 3.7761789e-03 2.1561917e-03
 6.3325174e-04 1.1600148e-03 1.7411708e-03 1.5405245e-03 6.3845129e-03
 3.4734835e-03 4.9582324e-03 2.2580887e-01 1.2016367e+00] 6 6
train:	 Loss = 1.4106,	 Acc = 0.4803
10762 0.274
21097 0.526
10887 0.583
1403 0.578
243 0.465
24 0.333
0 0.0
0 0.0
0.5463837879598265
0.5382721116355428
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3775,	 Acc = 0.5054
1367 0.378
6407 0.53
3023 0.526
308 0.344
8 0.875
0 0.0
0 0.0
0 0.0
0.5231890006156372
0.5382721116355428
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4666,	 Acc1 = 0.3825,	 Acc2 = 0.4048

 ===== Epoch 18	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664   3.5590656   2.4104538
 -0.36400968 -0.37184966  4.226086    1.8071951  -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.12834503 -0.01931061
  0.00116001  0.00174117  0.12030052 -0.09975337  0.00347348  0.00495823
 -0.00180886 -0.00223772] 0 3
train:	 Loss = 1.4125,	 Acc = 0.4786
10765 0.273
21093 0.524
10889 0.582
1403 0.577
242 0.471
24 0.375
0 0.0
0 0.0
0.544382039166741
0.5382721116355428
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3752,	 Acc = 0.5145
1367 0.393
6407 0.543
3023 0.53
308 0.331
8 0.0
0 0.0
0 0.0
0 0.0
0.5316027088036117
0.5382721116355428
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
Testing:	 Loss = 1.4406,	 Acc1 = 0.3990,	 Acc2 = 0.4247

 ===== Epoch 19	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966  2.0387385   3.291752   -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 1.4681835e-03  2.4594050e-03  4.6474081e-03  3.7761789e-03
  2.1561917e-03  6.3325174e-04  1.1600148e-03  1.7411708e-03
 -4.1796694e+00 -6.0802150e+00  3.4734835e-03  4.9582324e-03
 -1.8088582e-03 -2.2377169e-03] 4 1
train:	 Loss = 1.4118,	 Acc = 0.4826
10760 0.269
21094 0.53
10892 0.59
1403 0.585
243 0.473
24 0.417
0 0.0
0 0.0
0.5510161635369623
0.5382721116355428
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3347,	 Acc = 0.5422
1367 0.378
6407 0.555
3023 0.603
308 0.409
8 0.875
0 0.0
0 0.0
0 0.0
0.56525754155551
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
Testing:	 Loss = 1.4265,	 Acc1 = 0.3848,	 Acc2 = 0.4076

 ===== Epoch 20	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.579404    1.7416058  -0.4018844  -0.40971038  2.4199774   2.9769843
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
  0.2532172   0.05357144  0.00154052  0.00638451  0.47029287  0.06783748
 -0.00180886 -0.00223772] 3 3
train:	 Loss = 1.4117,	 Acc = 0.4786
10764 0.276
21096 0.52
10888 0.587
1401 0.572
243 0.461
24 0.333
0 0.0
0 0.0
0.5435041007963866
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3614,	 Acc = 0.5105
1367 0.39
6407 0.54
3023 0.519
308 0.338
8 0.875
0 0.0
0 0.0
0 0.0
0.5273958547096245
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
Testing:	 Loss = 1.4434,	 Acc1 = 0.3842,	 Acc2 = 0.4068

 ===== Epoch 21	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966  0.9530863   3.3414028  -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 1.4681835e-03  2.4594050e-03  4.6474081e-03  3.7761789e-03
  2.1561917e-03  6.3325174e-04  1.1600148e-03  1.7411708e-03
 -2.3197591e+00 -6.1618590e+00  3.4734835e-03  4.9582324e-03
 -1.8088582e-03 -2.2377169e-03] 1 1
train:	 Loss = 1.4088,	 Acc = 0.4833
10761 0.276
21099 0.529
10887 0.587
1402 0.585
243 0.465
24 0.375
0 0.0
0 0.0
0.5496062992125984
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3773,	 Acc = 0.5060
1367 0.386
6407 0.524
3023 0.536
308 0.383
8 0.0
0 0.0
0 0.0
0 0.0
0.5227785758259799
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
Testing:	 Loss = 1.4511,	 Acc1 = 0.3757,	 Acc2 = 0.3966

 ===== Epoch 22	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966  2.5216966   3.2743742   3.2301898   1.1194247
  3.6646519   2.9167032 ] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.18311511  0.13293353  0.5256504   0.28162694
 -0.19300887  0.2509757 ] 2 2
train:	 Loss = 1.4102,	 Acc = 0.4826
10763 0.276
21100 0.528
10883 0.587
1403 0.589
243 0.49
24 0.292
0 0.0
0 0.0
0.5488069414316703
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.4145,	 Acc = 0.4888
1367 0.386
6407 0.524
3023 0.473
308 0.351
8 0.875
0 0.0
0 0.0
0 0.0
0.5032833983172583
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
Testing:	 Loss = 1.4823,	 Acc1 = 0.3736,	 Acc2 = 0.3941

 ===== Epoch 23	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664   0.90398777  1.076999
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618 -0.19032058  0.6886964
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 6 6
train:	 Loss = 1.4109,	 Acc = 0.4791
10757 0.269
21100 0.526
10890 0.584
1402 0.569
243 0.461
24 0.208
0 0.0
0 0.0
0.5460649454826346
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3457,	 Acc = 0.5404
1367 0.379
6407 0.552
3023 0.599
308 0.435
8 0.875
0 0.0
0 0.0
0 0.0
0.5630002052123948
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
Testing:	 Loss = 1.4388,	 Acc1 = 0.3728,	 Acc2 = 0.3931

 ===== Epoch 24	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038  1.4939144   0.92802495
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.36780146  0.17682818
 -0.00180886 -0.00223772] 6 2
train:	 Loss = 1.4083,	 Acc = 0.4829
10762 0.277
21095 0.528
10891 0.586
1401 0.597
243 0.457
24 0.333
0 0.0
0 0.0
0.5487014916503239
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3521,	 Acc = 0.5137
1367 0.391
6407 0.539
3023 0.529
308 0.364
8 0.875
0 0.0
0 0.0
0 0.0
0.5308844654217115
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
Testing:	 Loss = 1.4555,	 Acc1 = 0.3646,	 Acc2 = 0.3832

 ===== Epoch 25	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 2.2750423e+00  2.3819456e+00  4.6474081e-03  3.7761789e-03
  2.1561917e-03  6.3325174e-04  1.1600148e-03  1.7411708e-03
  1.5405245e-03  6.3845129e-03  3.4734835e-03  4.9582324e-03
 -1.8088582e-03 -2.2377169e-03] 6 6
train:	 Loss = 1.4072,	 Acc = 0.4822
10761 0.274
21096 0.527
10890 0.59
1402 0.579
243 0.453
24 0.25
0 0.0
0 0.0
0.5487743277373347
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.4043,	 Acc = 0.4746
1367 0.397
6407 0.512
3023 0.447
308 0.308
8 0.375
0 0.0
0 0.0
0 0.0
0.485429919967166
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4746,	 Acc1 = 0.3769,	 Acc2 = 0.3981

 ===== Epoch 26	 =====
[ 2.1929116   1.0943686  -0.42025706 -0.3355664  -0.44155708 -0.3651737
  1.4039761   1.0455074  -0.4018844  -0.40971038  1.6890136   3.2346377
 -0.38724938 -0.38149557] [ 2.6521780e+00  5.1610399e-02  4.6474081e-03  3.7761789e-03
  2.1561917e-03  6.3325174e-04 -3.3498015e-02  8.4669605e-02
  1.5405245e-03  6.3845129e-03 -4.0349489e-01  8.8797227e-02
 -1.8088582e-03 -2.2377169e-03] 4 2
train:	 Loss = 1.4081,	 Acc = 0.4814
10766 0.275
21096 0.527
10889 0.585
1398 0.583
243 0.461
24 0.208
0 0.0
0 0.0
0.5474294205052006
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3348,	 Acc = 0.5275
1367 0.387
6407 0.542
3023 0.57
308 0.429
8 0.875
0 0.0
0 0.0
0 0.0
0.5471988508105889
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
Testing:	 Loss = 1.4199,	 Acc1 = 0.3872,	 Acc2 = 0.4105

 ===== Epoch 27	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664   2.8975906   2.9584796
 -0.36400968 -0.37184966  1.9310331   2.3061848  -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618 -0.5393366   0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 0 0
train:	 Loss = 1.4067,	 Acc = 0.4821
10762 0.271
21098 0.53
10887 0.586
1402 0.584
243 0.453
24 0.375
0 0.0
0 0.0
0.54974148689606
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.4205,	 Acc = 0.4724
1367 0.388
6407 0.507
3023 0.452
308 0.315
8 0.875
0 0.0
0 0.0
0 0.0
0.48430125179560846
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4969,	 Acc1 = 0.3749,	 Acc2 = 0.3956

 ===== Epoch 28	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038  2.7045138   1.5782933
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.01334213
 -0.00180886 -0.00223772] 2 0
train:	 Loss = 1.4086,	 Acc = 0.4821
10765 0.278
21095 0.527
10886 0.586
1403 0.579
243 0.473
24 0.333
0 0.0
0 0.0
0.5474725862530089
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3582,	 Acc = 0.5197
1367 0.4
6407 0.548
3023 0.526
308 0.383
8 0.875
0 0.0
0 0.0
0 0.0
0.5364252000820849
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
Testing:	 Loss = 1.4365,	 Acc1 = 0.3798,	 Acc2 = 0.4016

 ===== Epoch 29	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
  1.4117609   2.4006226 ] [ 2.1335416e+00  3.0816245e+00  4.6474081e-03  3.7761789e-03
  2.1561917e-03  6.3325174e-04  1.1600148e-03  1.7411708e-03
  1.5405245e-03  6.3845129e-03  3.4734835e-03  4.9582324e-03
 -1.0915205e-02  2.6430273e-01] 4 4
train:	 Loss = 1.4053,	 Acc = 0.4809
10757 0.275
21100 0.524
10890 0.588
1402 0.587
243 0.436
24 0.292
0 0.0
0 0.0
0.5467779791437654
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3484,	 Acc = 0.5229
1367 0.378
6407 0.543
3023 0.553
308 0.432
8 0.875
0 0.0
0 0.0
0 0.0
0.5431972091114303
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
Testing:	 Loss = 1.4439,	 Acc1 = 0.3631,	 Acc2 = 0.3815

 ===== Epoch 30	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664   1.1223131   1.3632147
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 1.4681835e-03  2.4594050e-03  4.6474081e-03  3.7761789e-03
 -3.2644410e+00 -1.9413999e+00  1.1600148e-03  1.7411708e-03
  1.5405245e-03  6.3845129e-03  3.4734835e-03  4.9582324e-03
 -1.8088582e-03 -2.2377169e-03] 2 1
train:	 Loss = 1.4053,	 Acc = 0.4819
10767 0.277
21095 0.528
10885 0.584
1402 0.577
243 0.424
24 0.25
0 0.0
0 0.0
0.5474456893221196
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3820,	 Acc = 0.4949
1367 0.385
6407 0.525
3023 0.484
308 0.451
8 0.875
0 0.0
0 0.0
0 0.0
0.5103632259388468
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
Testing:	 Loss = 1.4608,	 Acc1 = 0.3749,	 Acc2 = 0.3956

 ===== Epoch 31	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [3.1488128e+00 3.0295823e+00 4.6474081e-03 3.7761789e-03 2.1561917e-03
 6.3325174e-04 1.1600148e-03 1.7411708e-03 1.5405245e-03 6.3845129e-03
 3.4734835e-03 4.9582324e-03 4.4852204e+00 4.8843374e+00] 3 5
train:	 Loss = 1.4057,	 Acc = 0.4814
10763 0.276
21095 0.525
10888 0.588
1403 0.582
243 0.444
24 0.292
0 0.0
0 0.0
0.547113184560069
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3655,	 Acc = 0.4928
1367 0.369
6407 0.525
3023 0.49
308 0.39
8 0.875
0 0.0
0 0.0
0 0.0
0.5100554073466037
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4458,	 Acc1 = 0.3780,	 Acc2 = 0.3994

 ===== Epoch 32	 =====
[-0.36718738 -0.38160205  0.9990085   2.6450984  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 7.0861816e-01  1.9453690e+00 -2.7495341e+00 -3.1651437e+00
  2.1561917e-03  6.3325174e-04  1.1600148e-03  1.7411708e-03
  1.5405245e-03  6.3845129e-03  3.4734835e-03  4.9582324e-03
 -1.8088582e-03 -2.2377169e-03] 5 6
train:	 Loss = 1.4052,	 Acc = 0.4810
10764 0.269
21096 0.528
10887 0.588
1402 0.582
243 0.432
24 0.333
0 0.0
0 0.0
0.5488529656484012
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3707,	 Acc = 0.5032
1367 0.38
6407 0.536
3023 0.502
308 0.357
8 0.875
0 0.0
0 0.0
0 0.0
0.5204186332854505
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
Testing:	 Loss = 1.4513,	 Acc1 = 0.3827,	 Acc2 = 0.4051

 ===== Epoch 33	 =====
[ 0.15451553  1.0514038  -0.42025706 -0.3355664  -0.44155708 -0.3651737
  3.1521785   1.0818499  -0.4018844  -0.40971038  3.5335946   1.4138858
 -0.38724938 -0.38149557] [1.5422124e-01 4.8529559e-01 4.6474081e-03 3.7761789e-03 2.1561917e-03
 6.3325174e-04 1.0640105e+00 5.2004385e-01 1.5405245e-03 6.3845129e-03
 7.3736525e-01 3.8223374e-01 9.8851242e+00 6.2081547e+00] 2 2
train:	 Loss = 1.4050,	 Acc = 0.4809
10765 0.275
21090 0.525
10891 0.587
1403 0.582
243 0.416
24 0.292
0 0.0
0 0.0
0.5466107990847225
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3424,	 Acc = 0.5163
1367 0.377
6407 0.537
3023 0.547
308 0.383
8 0.875
0 0.0
0 0.0
0 0.0
0.5358095628975991
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
Testing:	 Loss = 1.4325,	 Acc1 = 0.3815,	 Acc2 = 0.4036

 ===== Epoch 34	 =====
[ 0.21165934  1.5290723  -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00107493  0.00824188  0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 0 0
train:	 Loss = 1.4036,	 Acc = 0.4795
10767 0.274
21096 0.523
10886 0.585
1400 0.589
243 0.44
24 0.292
0 0.0
0 0.0
0.5452465154982318
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3884,	 Acc = 0.4788
1367 0.26
6407 0.523
3023 0.5
308 0.331
8 0.0
0 0.0
0 0.0
0 0.0
0.5094397701621178
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4645,	 Acc1 = 0.3441,	 Acc2 = 0.3986

 ===== Epoch 35	 =====
[-0.36718738 -0.38160205  0.58333606  1.4700723  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.05539637  0.00618784  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 0 0
train:	 Loss = 1.4058,	 Acc = 0.4803
10761 0.276
21101 0.525
10885 0.584
1402 0.59
243 0.407
24 0.208
0 0.0
0 0.0
0.5457138612390432
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3721,	 Acc = 0.4866
1367 0.366
6407 0.504
3023 0.512
308 0.393
8 0.875
0 0.0
0 0.0
0 0.0
0.5034886107120871
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
Testing:	 Loss = 1.4552,	 Acc1 = 0.3654,	 Acc2 = 0.3842

 ===== Epoch 36	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038  0.8882364   1.2838322
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.18077633 -0.1501439
 -0.00180886 -0.00223772] 2 5
train:	 Loss = 1.4054,	 Acc = 0.4787
10765 0.275
21096 0.524
10885 0.581
1403 0.578
243 0.465
24 0.25
0 0.0
0 0.0
0.5439957207809575
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3695,	 Acc = 0.4964
1367 0.391
6407 0.525
3023 0.492
308 0.406
8 0.875
0 0.0
0 0.0
0 0.0
0.5112866817155757
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
Testing:	 Loss = 1.4421,	 Acc1 = 0.3895,	 Acc2 = 0.4133

 ===== Epoch 37	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 1.4681835e-03  2.4594050e-03  4.6660056e+00  2.6107795e+00
  2.1561917e-03  6.3325174e-04  1.1600148e-03  1.7411708e-03
  1.5405245e-03  6.3845129e-03  3.4734835e-03  4.9582324e-03
 -1.8088582e-03 -2.2377169e-03] 3 1
train:	 Loss = 1.4069,	 Acc = 0.4783
10767 0.273
21093 0.524
10888 0.581
1401 0.591
243 0.432
24 0.167
0 0.0
0 0.0
0.5440577728907249
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3540,	 Acc = 0.5120
1367 0.385
6407 0.53
3023 0.541
308 0.422
8 0.875
0 0.0
0 0.0
0 0.0
0.5298584034475682
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4405,	 Acc1 = 0.3753,	 Acc2 = 0.3961

 ===== Epoch 38	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966  1.2664648   2.991365   -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 0 0
train:	 Loss = 1.4052,	 Acc = 0.4807
10767 0.271
21092 0.526
10888 0.587
1402 0.6
243 0.428
24 0.25
0 0.0
0 0.0
0.547772593539184
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3684,	 Acc = 0.5121
1367 0.391
6407 0.541
3023 0.52
308 0.354
8 0.875
0 0.0
0 0.0
0 0.0
0.5290375538682537
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
Testing:	 Loss = 1.4617,	 Acc1 = 0.3716,	 Acc2 = 0.3917

 ===== Epoch 39	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038  1.9750599   1.8531238
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.37752607  0.3235464
 -0.00180886 -0.00223772] 2 2
train:	 Loss = 1.4031,	 Acc = 0.4818
10770 0.277
21091 0.525
10886 0.586
1402 0.601
243 0.469
24 0.292
0 0.0
0 0.0
0.547227010640195
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.4063,	 Acc = 0.4688
1367 0.358
6407 0.485
3023 0.493
308 0.386
8 0.875
0 0.0
0 0.0
0 0.0
0.48440385799302277
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
Testing:	 Loss = 1.5095,	 Acc1 = 0.3406,	 Acc2 = 0.3544

 ===== Epoch 40	 =====
[-0.36718738 -0.38160205  1.5571734   1.1071298  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.20689858  0.16535734  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 2
train:	 Loss = 1.4016,	 Acc = 0.4808
10765 0.275
21093 0.525
10889 0.587
1402 0.581
243 0.444
24 0.25
0 0.0
0 0.0
0.5467296662803482
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3461,	 Acc = 0.5076
1367 0.402
6407 0.531
3023 0.508
308 0.477
8 0.875
0 0.0
0 0.0
0 0.0
0.5224707572337369
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4284,	 Acc1 = 0.3703,	 Acc2 = 0.3902

 ===== Epoch 41	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966  1.4005247   0.87127864 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117 -0.06422015  0.1288513   0.00347348  0.00495823
 -0.00180886 -0.00223772] 2 1
train:	 Loss = 1.4019,	 Acc = 0.4809
10764 0.272
21097 0.528
10885 0.585
1403 0.577
243 0.428
24 0.333
0 0.0
0 0.0
0.5477237608463092
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3449,	 Acc = 0.5199
1367 0.384
6407 0.543
3023 0.546
308 0.383
8 0.875
0 0.0
0 0.0
0 0.0
0.5389903550174431
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
Testing:	 Loss = 1.4367,	 Acc1 = 0.3842,	 Acc2 = 0.4068

 ===== Epoch 42	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038  0.89314306  2.1377697
 -0.38724938 -0.38149557] [ 9.6314087e+00  2.8734553e+00  4.6474081e-03  3.7761789e-03
  2.1561917e-03  6.3325174e-04  1.1600148e-03  1.7411708e-03
  1.1108101e+01 -7.0313730e+00  2.7353844e-01  1.8521208e-01
  8.3671255e+00  4.4090066e+00] 4 2
train:	 Loss = 1.4035,	 Acc = 0.4803
10764 0.273
21093 0.528
10890 0.581
1402 0.591
243 0.42
24 0.208
0 0.0
0 0.0
0.5467728515392845
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3396,	 Acc = 0.5340
1367 0.38
6407 0.55
3023 0.578
308 0.445
8 0.875
0 0.0
0 0.0
0 0.0
0.5555099528011492
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
Testing:	 Loss = 1.4283,	 Acc1 = 0.3833,	 Acc2 = 0.4058

 ===== Epoch 43	 =====
[ 1.9885093   2.2089286  -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
  1.4469489   2.701888  ] [ 0.21498628  0.17882472  0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.0109152   0.22432166] 6 4
train:	 Loss = 1.3987,	 Acc = 0.4807
10764 0.27
21097 0.527
10889 0.588
1400 0.583
242 0.438
24 0.208
0 0.0
0 0.0
0.5480803518364435
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3835,	 Acc = 0.4886
1367 0.389
6407 0.527
3023 0.466
308 0.344
8 0.875
0 0.0
0 0.0
0 0.0
0.5025651549353581
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
Testing:	 Loss = 1.4531,	 Acc1 = 0.3846,	 Acc2 = 0.4073

 ===== Epoch 44	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038  2.3765812   1.9537314
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.6161716  -0.09564856
 -0.00180886 -0.00223772] 6 2
train:	 Loss = 1.3977,	 Acc = 0.4821
10762 0.274
21101 0.528
10884 0.585
1402 0.603
243 0.477
24 0.167
0 0.0
0 0.0
0.5486420633505675
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3848,	 Acc = 0.5021
1367 0.386
6407 0.533
3023 0.498
308 0.403
8 0.875
0 0.0
0 0.0
0 0.0
0.518366509337164
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
Testing:	 Loss = 1.4607,	 Acc1 = 0.3712,	 Acc2 = 0.3912

 ===== Epoch 45	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038  1.3346648   2.6064537
 -0.38724938 -0.38149557] [ 1.4681835e-03  2.4594050e-03  4.6474081e-03  3.7761789e-03
  2.1561917e-03  6.3325174e-04  1.1600148e-03  1.7411708e-03
  1.5405245e-03  6.3845129e-03  7.2299831e-02  7.3016548e-01
 -1.8088582e-03 -2.2377169e-03] 6 6
train:	 Loss = 1.3953,	 Acc = 0.4868
10762 0.278
21099 0.532
10886 0.593
1402 0.596
243 0.461
24 0.208
0 0.0
0 0.0
0.5534260414809532
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3582,	 Acc = 0.5101
1367 0.396
6407 0.536
3023 0.514
308 0.432
8 0.875
0 0.0
0 0.0
0 0.0
0.5261645803406526
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4383,	 Acc1 = 0.3794,	 Acc2 = 0.4011

 ===== Epoch 46	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 0 1
train:	 Loss = 1.3947,	 Acc = 0.4832
10761 0.275
21096 0.527
10891 0.589
1401 0.602
243 0.473
24 0.167
0 0.0
0 0.0
0.5496954390135196
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3510,	 Acc = 0.5084
1367 0.391
6407 0.536
3023 0.512
308 0.403
8 0.875
0 0.0
0 0.0
0 0.0
0.5248306997742663
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4336,	 Acc1 = 0.3920,	 Acc2 = 0.4163

 ===== Epoch 47	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966  1.5151051   3.254514   -0.4254193  -0.4215889
  2.8311431   2.363947  ] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117 -0.00533167 -0.03851997  0.00347348  0.00495823
 -0.0564367   0.02885867] 2 3
train:	 Loss = 1.3974,	 Acc = 0.4826
10760 0.27
21095 0.529
10892 0.59
1403 0.591
242 0.479
24 0.25
0 0.0
0 0.0
0.5507190396957452
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3660,	 Acc = 0.4958
1367 0.389
6407 0.527
3023 0.488
308 0.396
8 0.875
0 0.0
0 0.0
0 0.0
0.510773650728504
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4405,	 Acc1 = 0.3831,	 Acc2 = 0.4056

 ===== Epoch 48	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
  2.2993696   3.3437135 ] [ 1.4681835e-03  2.4594050e-03  4.6474081e-03  3.7761789e-03
  2.1561917e-03  6.3325174e-04  1.1600148e-03  1.7411708e-03
  1.5405245e-03  6.3845129e-03  3.4734835e-03  4.9582324e-03
 -7.3349446e-01  3.7091893e-01] 3 6
train:	 Loss = 1.3954,	 Acc = 0.4838
10759 0.274
21096 0.528
10891 0.59
1403 0.607
243 0.49
24 0.25
0 0.0
0 0.0
0.5509106575155243
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3574,	 Acc = 0.5003
1367 0.38
6407 0.533
3023 0.498
308 0.38
8 0.875
0 0.0
0 0.0
0 0.0
0.517135234968192
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
Testing:	 Loss = 1.4366,	 Acc1 = 0.3792,	 Acc2 = 0.4008

 ===== Epoch 49	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038  2.8924437   1.12924
  3.4469278   3.1131804 ] [ 1.4681835e-03  2.4594050e-03  4.6474081e-03  3.7761789e-03
  2.1561917e-03  6.3325174e-04  1.1600148e-03  1.7411708e-03
  4.5095840e+00  5.9868464e+00 -6.5739422e+00 -2.6443541e+00
  2.8291863e-01 -9.7511035e-01] 5 5
train:	 Loss = 1.3933,	 Acc = 0.4837
10766 0.274
21094 0.529
10887 0.59
1402 0.591
243 0.49
24 0.167
0 0.0
0 0.0
0.5506389301634472
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3816,	 Acc = 0.4963
1367 0.379
6407 0.528
3023 0.493
308 0.37
8 0.875
0 0.0
0 0.0
0 0.0
0.5127231684793762
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4641,	 Acc1 = 0.3753,	 Acc2 = 0.3961

 ===== Epoch 50	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 0 1
train:	 Loss = 1.3951,	 Acc = 0.4818
10765 0.273
21095 0.527
10887 0.586
1403 0.597
242 0.459
24 0.167
0 0.0
0 0.0
0.5483938070191079
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3819,	 Acc = 0.4935
1367 0.383
6407 0.525
3023 0.484
308 0.409
8 0.875
0 0.0
0 0.0
0 0.0
0.5090293453724605
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
Testing:	 Loss = 1.4657,	 Acc1 = 0.3701,	 Acc2 = 0.3899

 ===== Epoch 51	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664   0.70669204  2.0576763
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00614843 -0.32844043
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 5 5
train:	 Loss = 1.3940,	 Acc = 0.4846
10767 0.275
21092 0.531
10887 0.587
1403 0.595
243 0.481
24 0.25
0 0.0
0 0.0
0.5514576956224554
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3770,	 Acc = 0.5000
1367 0.388
6407 0.532
3023 0.492
308 0.403
8 0.875
0 0.0
0 0.0
0 0.0
0.5158013544018059
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4526,	 Acc1 = 0.3751,	 Acc2 = 0.3959

 ===== Epoch 52	 =====
[ 2.3324544   2.1179442  -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
  2.1854491   2.3508484 ] [ 0.022522   -0.00910553  0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
  0.34913474 -0.00223772] 0 0
train:	 Loss = 1.3943,	 Acc = 0.4862
10764 0.277
21098 0.531
10886 0.594
1402 0.591
242 0.496
24 0.125
0 0.0
0 0.0
0.553132057530013
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3573,	 Acc = 0.5078
1367 0.394
6407 0.534
3023 0.513
308 0.412
8 0.875
0 0.0
0 0.0
0 0.0
0.5238046378001231
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
Testing:	 Loss = 1.4479,	 Acc1 = 0.3726,	 Acc2 = 0.3929

 ===== Epoch 53	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 3 1
train:	 Loss = 1.3950,	 Acc = 0.4844
10766 0.276
21094 0.527
10887 0.595
1402 0.593
243 0.494
24 0.167
0 0.0
0 0.0
0.5510846953937593
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3562,	 Acc = 0.5256
1367 0.372
6407 0.544
3023 0.566
308 0.422
8 0.875
0 0.0
0 0.0
0 0.0
0.5471988508105889
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4370,	 Acc1 = 0.3825,	 Acc2 = 0.4048

 ===== Epoch 54	 =====
[-0.36718738 -0.38160205  1.9235826   3.0715559   2.1573496   0.76637715
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.50316834 -0.07339691  0.13553204 -0.0068457
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 2 3
train:	 Loss = 1.3952,	 Acc = 0.4857
10763 0.273
21092 0.531
10892 0.594
1402 0.6
243 0.494
24 0.208
0 0.0
0 0.0
0.5537099218494637
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3600,	 Acc = 0.5128
1367 0.388
6407 0.54
3023 0.522
308 0.399
8 0.875
0 0.0
0 0.0
0 0.0
0.5303714344346399
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4341,	 Acc1 = 0.3941,	 Acc2 = 0.4187

 ===== Epoch 55	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 6 1
train:	 Loss = 1.3943,	 Acc = 0.4866
10759 0.278
21101 0.531
10887 0.593
1402 0.6
243 0.473
24 0.292
0 0.0
0 0.0
0.5532578661199751
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.4069,	 Acc = 0.4766
1367 0.392
6407 0.499
3023 0.477
308 0.377
8 0.875
0 0.0
0 0.0
0 0.0
0.48850810588959576
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
Testing:	 Loss = 1.4787,	 Acc1 = 0.3796,	 Acc2 = 0.4013

 ===== Epoch 56	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966  2.3297734   3.1130092  -0.4254193  -0.4215889
  3.9131634   2.3141727 ] [ 1.4681835e-03  2.4594050e-03  4.6474081e-03  3.7761789e-03
  2.1561917e-03  6.3325174e-04  1.1600148e-03  1.7411708e-03
 -4.6782627e+00 -5.7862945e+00  6.3310223e+00  2.9393229e+00
 -8.8247877e-01  1.6591979e+00] 6 6
train:	 Loss = 1.3921,	 Acc = 0.4840
10766 0.271
21098 0.529
10882 0.591
1403 0.61
243 0.469
24 0.333
0 0.0
0 0.0
0.5520653789004457
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3787,	 Acc = 0.4926
1367 0.379
6407 0.517
3023 0.5
308 0.409
8 0.875
0 0.0
0 0.0
0 0.0
0.5085163143853889
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4648,	 Acc1 = 0.3714,	 Acc2 = 0.3914

 ===== Epoch 57	 =====
[-0.36718738 -0.38160205  3.971815   -4.5320883   4.0718045   1.1834979
  2.859026    3.6370063  -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 1.4681835e-03  2.4594050e-03 -7.2298747e-01  7.6197948e+00
 -1.0392973e+00  5.0492901e-02 -3.1706399e-01  6.3937493e-02
  1.5405245e-03  6.3845129e-03  3.4734835e-03  4.9582324e-03
 -1.8088582e-03 -2.2377169e-03] 3 3
train:	 Loss = 1.3936,	 Acc = 0.4853
10765 0.274
21098 0.53
10885 0.593
1401 0.607
243 0.457
24 0.292
0 0.0
0 0.0
0.5529701940506968
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3633,	 Acc = 0.5018
1367 0.385
6407 0.526
3023 0.509
308 0.438
8 0.875
0 0.0
0 0.0
0 0.0
0.5181612969423354
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
Testing:	 Loss = 1.4534,	 Acc1 = 0.3623,	 Acc2 = 0.3805

 ===== Epoch 58	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 0 1
train:	 Loss = 1.3934,	 Acc = 0.4855
10761 0.277
21101 0.528
10887 0.593
1401 0.611
242 0.471
24 0.208
0 0.0
0 0.0
0.5520427871044421
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3905,	 Acc = 0.5066
1367 0.402
6407 0.53
3023 0.509
308 0.461
8 0.875
0 0.0
0 0.0
0 0.0
0.5213420890621794
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4586,	 Acc1 = 0.3813,	 Acc2 = 0.4033

 ===== Epoch 59	 =====
[-0.36718738 -0.38160205  0.44857857  1.9328238  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.08681348  0.36311337  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 6 4
train:	 Loss = 1.3910,	 Acc = 0.4870
10767 0.281
21092 0.53
10887 0.589
1403 0.623
243 0.49
24 0.167
0 0.0
0 0.0
0.5527950310559007
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3749,	 Acc = 0.4951
1367 0.383
6407 0.52
3023 0.498
308 0.442
8 0.875
0 0.0
0 0.0
0 0.0
0.5108762569259183
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4532,	 Acc1 = 0.3712,	 Acc2 = 0.3912

 ===== Epoch 60	 =====
[-0.36718738 -0.38160205  1.4488782   1.6288595  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.11209366  0.3365851   0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 6 2
train:	 Loss = 1.3902,	 Acc = 0.4860
10764 0.276
21095 0.532
10888 0.588
1402 0.621
243 0.469
24 0.292
0 0.0
0 0.0
0.5532212052775467
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3919,	 Acc = 0.4937
1367 0.388
6407 0.534
3023 0.465
308 0.393
8 0.875
0 0.0
0 0.0
0 0.0
0.5086189205828032
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4613,	 Acc1 = 0.3813,	 Acc2 = 0.4033

 ===== Epoch 61	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 6 1
train:	 Loss = 1.3911,	 Acc = 0.4868
10766 0.278
21095 0.529
10886 0.597
1402 0.615
243 0.461
24 0.208
0 0.0
0 0.0
0.5535809806835067
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3870,	 Acc = 0.4984
1367 0.398
6407 0.527
3023 0.492
308 0.396
8 0.875
0 0.0
0 0.0
0 0.0
0.5125179560845475
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4511,	 Acc1 = 0.3870,	 Acc2 = 0.4103

 ===== Epoch 62	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664   1.669846    2.5391402
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 1.4681835e-03  2.4594050e-03  4.6474081e-03  3.7761789e-03
 -1.7115192e-01  7.2929740e-02  1.1098176e+01  3.1633875e+00
  1.1328934e+01  4.9254675e+00  1.1804167e+01  3.1195767e+00
 -1.8088582e-03 -2.2377169e-03] 1 2
train:	 Loss = 1.3916,	 Acc = 0.4852
10760 0.275
21100 0.528
10887 0.592
1402 0.628
243 0.469
24 0.25
0 0.0
0 0.0
0.5522640836700737
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.4284,	 Acc = 0.4758
1367 0.4
6407 0.519
3023 0.429
308 0.357
8 0.875
0 0.0
0 0.0
0 0.0
0.48645598194130923
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4999,	 Acc1 = 0.3720,	 Acc2 = 0.3921

 ===== Epoch 63	 =====
[-0.36718738 -0.38160205  4.3418894   2.2458618  -0.44155708 -0.3651737
  2.7457108   3.3965867  -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 1.4681835e-03  2.4594050e-03 -8.7467587e-01  3.5127744e-02
  8.3154917e+00  1.3568158e+00  4.7062287e-01  7.9486571e-02
  1.5405245e-03  6.3845129e-03  3.4734835e-03  4.9582324e-03
 -1.8088582e-03 -2.2377169e-03] 3 2
train:	 Loss = 1.3891,	 Acc = 0.4858
10761 0.273
21098 0.529
10888 0.595
1402 0.618
243 0.486
24 0.25
0 0.0
0 0.0
0.5537364433219433
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3705,	 Acc = 0.5026
1367 0.345
6407 0.526
3023 0.534
308 0.383
8 0.875
0 0.0
0 0.0
0 0.0
0.5246254873794377
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4490,	 Acc1 = 0.3901,	 Acc2 = 0.4140

 ===== Epoch 64	 =====
[ 1.4960904   2.7801092  -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
  1.1192627   3.1315181 ] [-0.34969333  0.3522988   0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.05974763  0.57082427] 1 6
train:	 Loss = 1.3892,	 Acc = 0.4870
10763 0.275
21096 0.531
10888 0.592
1402 0.621
243 0.543
24 0.208
0 0.0
0 0.0
0.5547796630315277
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3679,	 Acc = 0.5221
1367 0.396
6407 0.542
3023 0.55
308 0.377
8 0.875
0 0.0
0 0.0
0 0.0
0.5397085983993434
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4513,	 Acc1 = 0.3800,	 Acc2 = 0.4018

 ===== Epoch 65	 =====
[ 0.7068154   0.8921809  -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
  0.9793925   1.5911365 ] [-1.1399540e+00 -1.4547230e+00  4.6474081e-03  3.7761789e-03
  2.1561917e-03  6.3325174e-04  1.1600148e-03  1.7411708e-03
  1.5405245e-03  6.3845129e-03  3.4734835e-03  4.9582324e-03
  6.4659170e-03 -1.0728419e+00] 5 5
train:	 Loss = 1.3902,	 Acc = 0.4860
10759 0.277
21104 0.528
10884 0.595
1402 0.608
243 0.535
24 0.167
0 0.0
0 0.0
0.5528121936001427
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3743,	 Acc = 0.4906
1367 0.383
6407 0.513
3023 0.5
308 0.396
8 0.875
0 0.0
0 0.0
0 0.0
0.5056433408577878
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
Testing:	 Loss = 1.4610,	 Acc1 = 0.3660,	 Acc2 = 0.3849

 ===== Epoch 66	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664   2.208585    2.0621138
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.1419216  -0.03925447
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 2 3
train:	 Loss = 1.3916,	 Acc = 0.4855
10761 0.272
21100 0.53
10886 0.593
1402 0.621
243 0.506
24 0.167
0 0.0
0 0.0
0.5537661565889169
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3781,	 Acc = 0.4948
1367 0.391
6407 0.531
3023 0.476
308 0.39
8 0.875
0 0.0
0 0.0
0 0.0
0.5094397701621178
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4545,	 Acc1 = 0.3747,	 Acc2 = 0.3954

 ===== Epoch 67	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  1.280513    2.557914   -0.4018844  -0.40971038  2.4105434   2.0101697
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.00829099 -0.01899094  0.00154052  0.00638451  0.4964765   0.00495823
 -0.00180886 -0.00223772] 0 0
train:	 Loss = 1.3858,	 Acc = 0.4888
10766 0.28
21092 0.531
10888 0.597
1403 0.632
243 0.477
24 0.083
0 0.0
0 0.0
0.5556612184249629
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.4011,	 Acc = 0.4914
1367 0.385
6407 0.519
3023 0.49
308 0.403
8 0.875
0 0.0
0 0.0
0 0.0
0.506361584239688
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
Testing:	 Loss = 1.4770,	 Acc1 = 0.3755,	 Acc2 = 0.3964

 ===== Epoch 68	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664   2.5259404   2.2484868
 -0.36400968 -0.37184966  1.6892681   1.3479257  -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618 -0.01301777 -0.03676149
  0.00116001  0.00174117 -0.01416388  0.05945345  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 3
train:	 Loss = 1.3889,	 Acc = 0.4855
10762 0.272
21094 0.529
10891 0.595
1403 0.612
242 0.508
24 0.125
0 0.0
0 0.0
0.5536340405301005
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3925,	 Acc = 0.4874
1367 0.396
6407 0.515
3023 0.477
308 0.412
8 0.875
0 0.0
0 0.0
0 0.0
0.5002052123948286
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4560,	 Acc1 = 0.3757,	 Acc2 = 0.3966

 ===== Epoch 69	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664   1.623199    2.2440495
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.11237112  0.19757888
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 6 4
train:	 Loss = 1.3875,	 Acc = 0.4868
10765 0.277
21097 0.53
10885 0.594
1403 0.619
243 0.498
23 0.261
0 0.0
0 0.0
0.5538616980178895
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3954,	 Acc = 0.4871
1367 0.379
6407 0.525
3023 0.473
308 0.305
8 0.875
0 0.0
0 0.0
0 0.0
0.5022573363431151
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4840,	 Acc1 = 0.3773,	 Acc2 = 0.3986

 ===== Epoch 70	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 1 1
train:	 Loss = 1.3880,	 Acc = 0.4886
10763 0.277
21096 0.532
10888 0.597
1402 0.623
243 0.519
24 0.25
0 0.0
0 0.0
0.5562951297061183
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.4106,	 Acc = 0.4723
1367 0.21
6407 0.529
3023 0.485
308 0.338
8 0.0
0 0.0
0 0.0
0 0.0
0.5091319515698748
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4951,	 Acc1 = 0.3309,	 Acc2 = 0.3765

 ===== Epoch 71	 =====
[ 2.6443129   2.1331084  -0.42025706 -0.3355664  -0.44155708 -0.3651737
  1.5195447   1.5570978  -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 6.0405886e-01 -7.6660901e-01  4.6474081e-03  3.7761789e-03
  2.1561917e-03  6.3325174e-04 -3.5078135e+00 -3.5745473e+00
  1.5405245e-03  6.3845129e-03  3.4734835e-03  4.9582324e-03
 -1.8088582e-03 -2.2377169e-03] 5 5
train:	 Loss = 1.3903,	 Acc = 0.4860
10764 0.272
21095 0.53
10890 0.595
1400 0.618
243 0.506
24 0.167
0 0.0
0 0.0
0.5545881374063949
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3774,	 Acc = 0.4953
1367 0.367
6407 0.507
3023 0.535
308 0.416
8 0.875
0 0.0
0 0.0
0 0.0
0.5132361994664478
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4651,	 Acc1 = 0.3685,	 Acc2 = 0.3879

 ===== Epoch 72	 =====
[-0.36718738 -0.38160205  1.9268397   1.5585394  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.11860049  0.01824613  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 2 1
train:	 Loss = 1.3874,	 Acc = 0.4883
10765 0.278
21092 0.531
10890 0.596
1402 0.616
243 0.543
24 0.292
0 0.0
0 0.0
0.5554366883599299
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3734,	 Acc = 0.4970
1367 0.398
6407 0.53
3023 0.484
308 0.373
8 0.875
0 0.0
0 0.0
0 0.0
0.5108762569259183
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4460,	 Acc1 = 0.3844,	 Acc2 = 0.4071

 ===== Epoch 73	 =====
[-0.36718738 -0.38160205  2.9572663   2.17781    -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 1.4681835e-03  2.4594050e-03  3.3409813e-01 -7.3831353e+00
  2.1561917e-03  6.3325174e-04  1.1600148e-03  1.7411708e-03
  1.5405245e-03  6.3845129e-03  3.4734835e-03  4.9582324e-03
 -1.8088582e-03 -2.2377169e-03] 2 1
train:	 Loss = 1.3898,	 Acc = 0.4876
10762 0.277
21097 0.531
10889 0.595
1402 0.619
243 0.514
23 0.13
0 0.0
0 0.0
0.5550900338741308
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3614,	 Acc = 0.5162
1367 0.392
6407 0.546
3023 0.521
308 0.39
8 0.875
0 0.0
0 0.0
0 0.0
0.5336548327518982
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4399,	 Acc1 = 0.3951,	 Acc2 = 0.4200

 ===== Epoch 74	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 3 1
train:	 Loss = 1.3880,	 Acc = 0.4878
10761 0.274
21100 0.532
10887 0.597
1402 0.613
242 0.529
24 0.167
0 0.0
0 0.0
0.5562026444807606
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3881,	 Acc = 0.4919
1367 0.386
6407 0.53
3023 0.474
308 0.338
8 0.875
0 0.0
0 0.0
0 0.0
0.5067720090293454
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4619,	 Acc1 = 0.3813,	 Acc2 = 0.4033

 ===== Epoch 75	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  3.1916413   1.2915181  -0.4018844  -0.40971038  2.7022498   2.3757925
 -0.38724938 -0.38149557] [ 1.4681835e-03  2.4594050e-03  4.6474081e-03  3.7761789e-03
  2.1561917e-03  6.3325174e-04 -6.6228509e+00 -3.0821598e+00
  1.5405245e-03  6.3845129e-03  5.3088850e-01 -6.6156179e-01
 -1.8088582e-03 -2.2377169e-03] 5 5
train:	 Loss = 1.3884,	 Acc = 0.4876
10764 0.277
21097 0.529
10886 0.597
1403 0.624
242 0.533
24 0.25
0 0.0
0 0.0
0.5548258647331511
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3757,	 Acc = 0.4991
1367 0.399
6407 0.533
3023 0.482
308 0.406
8 0.875
0 0.0
0 0.0
0 0.0
0.5131335932690334
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4443,	 Acc1 = 0.3926,	 Acc2 = 0.4170

 ===== Epoch 76	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
train:	 Loss = 1.3884,	 Acc = 0.4875
10760 0.277
21098 0.532
10889 0.594
1402 0.611
243 0.531
24 0.167
0 0.0
0 0.0
0.5549084858569051
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.4045,	 Acc = 0.4875
1367 0.383
6407 0.526
3023 0.466
308 0.364
8 0.875
0 0.0
0 0.0
0 0.0
0.5022573363431151
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4790,	 Acc1 = 0.3720,	 Acc2 = 0.3921

 ===== Epoch 77	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 3 1
train:	 Loss = 1.3890,	 Acc = 0.4885
10764 0.275
21094 0.533
10891 0.595
1400 0.631
243 0.498
24 0.25
0 0.0
0 0.0
0.5567573992630452
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.4016,	 Acc = 0.4907
1367 0.388
6407 0.525
3023 0.471
308 0.412
8 0.875
0 0.0
0 0.0
0 0.0
0.5051303098707162
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4718,	 Acc1 = 0.3790,	 Acc2 = 0.4006

 ===== Epoch 78	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664   2.5848236   1.3254963
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 1.4681835e-03  2.4594050e-03  6.9342341e+00  3.0738182e+00
  1.6428505e-01 -6.4255619e-01  1.1600148e-03  1.7411708e-03
  1.5405245e-03  6.3845129e-03  3.4734835e-03  4.9582324e-03
 -1.8088582e-03 -2.2377169e-03] 5 5
train:	 Loss = 1.3902,	 Acc = 0.4888
10767 0.275
21092 0.531
10888 0.6
1402 0.627
243 0.519
24 0.083
0 0.0
0 0.0
0.5571042230081131
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3633,	 Acc = 0.5202
1367 0.39
6407 0.531
3023 0.567
308 0.393
8 0.875
0 0.0
0 0.0
0 0.0
0.5384773240303714
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
Testing:	 Loss = 1.4514,	 Acc1 = 0.3703,	 Acc2 = 0.3902

 ===== Epoch 79	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664   2.2158506   1.6006185
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618 -0.31730935 -0.01432464
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 2 3
train:	 Loss = 1.3851,	 Acc = 0.4887
10761 0.278
21100 0.532
10886 0.594
1402 0.636
243 0.494
24 0.125
0 0.0
0 0.0
0.5561432179468132
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3935,	 Acc = 0.4948
1367 0.381
6407 0.515
3023 0.513
308 0.399
8 0.875
0 0.0
0 0.0
0 0.0
0.510773650728504
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4649,	 Acc1 = 0.3763,	 Acc2 = 0.3974

 ===== Epoch 80	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 1.4681835e-03  2.4594050e-03  4.6474081e-03  3.7761789e-03
  2.1561917e-03  6.3325174e-04  1.1600148e-03  1.7411708e-03
  1.5405245e-03  6.3845129e-03  4.2183723e+00  5.3245425e+00
 -1.8088582e-03 -2.2377169e-03] 2 5
train:	 Loss = 1.3856,	 Acc = 0.4886
10769 0.277
21095 0.532
10888 0.595
1399 0.624
241 0.552
24 0.208
0 0.0
0 0.0
0.5562457277023212
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3761,	 Acc = 0.5071
1367 0.388
6407 0.528
3023 0.525
308 0.409
8 0.875
0 0.0
0 0.0
0 0.0
0.5238046378001231
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4488,	 Acc1 = 0.3763,	 Acc2 = 0.3974

 ===== Epoch 81	 =====
[ 2.246259    2.6891248  -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
  1.9483726   2.9481394 ] [-2.7760346e+00 -3.5103908e+00  4.6474081e-03  3.7761789e-03
  2.1561917e-03  6.3325174e-04  1.1600148e-03  1.7411708e-03
  1.5405245e-03  6.3845129e-03  3.4734835e-03  4.9582324e-03
 -1.8088582e-03 -2.2377169e-03] 0 0
train:	 Loss = 1.3868,	 Acc = 0.4885
10762 0.279
21098 0.531
10887 0.598
1402 0.613
243 0.514
24 0.167
0 0.0
0 0.0
0.5555654602721816
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3991,	 Acc = 0.4829
1367 0.367
6407 0.505
3023 0.496
308 0.396
8 0.875
0 0.0
0 0.0
0 0.0
0.4991791504206854
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
Testing:	 Loss = 1.4803,	 Acc1 = 0.3604,	 Acc2 = 0.3782

 ===== Epoch 82	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038  1.5388213   1.0629863
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 3 0
train:	 Loss = 1.3879,	 Acc = 0.4849
10761 0.27
21094 0.529
10892 0.593
1402 0.628
243 0.531
24 0.208
0 0.0
0 0.0
0.5535878769870747
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.4099,	 Acc = 0.4881
1367 0.386
6407 0.531
3023 0.456
308 0.36
8 0.875
0 0.0
0 0.0
0 0.0
0.5023599425405294
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4754,	 Acc1 = 0.3773,	 Acc2 = 0.3986

 ===== Epoch 83	 =====
[-0.36718738 -0.38160205  3.0439837   9.060105   -0.44155708 -0.3651737
  2.1718092   3.8215144  -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 1.4681835e-03  2.4594050e-03 -6.6373503e-01 -7.5302472e+00
  2.1561917e-03  6.3325174e-04 -2.4564734e-01 -2.4173962e-02
  1.5405245e-03  6.3845129e-03  3.4734835e-03  4.9582324e-03
 -1.8088582e-03 -2.2377169e-03] 3 3
train:	 Loss = 1.3858,	 Acc = 0.4897
10763 0.276
21097 0.534
10888 0.596
1401 0.63
243 0.564
24 0.125
0 0.0
0 0.0
0.558078031676225
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3683,	 Acc = 0.5094
1367 0.397
6407 0.539
3023 0.506
308 0.409
8 0.875
0 0.0
0 0.0
0 0.0
0.5251385183665094
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
Testing:	 Loss = 1.4434,	 Acc1 = 0.3815,	 Acc2 = 0.4036

 ===== Epoch 84	 =====
[ 0.96916974  1.6453303  -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
  0.7458334   2.0129075 ] [-0.07218327 -0.19992703  0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
  0.01805674 -0.29543224] 5 5
train:	 Loss = 1.3867,	 Acc = 0.4882
10766 0.275
21095 0.532
10888 0.596
1400 0.626
243 0.519
24 0.125
0 0.0
0 0.0
0.5566121842496285
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3858,	 Acc = 0.4951
1367 0.383
6407 0.526
3023 0.495
308 0.354
8 0.875
0 0.0
0 0.0
0 0.0
0.510773650728504
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
Testing:	 Loss = 1.4677,	 Acc1 = 0.3757,	 Acc2 = 0.3966

 ===== Epoch 85	 =====
[ 0.5980194   1.220736   -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00140714 -0.00043183  0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 0 0
train:	 Loss = 1.3876,	 Acc = 0.4866
10761 0.276
21097 0.531
10891 0.59
1401 0.621
242 0.529
24 0.333
0 0.0
0 0.0
0.5537661565889169
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3968,	 Acc = 0.4880
1367 0.383
6407 0.527
3023 0.46
308 0.412
8 0.875
0 0.0
0 0.0
0 0.0
0.5026677611327725
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4668,	 Acc1 = 0.3755,	 Acc2 = 0.3964

 ===== Epoch 86	 =====
[-0.36718738 -0.38160205  2.4247506   2.159663   -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.00562244  0.15329903  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 1 4
train:	 Loss = 1.3858,	 Acc = 0.4884
10761 0.278
21098 0.531
10888 0.596
1402 0.624
243 0.527
24 0.125
0 0.0
0 0.0
0.5556975189422076
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3969,	 Acc = 0.4888
1367 0.341
6407 0.521
3023 0.497
308 0.39
8 0.875
0 0.0
0 0.0
0 0.0
0.5095423763595321
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4661,	 Acc1 = 0.3784,	 Acc2 = 0.3999

 ===== Epoch 87	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  1.182984    2.8402674  -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 5.54936790e+00  3.78130341e+00  4.64740815e-03  3.77617893e-03
  2.15619174e-03  6.33251737e-04  3.57194543e-01  1.00218676e-01
  1.54052454e-03  6.38451288e-03  3.47348349e-03  4.95823240e-03
 -1.80885824e-03 -2.23771692e-03] 2 2
train:	 Loss = 1.3867,	 Acc = 0.4869
10763 0.274
21096 0.528
10891 0.599
1401 0.63
241 0.531
24 0.375
0 0.0
0 0.0
0.5551659584583841
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3937,	 Acc = 0.4772
1367 0.198
6407 0.516
3023 0.528
308 0.416
8 0.0
0 0.0
0 0.0
0 0.0
0.5163143853888775
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4722,	 Acc1 = 0.3419,	 Acc2 = 0.3897

 ===== Epoch 88	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664   4.0431285   2.3993604
 -0.36400968 -0.37184966  4.2805114   1.7500967  -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 1.4681835e-03  2.4594050e-03  4.6474081e-03  3.7761789e-03
  2.1561917e-03  6.3325174e-04  1.1600148e-03  1.7411708e-03
 -6.8647987e-01  6.3845129e-03  3.4734835e-03  4.9582324e-03
 -1.8088582e-03 -2.2377169e-03] 0 0
train:	 Loss = 1.3893,	 Acc = 0.4876
10763 0.274
21097 0.533
10890 0.594
1401 0.62
241 0.498
24 0.25
0 0.0
0 0.0
0.555938549312097
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3807,	 Acc = 0.4985
1367 0.395
6407 0.523
3023 0.5
308 0.412
8 0.875
0 0.0
0 0.0
0 0.0
0.5130309870716191
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4540,	 Acc1 = 0.3792,	 Acc2 = 0.4008

 ===== Epoch 89	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 0 1
train:	 Loss = 1.3851,	 Acc = 0.4882
10763 0.274
21097 0.533
10888 0.596
1402 0.628
242 0.517
24 0.208
0 0.0
0 0.0
0.5567111401658099
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3807,	 Acc = 0.4974
1367 0.39
6407 0.525
3023 0.498
308 0.386
8 0.875
0 0.0
0 0.0
0 0.0
0.5125179560845475
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4502,	 Acc1 = 0.3854,	 Acc2 = 0.4083

 ===== Epoch 90	 =====
[ 2.1852038   2.044651   -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
  2.18281     2.267018  ] [-1.0394998e-01 -5.2085406e-01  4.6474081e-03  3.7761789e-03
  2.1561917e-03  6.3325174e-04  1.1600148e-03  1.7411708e-03
  1.5405245e-03  6.3845129e-03  3.4734835e-03  4.9582324e-03
 -1.8088582e-03 -7.7076268e-01] 4 5
train:	 Loss = 1.3842,	 Acc = 0.4900
10763 0.278
21096 0.532
10887 0.6
1403 0.622
243 0.556
24 0.083
0 0.0
0 0.0
0.5577214512822036
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.4022,	 Acc = 0.4832
1367 0.393
6407 0.523
3023 0.452
308 0.36
8 0.875
0 0.0
0 0.0
0 0.0
0.49589575210342707
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
Testing:	 Loss = 1.4669,	 Acc1 = 0.3827,	 Acc2 = 0.4051

 ===== Epoch 91	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664   1.8434358   0.7952206
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 1.4681835e-03  2.4594050e-03  5.5943117e+00  3.6357348e+00
  4.6058720e-01  5.2985884e-02  1.1600148e-03  1.7411708e-03
  1.5405245e-03  6.3845129e-03  3.4734835e-03  4.9582324e-03
 -1.8088582e-03 -2.2377169e-03] 2 2
train:	 Loss = 1.3832,	 Acc = 0.4897
10761 0.276
21095 0.533
10890 0.599
1403 0.633
243 0.51
24 0.208
0 0.0
0 0.0
0.5581042935670777
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3815,	 Acc = 0.4970
1367 0.386
6407 0.52
3023 0.51
308 0.383
8 0.875
0 0.0
0 0.0
0 0.0
0.5125179560845475
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4650,	 Acc1 = 0.3693,	 Acc2 = 0.3889

 ===== Epoch 92	 =====
[ 1.6850705   2.0117958  -0.42025706 -0.3355664  -0.44155708 -0.3651737
  0.8035776   2.023959   -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 1.2778890e+00  1.2100002e-01  4.6474081e-03  3.7761789e-03
  2.1561917e-03  6.3325174e-04 -6.2268770e-01  2.4016039e-01
  1.5405245e-03  6.3845129e-03  3.4734835e-03  4.9582324e-03
 -1.8088582e-03 -2.2377169e-03] 1 1
train:	 Loss = 1.3861,	 Acc = 0.4900
10766 0.277
21098 0.533
10885 0.599
1400 0.623
243 0.531
24 0.375
0 0.0
0 0.0
0.5583358098068351
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3941,	 Acc = 0.4910
1367 0.389
6407 0.532
3023 0.459
308 0.393
8 0.875
0 0.0
0 0.0
0 0.0
0.5053355222655448
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
Testing:	 Loss = 1.4598,	 Acc1 = 0.3769,	 Acc2 = 0.3981

 ===== Epoch 93	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
  1.3229111   3.5323317 ] [ 2.2306447e+00  4.3422027e+00  4.6474081e-03  3.7761789e-03
  2.1561917e-03  6.3325174e-04  1.1600148e-03  1.7411708e-03
  1.5405245e-03  6.3845129e-03  3.4734835e-03  4.9582324e-03
 -3.2199748e+00 -6.6390953e+00] 4 5
train:	 Loss = 1.3856,	 Acc = 0.4890
10759 0.279
21105 0.532
10882 0.597
1403 0.613
243 0.527
24 0.125
0 0.0
0 0.0
0.556021035742936
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.4507,	 Acc = 0.4584
1367 0.387
6407 0.482
3023 0.446
308 0.396
8 0.875
0 0.0
0 0.0
0 0.0
0.4683972911963883
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
Testing:	 Loss = 1.5274,	 Acc1 = 0.3501,	 Acc2 = 0.3658

 ===== Epoch 94	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038  1.7082597   2.5647383
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.35209173  0.26066718
 -0.00180886 -0.00223772] 6 2
train:	 Loss = 1.3842,	 Acc = 0.4915
10763 0.277
21098 0.536
10885 0.6
1403 0.624
243 0.519
24 0.167
0 0.0
0 0.0
0.5600392238433424
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3590,	 Acc = 0.5120
1367 0.386
6407 0.53
3023 0.544
308 0.38
8 0.875
0 0.0
0 0.0
0 0.0
0.5297557972501539
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4503,	 Acc1 = 0.3695,	 Acc2 = 0.3892

 ===== Epoch 95	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 3 1
train:	 Loss = 1.3848,	 Acc = 0.4905
10764 0.278
21095 0.533
10890 0.6
1401 0.637
242 0.496
24 0.375
0 0.0
0 0.0
0.5585403542137168
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3898,	 Acc = 0.4642
1367 0.208
6407 0.521
3023 0.472
308 0.347
8 0.0
0 0.0
0 0.0
0 0.0
0.5001026061974143
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4722,	 Acc1 = 0.3373,	 Acc2 = 0.3842

 ===== Epoch 96	 =====
[-0.36718738 -0.38160205  1.5966641   1.2001338  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.02676726  0.01583447  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 2 2
train:	 Loss = 1.3862,	 Acc = 0.4900
10766 0.28
21096 0.532
10885 0.599
1402 0.617
243 0.506
24 0.25
0 0.0
0 0.0
0.5572065378900446
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3525,	 Acc = 0.5266
1367 0.385
6407 0.548
3023 0.557
308 0.393
8 0.875
0 0.0
0 0.0
0 0.0
0.5464806074286886
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4382,	 Acc1 = 0.3794,	 Acc2 = 0.4011

 ===== Epoch 97	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966  1.4560959   3.1924508  -0.4254193  -0.4215889
  2.1432235   1.0436196 ] [ 2.2472620e+00  1.4567504e+00  4.6474081e-03  3.7761789e-03
  2.1561917e-03  6.3325174e-04  1.1600148e-03  1.7411708e-03
 -3.8810828e-01  2.3498918e-01  3.4734835e-03  4.9582324e-03
 -9.6165352e-02  7.7724420e-02] 1 1
train:	 Loss = 1.3845,	 Acc = 0.4881
10763 0.277
21093 0.531
10890 0.597
1403 0.622
243 0.494
24 0.25
0 0.0
0 0.0
0.555671114016581
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3797,	 Acc = 0.5019
1367 0.383
6407 0.535
3023 0.5
308 0.351
8 0.875
0 0.0
0 0.0
0 0.0
0.5186743279294069
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4614,	 Acc1 = 0.3804,	 Acc2 = 0.4023

 ===== Epoch 98	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
train:	 Loss = 1.3834,	 Acc = 0.4890
10765 0.276
21095 0.533
10887 0.599
1402 0.621
243 0.514
24 0.292
0 0.0
0 0.0
0.557279129892128
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3776,	 Acc = 0.5106
1367 0.386
6407 0.533
3023 0.525
308 0.438
8 0.875
0 0.0
0 0.0
0 0.0
0.5280114918941105
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
Testing:	 Loss = 1.4596,	 Acc1 = 0.3687,	 Acc2 = 0.3882

 ===== Epoch 99	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 2.5383470e+00  3.7176960e+00  4.6474081e-03  3.7761789e-03
  2.1561917e-03  6.3325174e-04  1.1600148e-03  1.7411708e-03
  1.5405245e-03  6.3845129e-03  3.4734835e-03  4.9582324e-03
 -1.8088582e-03 -2.2377169e-03] 5 5
train:	 Loss = 1.3816,	 Acc = 0.4903
10763 0.278
21097 0.533
10887 0.601
1402 0.616
243 0.523
24 0.333
0 0.0
0 0.0
0.5582266068404006
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.4145,	 Acc = 0.4721
1367 0.377
6407 0.493
3023 0.475
308 0.412
8 0.875
0 0.0
0 0.0
0 0.0
0.4853273137697517
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4753,	 Acc1 = 0.3743,	 Acc2 = 0.3949

 ===== Epoch 100	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.3420632   2.3594282  -0.4018844  -0.40971038  2.7003634   3.0653226
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.26245204 -0.46991423  0.00154052  0.00638451 -0.21871528 -0.36393332
 -0.00180886 -0.00223772] 5 5
train:	 Loss = 1.3834,	 Acc = 0.4896
10760 0.28
21100 0.531
10886 0.599
1403 0.626
243 0.523
24 0.25
0 0.0
0 0.0
0.5566615165200856
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3587,	 Acc = 0.5104
1367 0.386
6407 0.544
3023 0.508
308 0.373
8 0.875
0 0.0
0 0.0
0 0.0
0.527908885696696
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
Testing:	 Loss = 1.4472,	 Acc1 = 0.3730,	 Acc2 = 0.3934

 ===== Epoch 101	 =====
[-0.36718738 -0.38160205  3.3228626   2.1211002  -0.44155708 -0.3651737
 -0.36400968 -0.37184966  1.047615    1.0450562  -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 1.4681835e-03  2.4594050e-03  8.7528104e-01 -4.4238076e-01
  2.1561917e-03  6.3325174e-04  7.6743684e+00  7.0195589e+00
 -2.4817030e+00 -2.3858001e+00  3.4734835e-03  4.9582324e-03
 -1.8088582e-03 -2.2377169e-03] 5 5
train:	 Loss = 1.3820,	 Acc = 0.4926
10759 0.28
21100 0.534
10888 0.604
1403 0.627
242 0.537
24 0.208
0 0.0
0 0.0
0.5605668954452269
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3575,	 Acc = 0.5089
1367 0.38
6407 0.54
3023 0.515
308 0.373
8 0.875
0 0.0
0 0.0
0 0.0
0.5268828237225528
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4394,	 Acc1 = 0.3786,	 Acc2 = 0.4001

 ===== Epoch 102	 =====
[ 0.21351217  1.5315996  -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [-0.00089416  0.00535064  0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 0 0
train:	 Loss = 1.3828,	 Acc = 0.4900
10764 0.274
21097 0.535
10887 0.601
1401 0.611
243 0.523
24 0.333
0 0.0
0 0.0
0.5591941043622964
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3638,	 Acc = 0.5180
1367 0.388
6407 0.537
3023 0.553
308 0.354
8 0.875
0 0.0
0 0.0
0 0.0
0.5363225938846706
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
Testing:	 Loss = 1.4720,	 Acc1 = 0.3627,	 Acc2 = 0.3810

 ===== Epoch 103	 =====
[ 2.3769226   1.9966315  -0.42025706 -0.3355664  -0.44155708 -0.3651737
  0.9935631   1.0035737  -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 5.4529279e-01 -3.3230644e-03  4.6474081e-03  3.7761789e-03
  2.1561917e-03  6.3325174e-04 -2.5279343e+00 -2.5483079e+00
  1.5405245e-03  6.3845129e-03  3.4734835e-03  4.9582324e-03
 -1.8088582e-03 -2.2377169e-03] 5 0
train:	 Loss = 1.3820,	 Acc = 0.4909
10763 0.277
21096 0.534
10888 0.6
1402 0.624
243 0.535
24 0.333
0 0.0
0 0.0
0.5591774878911241
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3968,	 Acc = 0.4904
1367 0.383
6407 0.522
3023 0.485
308 0.357
8 0.875
0 0.0
0 0.0
0 0.0
0.5055407346603735
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4736,	 Acc1 = 0.3697,	 Acc2 = 0.3894

 ===== Epoch 104	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038  0.83351856  1.3549937
 -0.38724938 -0.38149557] [ 1.4681835e-03  2.4594050e-03  4.6474081e-03  3.7761789e-03
  2.1561917e-03  6.3325174e-04  1.1600148e-03  1.7411708e-03
  1.5405245e-03  6.3845129e-03 -9.3779780e-02  7.9723668e-01
 -1.8088582e-03 -2.2377169e-03] 6 6
train:	 Loss = 1.3827,	 Acc = 0.4904
10762 0.278
21099 0.534
10887 0.599
1401 0.623
243 0.514
24 0.417
0 0.0
0 0.0
0.5583585903607298
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3783,	 Acc = 0.4855
1367 0.382
6407 0.525
3023 0.459
308 0.367
8 0.875
0 0.0
0 0.0
0 0.0
0.5
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
Testing:	 Loss = 1.4594,	 Acc1 = 0.3681,	 Acc2 = 0.3874

 ===== Epoch 105	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 3 1
train:	 Loss = 1.3830,	 Acc = 0.4902
10765 0.272
21099 0.536
10883 0.6
1402 0.618
243 0.523
24 0.292
0 0.0
0 0.0
0.5599239249947996
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3811,	 Acc = 0.5021
1367 0.385
6407 0.525
3023 0.513
308 0.442
8 0.875
0 0.0
0 0.0
0 0.0
0.5185717217319926
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4652,	 Acc1 = 0.3650,	 Acc2 = 0.3837

 ===== Epoch 106	 =====
[-0.36718738 -0.38160205  2.4764555   2.3887703  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 1.4681835e-03  2.4594050e-03  6.6433769e-01  1.2677079e-01
  2.1561917e-03  6.3325174e-04  1.1600148e-03  1.7411708e-03
  1.5405245e-03  6.3845129e-03  3.4734835e-03  4.9582324e-03
 -1.8088582e-03 -2.2377169e-03] 2 2
train:	 Loss = 1.3815,	 Acc = 0.4916
10762 0.278
21100 0.533
10884 0.605
1403 0.62
243 0.535
24 0.292
0 0.0
0 0.0
0.559933440304273
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.4001,	 Acc = 0.4782
1367 0.364
6407 0.521
3023 0.45
308 0.367
8 0.875
0 0.0
0 0.0
0 0.0
0.49425405294479785
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
Testing:	 Loss = 1.4764,	 Acc1 = 0.3658,	 Acc2 = 0.3847

 ===== Epoch 107	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.5534704   3.3015373  -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
  0.21436125  0.56669104  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 6 6
train:	 Loss = 1.3829,	 Acc = 0.4911
10763 0.278
21093 0.53
10890 0.608
1403 0.628
243 0.551
24 0.333
0 0.0
0 0.0
0.559415208153805
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3829,	 Acc = 0.4916
1367 0.387
6407 0.524
3023 0.478
308 0.409
8 0.875
0 0.0
0 0.0
0 0.0
0.5062589780422737
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4564,	 Acc1 = 0.3844,	 Acc2 = 0.4071

 ===== Epoch 108	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966  0.93990993  3.2296886  -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.09281782  0.08394681  0.00347348  0.00495823
 -0.00180886 -0.00223772] 1 4
train:	 Loss = 1.3805,	 Acc = 0.4894
10764 0.274
21096 0.534
10890 0.598
1399 0.624
243 0.543
24 0.125
0 0.0
0 0.0
0.5583323428028052
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3712,	 Acc = 0.5146
1367 0.377
6407 0.534
3023 0.549
308 0.377
8 0.875
0 0.0
0 0.0
0 0.0
0.5339626513441412
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
Testing:	 Loss = 1.4560,	 Acc1 = 0.3693,	 Acc2 = 0.3889

 ===== Epoch 109	 =====
[ 1.7688133   2.5046284  -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [-2.2686176e+00 -3.2993305e+00  4.6474081e-03  3.7761789e-03
  2.1561917e-03  6.3325174e-04  1.1600148e-03  1.7411708e-03
  1.5405245e-03  6.3845129e-03  3.4734835e-03  4.9582324e-03
 -1.8088582e-03 -2.2377169e-03] 6 1
train:	 Loss = 1.3816,	 Acc = 0.4918
10766 0.276
21093 0.532
10888 0.608
1402 0.636
243 0.531
24 0.333
0 0.0
0 0.0
0.5608618127786033
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3781,	 Acc = 0.5037
1367 0.383
6407 0.543
3023 0.489
308 0.37
8 0.875
0 0.0
0 0.0
0 0.0
0.5206238456802791
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4484,	 Acc1 = 0.3825,	 Acc2 = 0.4048

 ===== Epoch 110	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038  1.5116508   1.8015932
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451 -0.16859123 -0.2004473
 -0.00180886 -0.00223772] 5 5
train:	 Loss = 1.3808,	 Acc = 0.4906
10764 0.277
21094 0.533
10889 0.603
1403 0.618
242 0.504
24 0.208
0 0.0
0 0.0
0.5589266611196957
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3981,	 Acc = 0.4871
1367 0.383
6407 0.522
3023 0.475
308 0.341
8 0.875
0 0.0
0 0.0
0 0.0
0.5016416991586292
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4809,	 Acc1 = 0.3623,	 Acc2 = 0.3805

 ===== Epoch 111	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.2360764   3.7264647  -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 1.4681835e-03  2.4594050e-03  2.7082660e+00  1.5834128e+00
  2.1561917e-03  6.3325174e-04 -4.8426781e+00 -7.5965757e+00
  1.5405245e-03  6.3845129e-03  3.4734835e-03  4.9582324e-03
 -1.8088582e-03 -2.2377169e-03] 6 6
train:	 Loss = 1.3807,	 Acc = 0.4889
10765 0.271
21092 0.532
10889 0.604
1403 0.611
243 0.527
24 0.333
0 0.0
0 0.0
0.5585866690440106
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3692,	 Acc = 0.4979
1367 0.379
6407 0.533
3023 0.49
308 0.377
8 0.875
0 0.0
0 0.0
0 0.0
0.514570080032834
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4425,	 Acc1 = 0.3852,	 Acc2 = 0.4081

 ===== Epoch 112	 =====
[ 3.234764    2.4768274  -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
  2.2786965   2.7123666 ] [-8.6162198e-01  4.5827921e-02  4.6474081e-03  3.7761789e-03
  2.1561917e-03  6.3325174e-04  1.1600148e-03  1.7411708e-03
  1.5405245e-03  6.3845129e-03  3.4734835e-03  4.9582324e-03
 -3.1604704e-02  5.1070377e-02] 4 3
train:	 Loss = 1.3806,	 Acc = 0.4893
10762 0.271
21099 0.533
10886 0.603
1403 0.624
242 0.521
24 0.25
0 0.0
0 0.0
0.559101444107684
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3779,	 Acc = 0.5042
1367 0.392
6407 0.538
3023 0.494
308 0.383
8 0.875
0 0.0
0 0.0
0 0.0
0.5199056022983788
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
Testing:	 Loss = 1.4587,	 Acc1 = 0.3656,	 Acc2 = 0.3844

 ===== Epoch 113	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 2 1
train:	 Loss = 1.3813,	 Acc = 0.4900
10766 0.273
21093 0.533
10888 0.604
1403 0.619
242 0.537
24 0.292
0 0.0
0 0.0
0.559405646359584
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3515,	 Acc = 0.5191
1367 0.382
6407 0.543
3023 0.548
308 0.364
8 0.0
0 0.0
0 0.0
0 0.0
0.5383747178329571
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
Testing:	 Loss = 1.4459,	 Acc1 = 0.3747,	 Acc2 = 0.3954

 ===== Epoch 114	 =====
[-0.36718738 -0.38160205  1.9268397   1.3861418  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.18575378 -0.00587046  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 2 0
train:	 Loss = 1.3814,	 Acc = 0.4907
10760 0.276
21097 0.534
10891 0.602
1401 0.619
243 0.519
24 0.25
0 0.0
0 0.0
0.5594247682434038
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3882,	 Acc = 0.4910
1367 0.384
6407 0.527
3023 0.472
308 0.383
8 0.875
0 0.0
0 0.0
0 0.0
0.5060537656474451
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4667,	 Acc1 = 0.3720,	 Acc2 = 0.3921

 ===== Epoch 115	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038  0.8950293   2.9843457
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.17703448  0.17263621
 -0.00180886 -0.00223772] 1 4
train:	 Loss = 1.3798,	 Acc = 0.4933
10766 0.28
21091 0.536
10889 0.604
1403 0.616
243 0.514
24 0.333
0 0.0
0 0.0
0.5613967310549777
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3556,	 Acc = 0.5075
1367 0.387
6407 0.542
3023 0.498
308 0.406
8 0.875
0 0.0
0 0.0
0 0.0
0.5244202749846091
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
Testing:	 Loss = 1.4368,	 Acc1 = 0.3833,	 Acc2 = 0.4058

 ===== Epoch 116	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664   1.4844034   1.897928
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.27210513  0.0081122
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 2 2
train:	 Loss = 1.3807,	 Acc = 0.4926
10758 0.278
21098 0.536
10891 0.605
1402 0.621
243 0.539
24 0.083
0 0.0
0 0.0
0.5612335848832373
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.4012,	 Acc = 0.4793
1367 0.358
6407 0.521
3023 0.455
308 0.377
8 0.875
0 0.0
0 0.0
0 0.0
0.49620357069567
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4791,	 Acc1 = 0.3674,	 Acc2 = 0.3867

 ===== Epoch 117	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038  1.3954208   1.9733622
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451 -0.02644971 -0.05372906
 -0.00180886 -0.00223772] 4 3
train:	 Loss = 1.3811,	 Acc = 0.4918
10766 0.277
21097 0.534
10884 0.605
1402 0.621
243 0.547
24 0.25
0 0.0
0 0.0
0.5605349182763745
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3672,	 Acc = 0.4914
1367 0.183
6407 0.538
3023 0.546
308 0.36
8 0.0
0 0.0
0 0.0
0 0.0
0.5346808947260414
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4423,	 Acc1 = 0.3557,	 Acc2 = 0.4063

 ===== Epoch 118	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966  1.1770915   0.8241104  -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117 -0.10151507  0.35337377  0.00347348  0.00495823
 -0.00180886 -0.00223772] 6 6
train:	 Loss = 1.3809,	 Acc = 0.4900
10763 0.275
21095 0.532
10889 0.602
1402 0.631
243 0.531
24 0.375
0 0.0
0 0.0
0.5587614774314326
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.4181,	 Acc = 0.4779
1367 0.383
6407 0.515
3023 0.45
308 0.409
8 0.0
0 0.0
0 0.0
0 0.0
0.49127847321978246
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4768,	 Acc1 = 0.3790,	 Acc2 = 0.4006

 ===== Epoch 119	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664   0.47383764  2.7632315
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 1.4681835e-03  2.4594050e-03  4.6474081e-03  3.7761789e-03
 -1.9099115e+00 -3.5144718e+00  1.1600148e-03  1.7411708e-03
  1.5405245e-03  6.3845129e-03  3.4734835e-03  4.9582324e-03
 -1.8088582e-03 -2.2377169e-03] 6 1
train:	 Loss = 1.3815,	 Acc = 0.4890
10761 0.271
21096 0.532
10889 0.603
1403 0.636
243 0.502
24 0.125
0 0.0
0 0.0
0.5587579854404991
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3689,	 Acc = 0.5114
1367 0.386
6407 0.536
3023 0.53
308 0.37
8 0.875
0 0.0
0 0.0
0 0.0
0.5290375538682537
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
Testing:	 Loss = 1.4518,	 Acc1 = 0.3757,	 Acc2 = 0.3966

 ===== Epoch 120	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038  2.987918    1.6887164
 -0.38724938 -0.38149557] [ 3.6364734e+00  4.0386233e+00  4.6474081e-03  3.7761789e-03
  2.1561917e-03  6.3325174e-04  6.1714640e+00  2.5880716e+00
  1.5405245e-03  6.3845129e-03 -2.2710176e-02  8.4605269e-02
 -1.8088582e-03 -2.2377169e-03] 2 2
train:	 Loss = 1.3813,	 Acc = 0.4906
10763 0.274
21095 0.533
10889 0.603
1403 0.626
242 0.558
24 0.292
0 0.0
0 0.0
0.5599203637120019
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3830,	 Acc = 0.4854
1367 0.347
6407 0.522
3023 0.48
308 0.386
8 0.875
0 0.0
0 0.0
0 0.0
0.5047198850810589
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
Testing:	 Loss = 1.4673,	 Acc1 = 0.3687,	 Acc2 = 0.3882

 ===== Epoch 121	 =====
[ 1.4971551   2.843293   -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
  1.1122248   3.168194  ] [0.06382327 0.07763151 0.00464741 0.00377618 0.00215619 0.00063325
 0.00116001 0.00174117 0.00154052 0.00638451 0.00347348 0.00495823
 0.0685472  0.2065523 ] 1 4
train:	 Loss = 1.3797,	 Acc = 0.4929
10764 0.276
21095 0.535
10888 0.608
1402 0.63
243 0.523
24 0.25
0 0.0
0 0.0
0.5624331391893498
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.4069,	 Acc = 0.4803
1367 0.391
6407 0.51
3023 0.47
308 0.347
8 0.875
0 0.0
0 0.0
0 0.0
0.4929201723784117
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4773,	 Acc1 = 0.3687,	 Acc2 = 0.3882

 ===== Epoch 122	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 0 1
train:	 Loss = 1.3780,	 Acc = 0.4916
10758 0.277
21100 0.533
10889 0.604
1402 0.632
243 0.535
24 0.25
0 0.0
0 0.0
0.5601045813773843
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3852,	 Acc = 0.4906
1367 0.381
6407 0.532
3023 0.465
308 0.354
8 0.875
0 0.0
0 0.0
0 0.0
0.5059511594500308
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4610,	 Acc1 = 0.3809,	 Acc2 = 0.4028

 ===== Epoch 123	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 2 1
train:	 Loss = 1.3797,	 Acc = 0.4910
10758 0.275
21101 0.535
10888 0.601
1402 0.627
243 0.519
24 0.333
0 0.0
0 0.0
0.5598966070473588
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3593,	 Acc = 0.5164
1367 0.387
6407 0.537
3023 0.543
308 0.406
8 0.875
0 0.0
0 0.0
0 0.0
0.5345782885286271
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4365,	 Acc1 = 0.3800,	 Acc2 = 0.4018

 ===== Epoch 124	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038  1.3886279   2.6898842
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.4568264   0.2648591
 -0.00180886 -0.00223772] 2 4
train:	 Loss = 1.3793,	 Acc = 0.4919
10761 0.278
21095 0.533
10890 0.606
1403 0.631
243 0.519
24 0.375
0 0.0
0 0.0
0.5603030753231317
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3654,	 Acc = 0.4965
1367 0.385
6407 0.53
3023 0.485
308 0.393
8 0.875
0 0.0
0 0.0
0 0.0
0.5122101374923045
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
Testing:	 Loss = 1.4439,	 Acc1 = 0.3749,	 Acc2 = 0.3956

 ===== Epoch 125	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664   0.90398777  1.076999
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618 -0.19032058  0.6886964
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 6 6
train:	 Loss = 1.3788,	 Acc = 0.4913
10766 0.277
21093 0.534
10888 0.603
1402 0.626
243 0.535
24 0.375
0 0.0
0 0.0
0.5598811292719168
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3701,	 Acc = 0.4987
1367 0.388
6407 0.534
3023 0.482
308 0.409
8 0.875
0 0.0
0 0.0
0 0.0
0.5142622614405911
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4454,	 Acc1 = 0.3761,	 Acc2 = 0.3971

 ===== Epoch 126	 =====
[-0.36718738 -0.38160205  2.2358456   2.24813    -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.21954234  0.40652323  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 6 4
train:	 Loss = 1.3807,	 Acc = 0.4915
10760 0.277
21098 0.534
10888 0.603
1403 0.621
243 0.527
24 0.458
0 0.0
0 0.0
0.5601378654623247
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3641,	 Acc = 0.5171
1367 0.383
6407 0.538
3023 0.543
308 0.416
8 0.875
0 0.0
0 0.0
0 0.0
0.5358095628975991
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4410,	 Acc1 = 0.3889,	 Acc2 = 0.4125

 ===== Epoch 127	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664   1.490903   -3.97948
 -0.36400968 -0.37184966  2.7542963   1.1443577  -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 1.4681835e-03  2.4594050e-03  4.6474081e-03  3.7761789e-03
 -4.0343480e+00  4.0617018e+00  1.1600148e-03  1.7411708e-03
  4.8148662e-01  6.7995191e-01  3.4734835e-03  4.9582324e-03
 -1.8088582e-03 -2.2377169e-03] 2 6
train:	 Loss = 1.3781,	 Acc = 0.4923
10765 0.281
21097 0.534
10887 0.604
1401 0.617
242 0.517
24 0.292
0 0.0
0 0.0
0.5598942081958931
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3784,	 Acc = 0.4817
1367 0.368
6407 0.512
3023 0.477
308 0.393
8 0.875
0 0.0
0 0.0
0 0.0
0.49764005745947054
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4674,	 Acc1 = 0.3689,	 Acc2 = 0.3884

 ===== Epoch 128	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664   1.9237303   2.5280466
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 1.4681835e-03  2.4594050e-03  4.6474081e-03  3.7761789e-03
 -8.6998147e-01  1.4273325e-01  1.2821632e+01  3.2359498e+00
  1.0204154e+01  4.9989476e+00  1.0042377e+01  3.1866481e+00
 -1.8088582e-03 -2.2377169e-03] 1 1
train:	 Loss = 1.3780,	 Acc = 0.4918
10764 0.279
21096 0.534
10891 0.604
1399 0.628
242 0.508
24 0.333
0 0.0
0 0.0
0.559907286342565
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3802,	 Acc = 0.4964
1367 0.38
6407 0.516
3023 0.517
308 0.393
8 0.875
0 0.0
0 0.0
0 0.0
0.5126205622819618
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4692,	 Acc1 = 0.3619,	 Acc2 = 0.3800

 ===== Epoch 129	 =====
[ 2.1073897   1.7337875  -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
  1.5124853   2.018147  ] [-2.6284478e+00 -2.4175041e+00  4.6474081e-03  3.7761789e-03
  2.1561917e-03  6.3325174e-04  1.1600148e-03  1.7411708e-03
  1.5405245e-03  6.3845129e-03  3.4734835e-03  4.9582324e-03
 -2.4763414e-01  3.3538017e-01] 4 6
train:	 Loss = 1.3792,	 Acc = 0.4912
10763 0.277
21100 0.533
10884 0.605
1402 0.626
243 0.502
24 0.333
0 0.0
0 0.0
0.5596232133836508
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3722,	 Acc = 0.5090
1367 0.386
6407 0.541
3023 0.505
308 0.419
8 0.875
0 0.0
0 0.0
0 0.0
0.5263697927354812
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4509,	 Acc1 = 0.3864,	 Acc2 = 0.4095

 ===== Epoch 130	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664   2.3875277   1.0947487
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618 -0.02739428  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 5 0
train:	 Loss = 1.3774,	 Acc = 0.4922
10765 0.28
21099 0.532
10884 0.611
1401 0.607
243 0.481
24 0.333
0 0.0
0 0.0
0.559953641793706
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3832,	 Acc = 0.4776
1367 0.195
6407 0.53
3023 0.508
308 0.36
8 0.0
0 0.0
0 0.0
0 0.0
0.5173404473630208
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4689,	 Acc1 = 0.3423,	 Acc2 = 0.3902

 ===== Epoch 131	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038  1.1184319   1.7549702
 -0.38724938 -0.38149557] [ 1.4681835e-03  2.4594050e-03  4.6474081e-03  3.7761789e-03
  2.1561917e-03  6.3325174e-04  1.1600148e-03  1.7411708e-03
  1.5405245e-03  6.3845129e-03 -1.5063593e-01  1.1116329e+00
 -1.8088582e-03 -2.2377169e-03] 6 6
train:	 Loss = 1.3788,	 Acc = 0.4908
10763 0.274
21098 0.533
10888 0.605
1400 0.622
243 0.547
24 0.333
0 0.0
0 0.0
0.5601580839746828
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3965,	 Acc = 0.4878
1367 0.39
6407 0.522
3023 0.471
308 0.377
8 0.875
0 0.0
0 0.0
0 0.0
0.5015390929612149
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4712,	 Acc1 = 0.3732,	 Acc2 = 0.3936

 ===== Epoch 132	 =====
[-0.36718738 -0.38160205  2.0420558   2.1165636  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 1.4681835e-03  2.4594050e-03  1.9820891e-01 -7.4603090e+00
  2.1561917e-03  6.3325174e-04  1.1600148e-03  1.7411708e-03
  1.5405245e-03  6.3845129e-03  3.4734835e-03  4.9582324e-03
 -1.8088582e-03 -2.2377169e-03] 2 1
train:	 Loss = 1.3771,	 Acc = 0.4924
10765 0.279
21096 0.532
10888 0.608
1400 0.641
243 0.523
24 0.375
0 0.0
0 0.0
0.5605776945707408
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3493,	 Acc = 0.5140
1367 0.359
6407 0.542
3023 0.537
308 0.38
8 0.875
0 0.0
0 0.0
0 0.0
0.5357069567001846
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4323,	 Acc1 = 0.3932,	 Acc2 = 0.4177

 ===== Epoch 133	 =====
[ 2.1440058   1.8197172  -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
  2.1810508   2.0286257 ] [ 1.7521685e-01 -4.8326799e-01  4.6474081e-03  3.7761789e-03
  2.1561917e-03  6.3325174e-04  1.1600148e-03  1.7411708e-03
  1.5405245e-03  6.3845129e-03  3.4734835e-03  4.9582324e-03
  1.7449196e-01 -7.2189695e-01] 5 5
train:	 Loss = 1.3818,	 Acc = 0.4854
10762 0.267
21094 0.526
10891 0.604
1402 0.621
243 0.527
24 0.25
0 0.0
0 0.0
0.5551791763237653
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3770,	 Acc = 0.4827
1367 0.361
6407 0.513
3023 0.482
308 0.396
8 0.875
0 0.0
0 0.0
0 0.0
0.499692181407757
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4517,	 Acc1 = 0.3792,	 Acc2 = 0.4008

 ===== Epoch 134	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.056239    2.6893063  -0.4018844  -0.40971038  2.9505584   2.962261
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.17633128  0.15723197  0.00154052  0.00638451  0.01768694  0.21874766
 -0.00180886 -0.00223772] 3 4
train:	 Loss = 1.3782,	 Acc = 0.4935
10766 0.28
21095 0.536
10886 0.603
1402 0.641
243 0.51
24 0.333
0 0.0
0 0.0
0.5617830609212482
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3955,	 Acc = 0.4837
1367 0.377
6407 0.52
3023 0.462
308 0.396
8 0.875
0 0.0
0 0.0
0 0.0
0.49856351323619946
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
Testing:	 Loss = 1.4810,	 Acc1 = 0.3662,	 Acc2 = 0.3852

 ===== Epoch 135	 =====
[-0.36718738 -0.38160205  1.8584441   2.9785519  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.23297007 -0.00104714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 1 0
train:	 Loss = 1.3778,	 Acc = 0.4928
10765 0.278
21096 0.535
10886 0.607
1402 0.621
243 0.519
24 0.333
0 0.0
0 0.0
0.5615583489346527
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.4193,	 Acc = 0.4811
1367 0.364
6407 0.52
3023 0.459
308 0.393
8 0.875
0 0.0
0 0.0
0 0.0
0.49753745126205623
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4890,	 Acc1 = 0.3689,	 Acc2 = 0.3884

 ===== Epoch 136	 =====
[-0.36718738 -0.38160205  1.9138125   2.1301737  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.06250833 -0.03963368  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 1 2
train:	 Loss = 1.3760,	 Acc = 0.4934
10767 0.278
21094 0.538
10886 0.605
1402 0.619
243 0.506
24 0.125
0 0.0
0 0.0
0.5623644090463312
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3821,	 Acc = 0.4999
1367 0.383
6407 0.522
3023 0.509
308 0.455
8 0.875
0 0.0
0 0.0
0 0.0
0.5163143853888775
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
Testing:	 Loss = 1.4617,	 Acc1 = 0.3738,	 Acc2 = 0.3944

 ===== Epoch 137	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 1 1
train:	 Loss = 1.3784,	 Acc = 0.4922
10764 0.276
21094 0.535
10890 0.608
1401 0.602
243 0.564
24 0.25
0 0.0
0 0.0
0.5613633662189469
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3822,	 Acc = 0.5176
1367 0.394
6407 0.539
3023 0.537
308 0.422
8 0.875
0 0.0
0 0.0
0 0.0
0.5349887133182845
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
Testing:	 Loss = 1.4620,	 Acc1 = 0.3714,	 Acc2 = 0.3914

 ===== Epoch 138	 =====
[-0.36718738 -0.38160205  2.2969139   0.8802908  -0.44155708 -0.3651737
  3.0411184   3.033162   -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 1.4681835e-03  2.4594050e-03 -5.2682080e+00 -1.2888731e+00
  2.1561917e-03  6.3325174e-04  3.5299334e-01 -1.6153631e+00
  1.5405245e-03  6.3845129e-03  6.8517032e+00  6.1503563e+00
 -1.8088582e-03 -2.2377169e-03] 5 5
train:	 Loss = 1.3831,	 Acc = 0.4869
10760 0.262
21096 0.53
10892 0.608
1401 0.62
243 0.539
24 0.292
0 0.0
0 0.0
0.5589493700974566
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3702,	 Acc = 0.5034
1367 0.358
6407 0.535
3023 0.514
308 0.383
8 0.875
0 0.0
0 0.0
0 0.0
0.5237020316027088
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
Testing:	 Loss = 1.4511,	 Acc1 = 0.3823,	 Acc2 = 0.4046

 ===== Epoch 139	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038  1.1961699   1.8752084
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451 -0.14165828 -0.12080026
 -0.00180886 -0.00223772] 5 5
train:	 Loss = 1.3863,	 Acc = 0.4826
10763 0.259
21097 0.527
10887 0.601
1402 0.613
243 0.539
24 0.125
0 0.0
0 0.0
0.5540367872106499
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3724,	 Acc = 0.4973
1367 0.347
6407 0.523
3023 0.511
308 0.477
8 0.875
0 0.0
0 0.0
0 0.0
0.5184691155345783
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
Testing:	 Loss = 1.4530,	 Acc1 = 0.3776,	 Acc2 = 0.3989

 ===== Epoch 140	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 3 1
train:	 Loss = 1.3853,	 Acc = 0.4868
10764 0.266
21096 0.53
10887 0.605
1402 0.623
243 0.531
24 0.25
0 0.0
0 0.0
0.557589444906692
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.4013,	 Acc = 0.4890
1367 0.38
6407 0.52
3023 0.475
308 0.448
8 0.875
0 0.0
0 0.0
0 0.0
0.5042068540939872
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
Testing:	 Loss = 1.4778,	 Acc1 = 0.3736,	 Acc2 = 0.3941

 ===== Epoch 141	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
train:	 Loss = 1.3793,	 Acc = 0.4914
10765 0.28
21095 0.534
10886 0.602
1403 0.619
243 0.514
24 0.25
0 0.0
0 0.0
0.5591512882232326
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3432,	 Acc = 0.5290
1367 0.384
6407 0.548
3023 0.562
308 0.445
8 0.875
0 0.0
0 0.0
0 0.0
0.5493535809562897
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
Testing:	 Loss = 1.4365,	 Acc1 = 0.3780,	 Acc2 = 0.3994

 ===== Epoch 142	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 1 1
train:	 Loss = 1.3775,	 Acc = 0.4919
10760 0.279
21095 0.532
10891 0.607
1403 0.634
243 0.481
24 0.292
0 0.0
0 0.0
0.559900166389351
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3609,	 Acc = 0.5078
1367 0.383
6407 0.54
3023 0.504
308 0.412
8 0.875
0 0.0
0 0.0
0 0.0
0.525343730761338
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
Testing:	 Loss = 1.4374,	 Acc1 = 0.3776,	 Acc2 = 0.3989

 ===== Epoch 143	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038  0.71200514  3.1217608
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.03788781 -0.60287446
 -0.00180886 -0.00223772] 5 5
train:	 Loss = 1.3784,	 Acc = 0.4916
10763 0.278
21104 0.532
10883 0.608
1399 0.619
243 0.519
24 0.292
0 0.0
0 0.0
0.5598906486791668
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3704,	 Acc = 0.5041
1367 0.389
6407 0.535
3023 0.499
308 0.409
8 0.875
0 0.0
0 0.0
0 0.0
0.5202134208906218
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4447,	 Acc1 = 0.3906,	 Acc2 = 0.4145

 ===== Epoch 144	 =====
[-0.36718738 -0.38160205  2.8058167   2.9989672   2.6027944   1.1857166
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 1.4681835e-03  2.4594050e-03  4.3285471e-01 -7.6554304e-01
 -6.3568559e+00 -1.7419615e+00  1.1600148e-03  1.7411708e-03
  1.5405245e-03  6.3845129e-03  3.4734835e-03  4.9582324e-03
 -1.8088582e-03 -2.2377169e-03] 5 5
train:	 Loss = 1.3774,	 Acc = 0.4927
10765 0.278
21095 0.534
10887 0.607
1403 0.629
242 0.483
24 0.167
0 0.0
0 0.0
0.5612908977444949
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3702,	 Acc = 0.5044
1367 0.386
6407 0.533
3023 0.504
308 0.419
8 0.875
0 0.0
0 0.0
0 0.0
0.520931664272522
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4535,	 Acc1 = 0.3817,	 Acc2 = 0.4038

 ===== Epoch 145	 =====
[ 3.7526667   2.517265   -0.42025706 -0.3355664  -0.44155708 -0.3651737
  1.2833328   3.1533718  -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [-1.1038574e+00  1.6436854e-01  4.6474081e-03  3.7761789e-03
  2.1561917e-03  6.3325174e-04  2.3221460e-01  4.5784751e-01
  1.5405245e-03  6.3845129e-03  3.4734835e-03  4.9582324e-03
 -1.8088582e-03 -2.2377169e-03] 4 4
train:	 Loss = 1.3808,	 Acc = 0.4845
10762 0.26
21098 0.529
10888 0.601
1401 0.625
243 0.527
24 0.208
0 0.0
0 0.0
0.5561894574196232
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3682,	 Acc = 0.5105
1367 0.353
6407 0.538
3023 0.531
308 0.425
8 0.875
0 0.0
0 0.0
0 0.0
0.5325261645803406
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
Testing:	 Loss = 1.4492,	 Acc1 = 0.3753,	 Acc2 = 0.3961

 ===== Epoch 146	 =====
[ 2.2580705   1.7337875  -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
  2.2197576   1.9552743 ] [-0.06952564 -0.2461868   0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.02664086 -0.40649074] 5 5
train:	 Loss = 1.3802,	 Acc = 0.4885
10766 0.266
21092 0.53
10889 0.609
1402 0.628
243 0.539
24 0.375
0 0.0
0 0.0
0.5596136701337295
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3665,	 Acc = 0.5154
1367 0.383
6407 0.543
3023 0.523
308 0.448
8 0.875
0 0.0
0 0.0
0 0.0
0.5340652575415555
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
Testing:	 Loss = 1.4411,	 Acc1 = 0.3870,	 Acc2 = 0.4103

 ===== Epoch 147	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 6 1
train:	 Loss = 1.3766,	 Acc = 0.4946
10760 0.278
21095 0.537
10892 0.608
1402 0.638
243 0.51
24 0.125
0 0.0
0 0.0
0.5639113382457809
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3666,	 Acc = 0.5022
1367 0.389
6407 0.537
3023 0.486
308 0.422
8 0.875
0 0.0
0 0.0
0 0.0
0.518058690744921
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4385,	 Acc1 = 0.3934,	 Acc2 = 0.4180

 ===== Epoch 148	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664   2.2254093   2.5901709
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.02611456 -0.24367903
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 1 5
train:	 Loss = 1.3766,	 Acc = 0.4924
10762 0.28
21098 0.532
10888 0.61
1401 0.619
243 0.535
24 0.292
0 0.0
0 0.0
0.5603791525524455
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.4246,	 Acc = 0.4712
1367 0.383
6407 0.491
3023 0.474
308 0.403
8 0.875
0 0.0
0 0.0
0 0.0
0.48348040221629385
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4933,	 Acc1 = 0.3714,	 Acc2 = 0.3914

 ===== Epoch 149	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038  1.1086199   1.8727546
 -0.38724938 -0.38149557] [ 1.4681835e-03  2.4594050e-03  4.6474081e-03  3.7761789e-03
  2.1561917e-03  6.3325174e-04  1.1600148e-03  1.7411708e-03
  1.5405245e-03  6.3845129e-03 -8.9290954e-02  1.0571375e+00
 -1.8088582e-03 -2.2377169e-03] 6 6
train:	 Loss = 1.3830,	 Acc = 0.4846
10761 0.265
21096 0.526
10889 0.604
1403 0.627
243 0.51
24 0.25
0 0.0
0 0.0
0.5548061209329966
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3614,	 Acc = 0.5046
1367 0.355
6407 0.523
3023 0.538
308 0.458
8 0.875
0 0.0
0 0.0
0 0.0
0.5256515493535809
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
Testing:	 Loss = 1.4402,	 Acc1 = 0.3786,	 Acc2 = 0.4001

 ===== Epoch 150	 =====
[-0.36718738 -0.38160205  1.7497423   2.8946211  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.19681492  0.07853761  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 3 1
train:	 Loss = 1.3844,	 Acc = 0.4830
10770 0.258
21094 0.527
10883 0.602
1402 0.625
243 0.498
24 0.333
0 0.0
0 0.0
0.5551031326160614
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3681,	 Acc = 0.5080
1367 0.385
6407 0.54
3023 0.503
308 0.419
8 0.875
0 0.0
0 0.0
0 0.0
0.5252411245639237
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4411,	 Acc1 = 0.3833,	 Acc2 = 0.4058

 ===== Epoch 151	 =====
[-0.36718738 -0.38160205  1.6296405   2.2277145  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 0 0
train:	 Loss = 1.3755,	 Acc = 0.4916
10764 0.277
21095 0.532
10889 0.607
1401 0.636
243 0.477
24 0.292
0 0.0
0 0.0
0.5602638773326994
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3760,	 Acc = 0.5085
1367 0.394
6407 0.533
3023 0.516
308 0.422
8 0.875
0 0.0
0 0.0
0 0.0
0.5245228811820234
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4508,	 Acc1 = 0.3844,	 Acc2 = 0.4071

 ===== Epoch 152	 =====
[-0.36718738 -0.38160205  2.4182363   3.0828977   2.3550282   1.1213734
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 1.4681835e-03  2.4594050e-03  3.2935765e-01 -7.7280030e+00
 -5.8393245e+00 -1.6696650e+00  1.1600148e-03  1.7411708e-03
  1.5405245e-03  6.3845129e-03  3.4734835e-03  4.9582324e-03
 -1.8088582e-03 -2.2377169e-03] 2 1
train:	 Loss = 1.3768,	 Acc = 0.4913
10765 0.278
21095 0.533
10888 0.605
1402 0.628
242 0.496
24 0.208
0 0.0
0 0.0
0.5596267570057354
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3783,	 Acc = 0.5006
1367 0.35
6407 0.528
3023 0.515
308 0.448
8 0.875
0 0.0
0 0.0
0 0.0
0.5216499076544223
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4617,	 Acc1 = 0.3782,	 Acc2 = 0.3996

 ===== Epoch 153	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
  2.3517113   1.8321486 ] [1.4681835e-03 2.4594050e-03 4.6474081e-03 3.7761789e-03 2.1561917e-03
 6.3325174e-04 1.1600148e-03 1.7411708e-03 1.8262095e+00 3.8804171e+00
 3.4734835e-03 4.9582324e-03 2.8622958e-01 2.1099465e-01] 6 2
train:	 Loss = 1.3831,	 Acc = 0.4848
10758 0.258
21101 0.528
10889 0.606
1401 0.627
243 0.498
24 0.292
0 0.0
0 0.0
0.5571929407570265
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3996,	 Acc = 0.4911
1367 0.379
6407 0.51
3023 0.505
308 0.451
8 0.875
0 0.0
0 0.0
0 0.0
0.5068746152267597
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4740,	 Acc1 = 0.3674,	 Acc2 = 0.3867

 ===== Epoch 154	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 5 1
train:	 Loss = 1.3834,	 Acc = 0.4858
10764 0.264
21095 0.528
10888 0.605
1402 0.625
243 0.49
24 0.292
0 0.0
0 0.0
0.5566682515155117
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.4211,	 Acc = 0.4646
1367 0.341
6407 0.503
3023 0.452
308 0.331
8 0.875
0 0.0
0 0.0
0 0.0
0.481941309255079
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
Testing:	 Loss = 1.4983,	 Acc1 = 0.3627,	 Acc2 = 0.3810

 ===== Epoch 155	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 1.4681835e-03  2.4594050e-03  4.5435467e+00  3.6333230e+00
  2.1561917e-03  6.3325174e-04  1.1600148e-03  1.7411708e-03
  1.5405245e-03  6.3845129e-03  3.4734835e-03  4.9582324e-03
 -1.8088582e-03 -2.2377169e-03] 2 1
train:	 Loss = 1.3854,	 Acc = 0.4846
10764 0.26
21096 0.529
10887 0.604
1402 0.611
243 0.535
24 0.292
0 0.0
0 0.0
0.5564899560204446
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.4113,	 Acc = 0.4874
1367 0.377
6407 0.528
3023 0.464
308 0.351
8 0.875
0 0.0
0 0.0
0 0.0
0.5028729735276011
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4856,	 Acc1 = 0.3755,	 Acc2 = 0.3964

 ===== Epoch 156	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966  2.3630035   2.278877   -0.4254193  -0.4215889
  3.2718685   1.5754181 ] [ 1.4681835e-03  2.4594050e-03  4.6474081e-03  3.7761789e-03
  2.1561917e-03  6.3325174e-04  1.1600148e-03  1.7411708e-03
  2.8040189e-02 -1.6506898e-01  3.4734835e-03  4.9582324e-03
  9.9887860e-01 -2.5989351e-01] 5 5
train:	 Loss = 1.3831,	 Acc = 0.4863
10761 0.261
21096 0.53
10889 0.606
1403 0.617
243 0.519
24 0.333
0 0.0
0 0.0
0.5581637201010251
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3860,	 Acc = 0.4942
1367 0.363
6407 0.523
3023 0.499
308 0.416
8 0.875
0 0.0
0 0.0
0 0.0
0.5126205622819618
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
Testing:	 Loss = 1.4715,	 Acc1 = 0.3699,	 Acc2 = 0.3897

 ===== Epoch 157	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664   4.241189   -3.9839177
 -0.36400968 -0.37184966  5.540328    2.529613    0.4089784   1.6077396
  3.32729     1.4837288 ] [ 1.4681835e-03  2.4594050e-03  4.6474081e-03  3.7761789e-03
  8.2237983e-01 -9.4100080e-02  1.1600148e-03  1.7411708e-03
 -1.0178499e+01 -4.8269711e+00  4.5308450e-01 -7.0496857e-02
  6.0820377e-01 -6.8872832e-02] 4 2
train:	 Loss = 1.3803,	 Acc = 0.4871
10763 0.264
21098 0.529
10886 0.608
1402 0.628
243 0.51
24 0.375
0 0.0
0 0.0
0.5585831872344219
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3625,	 Acc = 0.5176
1367 0.385
6407 0.539
3023 0.542
308 0.422
8 0.875
0 0.0
0 0.0
0 0.0
0.5362199876872563
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4433,	 Acc1 = 0.3864,	 Acc2 = 0.4095

 ===== Epoch 158	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
train:	 Loss = 1.3836,	 Acc = 0.4854
10762 0.257
21094 0.529
10890 0.608
1403 0.627
243 0.498
24 0.333
0 0.0
0 0.0
0.5582991620609734
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3775,	 Acc = 0.4984
1367 0.347
6407 0.533
3023 0.5
308 0.422
8 0.875
0 0.0
0 0.0
0 0.0
0.5195977837061359
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4443,	 Acc1 = 0.3957,	 Acc2 = 0.4207

 ===== Epoch 159	 =====
[ 1.8635832   2.830656   -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
  1.627285    3.1184196 ] [-0.09442678 -0.14499356  0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.04815937 -0.2065854 ] 5 5
train:	 Loss = 1.3840,	 Acc = 0.4825
10764 0.257
21095 0.526
10888 0.602
1403 0.619
242 0.525
24 0.25
0 0.0
0 0.0
0.5545881374063949
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3572,	 Acc = 0.5095
1367 0.35
6407 0.532
3023 0.537
308 0.471
8 0.875
0 0.0
0 0.0
0 0.0
0.5318079211984403
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4339,	 Acc1 = 0.3906,	 Acc2 = 0.4145

 ===== Epoch 160	 =====
[ 0.21074964  1.5265449  -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00243498  0.00535064  0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 0 0
train:	 Loss = 1.3812,	 Acc = 0.4862
10766 0.259
21091 0.529
10889 0.609
1403 0.633
243 0.523
24 0.375
0 0.0
0 0.0
0.5587815750371471
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3957,	 Acc = 0.4724
1367 0.254
6407 0.52
3023 0.477
308 0.432
8 0.0
0 0.0
0 0.0
0 0.0
0.5030781859224297
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4646,	 Acc1 = 0.3476,	 Acc2 = 0.4028

 ===== Epoch 161	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038  1.9924183   2.194208
 -0.38724938 -0.38149557] [ 1.4681835e-03  2.4594050e-03  4.6474081e-03  3.7761789e-03
  2.1561917e-03  6.3325174e-04  1.1600148e-03  1.7411708e-03
  1.5405245e-03  6.3845129e-03  6.5567754e-02 -6.8252158e-01
 -1.8088582e-03 -2.2377169e-03] 5 5
train:	 Loss = 1.3828,	 Acc = 0.4828
10759 0.258
21100 0.526
10888 0.602
1402 0.621
243 0.531
24 0.25
0 0.0
0 0.0
0.5546245951807944
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3614,	 Acc = 0.5143
1367 0.36
6407 0.54
3023 0.541
308 0.39
8 0.875
0 0.0
0 0.0
0 0.0
0.5359121690950134
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
Testing:	 Loss = 1.4376,	 Acc1 = 0.3908,	 Acc2 = 0.4148

 ===== Epoch 162	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664   2.8023844   2.1930187
 -0.36400968 -0.37184966  1.865721    1.4621224  -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618 -0.03138899 -0.6949089
  0.00116001  0.00174117 -0.06029101 -1.030501    0.00347348  0.00495823
 -0.00180886 -0.00223772] 5 5
train:	 Loss = 1.3833,	 Acc = 0.4818
10767 0.251
21095 0.527
10887 0.605
1400 0.624
243 0.498
24 0.292
0 0.0
0 0.0
0.5557074504442926
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3806,	 Acc = 0.4920
1367 0.356
6407 0.526
3023 0.485
308 0.448
8 0.875
0 0.0
0 0.0
0 0.0
0.5111840755181613
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4521,	 Acc1 = 0.3934,	 Acc2 = 0.4180

 ===== Epoch 163	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038  1.041071    2.191754
 -0.38724938 -0.38149557] [ 1.4681835e-03  2.4594050e-03  4.6474081e-03  3.7761789e-03
  2.1561917e-03  6.3325174e-04  1.1600148e-03  1.7411708e-03
  1.5405245e-03  6.3845129e-03  1.6506428e-01  1.1158248e+00
 -1.8088582e-03 -2.2377169e-03] 6 6
train:	 Loss = 1.3841,	 Acc = 0.4873
10759 0.26
21101 0.53
10888 0.611
1401 0.635
243 0.506
24 0.208
0 0.0
0 0.0
0.5598835309148171
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3795,	 Acc = 0.5020
1367 0.353
6407 0.539
3023 0.499
308 0.416
8 0.875
0 0.0
0 0.0
0 0.0
0.5228811820233942
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
Testing:	 Loss = 1.4617,	 Acc1 = 0.3679,	 Acc2 = 0.3872

 ===== Epoch 164	 =====
[-0.36718738 -0.38160205  1.8270949   2.009949   -0.44155708 -0.3651737
  4.93477     3.3798134  -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 1.46818347e-03  2.45940499e-03 -1.12279825e-01  4.28228170e-01
  2.15619174e-03  6.33251737e-04  2.31275463e+00  8.93221736e-01
  1.54052454e-03  6.38451288e-03  3.47348349e-03  4.95823240e-03
 -1.80885824e-03 -2.23771692e-03] 4 4
train:	 Loss = 1.3825,	 Acc = 0.4855
10761 0.26
21097 0.529
10888 0.607
1403 0.619
243 0.514
24 0.333
0 0.0
0 0.0
0.5575694547615511
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3920,	 Acc = 0.4793
1367 0.353
6407 0.508
3023 0.478
308 0.455
8 0.875
0 0.0
0 0.0
0 0.0
0.4971270264723989
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
Testing:	 Loss = 1.4764,	 Acc1 = 0.3610,	 Acc2 = 0.3790

 ===== Epoch 165	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664   3.0734737   2.0776448
 -0.36400968 -0.37184966  3.410272    1.2734497  -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.8359589  -0.04174745
  0.00116001  0.00174117  0.03196629 -0.00586217  0.00347348  0.00495823
 -0.00180886 -0.00223772] 2 2
train:	 Loss = 1.3834,	 Acc = 0.4825
10764 0.257
21091 0.526
10892 0.603
1402 0.629
243 0.494
24 0.25
0 0.0
0 0.0
0.554707001069773
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3915,	 Acc = 0.4749
1367 0.354
6407 0.51
3023 0.464
308 0.37
8 0.875
0 0.0
0 0.0
0 0.0
0.4918941104042684
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4677,	 Acc1 = 0.3751,	 Acc2 = 0.3959

 ===== Epoch 166	 =====
[ 2.7584896   3.2754693  -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
  2.4080112   3.5297122 ] [ 0.0380263   0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
  0.35989654 -0.00223772] 0 0
train:	 Loss = 1.3838,	 Acc = 0.4834
10764 0.254
21099 0.528
10885 0.605
1401 0.622
243 0.527
24 0.25
0 0.0
0 0.0
0.5566088196838227
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3747,	 Acc = 0.5019
1367 0.342
6407 0.528
3023 0.526
308 0.412
8 0.875
0 0.0
0 0.0
0 0.0
0.5244202749846091
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4489,	 Acc1 = 0.3870,	 Acc2 = 0.4103

 ===== Epoch 167	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038  2.1244984   1.1243323
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451 -0.04515431  0.33193034
 -0.00180886 -0.00223772] 3 2
train:	 Loss = 1.3852,	 Acc = 0.4839
10767 0.259
21093 0.527
10887 0.603
1402 0.621
243 0.527
24 0.458
0 0.0
0 0.0
0.5556777318791049
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3750,	 Acc = 0.4820
1367 0.358
6407 0.511
3023 0.484
308 0.39
8 0.875
0 0.0
0 0.0
0 0.0
0.4994869690129284
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4533,	 Acc1 = 0.3769,	 Acc2 = 0.3981

 ===== Epoch 168	 =====
[-0.36718738 -0.38160205  2.9572663   2.17781    -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.4038066   0.3076452   0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 2 4
train:	 Loss = 1.3828,	 Acc = 0.4844
10764 0.256
21096 0.527
10887 0.61
1402 0.621
243 0.506
24 0.25
0 0.0
0 0.0
0.5575597289908475
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3926,	 Acc = 0.4777
1367 0.241
6407 0.526
3023 0.486
308 0.451
8 0.0
0 0.0
0 0.0
0 0.0
0.5108762569259183
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4588,	 Acc1 = 0.3503,	 Acc2 = 0.4061

 ===== Epoch 169	 =====
[ 2.9017403   0.8466886  -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966  1.6852567   1.2957923  -0.4254193  -0.4215889
  3.1715834   0.9781272 ] [-3.4726632e+00 -1.4026806e+00  4.6474081e-03  3.7761789e-03
  2.1561917e-03  6.3325174e-04  1.1600148e-03  1.7411708e-03
  3.2935783e-01  2.0641360e-01  3.4734835e-03  4.9582324e-03
 -1.8555799e-01  1.6212890e-01] 2 2
train:	 Loss = 1.3835,	 Acc = 0.4823
10762 0.254
21099 0.525
10886 0.608
1402 0.613
243 0.527
24 0.333
0 0.0
0 0.0
0.5554168895227908
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.4073,	 Acc = 0.4838
1367 0.348
6407 0.522
3023 0.468
308 0.429
8 0.875
0 0.0
0 0.0
0 0.0
0.5028729735276011
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4715,	 Acc1 = 0.3776,	 Acc2 = 0.3989

 ===== Epoch 170	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038  2.1924243   1.5022242
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451 -0.38628888  0.23551546
 -0.00180886 -0.00223772] 1 4
train:	 Loss = 1.3825,	 Acc = 0.4834
10763 0.257
21096 0.525
10890 0.611
1400 0.604
243 0.49
24 0.333
0 0.0
0 0.0
0.5557305440822512
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3696,	 Acc = 0.5019
1367 0.353
6407 0.536
3023 0.5
308 0.458
8 0.875
0 0.0
0 0.0
0 0.0
0.5227785758259799
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4418,	 Acc1 = 0.3870,	 Acc2 = 0.4103

 ===== Epoch 171	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 1.4681835e-03  2.4594050e-03  1.0112411e+00  2.6324847e+00
  2.1561917e-03  6.3325174e-04  1.1600148e-03  1.7411708e-03
  1.5405245e-03  6.3845129e-03  3.4734835e-03  4.9582324e-03
 -1.8088582e-03 -2.2377169e-03] 5 6
train:	 Loss = 1.3830,	 Acc = 0.4837
10766 0.255
21093 0.529
10888 0.604
1402 0.623
243 0.502
24 0.208
0 0.0
0 0.0
0.5569687964338782
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3926,	 Acc = 0.4793
1367 0.348
6407 0.516
3023 0.468
308 0.409
8 0.875
0 0.0
0 0.0
0 0.0
0.49774266365688485
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4613,	 Acc1 = 0.3875,	 Acc2 = 0.4108

 ===== Epoch 172	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038  1.6259933   1.2715629
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451 -0.28529605  0.01334213
 -0.00180886 -0.00223772] 1 1
train:	 Loss = 1.3831,	 Acc = 0.4840
10761 0.258
21098 0.526
10888 0.608
1402 0.62
243 0.539
24 0.25
0 0.0
0 0.0
0.5564403506165503
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3594,	 Acc = 0.5188
1367 0.356
6407 0.538
3023 0.562
308 0.396
8 0.875
0 0.0
0 0.0
0 0.0
0.5415555099528011
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4448,	 Acc1 = 0.3862,	 Acc2 = 0.4093

 ===== Epoch 173	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664   1.5249336   1.3254963
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.35196966  0.46931395
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 6 6
train:	 Loss = 1.3866,	 Acc = 0.4827
10759 0.257
21099 0.525
10890 0.606
1401 0.617
243 0.523
24 0.375
0 0.0
0 0.0
0.5549811331966604
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3604,	 Acc = 0.5197
1367 0.36
6407 0.547
3023 0.539
308 0.455
8 0.875
0 0.0
0 0.0
0 0.0
0.5420685409398728
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 6
Testing:	 Loss = 1.4376,	 Acc1 = 0.3976,	 Acc2 = 0.4230

 ===== Epoch 174	 =====
[ 2.764092    2.3226593  -0.42025706 -0.3355664  -0.44155708 -0.3651737
  0.7652425   1.9149315  -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [-0.6111057   0.29736534  0.00464741  0.00377618  0.00215619  0.00063325
 -0.14797501  0.5718741   0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 6 6
train:	 Loss = 1.3822,	 Acc = 0.4840
10759 0.25
21099 0.53
10889 0.609
1402 0.62
243 0.481
24 0.333
0 0.0
0 0.0
0.5589327628725079
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3763,	 Acc = 0.4987
1367 0.358
6407 0.536
3023 0.493
308 0.403
8 0.875
0 0.0
0 0.0
0 0.0
0.5184691155345783
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
Testing:	 Loss = 1.4537,	 Acc1 = 0.3761,	 Acc2 = 0.3971

 ===== Epoch 175	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 3 1
train:	 Loss = 1.3839,	 Acc = 0.4830
10763 0.252
21099 0.526
10887 0.608
1400 0.631
243 0.502
24 0.292
0 0.0
0 0.0
0.5568597153299855
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.4063,	 Acc = 0.4820
1367 0.354
6407 0.524
3023 0.459
308 0.39
8 0.875
0 0.0
0 0.0
0 0.0
0.5
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4750,	 Acc1 = 0.3839,	 Acc2 = 0.4066

 ===== Epoch 176	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664   1.1869311   2.7565756
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.39749405  0.12278938
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 2 2
train:	 Loss = 1.3835,	 Acc = 0.4841
10761 0.26
21096 0.525
10889 0.607
1403 0.627
243 0.539
24 0.292
0 0.0
0 0.0
0.5557866587431288
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3490,	 Acc = 0.5248
1367 0.355
6407 0.552
3023 0.555
308 0.419
8 0.875
0 0.0
0 0.0
0 0.0
0.5486353375743895
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4291,	 Acc1 = 0.3928,	 Acc2 = 0.4172

 ===== Epoch 177	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966  1.3386502   2.9938476  -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117 -0.00336558 -0.00586217  0.00347348  0.00495823
 -0.00180886 -0.00223772] 0 0
train:	 Loss = 1.3829,	 Acc = 0.4850
10767 0.26
21094 0.527
10886 0.609
1403 0.619
242 0.5
24 0.375
0 0.0
0 0.0
0.5569853487473625
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3920,	 Acc = 0.4876
1367 0.347
6407 0.526
3023 0.473
308 0.442
8 0.875
0 0.0
0 0.0
0 0.0
0.5072850400164169
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4571,	 Acc1 = 0.3864,	 Acc2 = 0.4095

 ===== Epoch 178	 =====
[ 1.7174227   3.457438   -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038  1.4667437   1.0605325
 -0.38724938 -0.38149557] [-2.2140012e+00 -4.3893261e+00  4.6474081e-03  3.7761789e-03
  2.1561917e-03  6.3325174e-04  1.1600148e-03  1.7411708e-03
  1.5405245e-03  6.3845129e-03  1.4636198e-01  9.1501819e-03
 -1.8088582e-03 -2.2377169e-03] 3 2
train:	 Loss = 1.3837,	 Acc = 0.4855
10763 0.259
21090 0.528
10894 0.61
1402 0.626
243 0.502
24 0.292
0 0.0
0 0.0
0.5577808813478738
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3769,	 Acc = 0.4965
1367 0.353
6407 0.526
3023 0.506
308 0.409
8 0.875
0 0.0
0 0.0
0 0.0
0.5167248101785348
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4550,	 Acc1 = 0.3763,	 Acc2 = 0.3974

 ===== Epoch 179	 =====
[ 0.2147825   1.5315996  -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00021887  0.00824188  0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 0 0
train:	 Loss = 1.3844,	 Acc = 0.4826
10768 0.256
21097 0.524
10883 0.608
1401 0.618
243 0.519
24 0.333
0 0.0
0 0.0
0.5550701378982406
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3876,	 Acc = 0.4918
1367 0.342
6407 0.512
3023 0.522
308 0.422
8 0.875
0 0.0
0 0.0
0 0.0
0.5127231684793762
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4621,	 Acc1 = 0.3831,	 Acc2 = 0.4056

 ===== Epoch 180	 =====
[ 0.07735974  1.956194   -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [-4.7098491e-01 -2.6719327e+00  4.6474081e-03  3.7761789e-03
  2.8438649e+00  1.2944912e+00  1.1600148e-03  1.7411708e-03
  1.5405245e-03  6.3845129e-03  3.4734835e-03  4.9582324e-03
 -1.8088582e-03 -2.2377169e-03] 6 6
train:	 Loss = 1.3832,	 Acc = 0.4839
10765 0.26
21098 0.525
10885 0.608
1401 0.613
243 0.514
24 0.417
0 0.0
0 0.0
0.5555258387566492
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3780,	 Acc = 0.4981
1367 0.347
6407 0.528
3023 0.511
308 0.416
8 0.875
0 0.0
0 0.0
0 0.0
0.5191873589164786
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4499,	 Acc1 = 0.3850,	 Acc2 = 0.4078

 ===== Epoch 181	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 3 1
train:	 Loss = 1.3828,	 Acc = 0.4818
10766 0.256
21089 0.525
10893 0.604
1402 0.618
242 0.508
24 0.417
0 0.0
0 0.0
0.5542347696879644
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3751,	 Acc = 0.4807
1367 0.337
6407 0.519
3023 0.472
308 0.396
8 0.875
0 0.0
0 0.0
0 0.0
0.5009234557767289
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
Testing:	 Loss = 1.4489,	 Acc1 = 0.3753,	 Acc2 = 0.3961

 ===== Epoch 182	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 3 1
train:	 Loss = 1.3834,	 Acc = 0.4846
10765 0.253
21097 0.529
10886 0.607
1402 0.634
242 0.529
24 0.333
0 0.0
0 0.0
0.558616385842917
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3876,	 Acc = 0.4875
1367 0.353
6407 0.524
3023 0.481
308 0.386
8 0.875
0 0.0
0 0.0
0 0.0
0.506361584239688
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4633,	 Acc1 = 0.3771,	 Acc2 = 0.3984

 ===== Epoch 183	 =====
[ 2.882219    1.1373335  -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
  2.175333    1.3213074 ] [-1.3421459e+00  2.8290918e-01  4.6474081e-03  3.7761789e-03
  2.1561917e-03  6.3325174e-04  1.1600148e-03  1.7411708e-03
  1.5405245e-03  6.3845129e-03  3.4734835e-03  4.9582324e-03
 -5.5388272e-01  4.4199640e-01] 6 6
train:	 Loss = 1.3832,	 Acc = 0.4856
10764 0.257
21094 0.528
10889 0.61
1402 0.64
243 0.523
24 0.333
0 0.0
0 0.0
0.5585997860454059
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3680,	 Acc = 0.5090
1367 0.347
6407 0.541
3023 0.517
308 0.471
8 0.875
0 0.0
0 0.0
0 0.0
0.5318079211984403
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4403,	 Acc1 = 0.3875,	 Acc2 = 0.4108

 ===== Epoch 184	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 1.4681835e-03  2.4594050e-03  4.6474081e-03  3.7761789e-03
  4.9299664e+00  3.4808369e+00  1.1600148e-03  1.7411708e-03
  1.5405245e-03  6.3845129e-03  3.4734835e-03  4.9582324e-03
 -1.8088582e-03 -2.2377169e-03] 2 1
train:	 Loss = 1.3831,	 Acc = 0.4844
10763 0.255
21094 0.527
10890 0.608
1402 0.629
243 0.539
24 0.333
0 0.0
0 0.0
0.5576620212165334
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3939,	 Acc = 0.5017
1367 0.355
6407 0.535
3023 0.505
308 0.425
8 0.875
0 0.0
0 0.0
0 0.0
0.5222655448389083
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 6
Testing:	 Loss = 1.4661,	 Acc1 = 0.3833,	 Acc2 = 0.4058

 ===== Epoch 185	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
train:	 Loss = 1.3834,	 Acc = 0.4826
10760 0.254
21101 0.525
10887 0.605
1401 0.637
243 0.519
24 0.292
0 0.0
0 0.0
0.5557701449964345
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3616,	 Acc = 0.5138
1367 0.352
6407 0.534
3023 0.55
308 0.451
8 0.875
0 0.0
0 0.0
0 0.0
0.5365278062794993
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4415,	 Acc1 = 0.3862,	 Acc2 = 0.4093

 ===== Epoch 186	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664   3.0295024   2.0266142
 -0.36400968 -0.37184966  3.1347048   1.2411766  -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.24015735  0.19757888
  0.00116001  0.00174117 -0.02790523  0.27172923  0.00347348  0.00495823
 -0.00180886 -0.00223772] 3 2
train:	 Loss = 1.3827,	 Acc = 0.4852
10766 0.258
21094 0.528
10888 0.607
1403 0.632
241 0.539
24 0.292
0 0.0
0 0.0
0.558038632986627
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3825,	 Acc = 0.4772
1367 0.342
6407 0.509
3023 0.481
308 0.367
8 0.875
0 0.0
0 0.0
0 0.0
0.49620357069567
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
Testing:	 Loss = 1.4651,	 Acc1 = 0.3697,	 Acc2 = 0.3894

 ===== Epoch 187	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664   1.6228161   2.4037979
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618 -0.14240137  0.09536658
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 1 1
train:	 Loss = 1.3850,	 Acc = 0.4824
10765 0.258
21096 0.526
10886 0.599
1402 0.629
243 0.551
24 0.25
0 0.0
0 0.0
0.5542777332025794
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3741,	 Acc = 0.4983
1367 0.353
6407 0.529
3023 0.503
308 0.442
8 0.875
0 0.0
0 0.0
0 0.0
0.5186743279294069
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4492,	 Acc1 = 0.3854,	 Acc2 = 0.4083

 ===== Epoch 188	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 1.4681835e-03  2.4594050e-03  3.1009183e+00  3.1944010e+00
  2.1561917e-03  6.3325174e-04  1.1600148e-03  1.7411708e-03
  1.5405245e-03  6.3845129e-03  3.4734835e-03  4.9582324e-03
 -1.8088582e-03 -2.2377169e-03] 5 6
train:	 Loss = 1.3807,	 Acc = 0.4861
10763 0.259
21095 0.528
10888 0.611
1403 0.628
243 0.519
24 0.292
0 0.0
0 0.0
0.5587911924642677
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3683,	 Acc = 0.4989
1367 0.342
6407 0.522
3023 0.524
308 0.451
8 0.875
0 0.0
0 0.0
0 0.0
0.520931664272522
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4466,	 Acc1 = 0.3806,	 Acc2 = 0.4026

 ===== Epoch 189	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664   1.7015811   0.84847003
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00375358  0.09785957
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 2
train:	 Loss = 1.3829,	 Acc = 0.4840
10764 0.254
21099 0.526
10885 0.609
1401 0.635
243 0.523
24 0.333
0 0.0
0 0.0
0.557470581243314
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3691,	 Acc = 0.4881
1367 0.347
6407 0.517
3023 0.5
308 0.396
8 0.875
0 0.0
0 0.0
0 0.0
0.5079006772009029
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4464,	 Acc1 = 0.3813,	 Acc2 = 0.4033

 ===== Epoch 190	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  1.2145537   2.524367   -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.00619041 -0.31442347  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 5 5
train:	 Loss = 1.3828,	 Acc = 0.4842
10766 0.256
21096 0.527
10884 0.608
1403 0.627
243 0.523
24 0.208
0 0.0
0 0.0
0.5570876671619613
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3857,	 Acc = 0.4929
1367 0.342
6407 0.523
3023 0.503
308 0.416
8 0.875
0 0.0
0 0.0
0 0.0
0.5140570490457623
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
Testing:	 Loss = 1.4675,	 Acc1 = 0.3650,	 Acc2 = 0.3837

 ===== Epoch 191	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966  3.3300636   1.4521923  -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 5 0
train:	 Loss = 1.3829,	 Acc = 0.4845
10763 0.26
21094 0.527
10889 0.606
1403 0.628
243 0.498
24 0.208
0 0.0
0 0.0
0.5562654146732832
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3497,	 Acc = 0.5106
1367 0.352
6407 0.545
3023 0.517
308 0.419
8 0.875
0 0.0
0 0.0
0 0.0
0.5328339831725836
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4291,	 Acc1 = 0.3922,	 Acc2 = 0.4165

 ===== Epoch 192	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038  0.79766864  1.5954702
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.09923048 -0.13756807
 -0.00180886 -0.00223772] 2 5
train:	 Loss = 1.3819,	 Acc = 0.4846
10763 0.257
21099 0.527
10885 0.607
1402 0.637
243 0.51
24 0.25
0 0.0
0 0.0
0.5573351558553472
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3930,	 Acc = 0.4812
1367 0.356
6407 0.516
3023 0.467
308 0.442
8 0.875
0 0.0
0 0.0
0 0.0
0.49887133182844245
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4599,	 Acc1 = 0.3837,	 Acc2 = 0.4063

 ===== Epoch 193	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [2.7663300e+00 3.8738227e+00 4.6474081e-03 3.7761789e-03 2.1561917e-03
 6.3325174e-04 1.1600148e-03 1.7411708e-03 1.5405245e-03 6.3845129e-03
 3.4734835e-03 4.9582324e-03 4.5456405e+00 6.1637316e+00] 3 1
train:	 Loss = 1.3835,	 Acc = 0.4828
10763 0.254
21097 0.526
10887 0.607
1402 0.622
243 0.551
24 0.292
0 0.0
0 0.0
0.5561168395091076
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3975,	 Acc = 0.4892
1367 0.349
6407 0.522
3023 0.488
308 0.425
8 0.875
0 0.0
0 0.0
0 0.0
0.5088241329776318
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4664,	 Acc1 = 0.3875,	 Acc2 = 0.4108

 ===== Epoch 194	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 2 1
train:	 Loss = 1.3813,	 Acc = 0.4846
10763 0.257
21096 0.527
10887 0.609
1403 0.636
243 0.523
24 0.292
0 0.0
0 0.0
0.5574837310195228
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.4000,	 Acc = 0.4863
1367 0.336
6407 0.521
3023 0.491
308 0.386
8 0.875
0 0.0
0 0.0
0 0.0
0.5073876462138314
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4677,	 Acc1 = 0.3831,	 Acc2 = 0.4056

 ===== Epoch 195	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  1.4823371   3.6453931  -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 1.4681835e-03  2.4594050e-03  3.8775382e+00  1.2120173e+00
  2.1561917e-03  6.3325174e-04 -3.4384973e+00 -7.4462681e+00
  1.5405245e-03  6.3845129e-03  3.4734835e-03  4.9582324e-03
 -1.8088582e-03 -2.2377169e-03] 6 2
train:	 Loss = 1.3833,	 Acc = 0.4844
10764 0.258
21096 0.525
10888 0.608
1402 0.64
243 0.519
23 0.391
0 0.0
0 0.0
0.5567276833472008
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3645,	 Acc = 0.5091
1367 0.334
6407 0.528
3023 0.558
308 0.409
8 0.875
0 0.0
0 0.0
0 0.0
0.5337574389493125
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4542,	 Acc1 = 0.3677,	 Acc2 = 0.3869

 ===== Epoch 196	 =====
[ 4.853019    2.9216406  -0.42025706 -0.3355664  -0.44155708 -0.3651737
  1.3740969   2.893383   -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [-5.5464315e+00 -3.7763844e+00  4.6474081e-03  3.7761789e-03
  2.1561917e-03  6.3325174e-04  3.6034378e-01  2.6089248e-01
  1.5405245e-03  6.3845129e-03  3.4734835e-03  4.9582324e-03
 -1.8088582e-03 -2.2377169e-03] 2 4
train:	 Loss = 1.3841,	 Acc = 0.4834
10767 0.255
21096 0.527
10884 0.604
1402 0.636
243 0.51
24 0.25
0 0.0
0 0.0
0.5564504145739844
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.4026,	 Acc = 0.4789
1367 0.326
6407 0.514
3023 0.482
308 0.377
8 0.875
0 0.0
0 0.0
0 0.0
0.5003078185922429
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4714,	 Acc1 = 0.3755,	 Acc2 = 0.3964

 ===== Epoch 197	 =====
[-0.36718738 -0.38160205  1.282365    0.82358104 -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.32954133  0.19670889  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
train:	 Loss = 1.3847,	 Acc = 0.4826
10763 0.256
21094 0.526
10890 0.604
1402 0.621
243 0.481
24 0.25
0 0.0
0 0.0
0.5549579532285384
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3811,	 Acc = 0.4829
1367 0.35
6407 0.517
3023 0.476
308 0.425
8 0.875
0 0.0
0 0.0
0 0.0
0.5016416991586292
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4532,	 Acc1 = 0.3788,	 Acc2 = 0.4003

 ===== Epoch 198	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
train:	 Loss = 1.3830,	 Acc = 0.4852
10764 0.261
21092 0.527
10891 0.607
1402 0.626
243 0.527
24 0.333
0 0.0
0 0.0
0.5567573992630452
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3737,	 Acc = 0.5021
1367 0.353
6407 0.534
3023 0.51
308 0.416
8 0.875
0 0.0
0 0.0
0 0.0
0.5230863944182229
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4475,	 Acc1 = 0.3856,	 Acc2 = 0.4085

 ===== Epoch 199	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966  0.66033155  2.0976517  -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.08496865 -0.5406338   0.00347348  0.00495823
 -0.00180886 -0.00223772] 5 5
train:	 Loss = 1.3815,	 Acc = 0.4872
10766 0.259
21094 0.531
10887 0.611
1402 0.618
243 0.494
24 0.333
0 0.0
0 0.0
0.5600891530460624
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3820,	 Acc = 0.5018
1367 0.343
6407 0.528
3023 0.526
308 0.403
8 0.875
0 0.0
0 0.0
0 0.0
0.5240098501949517
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4583,	 Acc1 = 0.3800,	 Acc2 = 0.4018

 ===== Epoch 200	 =====
[ 1.4197894   2.1255262  -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
  2.1643372   2.3875241 ] [ 1.26120925e+00  1.06543846e-01  4.64740815e-03  3.77617893e-03
  2.15619174e-03  6.33251737e-04  1.16001477e-03  1.74117077e-03
  1.54052454e-03  6.38451288e-03  3.47348349e-03  4.95823240e-03
 -5.76232791e-01  1.22147836e-01] 1 3
train:	 Loss = 1.3822,	 Acc = 0.4841
10762 0.258
21097 0.526
10890 0.608
1400 0.635
243 0.519
24 0.25
0 0.0
0 0.0
0.5565757413680394
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3879,	 Acc = 0.4960
1367 0.336
6407 0.529
3023 0.504
308 0.422
8 0.875
0 0.0
0 0.0
0 0.0
0.5184691155345783
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4533,	 Acc1 = 0.3943,	 Acc2 = 0.4190

 ===== Epoch 201	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664   2.5890288   2.8564181
 -0.36400968 -0.37184966  1.6354147   2.482445   -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 1.4681835e-03  2.4594050e-03  4.6474081e-03  3.7761789e-03
 -6.3281031e+00 -3.6191773e+00  1.1600148e-03  1.7411708e-03
  2.6261410e-01  3.9827824e-01  3.4734835e-03  4.9582324e-03
 -1.8088582e-03 -2.2377169e-03] 4 6
train:	 Loss = 1.3821,	 Acc = 0.4858
10762 0.255
21099 0.528
10889 0.61
1400 0.651
242 0.521
24 0.375
0 0.0
0 0.0
0.5595471563558566
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3784,	 Acc = 0.4855
1367 0.348
6407 0.524
3023 0.476
308 0.364
8 0.875
0 0.0
0 0.0
0 0.0
0.5047198850810589
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4585,	 Acc1 = 0.3796,	 Acc2 = 0.4013

 ===== Epoch 202	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664   0.74492806  2.0488014
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618 -0.06652909  0.1925929
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 1 6
train:	 Loss = 1.3824,	 Acc = 0.4833
10763 0.257
21098 0.528
10887 0.603
1401 0.615
243 0.51
24 0.333
0 0.0
0 0.0
0.5556116839509108
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3624,	 Acc = 0.4995
1367 0.35
6407 0.526
3023 0.526
308 0.341
8 0.875
0 0.0
0 0.0
0 0.0
0.5205212394828648
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
Testing:	 Loss = 1.4477,	 Acc1 = 0.3705,	 Acc2 = 0.3904

 ===== Epoch 203	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664   2.3508213   2.386048
 -0.36400968 -0.37184966  1.2280796   3.346368   -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 1.4681835e-03  2.4594050e-03  4.6474081e-03  3.7761789e-03
 -2.7394278e-02  7.0436768e-02  1.1600148e-03  1.7411708e-03
 -2.7908700e+00 -6.1700234e+00  3.4734835e-03  4.9582324e-03
 -1.8088582e-03 -2.2377169e-03] 4 3
train:	 Loss = 1.3829,	 Acc = 0.4823
10764 0.255
21097 0.526
10887 0.602
1401 0.631
243 0.506
24 0.292
0 0.0
0 0.0
0.5549447283965292
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3611,	 Acc = 0.5027
1367 0.345
6407 0.533
3023 0.518
308 0.429
8 0.875
0 0.0
0 0.0
0 0.0
0.5249333059716806
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
Testing:	 Loss = 1.4427,	 Acc1 = 0.3782,	 Acc2 = 0.3996

 ===== Epoch 204	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664   1.1869311   2.7565756
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.22737823  0.09785957
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 2 2
train:	 Loss = 1.3809,	 Acc = 0.4863
10759 0.256
21104 0.529
10884 0.612
1402 0.631
243 0.502
24 0.292
0 0.0
0 0.0
0.5600617999227501
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3666,	 Acc = 0.5101
1367 0.365
6407 0.534
3023 0.535
308 0.399
8 0.875
0 0.0
0 0.0
0 0.0
0.5304740406320542
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4404,	 Acc1 = 0.3866,	 Acc2 = 0.4098

 ===== Epoch 205	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 1 1
train:	 Loss = 1.3838,	 Acc = 0.4854
10764 0.255
21091 0.528
10894 0.613
1401 0.635
242 0.492
24 0.375
0 0.0
0 0.0
0.5591941043622964
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.4409,	 Acc = 0.4572
1367 0.361
6407 0.489
3023 0.444
308 0.331
8 0.875
0 0.0
0 0.0
0 0.0
0.4707572337369177
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.5173,	 Acc1 = 0.3555,	 Acc2 = 0.3723

 ===== Epoch 206	 =====
[ 5.1857104   1.8702643  -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.7158315   1.4005456  -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [-5.9000068e+00 -2.5736306e+00  4.6474081e-03  3.7761789e-03
  2.1561917e-03  6.3325174e-04 -3.1811264e-01  1.1990203e+00
  1.5405245e-03  6.3845129e-03  3.4734835e-03  4.9582324e-03
 -1.8088582e-03 -2.2377169e-03] 6 6
train:	 Loss = 1.3831,	 Acc = 0.4842
10769 0.26
21095 0.526
10883 0.605
1402 0.626
243 0.523
24 0.375
0 0.0
0 0.0
0.5559485243855321
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3980,	 Acc = 0.4732
1367 0.353
6407 0.514
3023 0.444
308 0.432
8 0.875
0 0.0
0 0.0
0 0.0
0.4900471988508106
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4682,	 Acc1 = 0.3806,	 Acc2 = 0.4026

 ===== Epoch 207	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 6 1
train:	 Loss = 1.3825,	 Acc = 0.4826
10764 0.255
21100 0.527
10884 0.605
1401 0.617
243 0.49
24 0.333
0 0.0
0 0.0
0.5554498989658861
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3649,	 Acc = 0.5033
1367 0.342
6407 0.528
3023 0.527
308 0.464
8 0.875
0 0.0
0 0.0
0 0.0
0.5258567617484096
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4373,	 Acc1 = 0.3914,	 Acc2 = 0.4155

 ===== Epoch 208	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 0 1
train:	 Loss = 1.3825,	 Acc = 0.4852
10762 0.258
21099 0.529
10886 0.606
1402 0.635
243 0.502
24 0.292
0 0.0
0 0.0
0.5580020205621917
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3842,	 Acc = 0.4962
1367 0.354
6407 0.523
3023 0.512
308 0.412
8 0.875
0 0.0
0 0.0
0 0.0
0.5161091729940488
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
Testing:	 Loss = 1.4615,	 Acc1 = 0.3786,	 Acc2 = 0.4001

 ===== Epoch 209	 =====
[-0.36718738 -0.38160205  1.8279092   3.101045   -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 1.4681835e-03  2.4594050e-03 -4.3580728e+00 -3.6498871e+00
  2.1561917e-03  6.3325174e-04  1.1600148e-03  1.7411708e-03
  1.5405245e-03  6.3845129e-03  3.4734835e-03  4.9582324e-03
 -1.8088582e-03 -2.2377169e-03] 4 1
train:	 Loss = 1.3834,	 Acc = 0.4832
10757 0.253
21099 0.527
10890 0.608
1403 0.624
243 0.498
24 0.25
0 0.0
0 0.0
0.5567010309278351
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.4343,	 Acc = 0.4647
1367 0.353
6407 0.495
3023 0.457
308 0.386
8 0.875
0 0.0
0 0.0
0 0.0
0.48040221629386415
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.5038,	 Acc1 = 0.3619,	 Acc2 = 0.3800

 ===== Epoch 210	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  1.5669003   3.033162   -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 5.5016870e+00  3.9200826e+00  4.6474081e-03  3.7761789e-03
  2.1561917e-03  6.3325174e-04  3.3618867e-01 -4.4906065e-02
  1.5405245e-03  6.3845129e-03  3.4734835e-03  4.9582324e-03
 -1.8088582e-03 -2.2377169e-03] 2 2
train:	 Loss = 1.3823,	 Acc = 0.4852
10761 0.253
21094 0.528
10891 0.613
1403 0.628
243 0.494
24 0.292
0 0.0
0 0.0
0.559471103847868
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3704,	 Acc = 0.5045
1367 0.347
6407 0.529
3023 0.53
308 0.442
8 0.875
0 0.0
0 0.0
0 0.0
0.5266776113277242
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
Testing:	 Loss = 1.4582,	 Acc1 = 0.3720,	 Acc2 = 0.3921

 ===== Epoch 211	 =====
[ 1.7790858   1.3824862  -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [-0.01781388  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 0 0
train:	 Loss = 1.3842,	 Acc = 0.4834
10766 0.256
21090 0.526
10890 0.607
1403 0.619
243 0.531
24 0.458
0 0.0
0 0.0
0.5561664190193165
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.4148,	 Acc = 0.4775
1367 0.352
6407 0.522
3023 0.445
308 0.416
8 0.875
0 0.0
0 0.0
0 0.0
0.49507490252411246
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
Testing:	 Loss = 1.4933,	 Acc1 = 0.3670,	 Acc2 = 0.3862

 ===== Epoch 212	 =====
[ 2.719831    3.0404258  -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
  2.273418    3.2965593 ] [ 0.07409179 -0.37340108  0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.23439042 -0.5930691 ] 5 5
train:	 Loss = 1.3831,	 Acc = 0.4837
10766 0.256
21096 0.525
10884 0.61
1403 0.629
243 0.502
24 0.208
0 0.0
0 0.0
0.5565527488855869
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3574,	 Acc = 0.5087
1367 0.356
6407 0.534
3023 0.534
308 0.396
8 0.875
0 0.0
0 0.0
0 0.0
0.5300636158423969
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4472,	 Acc1 = 0.3798,	 Acc2 = 0.4016

 ===== Epoch 213	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 1.4681835e-03  2.4594050e-03  4.6474081e-03  3.7761789e-03
  2.1561917e-03  6.3325174e-04  1.1600148e-03  1.7411708e-03
  2.8989697e+00  4.4519291e+00  3.4734835e-03  4.9582324e-03
 -1.8088582e-03 -2.2377169e-03] 1 1
train:	 Loss = 1.3816,	 Acc = 0.4828
10761 0.256
21096 0.525
10890 0.606
1402 0.636
243 0.519
24 0.333
0 0.0
0 0.0
0.5554598128064181
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3830,	 Acc = 0.4871
1367 0.353
6407 0.52
3023 0.482
308 0.432
8 0.875
0 0.0
0 0.0
0 0.0
0.5058485532526165
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 6
Testing:	 Loss = 1.4495,	 Acc1 = 0.3918,	 Acc2 = 0.4160

 ===== Epoch 214	 =====
[ 4.1372247   3.353817   -0.42025706 -0.3355664  -0.44155708 -0.3651737
  1.2799501   2.1329865  -0.4018844  -0.40971038  2.3901658   1.6715394
  4.1836686   3.5611486 ] [-4.7857032e+00 -4.2707853e+00  4.6474081e-03  3.7761789e-03
  2.1561917e-03  6.3325174e-04  1.2193877e-01  4.7339660e-01
  1.5405245e-03  6.3845129e-03 -5.5782137e+00 -3.5707748e+00
 -8.6033249e+00 -6.6879611e+00] 0 6
train:	 Loss = 1.3827,	 Acc = 0.4836
10765 0.254
21099 0.526
10884 0.609
1401 0.635
243 0.502
24 0.375
0 0.0
0 0.0
0.5572494130932216
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3957,	 Acc = 0.4861
1367 0.353
6407 0.528
3023 0.467
308 0.383
8 0.875
0 0.0
0 0.0
0 0.0
0.5047198850810589
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4609,	 Acc1 = 0.3910,	 Acc2 = 0.4150

 ===== Epoch 215	 =====
[-0.36718738 -0.38160205  2.254166    1.690106   -0.44155708 -0.3651737
  0.60118914  3.1198246  -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 1.46818347e-03  2.45940499e-03  1.07353166e-01 -3.65207642e-01
  2.15619174e-03  6.33251737e-04  5.89244738e-02 -8.37909162e-01
  1.54052454e-03  6.38451288e-03  3.47348349e-03  4.95823240e-03
 -1.80885824e-03 -2.23771692e-03] 5 5
train:	 Loss = 1.3838,	 Acc = 0.4843
10767 0.256
21095 0.527
10885 0.609
1402 0.631
243 0.498
24 0.25
0 0.0
0 0.0
0.5573122529644269
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.4002,	 Acc = 0.4798
1367 0.35
6407 0.516
3023 0.468
308 0.416
8 0.875
0 0.0
0 0.0
0 0.0
0.49794787605171353
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
Testing:	 Loss = 1.4662,	 Acc1 = 0.3780,	 Acc2 = 0.3994

 ===== Epoch 216	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.6825712   2.2112627  -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 1.4681835e-03  2.4594050e-03  6.1260161e+00  1.8390486e+00
  2.1561917e-03  6.3325174e-04 -2.0897750e-02  1.9090949e+00
  1.5405245e-03  6.3845129e-03  3.4734835e-03  4.9582324e-03
 -1.8088582e-03 -2.2377169e-03] 6 6
train:	 Loss = 1.3816,	 Acc = 0.4846
10763 0.258
21095 0.527
10888 0.607
1403 0.635
243 0.498
24 0.292
0 0.0
0 0.0
0.5569191453956557
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.4147,	 Acc = 0.4708
1367 0.322
6407 0.499
3023 0.486
308 0.393
8 0.875
0 0.0
0 0.0
0 0.0
0.49168889800943977
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4978,	 Acc1 = 0.3608,	 Acc2 = 0.3787

 ===== Epoch 217	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664   1.9237303   2.1686127
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618 -0.04736041  0.02057711
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 3
train:	 Loss = 1.3828,	 Acc = 0.4838
10763 0.258
21095 0.525
10888 0.607
1403 0.634
243 0.494
24 0.375
0 0.0
0 0.0
0.555938549312097
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.4039,	 Acc = 0.4705
1367 0.357
6407 0.508
3023 0.447
308 0.412
8 0.875
0 0.0
0 0.0
0 0.0
0.48645598194130923
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
Testing:	 Loss = 1.4792,	 Acc1 = 0.3747,	 Acc2 = 0.3954

 ===== Epoch 218	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664   1.6602874   1.2079039
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.42464715  0.18511395
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 2 2
train:	 Loss = 1.3821,	 Acc = 0.4854
10765 0.258
21096 0.529
10886 0.607
1402 0.628
243 0.49
24 0.333
0 0.0
0 0.0
0.5581112002615078
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3833,	 Acc = 0.4898
1367 0.349
6407 0.526
3023 0.486
308 0.393
8 0.875
0 0.0
0 0.0
0 0.0
0.5095423763595321
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4647,	 Acc1 = 0.3763,	 Acc2 = 0.3974

 ===== Epoch 219	 =====
[-0.36718738 -0.38160205  1.6609896   2.8152275  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 0 0
train:	 Loss = 1.3807,	 Acc = 0.4840
10762 0.258
21099 0.527
10886 0.604
1402 0.631
243 0.523
24 0.292
0 0.0
0 0.0
0.5562191715695014
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3882,	 Acc = 0.4838
1367 0.349
6407 0.52
3023 0.474
308 0.406
8 0.875
0 0.0
0 0.0
0 0.0
0.5027703673301868
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4576,	 Acc1 = 0.3792,	 Acc2 = 0.4008

 ===== Epoch 220	 =====
[ 1.5628722   2.1432178  -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
  1.7038189   2.3953834 ] [ 1.7279346e-01 -4.3411699e-01  4.6474081e-03  3.7761789e-03
  2.1561917e-03  6.3325174e-04  1.1600148e-03  1.7411708e-03
  1.5405245e-03  6.3845129e-03  3.4734835e-03  4.9582324e-03
  4.7023572e-02 -6.6414654e-01] 5 5
train:	 Loss = 1.3836,	 Acc = 0.4843
10766 0.257
21097 0.527
10884 0.606
1402 0.627
243 0.551
24 0.375
0 0.0
0 0.0
0.5570282317979197
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3740,	 Acc = 0.4865
1367 0.347
6407 0.52
3023 0.482
308 0.438
8 0.875
0 0.0
0 0.0
0 0.0
0.5060537656474451
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 6
Testing:	 Loss = 1.4473,	 Acc1 = 0.3858,	 Acc2 = 0.4088

 ===== Epoch 221	 =====
[-0.36718738 -0.38160205  2.1426148   2.9467943  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 1.4681835e-03  2.4594050e-03  1.9900022e-01  1.4606406e-01
  5.3468680e+00  1.3867315e+00  1.1600148e-03  1.7411708e-03
  1.5405245e-03  6.3845129e-03  3.4734835e-03  4.9582324e-03
 -1.8088582e-03 -2.2377169e-03] 2 4
train:	 Loss = 1.3839,	 Acc = 0.4838
10765 0.256
21098 0.526
10885 0.608
1401 0.63
243 0.531
24 0.333
0 0.0
0 0.0
0.5565659267183739
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.4000,	 Acc = 0.4825
1367 0.35
6407 0.522
3023 0.467
308 0.393
8 0.875
0 0.0
0 0.0
0 0.0
0.5011286681715575
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4688,	 Acc1 = 0.3854,	 Acc2 = 0.4083

 ===== Epoch 222	 =====
[ 2.426125    2.1507998  -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [-2.9671915e+00 -2.8945575e+00  4.6474081e-03  3.7761789e-03
  2.1561917e-03  6.3325174e-04  1.1600148e-03  1.7411708e-03
  1.5405245e-03  6.3845129e-03  3.4734835e-03  4.9582324e-03
 -1.8088582e-03 -2.2377169e-03] 6 1
train:	 Loss = 1.3821,	 Acc = 0.4821
10761 0.255
21096 0.525
10890 0.605
1402 0.619
243 0.56
24 0.417
0 0.0
0 0.0
0.5547764076660229
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3805,	 Acc = 0.4968
1367 0.361
6407 0.523
3023 0.508
308 0.442
8 0.875
0 0.0
0 0.0
0 0.0
0.5159039605992202
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4521,	 Acc1 = 0.3850,	 Acc2 = 0.4078

 ===== Epoch 223	 =====
[-0.36718738 -0.38160205  1.3190063   1.7082533  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.02142407 -0.05169198  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 3 3
train:	 Loss = 1.3834,	 Acc = 0.4849
10767 0.256
21095 0.529
10885 0.608
1402 0.627
243 0.498
24 0.375
0 0.0
0 0.0
0.5580849356593064
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.4010,	 Acc = 0.4784
1367 0.317
6407 0.502
3023 0.506
308 0.422
8 0.875
0 0.0
0 0.0
0 0.0
0.5009234557767289
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4739,	 Acc1 = 0.3743,	 Acc2 = 0.3949

 ===== Epoch 224	 =====
[ 5.5209804   1.56951    -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966  5.082005   -5.131495    0.50332004  1.7500623
  2.9899282   1.6435302 ] [-6.2563233e+00 -2.2295740e+00  4.6474081e-03  3.7761789e-03
  2.1561917e-03  6.3325174e-04  1.1600148e-03  1.7411708e-03
  8.3002567e-02 -8.3424464e-02  9.2498384e-02 -4.1153215e-02
  1.0783359e+00 -9.1084532e-02] 2 2
train:	 Loss = 1.3831,	 Acc = 0.4823
10762 0.258
21094 0.523
10891 0.605
1403 0.63
242 0.504
24 0.333
0 0.0
0 0.0
0.5540203244785167
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3483,	 Acc = 0.5229
1367 0.351
6407 0.527
3023 0.599
308 0.445
8 0.875
0 0.0
0 0.0
0 0.0
0.5469936384157603
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4349,	 Acc1 = 0.3852,	 Acc2 = 0.4081

 ===== Epoch 225	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966  2.3693047   2.1075819  -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.29205987 -0.14057563  0.00347348  0.00495823
 -0.00180886 -0.00223772] 2 3
train:	 Loss = 1.3832,	 Acc = 0.4822
10764 0.254
21093 0.525
10892 0.606
1400 0.619
243 0.514
24 0.375
0 0.0
0 0.0
0.5551824557232854
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.4138,	 Acc = 0.4672
1367 0.352
6407 0.501
3023 0.452
308 0.403
8 0.875
0 0.0
0 0.0
0 0.0
0.48337779601887954
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4760,	 Acc1 = 0.3850,	 Acc2 = 0.4078

 ===== Epoch 226	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 5 1
train:	 Loss = 1.3806,	 Acc = 0.4878
10764 0.261
21097 0.53
10888 0.613
1400 0.62
243 0.543
24 0.25
0 0.0
0 0.0
0.5602935932485439
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3999,	 Acc = 0.4797
1367 0.35
6407 0.515
3023 0.476
308 0.347
8 0.875
0 0.0
0 0.0
0 0.0
0.49794787605171353
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4735,	 Acc1 = 0.3773,	 Acc2 = 0.3986

 ===== Epoch 227	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 5 1
train:	 Loss = 1.3829,	 Acc = 0.4833
10764 0.26
21097 0.526
10886 0.603
1402 0.628
243 0.519
24 0.375
0 0.0
0 0.0
0.5548258647331511
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3701,	 Acc = 0.4954
1367 0.333
6407 0.524
3023 0.514
308 0.438
8 0.875
0 0.0
0 0.0
0 0.0
0.5181612969423354
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4454,	 Acc1 = 0.3788,	 Acc2 = 0.4003

 ===== Epoch 228	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 1.4681835e-03  2.4594050e-03  2.1749816e+00  3.6092064e+00
  2.1561917e-03  6.3325174e-04  1.1600148e-03  1.7411708e-03
  1.5405245e-03  6.3845129e-03  3.4734835e-03  4.9582324e-03
 -1.8088582e-03 -2.2377169e-03] 5 1
train:	 Loss = 1.3834,	 Acc = 0.4846
10764 0.257
21096 0.527
10889 0.608
1401 0.636
242 0.488
24 0.375
0 0.0
0 0.0
0.5572922857482467
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.4208,	 Acc = 0.4742
1367 0.349
6407 0.502
3023 0.475
308 0.425
8 0.875
0 0.0
0 0.0
0 0.0
0.4917915042068541
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4772,	 Acc1 = 0.3895,	 Acc2 = 0.4133

 ===== Epoch 229	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  3.1848757   2.3398592  -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 1.4681835e-03  2.4594050e-03  4.6474081e-03  3.7761789e-03
  2.1561917e-03  6.3325174e-04  2.0910817e-01 -8.6900729e-01
  1.5405245e-03  6.3845129e-03  7.1352367e+00  5.7185860e+00
 -1.8088582e-03 -2.2377169e-03] 5 5
train:	 Loss = 1.3810,	 Acc = 0.4866
10762 0.262
21101 0.528
10884 0.612
1402 0.616
243 0.547
24 0.292
0 0.0
0 0.0
0.558566589409877
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.4088,	 Acc = 0.4871
1367 0.35
6407 0.53
3023 0.465
308 0.399
8 0.875
0 0.0
0 0.0
0 0.0
0.506361584239688
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4844,	 Acc1 = 0.3773,	 Acc2 = 0.3986

 ===== Epoch 230	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.1300914   3.4720674  -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 1.4681835e-03  2.4594050e-03  5.3849483e+00  1.5351796e+00
  2.1561917e-03  6.3325174e-04 -3.5592318e-01 -1.9003083e-01
  1.5405245e-03  6.3845129e-03  3.4734835e-03  4.9582324e-03
 -1.8088582e-03 -2.2377169e-03] 1 2
train:	 Loss = 1.3817,	 Acc = 0.4855
10763 0.258
21094 0.529
10890 0.607
1402 0.631
243 0.498
24 0.208
0 0.0
0 0.0
0.5581077467090602
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3616,	 Acc = 0.5134
1367 0.352
6407 0.536
3023 0.542
308 0.471
8 0.875
0 0.0
0 0.0
0 0.0
0.5360147752924277
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4393,	 Acc1 = 0.3856,	 Acc2 = 0.4085

 ===== Epoch 231	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 6.3853574e-01  4.0328407e+00  4.6474081e-03  3.7761789e-03
  2.1561917e-03  6.3325174e-04  1.1600148e-03  1.7411708e-03
  1.5405245e-03  6.3845129e-03  3.4734835e-03  4.9582324e-03
 -1.8088582e-03 -2.2377169e-03] 1 1
train:	 Loss = 1.3819,	 Acc = 0.4829
10764 0.255
21104 0.527
10881 0.605
1402 0.62
242 0.504
23 0.217
0 0.0
0 0.0
0.5555984785451088
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3547,	 Acc = 0.5190
1367 0.352
6407 0.544
3023 0.555
308 0.367
8 0.875
0 0.0
0 0.0
0 0.0
0.54247896572953
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 6
Testing:	 Loss = 1.4341,	 Acc1 = 0.3899,	 Acc2 = 0.4138

 ===== Epoch 232	 =====
[-0.36718738 -0.38160205  1.7688771   0.9370005  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.00483357  0.08336093  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 3
train:	 Loss = 1.3815,	 Acc = 0.4849
10762 0.254
21103 0.529
10882 0.609
1403 0.622
242 0.5
24 0.208
0 0.0
0 0.0
0.5585963035597552
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3766,	 Acc = 0.5047
1367 0.348
6407 0.527
3023 0.531
308 0.464
8 0.875
0 0.0
0 0.0
0 0.0
0.5266776113277242
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4479,	 Acc1 = 0.3856,	 Acc2 = 0.4085

 ===== Epoch 233	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664   2.1902323   1.0969676
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.4270445  -0.00435271
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 0 2
train:	 Loss = 1.3840,	 Acc = 0.4845
10764 0.257
21100 0.527
10885 0.608
1400 0.63
243 0.502
24 0.25
0 0.0
0 0.0
0.5571734220848686
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.4117,	 Acc = 0.4761
1367 0.338
6407 0.516
3023 0.463
308 0.367
8 0.875
0 0.0
0 0.0
0 0.0
0.49548532731376976
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 6
Testing:	 Loss = 1.4834,	 Acc1 = 0.3765,	 Acc2 = 0.3976

 ===== Epoch 234	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  4.390183    1.6745119  -0.4018844  -0.40971038  3.9456813   1.8776622
  4.6692586  -4.7066154 ] [ 9.6294349e-01  2.6190267e+00  4.6474081e-03  3.7761789e-03
  2.1561917e-03  6.3325174e-04 -9.9664807e-02  1.6241498e-01
  1.5405245e-03  6.3845129e-03  9.1541505e-01  9.2989177e-02
 -2.6832622e-01  1.3915616e+01] 2 2
train:	 Loss = 1.3830,	 Acc = 0.4824
10764 0.254
21095 0.527
10888 0.604
1402 0.613
243 0.523
24 0.458
0 0.0
0 0.0
0.5555687626292642
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.4223,	 Acc = 0.4783
1367 0.35
6407 0.52
3023 0.455
308 0.386
8 0.875
0 0.0
0 0.0
0 0.0
0.49620357069567
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4870,	 Acc1 = 0.3792,	 Acc2 = 0.4008

 ===== Epoch 235	 =====
[-0.36718738 -0.38160205  1.5571734   1.1071298  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.13658497  0.1436524   0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 2
train:	 Loss = 1.3815,	 Acc = 0.4859
10767 0.259
21092 0.529
10889 0.61
1401 0.625
243 0.51
24 0.292
0 0.0
0 0.0
0.5584415584415584
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3639,	 Acc = 0.5164
1367 0.342
6407 0.536
3023 0.562
308 0.432
8 0.875
0 0.0
0 0.0
0 0.0
0.5408372665709009
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4421,	 Acc1 = 0.3842,	 Acc2 = 0.4068

 ===== Epoch 236	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966  2.3017027   1.0773292  -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.17427985  0.18600246  0.00347348  0.00495823
 -0.00180886 -0.00223772] 2 2
train:	 Loss = 1.3838,	 Acc = 0.4826
10761 0.261
21101 0.525
10885 0.603
1402 0.615
243 0.477
24 0.25
0 0.0
0 0.0
0.5536175902540484
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3842,	 Acc = 0.4940
1367 0.356
6407 0.53
3023 0.485
308 0.432
8 0.875
0 0.0
0 0.0
0 0.0
0.5133388056638621
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4563,	 Acc1 = 0.3842,	 Acc2 = 0.4068

 ===== Epoch 237	 =====
[ 2.2088628   1.708514   -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966  1.8216083   3.2991996  -0.4254193  -0.4215889
  2.8443394   1.8950212 ] [-2.7362907e+00 -2.3885918e+00  4.6474081e-03  3.7761789e-03
  2.1561917e-03  6.3325174e-04  1.1600148e-03  1.7411708e-03
 -6.6586930e-01  6.7617908e-02  3.4734835e-03  4.9582324e-03
 -9.9670345e-01  8.6609103e-02] 1 1
train:	 Loss = 1.3828,	 Acc = 0.4839
10759 0.257
21099 0.526
10888 0.61
1403 0.617
243 0.506
24 0.292
0 0.0
0 0.0
0.5565855542680571
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.4341,	 Acc = 0.4750
1367 0.352
6407 0.515
3023 0.458
308 0.357
8 0.875
0 0.0
0 0.0
0 0.0
0.4923045351939257
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.5032,	 Acc1 = 0.3722,	 Acc2 = 0.3924

 ===== Epoch 238	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664   2.363439    2.8209186
 -0.36400968 -0.37184966  1.9178568   2.1373725  -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.21539906 -0.08412816
  0.00116001  0.00174117  0.22335614 -0.07934224  0.00347348  0.00495823
 -0.00180886 -0.00223772] 5 2
train:	 Loss = 1.3844,	 Acc = 0.4829
10765 0.258
21093 0.524
10889 0.608
1402 0.615
243 0.527
24 0.333
0 0.0
0 0.0
0.5549612195774272
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.4021,	 Acc = 0.4814
1367 0.351
6407 0.525
3023 0.454
308 0.416
8 0.875
0 0.0
0 0.0
0 0.0
0.499692181407757
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4705,	 Acc1 = 0.3875,	 Acc2 = 0.4108

 ===== Epoch 239	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038  1.0546569   1.568478
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.18601213 -0.11241636
 -0.00180886 -0.00223772] 2 2
train:	 Loss = 1.3840,	 Acc = 0.4814
10768 0.255
21092 0.524
10889 0.602
1400 0.635
243 0.51
24 0.333
0 0.0
0 0.0
0.5537624821683309
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3847,	 Acc = 0.4982
1367 0.36
6407 0.532
3023 0.496
308 0.422
8 0.875
0 0.0
0 0.0
0 0.0
0.5175456597578494
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 6
Testing:	 Loss = 1.4559,	 Acc1 = 0.3897,	 Acc2 = 0.4135

 ===== Epoch 240	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664   1.737523    2.6256707
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 1.4681835e-03  2.4594050e-03  4.6474081e-03  3.7761789e-03
 -4.5494852e+00 -3.3599072e+00  1.1600148e-03  1.7411708e-03
  2.8233967e+00  3.2721655e+00  3.4734835e-03  4.9582324e-03
 -1.8088582e-03 -2.2377169e-03] 6 6
train:	 Loss = 1.3828,	 Acc = 0.4843
10764 0.258
21095 0.527
10889 0.607
1401 0.63
243 0.473
24 0.25
0 0.0
0 0.0
0.5566385355996671
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3611,	 Acc = 0.5016
1367 0.347
6407 0.526
3023 0.531
308 0.38
8 0.875
0 0.0
0 0.0
0 0.0
0.5231890006156372
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
Testing:	 Loss = 1.4435,	 Acc1 = 0.3757,	 Acc2 = 0.3966

 ===== Epoch 241	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 1 1
train:	 Loss = 1.3851,	 Acc = 0.4842
10762 0.257
21097 0.528
10888 0.608
1402 0.611
243 0.494
24 0.292
0 0.0
0 0.0
0.5568728828668211
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.4119,	 Acc = 0.4795
1367 0.363
6407 0.512
3023 0.47
308 0.403
8 0.875
0 0.0
0 0.0
0 0.0
0.49589575210342707
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4722,	 Acc1 = 0.3872,	 Acc2 = 0.4105

 ===== Epoch 242	 =====
[-0.36718738 -0.38160205  1.0861325   3.008041   -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 0 0
train:	 Loss = 1.3849,	 Acc = 0.4819
10763 0.258
21094 0.527
10891 0.599
1401 0.616
243 0.535
24 0.208
0 0.0
0 0.0
0.5536802068166286
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.4066,	 Acc = 0.4746
1367 0.35
6407 0.51
3023 0.46
308 0.422
8 0.875
0 0.0
0 0.0
0 0.0
0.49209932279909707
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4853,	 Acc1 = 0.3738,	 Acc2 = 0.3944

 ===== Epoch 243	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038  0.8327637   2.9548993
 -0.38724938 -0.38149557] [ 1.4681835e-03  2.4594050e-03  4.6474081e-03  3.7761789e-03
  2.1561917e-03  6.3325174e-04  1.1600148e-03  1.7411708e-03
  1.5405245e-03  6.3845129e-03 -2.4907804e+00 -5.7631650e+00
 -1.8088582e-03 -2.2377169e-03] 6 1
train:	 Loss = 1.3815,	 Acc = 0.4830
10762 0.254
21094 0.526
10891 0.606
1402 0.626
243 0.519
24 0.375
0 0.0
0 0.0
0.5563083140191359
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.4237,	 Acc = 0.4614
1367 0.321
6407 0.502
3023 0.444
308 0.396
8 0.875
0 0.0
0 0.0
0 0.0
0.4811204596757644
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.5010,	 Acc1 = 0.3643,	 Acc2 = 0.3830

 ===== Epoch 244	 =====
[ 2.2191753   1.8854283  -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
  1.9505711   2.1072166 ] [-1.9634829e-01  7.8309280e-01  4.6474081e-03  3.7761789e-03
  2.1561917e-03  6.3325174e-04  1.1600148e-03  1.7411708e-03
  1.5405245e-03  6.3845129e-03  3.4734835e-03  4.9582324e-03
 -2.6915523e-01  1.2282908e+00] 6 6
train:	 Loss = 1.3826,	 Acc = 0.4856
10765 0.254
21098 0.53
10885 0.61
1401 0.625
243 0.498
24 0.292
0 0.0
0 0.0
0.5596861906035482
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3998,	 Acc = 0.4878
1367 0.348
6407 0.521
3023 0.484
308 0.435
8 0.875
0 0.0
0 0.0
0 0.0
0.5073876462138314
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4630,	 Acc1 = 0.3945,	 Acc2 = 0.4192

 ===== Epoch 245	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.88214     1.0706676  -0.4018844  -0.40971038  2.9905603   2.1230466
 -0.38724938 -0.38149557] [ 1.4681835e-03  2.4594050e-03  4.6474081e-03  3.7761789e-03
  2.1561917e-03  6.3325174e-04 -6.0462646e+00 -2.6727004e+00
  1.5405245e-03  6.3845129e-03 -6.6832638e-01 -2.8009436e-01
 -1.8088582e-03 -2.2377169e-03] 5 5
train:	 Loss = 1.3838,	 Acc = 0.4836
10765 0.257
21095 0.526
10887 0.606
1402 0.64
243 0.494
24 0.25
0 0.0
0 0.0
0.5561796083325904
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3743,	 Acc = 0.4986
1367 0.346
6407 0.531
3023 0.507
308 0.419
8 0.875
0 0.0
0 0.0
0 0.0
0.5200082084957931
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4445,	 Acc1 = 0.3908,	 Acc2 = 0.4148

 ===== Epoch 246	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038  1.9837391   2.3021772
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451 -0.30324906  0.03430188
 -0.00180886 -0.00223772] 1 1
train:	 Loss = 1.3824,	 Acc = 0.4841
10761 0.258
21098 0.528
10888 0.606
1402 0.613
243 0.519
24 0.375
0 0.0
0 0.0
0.5565592036844451
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3830,	 Acc = 0.4927
1367 0.339
6407 0.516
3023 0.516
308 0.448
8 0.875
0 0.0
0 0.0
0 0.0
0.5142622614405911
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
Testing:	 Loss = 1.4599,	 Acc1 = 0.3689,	 Acc2 = 0.3884

 ===== Epoch 247	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 2 1
train:	 Loss = 1.3836,	 Acc = 0.4827
10766 0.254
21097 0.525
10883 0.608
1403 0.622
243 0.514
24 0.25
0 0.0
0 0.0
0.5557503714710252
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3861,	 Acc = 0.4748
1367 0.327
6407 0.509
3023 0.478
308 0.367
8 0.875
0 0.0
0 0.0
0 0.0
0.49548532731376976
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
Testing:	 Loss = 1.4605,	 Acc1 = 0.3701,	 Acc2 = 0.3899

 ===== Epoch 248	 =====
[ 1.312432    2.6865973  -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
  1.3132355   3.021491  ] [ 0.27096418 -0.03801788  0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.04981739 -0.09996922] 3 2
train:	 Loss = 1.3833,	 Acc = 0.4835
10764 0.257
21094 0.527
10888 0.606
1403 0.621
243 0.535
24 0.167
0 0.0
0 0.0
0.5560739331986212
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3835,	 Acc = 0.5013
1367 0.353
6407 0.533
3023 0.514
308 0.354
8 0.875
0 0.0
0 0.0
0 0.0
0.5220603324440797
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4576,	 Acc1 = 0.3771,	 Acc2 = 0.3984

 ===== Epoch 249	 =====
[-0.36718738 -0.38160205  1.9785445   1.061762   -0.44155708 -0.3651737
  4.85528     2.6222122  -0.4018844  -0.40971038  4.1962547  -4.995553
 -0.38724938 -0.38149557] [ 1.4681835e-03  2.4594050e-03  1.7450891e-01  1.1953582e-01
  2.1561917e-03  6.3325174e-04  1.0587606e+00  1.9869617e-01
  1.5405245e-03  6.3845129e-03 -1.4091128e-01  1.3343741e+01
 -1.8088582e-03 -2.2377169e-03] 3 2
train:	 Loss = 1.3824,	 Acc = 0.4848
10760 0.257
21100 0.527
10888 0.609
1401 0.628
243 0.477
24 0.292
0 0.0
0 0.0
0.5574340385072498
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3996,	 Acc = 0.4870
1367 0.343
6407 0.517
3023 0.493
308 0.422
8 0.875
0 0.0
0 0.0
0 0.0
0.5071824338190026
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4747,	 Acc1 = 0.3724,	 Acc2 = 0.3926

 ===== Epoch 250	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 5 1
train:	 Loss = 1.3843,	 Acc = 0.4838
10761 0.256
21098 0.527
10888 0.609
1402 0.617
243 0.51
24 0.375
0 0.0
0 0.0
0.5567374832862874
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3699,	 Acc = 0.4857
1367 0.346
6407 0.521
3023 0.479
308 0.435
8 0.875
0 0.0
0 0.0
0 0.0
0.5053355222655448
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4452,	 Acc1 = 0.3804,	 Acc2 = 0.4023

 ===== Epoch 251	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966  2.3481073   2.81014    -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 1.4681835e-03  2.4594050e-03  4.6474081e-03  3.7761789e-03
  7.1718078e+00  3.3935823e+00  1.1600148e-03  1.7411708e-03
  1.9207979e-02 -1.1448034e+00  3.4734835e-03  4.9582324e-03
 -1.8088582e-03 -2.2377169e-03] 5 5
train:	 Loss = 1.3832,	 Acc = 0.4834
10762 0.256
21097 0.525
10890 0.609
1401 0.619
243 0.531
23 0.435
0 0.0
0 0.0
0.5561894574196232
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3589,	 Acc = 0.5144
1367 0.353
6407 0.54
3023 0.54
308 0.451
8 0.875
0 0.0
0 0.0
0 0.0
0.5371434434639852
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4325,	 Acc1 = 0.3877,	 Acc2 = 0.4110

 ===== Epoch 252	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  1.2139908   3.150576   -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 3.9521074e+00  3.5933728e+00  4.6474081e-03  3.7761789e-03
  2.1561917e-03  6.3325174e-04 -3.0348755e-02  4.6821359e-01
  1.5405245e-03  6.3845129e-03  3.4734835e-03  4.9582324e-03
 -1.8088582e-03 -2.2377169e-03] 4 4
train:	 Loss = 1.3838,	 Acc = 0.4846
10764 0.26
21098 0.528
10886 0.605
1401 0.625
243 0.519
24 0.25
0 0.0
0 0.0
0.5565493878521336
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3544,	 Acc = 0.5081
1367 0.353
6407 0.536
3023 0.527
308 0.432
8 0.875
0 0.0
0 0.0
0 0.0
0.5299610096449826
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4400,	 Acc1 = 0.3819,	 Acc2 = 0.4041

 ===== Epoch 253	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 5 1
train:	 Loss = 1.3828,	 Acc = 0.4835
10765 0.258
21095 0.525
10888 0.607
1401 0.632
243 0.519
24 0.375
0 0.0
0 0.0
0.5555555555555556
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3896,	 Acc = 0.4834
1367 0.356
6407 0.521
3023 0.465
308 0.442
8 0.875
0 0.0
0 0.0
0 0.0
0.5013338805663862
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4675,	 Acc1 = 0.3736,	 Acc2 = 0.3941

 ===== Epoch 254	 =====
[ 2.502789    2.3074954  -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
  2.204802    2.5316076 ] [-3.7983733e-01 -5.5554891e-01  4.6474081e-03  3.7761789e-03
  2.1561917e-03  6.3325174e-04  1.1600148e-03  1.7411708e-03
  1.5405245e-03  6.3845129e-03  3.4734835e-03  4.9582324e-03
 -4.6503905e-02 -8.5516715e-01] 5 5
train:	 Loss = 1.3836,	 Acc = 0.4853
10766 0.258
21094 0.528
10889 0.609
1400 0.617
243 0.539
24 0.333
0 0.0
0 0.0
0.5578306092124814
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3996,	 Acc = 0.4777
1367 0.34
6407 0.51
3023 0.475
308 0.438
8 0.875
0 0.0
0 0.0
0 0.0
0.4970244202749846
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4723,	 Acc1 = 0.3730,	 Acc2 = 0.3934

 ===== Epoch 255	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966  2.6488812   3.3389204   3.589445    1.2519321
  4.7013683   3.0372093 ] [ 1.4681835e-03  2.4594050e-03  4.6474081e-03  3.7761789e-03
  2.1561917e-03  6.3325174e-04  1.1600148e-03  1.7411708e-03
  3.0874428e-01 -2.2191070e-02  9.3112475e-01 -3.2769315e-02
 -1.6936243e+00 -5.1103465e-02] 2 2
train:	 Loss = 1.3829,	 Acc = 0.4837
10764 0.26
21094 0.526
10889 0.604
1402 0.633
243 0.514
24 0.333
0 0.0
0 0.0
0.5554201830500416
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.4005,	 Acc = 0.4742
1367 0.353
6407 0.498
3023 0.479
308 0.461
8 0.875
0 0.0
0 0.0
0 0.0
0.49117586702236815
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4698,	 Acc1 = 0.3705,	 Acc2 = 0.3904

 ===== Epoch 256	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966  2.5686748   2.3682485  -0.4254193  -0.4215889
  3.1746633   1.0829151 ] [ 1.4681835e-03  2.4594050e-03  4.6474081e-03  3.7761789e-03
  2.1561917e-03  6.3325174e-04  1.1600148e-03  1.7411708e-03
  2.7930155e-01 -5.7329160e-01  3.4734835e-03  4.9582324e-03
 -6.7045870e+00 -2.4855063e+00] 5 5
train:	 Loss = 1.3818,	 Acc = 0.4849
10761 0.26
21106 0.527
10880 0.608
1402 0.614
243 0.519
24 0.333
0 0.0
0 0.0
0.5567969098202348
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.4227,	 Acc = 0.4743
1367 0.33
6407 0.51
3023 0.472
308 0.39
8 0.875
0 0.0
0 0.0
0 0.0
0.49456187153704084
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4928,	 Acc1 = 0.3740,	 Acc2 = 0.3946

 ===== Epoch 257	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 0 1
train:	 Loss = 1.3804,	 Acc = 0.4860
10762 0.258
21095 0.53
10889 0.608
1403 0.619
243 0.535
24 0.292
0 0.0
0 0.0
0.5588637309086587
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3794,	 Acc = 0.5046
1367 0.352
6407 0.536
3023 0.516
308 0.406
8 0.875
0 0.0
0 0.0
0 0.0
0.5260619741432383
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
Testing:	 Loss = 1.4603,	 Acc1 = 0.3670,	 Acc2 = 0.3862

 ===== Epoch 258	 =====
[-0.36718738 -0.38160205  2.2936566   3.101045    2.8299139   1.2655908
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.22744316 -0.07580856  0.31203493 -0.13149482
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 5 3
train:	 Loss = 1.3837,	 Acc = 0.4833
10766 0.255
21095 0.527
10886 0.606
1402 0.618
243 0.531
24 0.333
0 0.0
0 0.0
0.5562852897473997
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3783,	 Acc = 0.5009
1367 0.356
6407 0.534
3023 0.5
308 0.464
8 0.875
0 0.0
0 0.0
0 0.0
0.5213420890621794
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4475,	 Acc1 = 0.3926,	 Acc2 = 0.4170

 ===== Epoch 259	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038  1.5033484   2.3684309
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.02516832 -0.03696126
 -0.00180886 -0.00223772] 2 3
train:	 Loss = 1.3839,	 Acc = 0.4828
10762 0.257
21099 0.525
10885 0.604
1403 0.627
243 0.539
24 0.333
0 0.0
0 0.0
0.5549711772746182
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3834,	 Acc = 0.4904
1367 0.343
6407 0.519
3023 0.499
308 0.448
8 0.875
0 0.0
0 0.0
0 0.0
0.5110814693207469
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4557,	 Acc1 = 0.3848,	 Acc2 = 0.4076

 ===== Epoch 260	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966  1.3048483   1.2933098  -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 1.4681835e-03  2.4594050e-03  4.6474081e-03  3.7761789e-03
  4.8668737e+00  3.3736386e+00  1.1600148e-03  1.7411708e-03
 -2.9223883e+00 -2.7940228e+00  3.4734835e-03  4.9582324e-03
 -1.8088582e-03 -2.2377169e-03] 5 1
train:	 Loss = 1.3834,	 Acc = 0.4845
10763 0.259
21096 0.527
10887 0.607
1403 0.624
243 0.523
24 0.25
0 0.0
0 0.0
0.5566517101001397
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3755,	 Acc = 0.5058
1367 0.356
6407 0.537
3023 0.512
308 0.448
8 0.875
0 0.0
0 0.0
0 0.0
0.5268828237225528
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4439,	 Acc1 = 0.3939,	 Acc2 = 0.4185

 ===== Epoch 261	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966  1.0670929   3.083219   -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 1.4681835e-03  2.4594050e-03  4.6474081e-03  3.7761789e-03
  2.1561917e-03  6.3325174e-04  1.1600148e-03  1.7411708e-03
 -2.5150719e+00 -5.7373075e+00  3.4734835e-03  4.9582324e-03
 -1.8088582e-03 -2.2377169e-03] 6 1
train:	 Loss = 1.3826,	 Acc = 0.4834
10769 0.255
21096 0.525
10883 0.608
1401 0.628
243 0.539
24 0.375
0 0.0
0 0.0
0.556572651350789
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3974,	 Acc = 0.4686
1367 0.342
6407 0.503
3023 0.466
308 0.328
8 0.875
0 0.0
0 0.0
0 0.0
0.4863533757438949
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4763,	 Acc1 = 0.3679,	 Acc2 = 0.3872

 ===== Epoch 262	 =====
[-0.36718738 -0.38160205  1.622312    1.6129808  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.24011876  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 2
train:	 Loss = 1.3830,	 Acc = 0.4848
10761 0.259
21102 0.526
10885 0.609
1403 0.629
242 0.517
23 0.348
0 0.0
0 0.0
0.5570049026890507
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3881,	 Acc = 0.4831
1367 0.353
6407 0.525
3023 0.457
308 0.422
8 0.875
0 0.0
0 0.0
0 0.0
0.5013338805663862
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4559,	 Acc1 = 0.3920,	 Acc2 = 0.4163

 ===== Epoch 263	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664   1.6209048   1.9911145
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.16587998  0.12029641
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 6 2
train:	 Loss = 1.3834,	 Acc = 0.4838
10765 0.254
21096 0.526
10887 0.611
1401 0.623
243 0.519
24 0.375
0 0.0
0 0.0
0.557279129892128
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3797,	 Acc = 0.4986
1367 0.359
6407 0.533
3023 0.496
308 0.416
8 0.875
0 0.0
0 0.0
0 0.0
0.5181612969423354
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4497,	 Acc1 = 0.3879,	 Acc2 = 0.4113

 ===== Epoch 264	 =====
[ 0.94202983  3.194594   -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
  0.8549157   3.5454307 ] [ 6.7963049e-02 -6.3939470e-01  4.6474081e-03  3.7761789e-03
  2.1561917e-03  6.3325174e-04  1.1600148e-03  1.7411708e-03
  1.5405245e-03  6.3845129e-03  3.4734835e-03  4.9582324e-03
 -2.3604845e-01 -8.3295548e-01] 5 5
train:	 Loss = 1.3823,	 Acc = 0.4832
10760 0.253
21094 0.526
10892 0.61
1403 0.619
243 0.531
24 0.375
0 0.0
0 0.0
0.5566318041359639
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3830,	 Acc = 0.4973
1367 0.349
6407 0.518
3023 0.525
308 0.455
8 0.875
0 0.0
0 0.0
0 0.0
0.5181612969423354
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4697,	 Acc1 = 0.3619,	 Acc2 = 0.3800

 ===== Epoch 265	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  3.4346182   1.2635623  -0.4018844  -0.40971038  2.9958436   1.9267392
 -0.38724938 -0.38149557] [ 1.4681835e-03  2.4594050e-03  4.6474081e-03  3.7761789e-03
  2.1561917e-03  6.3325174e-04 -2.7085435e-01 -9.6736327e-02
  1.5405245e-03  6.3845129e-03  9.1167325e-01  1.3108993e+01
 -1.8088582e-03 -2.2377169e-03] 2 3
train:	 Loss = 1.3833,	 Acc = 0.4839
10766 0.257
21098 0.529
10885 0.604
1401 0.615
242 0.492
24 0.208
0 0.0
0 0.0
0.5564933135215453
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3546,	 Acc = 0.5225
1367 0.355
6407 0.546
3023 0.562
308 0.38
8 0.875
0 0.0
0 0.0
0 0.0
0.5460701826390314
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4293,	 Acc1 = 0.4002,	 Acc2 = 0.4262

 ===== Epoch 266	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 1 1
train:	 Loss = 1.3837,	 Acc = 0.4847
10767 0.26
21094 0.526
10887 0.61
1402 0.624
242 0.541
24 0.292
0 0.0
0 0.0
0.5567476002258611
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3890,	 Acc = 0.4706
1367 0.332
6407 0.503
3023 0.473
308 0.37
8 0.875
0 0.0
0 0.0
0 0.0
0.4900471988508106
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
Testing:	 Loss = 1.4746,	 Acc1 = 0.3650,	 Acc2 = 0.3837

 ===== Epoch 267	 =====
[-0.36718738 -0.38160205  1.7025152   2.6745872  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.21954234 -0.00828212  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 2 0
train:	 Loss = 1.3829,	 Acc = 0.4857
10760 0.258
21099 0.528
10887 0.611
1403 0.622
243 0.51
24 0.25
0 0.0
0 0.0
0.5586225338721179
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3809,	 Acc = 0.4835
1367 0.338
6407 0.514
3023 0.491
308 0.412
8 0.875
0 0.0
0 0.0
0 0.0
0.5038990355017443
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4535,	 Acc1 = 0.3891,	 Acc2 = 0.4128

 ===== Epoch 268	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664   2.8975906   2.9584796
 -0.36400968 -0.37184966  1.8141612   2.3061848  -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 0 0
train:	 Loss = 1.3825,	 Acc = 0.4848
10760 0.253
21094 0.529
10893 0.609
1402 0.626
243 0.519
24 0.417
0 0.0
0 0.0
0.5588602329450915
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3701,	 Acc = 0.4926
1367 0.326
6407 0.515
3023 0.53
308 0.386
8 0.875
0 0.0
0 0.0
0 0.0
0.5159039605992202
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4516,	 Acc1 = 0.3751,	 Acc2 = 0.3959

 ===== Epoch 269	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038  1.1939055   1.7549702
 -0.38724938 -0.38149557] [ 1.4681835e-03  2.4594050e-03  4.6474081e-03  3.7761789e-03
  2.1561917e-03  6.3325174e-04  1.1600148e-03  1.7411708e-03
  1.5405245e-03  6.3845129e-03  1.7329495e-01  6.8824601e-01
 -1.8088582e-03 -2.2377169e-03] 4 6
train:	 Loss = 1.3826,	 Acc = 0.4856
10767 0.257
21096 0.528
10885 0.611
1402 0.626
242 0.533
24 0.333
0 0.0
0 0.0
0.5589170554845612
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3601,	 Acc = 0.5113
1367 0.356
6407 0.541
3023 0.531
308 0.373
8 0.875
0 0.0
0 0.0
0 0.0
0.5331418017648266
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
Testing:	 Loss = 1.4581,	 Acc1 = 0.3707,	 Acc2 = 0.3907

 ===== Epoch 270	 =====
[-0.36718738 -0.38160205  1.4720844   1.8398198  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.2290209   0.20876719  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 2 2
train:	 Loss = 1.3824,	 Acc = 0.4853
10769 0.259
21100 0.527
10880 0.609
1401 0.631
242 0.508
24 0.333
0 0.0
0 0.0
0.5576425832912295
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3591,	 Acc = 0.5117
1367 0.353
6407 0.527
3023 0.556
308 0.448
8 0.875
0 0.0
0 0.0
0 0.0
0.5339626513441412
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
Testing:	 Loss = 1.4477,	 Acc1 = 0.3740,	 Acc2 = 0.3946

 ===== Epoch 271	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
  1.9796009   2.9324212 ] [ 2.9837964e+00  3.4401376e+00  4.6474081e-03  3.7761789e-03
  2.1561917e-03  6.3325174e-04  1.1600148e-03  1.7411708e-03
  1.5405245e-03  6.3845129e-03  3.4734835e-03  4.9582324e-03
 -8.2921617e-02 -7.7757515e-02] 3 3
train:	 Loss = 1.3845,	 Acc = 0.4848
10766 0.261
21097 0.527
10883 0.606
1403 0.628
243 0.527
24 0.25
0 0.0
0 0.0
0.5562852897473997
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3941,	 Acc = 0.4821
1367 0.331
6407 0.516
3023 0.487
308 0.386
8 0.875
0 0.0
0 0.0
0 0.0
0.5033860045146726
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4660,	 Acc1 = 0.3745,	 Acc2 = 0.3951

 ===== Epoch 272	 =====
[ 2.7582543   3.2754693  -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
  2.3996544   3.5218532 ] [0.00171805 0.0024594  0.00464741 0.00377618 0.00215619 0.00063325
 0.00116001 0.00174117 0.00154052 0.00638451 0.00347348 0.00495823
 0.0139168  0.01108931] 0 0
train:	 Loss = 1.3833,	 Acc = 0.4826
10767 0.254
21095 0.528
10887 0.604
1400 0.609
243 0.523
24 0.25
0 0.0
0 0.0
0.5556777318791049
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.4032,	 Acc = 0.5000
1367 0.355
6407 0.53
3023 0.507
308 0.435
8 0.875
0 0.0
0 0.0
0 0.0
0.5203160270880361
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4725,	 Acc1 = 0.3765,	 Acc2 = 0.3976

 ===== Epoch 273	 =====
[-0.36718738 -0.38160205  3.111565    3.003504    2.633383    1.2456223
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 1.4681835e-03  2.4594050e-03 -6.8490944e+00 -3.5461860e+00
 -1.8792574e-01  1.3026834e-01  1.1600148e-03  1.7411708e-03
  3.8539526e+00  2.1413889e+00  3.4734835e-03  4.9582324e-03
 -1.8088582e-03 -2.2377169e-03] 2 2
train:	 Loss = 1.3837,	 Acc = 0.4838
10757 0.258
21103 0.526
10888 0.608
1401 0.617
243 0.502
24 0.375
0 0.0
0 0.0
0.5560771264743456
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3815,	 Acc = 0.4910
1367 0.356
6407 0.524
3023 0.491
308 0.39
8 0.875
0 0.0
0 0.0
0 0.0
0.5099528011491894
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 6
Testing:	 Loss = 1.4573,	 Acc1 = 0.3817,	 Acc2 = 0.4038

 ===== Epoch 274	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966  1.415993    2.038071   -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 1.4681835e-03  2.4594050e-03  4.6474081e-03  3.7761789e-03
  2.1561917e-03  6.3325174e-04  1.1600148e-03  1.7411708e-03
 -2.0457067e-01  7.3302078e-01  3.4734835e-03  4.9582324e-03
 -1.8088582e-03 -2.2377169e-03] 6 6
train:	 Loss = 1.3828,	 Acc = 0.4835
10761 0.257
21100 0.525
10885 0.608
1403 0.621
243 0.523
24 0.333
0 0.0
0 0.0
0.5557866587431288
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3816,	 Acc = 0.4929
1367 0.351
6407 0.531
3023 0.489
308 0.354
8 0.875
0 0.0
0 0.0
0 0.0
0.5128257746767905
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
Testing:	 Loss = 1.4634,	 Acc1 = 0.3776,	 Acc2 = 0.3989

 ===== Epoch 275	 =====
[ 3.583295    3.035371   -0.42025706 -0.3355664  -0.44155708 -0.3651737
  1.1564871   2.7312398  -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [-4.1970015e+00 -3.9064898e+00  4.6474081e-03  3.7761789e-03
  2.1561917e-03  6.3325174e-04  3.1308547e-01  4.3193236e-01
  1.5405245e-03  6.3845129e-03  3.4734835e-03  4.9582324e-03
 -1.8088582e-03 -2.2377169e-03] 4 6
train:	 Loss = 1.3827,	 Acc = 0.4832
10761 0.256
21098 0.526
10890 0.606
1400 0.628
243 0.527
24 0.333
0 0.0
0 0.0
0.5560243648789185
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3861,	 Acc = 0.4923
1367 0.319
6407 0.511
3023 0.533
308 0.468
8 0.875
0 0.0
0 0.0
0 0.0
0.5166222039811205
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
Testing:	 Loss = 1.4739,	 Acc1 = 0.3573,	 Acc2 = 0.3745

 ===== Epoch 276	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 0 1
train:	 Loss = 1.3830,	 Acc = 0.4834
10761 0.258
21095 0.525
10890 0.608
1403 0.624
243 0.527
24 0.333
0 0.0
0 0.0
0.5555489526073392
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3897,	 Acc = 0.4905
1367 0.354
6407 0.528
3023 0.477
308 0.451
8 0.875
0 0.0
0 0.0
0 0.0
0.5096449825569465
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
Testing:	 Loss = 1.4655,	 Acc1 = 0.3794,	 Acc2 = 0.4011

 ===== Epoch 277	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038  1.2044722   2.7904918
 -0.38724938 -0.38149557] [ 1.4681835e-03  2.4594050e-03  4.6474081e-03  3.7761789e-03
  2.1561917e-03  6.3325174e-04  1.1600148e-03  1.7411708e-03
  1.5405245e-03  6.3845129e-03  1.7927773e-01 -7.9151219e-01
 -1.8088582e-03 -2.2377169e-03] 5 5
train:	 Loss = 1.3844,	 Acc = 0.4840
10765 0.257
21097 0.525
10884 0.607
1403 0.647
243 0.51
24 0.333
0 0.0
0 0.0
0.5565956435172803
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3549,	 Acc = 0.5092
1367 0.35
6407 0.538
3023 0.528
308 0.432
8 0.875
0 0.0
0 0.0
0 0.0
0.5315001026061974
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4370,	 Acc1 = 0.3825,	 Acc2 = 0.4048

 ===== Epoch 278	 =====
[ 3.6917622   2.8988943  -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
  2.3759031   3.1341379 ] [-0.06454257  0.13256496  0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
  0.1331053   0.20210996] 4 4
train:	 Loss = 1.3821,	 Acc = 0.4842
10760 0.256
21098 0.527
10891 0.607
1400 0.636
243 0.539
24 0.208
0 0.0
0 0.0
0.557018065129546
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3994,	 Acc = 0.4953
1367 0.353
6407 0.526
3023 0.497
308 0.461
8 0.875
0 0.0
0 0.0
0 0.0
0.5152883234147343
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4656,	 Acc1 = 0.3802,	 Acc2 = 0.4021

 ===== Epoch 279	 =====
[-0.36718738 -0.38160205  0.6252696   1.71279    -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.12412984  0.31488016  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 6 4
train:	 Loss = 1.3829,	 Acc = 0.4831
10756 0.254
21102 0.526
10888 0.608
1403 0.629
243 0.523
24 0.25
0 0.0
0 0.0
0.5563279857397504
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3668,	 Acc = 0.4996
1367 0.353
6407 0.531
3023 0.514
308 0.354
8 0.875
0 0.0
0 0.0
0 0.0
0.5202134208906218
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
Testing:	 Loss = 1.4567,	 Acc1 = 0.3722,	 Acc2 = 0.3924

 ===== Epoch 280	 =====
[-0.36718738 -0.38160205  1.4985478   2.865132   -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.17311493 -0.26874128  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 5 5
train:	 Loss = 1.3818,	 Acc = 0.4855
10758 0.258
21101 0.528
10889 0.607
1401 0.637
243 0.506
24 0.417
0 0.0
0 0.0
0.5581733911700042
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3946,	 Acc = 0.4937
1367 0.358
6407 0.524
3023 0.498
308 0.416
8 0.875
0 0.0
0 0.0
0 0.0
0.5126205622819618
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4695,	 Acc1 = 0.3743,	 Acc2 = 0.3949

 ===== Epoch 281	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966  1.2022986   2.0827565  -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 1.4681835e-03  2.4594050e-03  4.6474081e-03  3.7761789e-03
  2.1561917e-03  6.3325174e-04  1.1600148e-03  1.7411708e-03
  1.5367238e-01  1.2718745e+00  3.4734835e-03  4.9582324e-03
 -1.8088582e-03 -2.2377169e-03] 6 6
train:	 Loss = 1.3836,	 Acc = 0.4832
10765 0.255
21097 0.526
10887 0.609
1400 0.612
243 0.519
24 0.25
0 0.0
0 0.0
0.5561201747347776
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.4173,	 Acc = 0.4675
1367 0.355
6407 0.505
3023 0.449
308 0.364
8 0.875
0 0.0
0 0.0
0 0.0
0.4832751898214652
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4895,	 Acc1 = 0.3753,	 Acc2 = 0.3961

 ===== Epoch 282	 =====
[-0.36718738 -0.38160205  2.0778828   1.6356647  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.36253864  0.03271609  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 2 2
train:	 Loss = 1.3819,	 Acc = 0.4835
10765 0.255
21092 0.526
10889 0.61
1403 0.618
243 0.523
24 0.292
0 0.0
0 0.0
0.5566253603161867
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.4098,	 Acc = 0.4777
1367 0.34
6407 0.512
3023 0.471
308 0.438
8 0.875
0 0.0
0 0.0
0 0.0
0.4970244202749846
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4694,	 Acc1 = 0.3868,	 Acc2 = 0.4100

 ===== Epoch 283	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.7519116   2.1301908  -0.4018844  -0.40971038  2.933955    3.0824993
 -0.38724938 -0.38149557] [ 1.4681835e-03  2.4594050e-03  4.6474081e-03  3.7761789e-03
  2.1561917e-03  6.3325174e-04 -2.6140013e-01 -6.8241835e-01
  1.5405245e-03  6.3845129e-03 -3.0474764e-01 -5.7353085e-01
 -1.8088582e-03 -2.2377169e-03] 5 5
train:	 Loss = 1.3820,	 Acc = 0.4856
10765 0.258
21097 0.528
10885 0.609
1402 0.627
243 0.498
24 0.292
0 0.0
0 0.0
0.5583192178538527
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3666,	 Acc = 0.5097
1367 0.353
6407 0.541
3023 0.519
308 0.448
8 0.875
0 0.0
0 0.0
0 0.0
0.5316027088036117
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
Testing:	 Loss = 1.4447,	 Acc1 = 0.3788,	 Acc2 = 0.4003

 ===== Epoch 284	 =====
[ 1.480086    0.96041924 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
  1.7636377   1.1719848 ] [-1.9617658e+00 -1.5327864e+00  4.6474081e-03  3.7761789e-03
  2.1561917e-03  6.3325174e-04  1.1600148e-03  1.7411708e-03
  1.5405245e-03  6.3845129e-03  3.4734835e-03  4.9582324e-03
 -4.0493307e+00 -2.6365459e+00] 5 1
train:	 Loss = 1.3826,	 Acc = 0.4861
10763 0.257
21094 0.53
10892 0.61
1400 0.623
243 0.51
24 0.417
0 0.0
0 0.0
0.5592666329896294
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3920,	 Acc = 0.4893
1367 0.355
6407 0.52
3023 0.494
308 0.403
8 0.875
0 0.0
0 0.0
0 0.0
0.5082084957931459
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4645,	 Acc1 = 0.3776,	 Acc2 = 0.3989

 ===== Epoch 285	 =====
[ 2.9315388   1.789389   -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966  1.940772    3.333955   -0.4254193  -0.4215889
  2.6151786   1.9867105 ] [ 0.02217844  0.01691558  0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117 -0.00925474  0.00230229  0.00347348  0.00495823
  0.4691522   0.00220462] 0 0
train:	 Loss = 1.3848,	 Acc = 0.4840
10759 0.258
21098 0.528
10890 0.604
1402 0.616
243 0.494
24 0.375
0 0.0
0 0.0
0.5562587277535134
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3767,	 Acc = 0.5026
1367 0.356
6407 0.531
3023 0.517
308 0.412
8 0.875
0 0.0
0 0.0
0 0.0
0.5231890006156372
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4479,	 Acc1 = 0.3914,	 Acc2 = 0.4155

 ===== Epoch 286	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664   1.7214644   1.4408703
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618 -0.23504749  0.11531045
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
train:	 Loss = 1.3798,	 Acc = 0.4864
10765 0.262
21096 0.528
10886 0.608
1402 0.63
243 0.531
24 0.375
0 0.0
0 0.0
0.55825978425604
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3721,	 Acc = 0.4755
1367 0.334
6407 0.51
3023 0.475
308 0.383
8 0.875
0 0.0
0 0.0
0 0.0
0.4952801149189411
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
Testing:	 Loss = 1.4554,	 Acc1 = 0.3615,	 Acc2 = 0.3795

 ===== Epoch 287	 =====
[-0.36718738 -0.38160205  1.2168186   1.2114757  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 1.4681835e-03  2.4594050e-03  2.3609374e-02  7.3692054e-01
  2.1561917e-03  6.3325174e-04  1.1600148e-03  1.7411708e-03
  1.5405245e-03  6.3845129e-03  3.4734835e-03  4.9582324e-03
 -1.8088582e-03 -2.2377169e-03] 6 6
train:	 Loss = 1.3842,	 Acc = 0.4839
10761 0.257
21099 0.528
10886 0.605
1403 0.615
243 0.531
24 0.417
0 0.0
0 0.0
0.5563512108156292
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3828,	 Acc = 0.4985
1367 0.343
6407 0.531
3023 0.505
308 0.445
8 0.875
0 0.0
0 0.0
0 0.0
0.5203160270880361
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4572,	 Acc1 = 0.3776,	 Acc2 = 0.3989

 ===== Epoch 288	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 2 1
train:	 Loss = 1.3837,	 Acc = 0.4829
10761 0.254
21101 0.526
10885 0.606
1403 0.637
242 0.525
24 0.25
0 0.0
0 0.0
0.5559649383449711
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3775,	 Acc = 0.5008
1367 0.353
6407 0.528
3023 0.519
308 0.403
8 0.875
0 0.0
0 0.0
0 0.0
0.521547301457008
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4521,	 Acc1 = 0.3903,	 Acc2 = 0.4143

 ===== Epoch 289	 =====
[-0.36718738 -0.38160205  3.4388914   2.4409432   3.6500666   1.0503743
  2.817871    3.5363655  -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.12078578  0.1315941   0.03809622  0.11531045
 -0.2876558   0.15723197  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 3 2
train:	 Loss = 1.3853,	 Acc = 0.4851
10767 0.262
21096 0.527
10884 0.606
1402 0.625
243 0.535
24 0.375
0 0.0
0 0.0
0.5566287259651104
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3804,	 Acc = 0.4968
1367 0.359
6407 0.531
3023 0.493
308 0.416
8 0.875
0 0.0
0 0.0
0 0.0
0.5161091729940488
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
Testing:	 Loss = 1.4593,	 Acc1 = 0.3790,	 Acc2 = 0.4006

 ===== Epoch 290	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 0 1
train:	 Loss = 1.3823,	 Acc = 0.4839
10765 0.254
21094 0.528
10888 0.608
1402 0.613
243 0.514
24 0.333
0 0.0
0 0.0
0.5575762978811922
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3829,	 Acc = 0.4857
1367 0.343
6407 0.526
3023 0.475
308 0.386
8 0.875
0 0.0
0 0.0
0 0.0
0.5057459470552021
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4525,	 Acc1 = 0.3856,	 Acc2 = 0.4085

 ===== Epoch 291	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664   2.5465875   2.1974561
 -0.36400968 -0.37184966  1.623956    1.3106877  -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618 -0.3796026   0.21752271
  0.00116001  0.00174117  0.00056052  0.5044161   0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
train:	 Loss = 1.3843,	 Acc = 0.4828
10764 0.252
21096 0.526
10887 0.61
1403 0.621
242 0.488
24 0.375
0 0.0
0 0.0
0.5567573992630452
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3829,	 Acc = 0.5020
1367 0.347
6407 0.531
3023 0.513
308 0.477
8 0.875
0 0.0
0 0.0
0 0.0
0.5237020316027088
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4546,	 Acc1 = 0.3819,	 Acc2 = 0.4041

 ===== Epoch 292	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 1.4681835e-03  2.4594050e-03  4.6474081e-03  3.7761789e-03
  2.1561917e-03  6.3325174e-04  1.1600148e-03  1.7411708e-03
  1.5405245e-03  6.3845129e-03  2.9839959e+00  3.3249824e+00
 -1.8088582e-03 -2.2377169e-03] 6 1
train:	 Loss = 1.3819,	 Acc = 0.4835
10765 0.254
21096 0.528
10886 0.606
1402 0.611
243 0.527
24 0.375
0 0.0
0 0.0
0.5567442275118124
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3688,	 Acc = 0.5063
1367 0.35
6407 0.526
3023 0.541
308 0.448
8 0.875
0 0.0
0 0.0
0 0.0
0.5282167042889391
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4429,	 Acc1 = 0.3732,	 Acc2 = 0.3936

 ===== Epoch 293	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038  1.2365482   1.5292165
 -0.38724938 -0.38149557] [ 1.4681835e-03  2.4594050e-03  4.6474081e-03  3.7761789e-03
  2.1561917e-03  6.3325174e-04  1.1600148e-03  1.7411708e-03
  1.5405245e-03  6.3845129e-03  2.9373932e-01 -1.0388372e+00
 -1.8088582e-03 -2.2377169e-03] 5 5
train:	 Loss = 1.3823,	 Acc = 0.4839
10763 0.251
21094 0.527
10890 0.612
1402 0.621
243 0.51
24 0.333
0 0.0
0 0.0
0.5582860369060708
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3874,	 Acc = 0.4880
1367 0.342
6407 0.519
3023 0.494
308 0.419
8 0.875
0 0.0
0 0.0
0 0.0
0.5085163143853889
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4652,	 Acc1 = 0.3747,	 Acc2 = 0.3954

 ===== Epoch 294	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966  2.3693047   1.8096775  -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.32837477  0.01863119  0.00347348  0.00495823
 -0.00180886 -0.00223772] 2 2
train:	 Loss = 1.3824,	 Acc = 0.4863
10758 0.261
21103 0.528
10885 0.61
1403 0.626
243 0.531
24 0.333
0 0.0
0 0.0
0.5582625230257294
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.4033,	 Acc = 0.4710
1367 0.353
6407 0.506
3023 0.453
308 0.435
8 0.875
0 0.0
0 0.0
0 0.0
0.48758465011286684
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4731,	 Acc1 = 0.3738,	 Acc2 = 0.3944

 ===== Epoch 295	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664   1.5669922   2.1974561
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618 -0.34605986  0.04301396
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 1 1
train:	 Loss = 1.3832,	 Acc = 0.4836
10758 0.259
21100 0.527
10888 0.604
1403 0.619
243 0.514
24 0.333
0 0.0
0 0.0
0.5552617505496464
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3517,	 Acc = 0.5121
1367 0.255
6407 0.548
3023 0.565
308 0.406
8 0.0
0 0.0
0 0.0
0 0.0
0.5481223065873179
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4396,	 Acc1 = 0.3497,	 Acc2 = 0.4053

 ===== Epoch 296	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038  1.4697628   2.7684073
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.11045138  0.22293963
 -0.00180886 -0.00223772] 4 4
train:	 Loss = 1.3840,	 Acc = 0.4842
10765 0.256
21096 0.526
10887 0.61
1401 0.627
243 0.51
24 0.25
0 0.0
0 0.0
0.5571899794954087
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3878,	 Acc = 0.4902
1367 0.354
6407 0.521
3023 0.493
308 0.409
8 0.875
0 0.0
0 0.0
0 0.0
0.5093371639647035
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4655,	 Acc1 = 0.3705,	 Acc2 = 0.3904

 ===== Epoch 297	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 6 1
train:	 Loss = 1.3844,	 Acc = 0.4828
10765 0.26
21096 0.524
10885 0.605
1403 0.621
243 0.502
24 0.333
0 0.0
0 0.0
0.5542182996047665
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.4008,	 Acc = 0.4749
1367 0.356
6407 0.508
3023 0.466
308 0.386
8 0.875
0 0.0
0 0.0
0 0.0
0.49168889800943977
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4728,	 Acc1 = 0.3705,	 Acc2 = 0.3904

 ===== Epoch 298	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 2 1
train:	 Loss = 1.3824,	 Acc = 0.4855
10768 0.26
21094 0.528
10886 0.609
1401 0.62
243 0.519
24 0.167
0 0.0
0 0.0
0.5576260104612458
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3810,	 Acc = 0.4939
1367 0.334
6407 0.524
3023 0.512
308 0.396
8 0.875
0 0.0
0 0.0
0 0.0
0.5164169915862918
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 1
Testing:	 Loss = 1.4629,	 Acc1 = 0.3631,	 Acc2 = 0.3815

 ===== Epoch 299	 =====
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
 -0.36400968 -0.37184966  1.8107235   1.2883447  -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 1.4681835e-03  2.4594050e-03  4.6474081e-03  3.7761789e-03
  6.6007671e+00  2.4238122e+00  1.1600148e-03  1.7411708e-03
  3.3033782e-01  2.7172923e-01  3.4734835e-03  4.9582324e-03
 -1.8088582e-03 -2.2377169e-03] 2 2
train:	 Loss = 1.3847,	 Acc = 0.4837
10764 0.256
21096 0.527
10888 0.607
1401 0.619
243 0.543
24 0.25
0 0.0
0 0.0
0.556519671936289
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3662,	 Acc = 0.5044
1367 0.352
6407 0.525
3023 0.535
308 0.438
8 0.875
0 0.0
0 0.0
0 0.0
0.5257541555509953
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4395,	 Acc1 = 0.3806,	 Acc2 = 0.4026

 ===== Epoch 300	 =====
[-0.36718738 -0.38160205  1.4663844   1.9577761  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 1.4681835e-03  2.4594050e-03 -2.3793098e-02  7.7791876e-01
  2.1561917e-03  6.3325174e-04  1.1600148e-03  1.7411708e-03
  1.5405245e-03  6.3845129e-03  3.4734835e-03  4.9582324e-03
 -1.8088582e-03 -2.2377169e-03] 6 6
train:	 Loss = 1.3843,	 Acc = 0.4829
10760 0.259
21099 0.524
10888 0.607
1402 0.611
243 0.506
24 0.25
0 0.0
0 0.0
0.5543736629427145
0.56525754155551
[-0.36718738 -0.38160205 -0.42025706 -0.3355664  -0.44155708 -0.3651737
  2.8054698   1.7248323  -0.4018844  -0.40971038  2.6664      3.0039763
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594   0.00464741  0.00377618  0.00215619  0.00063325
 -0.31181416  0.04838841  0.00154052  0.00638451 -0.50897884  0.05945358
 -0.00180886 -0.00223772] 3 3
val:	 Loss = 1.3964,	 Acc = 0.4898
1367 0.358
6407 0.523
3023 0.487
308 0.396
8 0.875
0 0.0
0 0.0
0 0.0
0.5083111019905602
0.56525754155551
[-0.36718738 -0.38160205  1.0783975   2.3388655  -0.44155708 -0.3651737
 -0.36400968 -0.37184966 -0.4018844  -0.40971038 -0.4254193  -0.4215889
 -0.38724938 -0.38149557] [ 0.00146818  0.0024594  -0.03406538  0.22323714  0.00215619  0.00063325
  0.00116001  0.00174117  0.00154052  0.00638451  0.00347348  0.00495823
 -0.00180886 -0.00223772] 4 4
Testing:	 Loss = 1.4663,	 Acc1 = 0.3763,	 Acc2 = 0.3974
