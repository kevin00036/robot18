(0.24300000000005184, array([-1.000000e+00, -1.000000e+00,  1.189783e+03,  1.790000e-01,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 4, array([  0.   ,   0.   , -15.851,   0.091,   0.   ,   0.   ,   0.   ,
         0.   ,   0.   ,   0.   ,   0.   ,   0.   ,   0.   ,   0.   ]), 1)
(0.24, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.543573e+03, -3.290000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([   0.   ,    0.   ,    0.   ,    0.   , -173.065,    3.128,
          0.   ,    0.   ,    0.   ,    0.   ,    0.   ,    0.   ,
          0.   ,    0.   ]))
(0.238, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.370508e+03,  2.799000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([  0.   ,   0.   ,   0.   ,   0.   , -14.234,  -3.14 ,   0.   ,
         0.   ,   0.   ,   0.   ,   0.   ,   0.   ,   0.   ,   0.   ]))
(0.236, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.356274e+03, -3.410000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 7.0844e+01,
       8.0000e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
       0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]))
(0.2350000000000001, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.427118e+03, -3.330000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([ 0.00000e+00,  0.00000e+00,  0.00000e+00,  0.00000e+00,
       -1.81476e+02, -1.00000e-03,  0.00000e+00,  0.00000e+00,
        0.00000e+00,  0.00000e+00,  0.00000e+00,  0.00000e+00,
        0.00000e+00,  0.00000e+00]))
(0.248, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.245642e+03, -3.340000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([ 0.00000e+00,  0.00000e+00,  0.00000e+00,  0.00000e+00,
        1.69507e+02, -9.00000e-03,  0.00000e+00,  0.00000e+00,
        0.00000e+00,  0.00000e+00,  0.00000e+00,  0.00000e+00,
        0.00000e+00,  0.00000e+00]))
(0.256, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.415149e+03, -3.430000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -2.0412e+02,
        3.0000e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
        0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]))
(0.256, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.211029e+03, -3.400000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([0.000000e+00, 0.000000e+00, 2.399328e+03, 1.498000e+00,
       1.390990e+02, 4.000000e-03, 0.000000e+00, 0.000000e+00,
       0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,
       0.000000e+00, 0.000000e+00]))
(0.2670000000000001, array([-1.000000e+00, -1.000000e+00,  2.398328e+03,  4.980000e-01,
        2.350128e+03, -3.360000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([ 0.000000e+00,  0.000000e+00, -2.399328e+03, -1.498000e+00,
        3.364300e+01, -3.000000e-03,  0.000000e+00,  0.000000e+00,
        0.000000e+00,  0.000000e+00,  0.000000e+00,  0.000000e+00,
        0.000000e+00,  0.000000e+00]))
(0.2609999999999997, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.383771e+03, -3.390000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  5.1434e+01,
       -1.1000e-02,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
        0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]))
(0.28500000000000014, array([-1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
        2.435205e+03, -3.500000e-01, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00, -1.000000e+00, -1.000000e+00,
       -1.000000e+00, -1.000000e+00]), 0, array([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -2.9114e+01,
        3.0000e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,
        0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]))
14 1 14

 ===== Epoch 1	 =====
[-0.36602148 -0.3783333   1.8832754   2.3276093  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10  3.71730849e-02  1.74017064e-02
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 1 3
train:	 Loss = 1.9474,	 Acc = 0.1867
2923 0.179
5679 0.176
2914 0.202
377 0.249
69 0.42
6 0.333
0 0.0
0 0.0
0.18927584300718628
0.0
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 2
val:	 Loss = 1.8959,	 Acc = 0.2313
380 0.305
1718 0.227
815 0.211
85 0.188
2 0.0
0 0.0
0 0.0
0 0.0
0.22061068702290076
0.22061068702290076
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 5
Testing:	 Loss = 1.8837,	 Acc1 = 0.2393,	 Acc2 = 0.2324

 ===== Epoch 2	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10  7.83936834e+00 -5.18683052e+00
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 2 4
train:	 Loss = 1.8083,	 Acc = 0.2970
2927 0.229
5675 0.31
2914 0.328
377 0.361
69 0.435
6 0.333
0 0.0
0 0.0
0.31899126202853667
0.22061068702290076
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 2
val:	 Loss = 1.7024,	 Acc = 0.3700
380 0.321
1718 0.404
815 0.335
85 0.247
2 0.0
0 0.0
0 0.0
0 0.0
0.37709923664122136
0.37709923664122136
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.7290,	 Acc1 = 0.3254,	 Acc2 = 0.3360

 ===== Epoch 3	 =====
[ 2.7500618   1.9404247  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.2051675   2.1657557 ] [-6.59231663e-01 -1.39316320e-01 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
 -3.71774957e-02 -2.63923585e-01] 5 5
train:	 Loss = 1.7030,	 Acc = 0.3463
2922 0.239
5681 0.366
2914 0.406
376 0.42
69 0.333
6 0.167
0 0.0
0 0.0
0.3810523988503206
0.37709923664122136
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 2
val:	 Loss = 1.6422,	 Acc = 0.3643
380 0.326
1718 0.395
815 0.33
85 0.259
2 0.0
0 0.0
0 0.0
0 0.0
0.3698473282442748
0.37709923664122136
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.6746,	 Acc1 = 0.3198,	 Acc2 = 0.3293

 ===== Epoch 4	 =====
[-0.36602148 -0.3783333   1.9283583   3.0910199  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -5.85116243e+00 -3.80154943e+00
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 2 1
train:	 Loss = 1.6791,	 Acc = 0.3615
2927 0.241
5673 0.379
2916 0.437
377 0.446
69 0.406
6 0.0
0 0.0
0 0.0
0.4005087932750802
0.37709923664122136
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 2
val:	 Loss = 1.6470,	 Acc = 0.3547
380 0.303
1718 0.375
815 0.344
85 0.294
2 0.0
0 0.0
0 0.0
0 0.0
0.36221374045801524
0.37709923664122136
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.6847,	 Acc1 = 0.3148,	 Acc2 = 0.3233

 ===== Epoch 5	 =====
[-0.36602148 -0.3783333   1.5770366   2.4929774  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.57745679e-02 -9.07055363e-02
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 5 2
train:	 Loss = 1.6770,	 Acc = 0.3607
2923 0.241
5681 0.381
2912 0.432
377 0.422
69 0.464
6 0.167
0 0.0
0 0.0
0.3995577667219458
0.37709923664122136
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 0
val:	 Loss = 1.6180,	 Acc = 0.3997
380 0.334
1718 0.414
815 0.413
85 0.259
2 0.5
0 0.0
0 0.0
0 0.0
0.40916030534351144
0.40916030534351144
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.6631,	 Acc1 = 0.3466,	 Acc2 = 0.3616

 ===== Epoch 6	 =====
[ 0.21495906  1.5404643  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-9.87129868e-04 -6.19183667e-03 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 0 0
train:	 Loss = 1.6677,	 Acc = 0.3737
2922 0.234
5683 0.393
2914 0.456
374 0.511
69 0.464
6 0.333
0 0.0
0 0.0
0.41874861817377845
0.40916030534351144
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 2
val:	 Loss = 1.6519,	 Acc = 0.3653
380 0.324
1718 0.398
815 0.329
85 0.247
2 0.5
0 0.0
0 0.0
0 0.0
0.3713740458015267
0.40916030534351144
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.6717,	 Acc1 = 0.3404,	 Acc2 = 0.3541

 ===== Epoch 7	 =====
[-0.36602148 -0.3783333   1.5437319   2.9460402  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.89322472e+00 -3.64064574e+00
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 1 1
train:	 Loss = 1.6633,	 Acc = 0.3704
2924 0.237
5679 0.387
2914 0.464
376 0.436
69 0.42
6 0.333
0 0.0
0 0.0
0.41364440513047324
0.40916030534351144
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 0
val:	 Loss = 1.6177,	 Acc = 0.3853
380 0.334
1718 0.401
815 0.394
85 0.212
2 0.5
0 0.0
0 0.0
0 0.0
0.39274809160305346
0.40916030534351144
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.6987,	 Acc1 = 0.3074,	 Acc2 = 0.3144

 ===== Epoch 8	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.2589574   3.6012726 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
 -6.02686357e+00 -8.00744152e+00] 6 6
train:	 Loss = 1.6542,	 Acc = 0.3776
2922 0.24
5684 0.392
2912 0.472
375 0.477
69 0.478
6 0.167
0 0.0
0 0.0
0.4219544550077382
0.40916030534351144
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.6201,	 Acc = 0.3763
380 0.337
1718 0.407
815 0.337
85 0.306
2 0.0
0 0.0
0 0.0
0 0.0
0.38206106870229006
0.40916030534351144
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.6599,	 Acc1 = 0.3377,	 Acc2 = 0.3509

 ===== Epoch 9	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 3 1
train:	 Loss = 1.6519,	 Acc = 0.3787
2921 0.24
5678 0.394
2917 0.475
377 0.472
69 0.449
6 0.167
0 0.0
0 0.0
0.42356582292472644
0.40916030534351144
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.6490,	 Acc = 0.3937
380 0.332
1718 0.383
815 0.45
85 0.341
2 0.5
0 0.0
0 0.0
0 0.0
0.4026717557251908
0.40916030534351144
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.7317,	 Acc1 = 0.2917,	 Acc2 = 0.2955

 ===== Epoch 10	 =====
[-0.36602148 -0.3783333   2.7353861   1.5528723  -0.4409929  -0.3635196
  2.6084757   3.464231   -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -1.25688985e-01  2.31102064e-01
  5.23268769e-04  1.12748065e-04  2.12450102e-01  5.57634056e-01
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 6 6
train:	 Loss = 1.6566,	 Acc = 0.3727
2926 0.223
5679 0.391
2913 0.475
375 0.451
69 0.464
6 0.333
0 0.0
0 0.0
0.4212563592125636
0.40916030534351144
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 2
val:	 Loss = 1.5955,	 Acc = 0.3963
380 0.329
1718 0.41
815 0.41
85 0.306
2 0.0
0 0.0
0 0.0
0 0.0
0.40610687022900765
0.40916030534351144
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.6713,	 Acc1 = 0.3152,	 Acc2 = 0.3238

 ===== Epoch 11	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 1
train:	 Loss = 1.6539,	 Acc = 0.3754
2920 0.239
5685 0.39
2912 0.473
376 0.452
69 0.435
6 0.167
0 0.0
0 0.0
0.41942970822281167
0.40916030534351144
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 0
val:	 Loss = 1.6399,	 Acc = 0.3847
380 0.326
1718 0.402
815 0.39
85 0.259
2 0.0
0 0.0
0 0.0
0 0.0
0.3931297709923664
0.40916030534351144
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.6926,	 Acc1 = 0.3229,	 Acc2 = 0.3330

 ===== Epoch 12	 =====
[ 0.21533065  1.5354013  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-4.39220632e-04  3.09591810e-03 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 0 1
train:	 Loss = 1.6542,	 Acc = 0.3719
2926 0.221
5679 0.388
2912 0.475
377 0.496
68 0.412
6 0.333
0 0.0
0 0.0
0.4205927892059279
0.40916030534351144
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 2
val:	 Loss = 1.6703,	 Acc = 0.3447
380 0.316
1718 0.358
815 0.342
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.34885496183206105
0.40916030534351144
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 4
Testing:	 Loss = 1.6841,	 Acc1 = 0.3287,	 Acc2 = 0.3400

 ===== Epoch 13	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 6 1
train:	 Loss = 1.6377,	 Acc = 0.3836
2925 0.235
5678 0.396
2914 0.492
376 0.503
69 0.493
6 0.167
0 0.0
0 0.0
0.4318257215525821
0.40916030534351144
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 0
val:	 Loss = 1.6348,	 Acc = 0.3707
380 0.337
1718 0.381
815 0.379
85 0.235
2 0.0
0 0.0
0 0.0
0 0.0
0.3755725190839695
0.40916030534351144
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.6922,	 Acc1 = 0.3295,	 Acc2 = 0.3410

 ===== Epoch 14	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 0 1
train:	 Loss = 1.6317,	 Acc = 0.3861
2922 0.232
5684 0.403
2911 0.493
376 0.495
69 0.435
6 0.167
0 0.0
0 0.0
0.4359938094185275
0.40916030534351144
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 2
val:	 Loss = 1.5783,	 Acc = 0.3983
380 0.324
1718 0.423
815 0.399
85 0.235
2 0.5
0 0.0
0 0.0
0 0.0
0.40916030534351144
0.40916030534351144
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.6332,	 Acc1 = 0.3462,	 Acc2 = 0.3611

 ===== Epoch 15	 =====
[ 2.324227    2.1252165  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.1879728   2.357333  ] [-2.10200623e-02 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 0 0
train:	 Loss = 1.6099,	 Acc = 0.3936
2924 0.24
5682 0.41
2911 0.504
376 0.484
69 0.42
6 0.333
0 0.0
0 0.0
0.4431667403803627
0.40916030534351144
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.5647,	 Acc = 0.4233
380 0.334
1718 0.446
815 0.431
85 0.282
2 0.5
0 0.0
0 0.0
0 0.0
0.4362595419847328
0.4362595419847328
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.6080,	 Acc1 = 0.3648,	 Acc2 = 0.3834

 ===== Epoch 16	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.5211736   1.8127627
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -2.15231523e-01 -2.45991185e-01
  5.05659230e-11 -3.35663330e-10] 5 5
train:	 Loss = 1.6033,	 Acc = 0.3961
2925 0.233
5678 0.421
2913 0.497
377 0.496
69 0.435
6 0.333
0 0.0
0 0.0
0.4489660510892403
0.4362595419847328
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 0
val:	 Loss = 1.6260,	 Acc = 0.3917
380 0.326
1718 0.421
815 0.379
85 0.212
2 0.0
0 0.0
0 0.0
0 0.0
0.40114503816793895
0.4362595419847328
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.6538,	 Acc1 = 0.3654,	 Acc2 = 0.3842

 ===== Epoch 17	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   2.9823236   2.0458791  -0.42342317 -0.42019257
  3.376617    0.90607   ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
  4.15328890e-01  1.09334268e-01 -4.94477126e-10 -1.99525854e-10
  7.25465894e-01  9.50124860e-02] 6 2
train:	 Loss = 1.5967,	 Acc = 0.4013
2926 0.238
5679 0.424
2912 0.51
376 0.487
69 0.391
6 0.333
0 0.0
0 0.0
0.45410307454103077
0.4362595419847328
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.5945,	 Acc = 0.3893
380 0.332
1718 0.42
815 0.371
85 0.224
2 0.0
0 0.0
0 0.0
0 0.0
0.3977099236641221
0.4362595419847328
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.6465,	 Acc1 = 0.3536,	 Acc2 = 0.3700

 ===== Epoch 18	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 3 1
train:	 Loss = 1.5913,	 Acc = 0.4021
2924 0.24
5684 0.428
2910 0.504
375 0.483
69 0.406
6 0.0
0 0.0
0 0.0
0.45444493586908447
0.4362595419847328
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 2
val:	 Loss = 1.5606,	 Acc = 0.3937
380 0.226
1718 0.431
815 0.401
85 0.318
2 0.0
0 0.0
0 0.0
0 0.0
0.4179389312977099
0.4362595419847328
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.6081,	 Acc1 = 0.3379,	 Acc2 = 0.3912

 ===== Epoch 19	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.2713953e-11 -3.1991790e-10 -2.5436093e-04 -1.9714674e-04
  1.2271482e+01  3.3869174e+00 -1.5722629e-10 -9.7300190e-10
  9.1905651e+00  4.7061272e+00 -4.9447713e-10 -1.9952585e-10
  5.0565923e-11 -3.3566333e-10] 2 6
train:	 Loss = 1.5789,	 Acc = 0.4081
2924 0.236
5679 0.434
2914 0.516
376 0.511
69 0.507
6 0.333
0 0.0
0 0.0
0.463843432109686
0.4362595419847328
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.5420,	 Acc = 0.4263
380 0.342
1718 0.44
815 0.453
85 0.282
2 0.0
0 0.0
0 0.0
0 0.0
0.4385496183206107
0.4385496183206107
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5857,	 Acc1 = 0.3926,	 Acc2 = 0.4170

 ===== Epoch 20	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   1.5971525   2.1877236  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -1.12999021e-03 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 0 0
train:	 Loss = 1.5800,	 Acc = 0.4113
2922 0.25
5679 0.435
2916 0.515
376 0.513
69 0.435
6 0.0
0 0.0
0 0.0
0.4635197877514924
0.4385496183206107
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 2
val:	 Loss = 1.5813,	 Acc = 0.3977
380 0.339
1718 0.423
815 0.385
85 0.282
2 0.0
0 0.0
0 0.0
0 0.0
0.40610687022900765
0.4385496183206107
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.6345,	 Acc1 = 0.3660,	 Acc2 = 0.3849

 ===== Epoch 21	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   2.8472607  -4.1131206
 -0.3640846  -0.3725409   1.8031255   2.150396   -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.2713953e-11 -3.1991790e-10 -2.5436093e-04 -1.9714674e-04
 -8.2052684e-01  7.9454966e+00 -1.5722629e-10 -9.7300190e-10
  2.2407728e-01  1.8063922e-01 -4.9447713e-10 -1.9952585e-10
  5.0565923e-11 -3.3566333e-10] 0 4
train:	 Loss = 1.5696,	 Acc = 0.4098
2925 0.238
5676 0.438
2915 0.516
377 0.496
69 0.435
6 0.167
0 0.0
0 0.0
0.465442883998673
0.4385496183206107
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.5449,	 Acc = 0.4143
380 0.337
1718 0.43
815 0.429
85 0.306
2 0.5
0 0.0
0 0.0
0 0.0
0.4255725190839695
0.4385496183206107
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5951,	 Acc1 = 0.3829,	 Acc2 = 0.4053

 ===== Epoch 22	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 5 1
train:	 Loss = 1.5779,	 Acc = 0.4088
2922 0.245
5681 0.433
2913 0.512
377 0.517
69 0.406
6 0.333
0 0.0
0 0.0
0.46152995799248286
0.4385496183206107
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 2
val:	 Loss = 1.5254,	 Acc = 0.4453
380 0.339
1718 0.465
815 0.469
85 0.306
2 0.0
0 0.0
0 0.0
0 0.0
0.46068702290076335
0.46068702290076335
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5957,	 Acc1 = 0.3736,	 Acc2 = 0.3941

 ===== Epoch 23	 =====
[ 3.5227978   1.6518456  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.478521    1.840337  ] [-9.84987497e-01 -6.19183667e-03 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  3.06461960e-01 -3.69493030e-02] 3 3
train:	 Loss = 1.5681,	 Acc = 0.4148
2922 0.246
5682 0.439
2913 0.528
376 0.492
69 0.391
6 0.0
0 0.0
0 0.0
0.46915763873535266
0.46068702290076335
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 2
val:	 Loss = 1.5775,	 Acc = 0.4207
380 0.332
1718 0.46
815 0.399
85 0.247
2 0.0
0 0.0
0 0.0
0 0.0
0.433587786259542
0.46068702290076335
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.6138,	 Acc1 = 0.3829,	 Acc2 = 0.4053

 ===== Epoch 24	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 1 1
train:	 Loss = 1.5673,	 Acc = 0.4141
2926 0.245
5680 0.438
2911 0.524
376 0.511
69 0.464
6 0.167
0 0.0
0 0.0
0.4688122096881221
0.46068702290076335
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.5052,	 Acc = 0.4390
380 0.342
1718 0.462
815 0.455
85 0.271
2 0.0
0 0.0
0 0.0
0 0.0
0.4530534351145038
0.46068702290076335
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5792,	 Acc1 = 0.3800,	 Acc2 = 0.4018

 ===== Epoch 25	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   1.9477656   2.7570732
 -0.3640846  -0.3725409   0.71890634  1.4088224  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.2713953e-11 -3.1991790e-10 -2.5436093e-04 -1.9714674e-04
 -2.7282703e-01 -2.4684176e-01 -1.5722629e-10 -9.7300190e-10
  1.8333468e-01 -4.6110541e-01 -4.9447713e-10 -1.9952585e-10
  5.0565923e-11 -3.3566333e-10] 5 5
train:	 Loss = 1.5662,	 Acc = 0.4144
2919 0.245
5685 0.44
2912 0.519
377 0.525
69 0.42
6 0.333
0 0.0
0 0.0
0.46889159023096477
0.46068702290076335
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 3
val:	 Loss = 1.5031,	 Acc = 0.4430
380 0.342
1718 0.468
815 0.459
85 0.247
2 0.0
0 0.0
0 0.0
0 0.0
0.45763358778625957
0.46068702290076335
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5929,	 Acc1 = 0.3687,	 Acc2 = 0.3882

 ===== Epoch 26	 =====
[-0.36602148 -0.3783333   2.103818    1.1813611  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -6.56752825e-01  1.17966577e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
train:	 Loss = 1.5701,	 Acc = 0.4137
2921 0.249
5682 0.435
2915 0.526
375 0.504
69 0.435
6 0.333
0 0.0
0 0.0
0.4668951033491765
0.46068702290076335
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 2
val:	 Loss = 1.5131,	 Acc = 0.4323
380 0.337
1718 0.462
815 0.432
85 0.271
2 0.0
0 0.0
0 0.0
0 0.0
0.44618320610687023
0.46068702290076335
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5934,	 Acc1 = 0.3677,	 Acc2 = 0.3869

 ===== Epoch 27	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 1 1
train:	 Loss = 1.5644,	 Acc = 0.4154
2922 0.249
5682 0.44
2914 0.522
375 0.504
69 0.42
6 0.333
0 0.0
0 0.0
0.4690470926376299
0.46068702290076335
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 2
val:	 Loss = 1.5370,	 Acc = 0.4067
380 0.332
1718 0.432
815 0.402
85 0.282
2 0.0
0 0.0
0 0.0
0 0.0
0.41755725190839693
0.46068702290076335
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5953,	 Acc1 = 0.3743,	 Acc2 = 0.3949

 ===== Epoch 28	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   1.5952245   1.0278556
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.2713953e-11 -3.1991790e-10 -2.5436093e-04 -1.9714674e-04
 -1.7106737e-01 -2.0046804e-02 -1.5722629e-10 -9.7300190e-10
 -6.1223204e-10 -1.8893141e-10 -4.9447713e-10 -1.9952585e-10
  5.0565923e-11 -3.3566333e-10] 5 0
train:	 Loss = 1.5667,	 Acc = 0.4153
2926 0.247
5679 0.442
2912 0.518
376 0.521
69 0.449
6 0.167
0 0.0
0 0.0
0.4696969696969697
0.46068702290076335
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 2
val:	 Loss = 1.5069,	 Acc = 0.4363
380 0.345
1718 0.459
815 0.448
85 0.282
2 0.0
0 0.0
0 0.0
0 0.0
0.449618320610687
0.46068702290076335
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5812,	 Acc1 = 0.3811,	 Acc2 = 0.4031

 ===== Epoch 29	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   2.5821872   1.3301352
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.2713953e-11 -3.1991790e-10  8.6081457e+00  3.6125960e+00
  7.3976719e-01 -2.0904259e-01 -1.5722629e-10 -9.7300190e-10
 -6.1223204e-10 -1.8893141e-10 -4.9447713e-10 -1.9952585e-10
  5.0565923e-11 -3.3566333e-10] 5 5
train:	 Loss = 1.5627,	 Acc = 0.4108
2925 0.246
5682 0.438
2910 0.512
376 0.497
69 0.449
6 0.333
0 0.0
0 0.0
0.4642264735154263
0.46068702290076335
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 3
val:	 Loss = 1.5044,	 Acc = 0.4400
380 0.347
1718 0.453
815 0.471
85 0.306
2 0.0
0 0.0
0 0.0
0 0.0
0.4534351145038168
0.46068702290076335
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5693,	 Acc1 = 0.3920,	 Acc2 = 0.4163

 ===== Epoch 30	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.5749682   1.3937756
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -6.64422661e-02  4.86962169e-01
  5.05659230e-11 -3.35663330e-10] 0 6
train:	 Loss = 1.5657,	 Acc = 0.4134
2920 0.249
5686 0.437
2910 0.518
377 0.523
69 0.464
6 0.0
0 0.0
0 0.0
0.46640141467727675
0.46068702290076335
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.5197,	 Acc = 0.4427
380 0.342
1718 0.471
815 0.442
85 0.329
2 0.0
0 0.0
0 0.0
0 0.0
0.45725190839694657
0.46068702290076335
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5829,	 Acc1 = 0.3879,	 Acc2 = 0.4113

 ===== Epoch 31	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   2.8946233   2.9660015
 -0.3640846  -0.3725409   1.9284266   2.3146374  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 0 0
train:	 Loss = 1.5595,	 Acc = 0.4123
2925 0.244
5679 0.44
2912 0.52
377 0.472
69 0.478
6 0.167
0 0.0
0 0.0
0.4668804600243282
0.46068702290076335
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 2
val:	 Loss = 1.5546,	 Acc = 0.4270
380 0.35
1718 0.464
815 0.402
85 0.259
2 0.0
0 0.0
0 0.0
0 0.0
0.4381679389312977
0.46068702290076335
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5997,	 Acc1 = 0.3868,	 Acc2 = 0.4100

 ===== Epoch 32	 =====
[ 1.9249307   1.788541   -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 1.97217569e-01 -1.39316320e-01 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 6 5
train:	 Loss = 1.5594,	 Acc = 0.4158
2922 0.251
5684 0.442
2911 0.517
376 0.527
69 0.435
6 0.0
0 0.0
0 0.0
0.4690470926376299
0.46068702290076335
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.5459,	 Acc = 0.4163
380 0.35
1718 0.451
815 0.389
85 0.282
2 0.0
0 0.0
0 0.0
0 0.0
0.4259541984732824
0.46068702290076335
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.6047,	 Acc1 = 0.3806,	 Acc2 = 0.4026

 ===== Epoch 33	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 5 1
train:	 Loss = 1.5657,	 Acc = 0.4117
2923 0.244
5684 0.443
2910 0.512
376 0.473
69 0.362
6 0.167
0 0.0
0 0.0
0.4657822001105583
0.46068702290076335
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.5238,	 Acc = 0.4377
380 0.342
1718 0.474
815 0.42
85 0.318
2 0.0
0 0.0
0 0.0
0 0.0
0.45152671755725193
0.46068702290076335
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5871,	 Acc1 = 0.3848,	 Acc2 = 0.4076

 ===== Epoch 34	 =====
[-0.36602148 -0.3783333   1.9121128   0.90499276 -0.4409929  -0.3635196
  3.2915025   3.0819492  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -1.25685871e-01  4.50570472e-02
  5.23268769e-04  1.12748065e-04  8.83534014e-01  2.51615375e-01
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 3 4
train:	 Loss = 1.5603,	 Acc = 0.4163
2919 0.246
5682 0.45
2915 0.511
377 0.499
69 0.435
6 0.0
0 0.0
0 0.0
0.4713227980992375
0.46068702290076335
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 2
val:	 Loss = 1.5131,	 Acc = 0.4407
380 0.339
1718 0.476
815 0.431
85 0.294
2 0.0
0 0.0
0 0.0
0 0.0
0.45534351145038165
0.46068702290076335
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5839,	 Acc1 = 0.3755,	 Acc2 = 0.3964

 ===== Epoch 35	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   2.1979084   1.8492874  -0.42342317 -0.42019257
  3.6517348   1.5464102 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
  3.16872835e-01  1.90146565e-02 -4.94477126e-10 -1.99525854e-10
 -7.73695111e-01 -1.05569437e-02] 2 2
train:	 Loss = 1.5600,	 Acc = 0.4170
2925 0.252
5677 0.443
2916 0.525
376 0.476
68 0.471
6 0.167
0 0.0
0 0.0
0.47052969147406837
0.46068702290076335
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 2
val:	 Loss = 1.5499,	 Acc = 0.4273
380 0.347
1718 0.465
815 0.402
85 0.271
2 0.0
0 0.0
0 0.0
0 0.0
0.4389312977099237
0.46068702290076335
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5944,	 Acc1 = 0.3829,	 Acc2 = 0.4053

 ===== Epoch 36	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10  6.12175226e+00  3.72321725e+00
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 5
train:	 Loss = 1.5578,	 Acc = 0.4188
2921 0.251
5685 0.449
2911 0.517
376 0.503
69 0.42
6 0.333
0 0.0
0 0.0
0.4728639327954018
0.46068702290076335
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.5495,	 Acc = 0.4137
380 0.345
1718 0.448
815 0.388
85 0.294
2 0.0
0 0.0
0 0.0
0 0.0
0.42366412213740456
0.46068702290076335
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.6116,	 Acc1 = 0.3720,	 Acc2 = 0.3921

 ===== Epoch 37	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  1.0716692   2.2448351  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -7.37334341e-02  1.36008291e-02
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 0
train:	 Loss = 1.5630,	 Acc = 0.4125
2923 0.241
5684 0.443
2910 0.513
376 0.513
69 0.377
6 0.333
0 0.0
0 0.0
0.4679933665008292
0.46068702290076335
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 2
val:	 Loss = 1.5705,	 Acc = 0.4047
380 0.342
1718 0.436
815 0.379
85 0.306
2 0.0
0 0.0
0 0.0
0 0.0
0.41374045801526715
0.46068702290076335
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.6129,	 Acc1 = 0.3790,	 Acc2 = 0.4006

 ===== Epoch 38	 =====
[-0.36602148 -0.3783333   2.0091832   1.2379938  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -8.32020119e-02 -1.03276156e-01
  5.23268769e-04  1.12748065e-04  3.31306696e+00  1.02958288e+01
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 2 5
train:	 Loss = 1.5612,	 Acc = 0.4144
2926 0.245
5682 0.443
2911 0.517
375 0.515
68 0.382
6 0.0
0 0.0
0 0.0
0.46914399469143997
0.46068702290076335
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.5486,	 Acc = 0.4290
380 0.342
1718 0.471
815 0.398
85 0.282
2 0.0
0 0.0
0 0.0
0 0.0
0.4416030534351145
0.46068702290076335
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5937,	 Acc1 = 0.3908,	 Acc2 = 0.4148

 ===== Epoch 39	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   1.3333926   2.9989755  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -1.35801658e-01 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 0 0
train:	 Loss = 1.5564,	 Acc = 0.4170
2921 0.239
5682 0.448
2913 0.525
377 0.496
69 0.449
6 0.167
0 0.0
0 0.0
0.4745219409749088
0.46068702290076335
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 2
val:	 Loss = 1.5225,	 Acc = 0.4363
380 0.347
1718 0.449
815 0.466
85 0.306
2 0.0
0 0.0
0 0.0
0 0.0
0.449236641221374
0.46068702290076335
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5825,	 Acc1 = 0.3914,	 Acc2 = 0.4155

 ===== Epoch 40	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.7217026   1.5934107
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10  7.89803267e-01 -1.00404574e-02
  5.05659230e-11 -3.35663330e-10] 0 3
train:	 Loss = 1.5631,	 Acc = 0.4164
2920 0.245
5680 0.446
2916 0.525
377 0.467
69 0.391
6 0.333
0 0.0
0 0.0
0.47181697612732093
0.46068702290076335
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.5010,	 Acc = 0.4457
380 0.339
1718 0.469
815 0.465
85 0.282
2 0.0
0 0.0
0 0.0
0 0.0
0.46106870229007635
0.46106870229007635
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5784,	 Acc1 = 0.3786,	 Acc2 = 0.4001

 ===== Epoch 41	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   1.0644841   2.0906723  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
  3.84773672e-01 -2.09161207e-01 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 2 5
train:	 Loss = 1.5602,	 Acc = 0.4109
2925 0.234
5678 0.445
2913 0.511
377 0.509
69 0.435
6 0.167
0 0.0
0 0.0
0.4683180360499834
0.46106870229007635
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 2
val:	 Loss = 1.5321,	 Acc = 0.4467
380 0.342
1718 0.471
815 0.461
85 0.282
2 0.0
0 0.0
0 0.0
0 0.0
0.4618320610687023
0.4618320610687023
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.6081,	 Acc1 = 0.3782,	 Acc2 = 0.3996

 ===== Epoch 42	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  3.9458802   1.9434742  -0.40141198 -0.40778467  3.9945893   2.451102
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -3.23672205e-01  1.36008291e-02
 -6.12232043e-10 -1.88931412e-10 -2.39560127e-01 -1.00404574e-02
  5.05659230e-11 -3.35663330e-10] 3 3
train:	 Loss = 1.5544,	 Acc = 0.4180
2925 0.25
5680 0.448
2912 0.519
376 0.497
69 0.377
6 0.333
0 0.0
0 0.0
0.4722990158133363
0.4618320610687023
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 3
val:	 Loss = 1.4870,	 Acc = 0.4487
380 0.339
1718 0.468
815 0.475
85 0.306
2 0.0
0 0.0
0 0.0
0 0.0
0.46450381679389313
0.46450381679389313
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5744,	 Acc1 = 0.3765,	 Acc2 = 0.3976

 ===== Epoch 43	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   2.393885    2.5348086
 -0.3640846  -0.3725409   1.5056084   2.0931606  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.2713953e-11 -3.1991790e-10 -2.5436093e-04 -1.9714674e-04
  7.8067029e-01  1.6390909e-01 -1.5722629e-10 -9.7300190e-10
  1.8107121e-01  2.7095884e-01 -4.9447713e-10 -1.9952585e-10
  5.0565923e-11 -3.3566333e-10] 6 6
train:	 Loss = 1.5562,	 Acc = 0.4184
2925 0.249
5681 0.447
2912 0.52
375 0.517
69 0.449
6 0.167
0 0.0
0 0.0
0.47318367798297023
0.46450381679389313
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 2
val:	 Loss = 1.5069,	 Acc = 0.4497
380 0.342
1718 0.475
815 0.465
85 0.282
2 0.0
0 0.0
0 0.0
0 0.0
0.46526717557251906
0.46526717557251906
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5739,	 Acc1 = 0.3887,	 Acc2 = 0.4123

 ===== Epoch 44	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   1.1345917   2.7859676
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.2713953e-11 -3.1991790e-10 -2.5436093e-04 -1.9714674e-04
  1.3420540e-01 -2.5086690e-02 -1.5722629e-10 -9.7300190e-10
 -6.1223204e-10 -1.8893141e-10 -4.9447713e-10 -1.9952585e-10
  5.0565923e-11 -3.3566333e-10] 2 0
train:	 Loss = 1.5558,	 Acc = 0.4184
2926 0.249
5679 0.45
2912 0.518
376 0.503
69 0.377
6 0.333
0 0.0
0 0.0
0.4732360097323601
0.46526717557251906
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 2
val:	 Loss = 1.4943,	 Acc = 0.4470
380 0.345
1718 0.479
815 0.445
85 0.282
2 0.0
0 0.0
0 0.0
0 0.0
0.4618320610687023
0.46526717557251906
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5818,	 Acc1 = 0.3654,	 Acc2 = 0.3842

 ===== Epoch 45	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   1.5971525   2.1877236  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 0 0
train:	 Loss = 1.5639,	 Acc = 0.4158
2926 0.24
5684 0.448
2908 0.52
375 0.501
69 0.435
6 0.0
0 0.0
0 0.0
0.4726830347268303
0.46526717557251906
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 2
val:	 Loss = 1.5610,	 Acc = 0.4257
380 0.337
1718 0.462
815 0.406
85 0.294
2 0.0
0 0.0
0 0.0
0 0.0
0.4385496183206107
0.46526717557251906
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.6227,	 Acc1 = 0.3629,	 Acc2 = 0.3812

 ===== Epoch 46	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   2.291522    1.319022
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.2713953e-11 -3.1991790e-10 -2.5436093e-04 -1.9714674e-04
  5.4722387e-01  5.3031571e-02 -1.5722629e-10 -9.7300190e-10
 -6.1223204e-10 -1.8893141e-10 -4.9447713e-10 -1.9952585e-10
  5.0565923e-11 -3.3566333e-10] 2 2
train:	 Loss = 1.5584,	 Acc = 0.4163
2923 0.247
5678 0.446
2917 0.515
376 0.511
68 0.471
6 0.167
0 0.0
0 0.0
0.4709784411276949
0.46526717557251906
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 3
val:	 Loss = 1.5240,	 Acc = 0.4353
380 0.337
1718 0.458
815 0.449
85 0.294
2 0.0
0 0.0
0 0.0
0 0.0
0.449618320610687
0.46526717557251906
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5901,	 Acc1 = 0.3749,	 Acc2 = 0.3956

 ===== Epoch 47	 =====
[-0.36602148 -0.3783333   0.54987454  1.471321   -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10  7.56105632e-02 -2.71126884e-03
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 0 0
train:	 Loss = 1.5505,	 Acc = 0.4142
2923 0.246
5678 0.445
2916 0.515
376 0.487
69 0.406
6 0.167
0 0.0
0 0.0
0.4684355997788834
0.46526717557251906
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 3
val:	 Loss = 1.5045,	 Acc = 0.4493
380 0.337
1718 0.483
815 0.45
85 0.271
2 0.0
0 0.0
0 0.0
0 0.0
0.46564885496183206
0.46564885496183206
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5987,	 Acc1 = 0.3662,	 Acc2 = 0.3852

 ===== Epoch 48	 =====
[ 1.0162216   3.198528   -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.0015334   3.5251667 ] [-1.81585774e-01 -2.47673467e-02 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
 -2.30100408e-01 -3.69493030e-02] 1 0
train:	 Loss = 1.5528,	 Acc = 0.4174
2926 0.247
5680 0.447
2911 0.521
376 0.503
69 0.391
6 0.333
0 0.0
0 0.0
0.4725724397257244
0.46564885496183206
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 2
val:	 Loss = 1.4874,	 Acc = 0.4547
380 0.337
1718 0.494
815 0.448
85 0.271
2 0.0
0 0.0
0 0.0
0 0.0
0.4717557251908397
0.4717557251908397
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5729,	 Acc1 = 0.3839,	 Acc2 = 0.4066

 ===== Epoch 49	 =====
[ 1.7562681   0.89495844 -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   1.2269739   3.342389   -0.42342317 -0.42019257
  2.1006765   1.0950228 ] [-2.35527158e+00 -1.55724692e+00 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -3.22089696e+00 -7.16377115e+00 -4.94477126e-10 -1.99525854e-10
 -8.92259896e-01 -2.11138874e-02] 1 5
train:	 Loss = 1.5530,	 Acc = 0.4201
2926 0.244
5678 0.454
2913 0.52
376 0.516
69 0.377
6 0.167
0 0.0
0 0.0
0.4772174297721743
0.4717557251908397
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4880,	 Acc = 0.4660
380 0.342
1718 0.484
815 0.501
85 0.329
2 0.0
0 0.0
0 0.0
0 0.0
0.48396946564885496
0.48396946564885496
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5693,	 Acc1 = 0.3897,	 Acc2 = 0.4135

 ===== Epoch 50	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 1
train:	 Loss = 1.5541,	 Acc = 0.4166
2927 0.248
5679 0.448
2911 0.515
377 0.496
68 0.441
6 0.333
0 0.0
0 0.0
0.4712974228514545
0.48396946564885496
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 2
val:	 Loss = 1.5524,	 Acc = 0.4130
380 0.189
1718 0.469
815 0.418
85 0.247
2 0.0
0 0.0
0 0.0
0 0.0
0.4454198473282443
0.48396946564885496
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.6047,	 Acc1 = 0.3596,	 Acc2 = 0.4095

 ===== Epoch 51	 =====
[-0.36602148 -0.3783333   2.339387    2.9483051  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -1.32768586e-01 -1.46016225e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 2 5
train:	 Loss = 1.5501,	 Acc = 0.4186
2923 0.24
5682 0.451
2914 0.523
374 0.5
69 0.435
6 0.333
0 0.0
0 0.0
0.47628524046434495
0.48396946564885496
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 2
val:	 Loss = 1.5221,	 Acc = 0.4307
380 0.329
1718 0.478
815 0.398
85 0.259
2 0.0
0 0.0
0 0.0
0 0.0
0.4454198473282443
0.48396946564885496
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5934,	 Acc1 = 0.3755,	 Acc2 = 0.3964

 ===== Epoch 52	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 1
train:	 Loss = 1.5559,	 Acc = 0.4151
2922 0.248
5679 0.444
2915 0.511
377 0.52
69 0.478
6 0.333
0 0.0
0 0.0
0.46915763873535266
0.48396946564885496
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.5205,	 Acc = 0.4437
380 0.345
1718 0.469
815 0.453
85 0.306
2 0.0
0 0.0
0 0.0
0 0.0
0.4580152671755725
0.48396946564885496
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5697,	 Acc1 = 0.4021,	 Acc2 = 0.4284

 ===== Epoch 53	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  1.7732302   1.0979892  -0.40141198 -0.40778467  1.7939348   3.197885
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04  2.36193448e-01 -7.48045668e-02
 -6.12232043e-10 -1.88931412e-10 -5.80183789e-02 -1.75707996e-01
  5.05659230e-11 -3.35663330e-10] 3 5
train:	 Loss = 1.5476,	 Acc = 0.4164
2920 0.25
5682 0.447
2914 0.508
377 0.525
69 0.478
6 0.333
0 0.0
0 0.0
0.47004862953138815
0.48396946564885496
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 3
val:	 Loss = 1.5177,	 Acc = 0.4057
380 0.192
1718 0.445
815 0.436
85 0.294
2 0.0
0 0.0
0 0.0
0 0.0
0.4366412213740458
0.48396946564885496
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5920,	 Acc1 = 0.3478,	 Acc2 = 0.3954

 ===== Epoch 54	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   1.9382172   0.832263
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  1.03279054e-01  7.06711784e-02 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 3 2
train:	 Loss = 1.5562,	 Acc = 0.4196
2924 0.25
5679 0.448
2914 0.518
376 0.545
69 0.42
6 0.333
0 0.0
0 0.0
0.4745687748783724
0.48396946564885496
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 2
val:	 Loss = 1.5380,	 Acc = 0.4257
380 0.339
1718 0.451
815 0.428
85 0.294
2 0.0
0 0.0
0 0.0
0 0.0
0.4381679389312977
0.48396946564885496
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5923,	 Acc1 = 0.3936,	 Acc2 = 0.4182

 ===== Epoch 55	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.6878611   1.1054137
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -2.62031537e-02  1.20485485e-01
  5.05659230e-11 -3.35663330e-10] 3 2
train:	 Loss = 1.5542,	 Acc = 0.4093
2926 0.242
5678 0.439
2913 0.511
376 0.476
69 0.42
6 0.167
0 0.0
0 0.0
0.4636142446361424
0.48396946564885496
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.5382,	 Acc = 0.4183
380 0.337
1718 0.445
815 0.415
85 0.294
2 0.0
0 0.0
0 0.0
0 0.0
0.4301526717557252
0.48396946564885496
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5941,	 Acc1 = 0.3877,	 Acc2 = 0.4110

 ===== Epoch 56	 =====
[-0.36602148 -0.3783333   1.5303289   2.4091606  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.08633527e-01  3.04011613e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 6 6
train:	 Loss = 1.5480,	 Acc = 0.4169
2925 0.243
5683 0.448
2910 0.52
375 0.504
69 0.406
6 0.333
0 0.0
0 0.0
0.473073095211766
0.48396946564885496
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 2
val:	 Loss = 1.5094,	 Acc = 0.4397
380 0.339
1718 0.468
815 0.443
85 0.294
2 0.0
0 0.0
0 0.0
0 0.0
0.4541984732824427
0.48396946564885496
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5833,	 Acc1 = 0.3771,	 Acc2 = 0.3984

 ===== Epoch 57	 =====
[ 1.9108452   0.98862004 -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.0354245   1.1842506 ] [ 1.60595477e-01  7.43020400e-02 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
 -4.01935680e-03  1.47797212e-01] 2 2
train:	 Loss = 1.5505,	 Acc = 0.4185
2925 0.248
5678 0.451
2913 0.515
377 0.509
69 0.449
6 0.167
0 0.0
0 0.0
0.4738471746101957
0.48396946564885496
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.5231,	 Acc = 0.4530
380 0.337
1718 0.479
815 0.474
85 0.259
2 0.0
0 0.0
0 0.0
0 0.0
0.46984732824427483
0.48396946564885496
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.6081,	 Acc1 = 0.3759,	 Acc2 = 0.3969

 ===== Epoch 58	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 6 1
train:	 Loss = 1.5509,	 Acc = 0.4180
2923 0.237
5683 0.449
2912 0.53
375 0.488
69 0.42
6 0.167
0 0.0
0 0.0
0.47639579878385846
0.48396946564885496
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 2
val:	 Loss = 1.5588,	 Acc = 0.4017
380 0.339
1718 0.426
815 0.393
85 0.282
2 0.0
0 0.0
0 0.0
0 0.0
0.41068702290076337
0.48396946564885496
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.6177,	 Acc1 = 0.3627,	 Acc2 = 0.3810

 ===== Epoch 59	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   1.5347884   1.2371157  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -1.82201192e-01  1.85392886e-01 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
train:	 Loss = 1.5491,	 Acc = 0.4169
2922 0.248
5681 0.446
2913 0.519
377 0.488
69 0.435
6 0.333
0 0.0
0 0.0
0.4714791067875304
0.48396946564885496
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.5479,	 Acc = 0.4340
380 0.329
1718 0.464
815 0.437
85 0.282
2 0.0
0 0.0
0 0.0
0 0.0
0.449236641221374
0.48396946564885496
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 4
Testing:	 Loss = 1.5857,	 Acc1 = 0.3994,	 Acc2 = 0.4252

 ===== Epoch 60	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.235657    1.5317948
 -0.38555372 -0.3798593 ] [ 3.03303218e+00  4.67793274e+00 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -3.27524953e-02  9.03641135e-02
  5.05659230e-11 -3.35663330e-10] 2 4
train:	 Loss = 1.5470,	 Acc = 0.4185
2919 0.25
5683 0.448
2916 0.52
376 0.495
69 0.42
5 0.2
0 0.0
0 0.0
0.4728699303790474
0.48396946564885496
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 3
val:	 Loss = 1.5083,	 Acc = 0.4467
380 0.342
1718 0.483
815 0.438
85 0.271
2 0.0
0 0.0
0 0.0
0 0.0
0.4618320610687023
0.48396946564885496
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5721,	 Acc1 = 0.3941,	 Acc2 = 0.4187

 ===== Epoch 61	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   1.730435    1.8346757
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.2713953e-11 -3.1991790e-10 -2.5436093e-04 -1.9714674e-04
  8.5649353e-01 -2.0400272e-01 -1.5722629e-10 -9.7300190e-10
 -6.1223204e-10 -1.8893141e-10 -4.9447713e-10 -1.9952585e-10
  5.0565923e-11 -3.3566333e-10] 5 5
train:	 Loss = 1.5487,	 Acc = 0.4200
2920 0.243
5682 0.451
2915 0.526
376 0.487
69 0.478
6 0.5
0 0.0
0 0.0
0.4771220159151194
0.48396946564885496
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 3
val:	 Loss = 1.5200,	 Acc = 0.4287
380 0.337
1718 0.439
815 0.466
85 0.271
2 0.0
0 0.0
0 0.0
0 0.0
0.44198473282442746
0.48396946564885496
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5862,	 Acc1 = 0.3959,	 Acc2 = 0.4210

 ===== Epoch 62	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 6 1
train:	 Loss = 1.5489,	 Acc = 0.4203
2923 0.249
5681 0.448
2912 0.524
377 0.538
69 0.391
6 0.333
0 0.0
0 0.0
0.47573244886677724
0.48396946564885496
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.5196,	 Acc = 0.4410
380 0.337
1718 0.48
815 0.426
85 0.282
2 0.0
0 0.0
0 0.0
0 0.0
0.45610687022900764
0.48396946564885496
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5770,	 Acc1 = 0.3953,	 Acc2 = 0.4202

 ===== Epoch 63	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.4038596   1.8669845
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.69764888e-01 -2.86153018e-01
  5.05659230e-11 -3.35663330e-10] 2 5
train:	 Loss = 1.5474,	 Acc = 0.4199
2926 0.252
5684 0.45
2908 0.519
375 0.504
69 0.435
6 0.5
0 0.0
0 0.0
0.47423136474231364
0.48396946564885496
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 2
val:	 Loss = 1.5209,	 Acc = 0.4237
380 0.337
1718 0.454
815 0.418
85 0.259
2 0.0
0 0.0
0 0.0
0 0.0
0.4362595419847328
0.48396946564885496
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5940,	 Acc1 = 0.3804,	 Acc2 = 0.4023

 ===== Epoch 64	 =====
[-0.36602148 -0.3783333   1.732593    2.9460402  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -1.22652635e-01 -7.77637625e+00
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 1
train:	 Loss = 1.5443,	 Acc = 0.4198
2925 0.25
5677 0.448
2915 0.527
376 0.473
69 0.478
6 0.333
0 0.0
0 0.0
0.47484241955103396
0.48396946564885496
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 2
val:	 Loss = 1.5432,	 Acc = 0.4367
380 0.339
1718 0.479
815 0.412
85 0.259
2 0.0
0 0.0
0 0.0
0 0.0
0.45076335877862594
0.48396946564885496
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.6101,	 Acc1 = 0.3819,	 Acc2 = 0.4041

 ===== Epoch 65	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  1.6271892   3.7879152  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04  1.01224110e-01 -1.76810801e-01
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 5 5
train:	 Loss = 1.5464,	 Acc = 0.4174
2928 0.245
5678 0.452
2912 0.513
376 0.511
68 0.412
6 0.333
0 0.0
0 0.0
0.47345132743362833
0.48396946564885496
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.5243,	 Acc = 0.4320
380 0.332
1718 0.466
815 0.427
85 0.247
2 0.0
0 0.0
0 0.0
0 0.0
0.44656488549618323
0.48396946564885496
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5965,	 Acc1 = 0.3813,	 Acc2 = 0.4033

 ===== Epoch 66	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.3188752   1.6722788
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10  2.08682179e-01  3.01213712e-02
  5.05659230e-11 -3.35663330e-10] 2 4
train:	 Loss = 1.5430,	 Acc = 0.4200
2923 0.25
5677 0.448
2916 0.525
377 0.523
69 0.377
6 0.333
0 0.0
0 0.0
0.47506909894969596
0.48396946564885496
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 3
val:	 Loss = 1.5129,	 Acc = 0.4403
380 0.342
1718 0.461
815 0.456
85 0.318
2 0.0
0 0.0
0 0.0
0 0.0
0.4545801526717557
0.48396946564885496
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5855,	 Acc1 = 0.3889,	 Acc2 = 0.4125

 ===== Epoch 67	 =====
[ 2.286618    2.6947806  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.9569452   2.955684  ] [ 8.24797004e-02  1.23836733e-02 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 0 0
train:	 Loss = 1.5426,	 Acc = 0.4254
2926 0.252
5677 0.449
2914 0.546
376 0.5
69 0.391
6 0.333
0 0.0
0 0.0
0.48153063481530634
0.48396946564885496
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.5434,	 Acc = 0.4380
380 0.339
1718 0.482
815 0.404
85 0.329
2 0.0
0 0.0
0 0.0
0 0.0
0.45229007633587787
0.48396946564885496
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5828,	 Acc1 = 0.3980,	 Acc2 = 0.4235

 ===== Epoch 68	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.5749682   1.3937756
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -6.64422661e-02  4.86962169e-01
  5.05659230e-11 -3.35663330e-10] 0 6
train:	 Loss = 1.5440,	 Acc = 0.4197
2927 0.241
5682 0.449
2909 0.529
375 0.507
69 0.493
6 0.167
0 0.0
0 0.0
0.47749142793938726
0.48396946564885496
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.5023,	 Acc = 0.4633
380 0.337
1718 0.498
815 0.47
85 0.259
2 0.5
0 0.0
0 0.0
0 0.0
0.4816793893129771
0.48396946564885496
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5693,	 Acc1 = 0.4033,	 Acc2 = 0.4299

 ===== Epoch 69	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 2 1
train:	 Loss = 1.5402,	 Acc = 0.4222
2926 0.249
5677 0.451
2913 0.53
377 0.507
69 0.391
6 0.333
0 0.0
0 0.0
0.4781021897810219
0.48396946564885496
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 2
val:	 Loss = 1.5608,	 Acc = 0.4170
380 0.339
1718 0.45
815 0.398
85 0.282
2 0.5
0 0.0
0 0.0
0 0.0
0.42824427480916033
0.48396946564885496
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.6125,	 Acc1 = 0.3901,	 Acc2 = 0.4140

 ===== Epoch 70	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   0.6640267   0.7789196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.2713953e-11 -3.1991790e-10 -2.5436093e-04 -1.9714674e-04
 -2.8857148e+00 -1.2951384e+00 -1.5722629e-10 -9.7300190e-10
 -6.1223204e-10 -1.8893141e-10 -4.9447713e-10 -1.9952585e-10
  5.0565923e-11 -3.3566333e-10] 5 5
train:	 Loss = 1.5415,	 Acc = 0.4185
2924 0.246
5678 0.447
2916 0.525
375 0.52
69 0.377
6 0.167
0 0.0
0 0.0
0.4744582043343653
0.48396946564885496
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.5126,	 Acc = 0.4550
380 0.339
1718 0.495
815 0.438
85 0.318
2 0.5
0 0.0
0 0.0
0 0.0
0.4717557251908397
0.48396946564885496
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5724,	 Acc1 = 0.3959,	 Acc2 = 0.4210

 ===== Epoch 71	 =====
[ 2.3045874   2.1252165  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.1879728   2.357333  ] [-1.92750916e-02 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  1.00639230e-03 -3.35663330e-10] 0 0
train:	 Loss = 1.5378,	 Acc = 0.4214
2927 0.25
5677 0.45
2913 0.527
377 0.517
68 0.397
6 0.333
0 0.0
0 0.0
0.4769383917708218
0.48396946564885496
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.5270,	 Acc = 0.4357
380 0.337
1718 0.475
815 0.412
85 0.318
2 0.0
0 0.0
0 0.0
0 0.0
0.45
0.48396946564885496
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5845,	 Acc1 = 0.3903,	 Acc2 = 0.4143

 ===== Epoch 72	 =====
[-0.36602148 -0.3783333   1.7784884   3.01853    -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -8.34712014e-03 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 0 0
train:	 Loss = 1.5377,	 Acc = 0.4234
2926 0.244
5676 0.453
2914 0.535
377 0.517
69 0.435
6 0.167
0 0.0
0 0.0
0.48153063481530634
0.48396946564885496
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 3
val:	 Loss = 1.5237,	 Acc = 0.4430
380 0.316
1718 0.455
815 0.488
85 0.329
2 0.5
0 0.0
0 0.0
0 0.0
0.4614503816793893
0.48396946564885496
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5735,	 Acc1 = 0.4005,	 Acc2 = 0.4264

 ===== Epoch 73	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   3.0924745   1.7746643
 -0.3640846  -0.3725409   2.0451443   1.976201   -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.2713953e-11 -3.1991790e-10 -2.5436093e-04 -1.9714674e-04
 -8.0157256e-01 -8.3045408e-02 -1.5722629e-10 -9.7300190e-10
  2.9310808e-01 -2.0916121e-01 -4.9447713e-10 -1.9952585e-10
  5.0565923e-11 -3.3566333e-10] 3 5
train:	 Loss = 1.5382,	 Acc = 0.4238
2924 0.249
5684 0.449
2911 0.537
375 0.528
68 0.412
6 0.5
0 0.0
0 0.0
0.4803184431667404
0.48396946564885496
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.5312,	 Acc = 0.4297
380 0.339
1718 0.467
815 0.407
85 0.294
2 0.5
0 0.0
0 0.0
0 0.0
0.44274809160305345
0.48396946564885496
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5893,	 Acc1 = 0.3875,	 Acc2 = 0.4108

 ===== Epoch 74	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 1 1
train:	 Loss = 1.5358,	 Acc = 0.4208
2926 0.242
5676 0.451
2914 0.528
377 0.52
69 0.435
6 0.333
0 0.0
0 0.0
0.4787657597876576
0.48396946564885496
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.5027,	 Acc = 0.4553
380 0.342
1718 0.482
815 0.465
85 0.341
2 0.0
0 0.0
0 0.0
0 0.0
0.4717557251908397
0.48396946564885496
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5723,	 Acc1 = 0.3934,	 Acc2 = 0.4180

 ===== Epoch 75	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 0 1
train:	 Loss = 1.5374,	 Acc = 0.4239
2922 0.247
5682 0.453
2914 0.532
376 0.503
68 0.515
6 0.333
0 0.0
0 0.0
0.48109661728940967
0.48396946564885496
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.5663,	 Acc = 0.4333
380 0.334
1718 0.471
815 0.412
85 0.306
2 0.5
0 0.0
0 0.0
0 0.0
0.44770992366412216
0.48396946564885496
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.6197,	 Acc1 = 0.3854,	 Acc2 = 0.4083

 ===== Epoch 76	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 1 1
train:	 Loss = 1.5387,	 Acc = 0.4262
2926 0.246
5679 0.455
2913 0.539
375 0.533
69 0.362
6 0.333
0 0.0
0 0.0
0.48440610484406105
0.48396946564885496
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.5381,	 Acc = 0.4307
380 0.337
1718 0.463
815 0.413
85 0.353
2 0.5
0 0.0
0 0.0
0 0.0
0.4442748091603053
0.48396946564885496
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5878,	 Acc1 = 0.4019,	 Acc2 = 0.4282

 ===== Epoch 77	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   1.6126003   2.4440396  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -1.79937705e-01 -1.37856245e-01 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 2 5
train:	 Loss = 1.5300,	 Acc = 0.4272
2923 0.249
5678 0.455
2915 0.537
377 0.531
69 0.507
6 0.167
0 0.0
0 0.0
0.4847982310668878
0.48396946564885496
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 3
val:	 Loss = 1.5407,	 Acc = 0.4377
380 0.332
1718 0.446
815 0.483
85 0.306
2 0.5
0 0.0
0 0.0
0 0.0
0.4530534351145038
0.48396946564885496
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.6247,	 Acc1 = 0.3839,	 Acc2 = 0.4066

 ===== Epoch 78	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 3 1
train:	 Loss = 1.5379,	 Acc = 0.4223
2922 0.242
5679 0.451
2917 0.532
376 0.54
68 0.456
6 0.333
0 0.0
0 0.0
0.48065443289851867
0.48396946564885496
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.5542,	 Acc = 0.4063
380 0.339
1718 0.433
815 0.388
85 0.341
2 0.5
0 0.0
0 0.0
0 0.0
0.41603053435114506
0.48396946564885496
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 4
Testing:	 Loss = 1.5858,	 Acc1 = 0.3994,	 Acc2 = 0.4252

 ===== Epoch 79	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   1.0559006   3.3548317  -0.42342317 -0.42019257
  2.3802016   2.2287402 ] [ 3.55027771e+00  2.79871011e+00 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -2.88251925e+00 -7.18753958e+00 -4.94477126e-10 -1.99525854e-10
 -7.03232083e-03  2.11138859e-01] 2 4
train:	 Loss = 1.5350,	 Acc = 0.4257
2921 0.244
5682 0.454
2914 0.536
376 0.551
69 0.435
6 0.333
0 0.0
0 0.0
0.4844699900519509
0.48396946564885496
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 2
val:	 Loss = 1.5422,	 Acc = 0.4120
380 0.337
1718 0.447
815 0.39
85 0.259
2 0.0
0 0.0
0 0.0
0 0.0
0.42290076335877863
0.48396946564885496
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5850,	 Acc1 = 0.3941,	 Acc2 = 0.4187

 ===== Epoch 80	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   2.4239066   1.54569    -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
  2.08232924e-01  4.46844369e-01 -4.94477126e-10 -1.99525854e-10
  8.26655579e+00  2.67618537e+00] 6 6
train:	 Loss = 1.5355,	 Acc = 0.4196
2922 0.241
5680 0.442
2915 0.539
376 0.524
69 0.522
6 0.333
0 0.0
0 0.0
0.47722750386911345
0.48396946564885496
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 3
val:	 Loss = 1.5217,	 Acc = 0.4397
380 0.334
1718 0.461
815 0.459
85 0.294
2 0.5
0 0.0
0 0.0
0 0.0
0.4549618320610687
0.48396946564885496
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5909,	 Acc1 = 0.3910,	 Acc2 = 0.4150

 ===== Epoch 81	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  3.6740189   1.5500307  -0.40141198 -0.40778467  3.7354655   2.1233063
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -4.28647518e-01 -2.24413708e-01
 -6.12232043e-10 -1.88931412e-10  1.74992412e-01 -1.40566409e-01
  5.05659230e-11 -3.35663330e-10] 3 5
train:	 Loss = 1.5325,	 Acc = 0.4268
2924 0.244
5679 0.454
2914 0.543
377 0.531
68 0.456
6 0.167
0 0.0
0 0.0
0.48584697036709423
0.48396946564885496
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.5256,	 Acc = 0.4407
380 0.332
1718 0.473
815 0.438
85 0.306
2 0.0
0 0.0
0 0.0
0 0.0
0.45648854961832064
0.48396946564885496
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5795,	 Acc1 = 0.3889,	 Acc2 = 0.4125

 ===== Epoch 82	 =====
[-0.36602148 -0.3783333   1.647301    2.8078558  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10  2.30107568e-02  7.34521914e-03
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 0 0
train:	 Loss = 1.5295,	 Acc = 0.4230
2925 0.252
5679 0.448
2914 0.528
375 0.552
69 0.493
6 0.333
0 0.0
0 0.0
0.47838106822956983
0.48396946564885496
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 3
val:	 Loss = 1.4894,	 Acc = 0.4497
380 0.337
1718 0.469
815 0.474
85 0.329
2 0.5
0 0.0
0 0.0
0 0.0
0.46603053435114505
0.48396946564885496
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5593,	 Acc1 = 0.4017,	 Acc2 = 0.4279

 ===== Epoch 83	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   3.0657377   2.0524948
 -0.3640846  -0.3725409   3.4589226   1.1848571  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.2713953e-11 -3.1991790e-10 -2.5436093e-04 -1.9714674e-04
  7.9334877e-02  1.1351023e-01 -1.5722629e-10 -9.7300190e-10
 -1.3580516e-01  2.1866854e-01 -4.9447713e-10 -1.9952585e-10
  5.0565923e-11 -3.3566333e-10] 2 2
train:	 Loss = 1.5272,	 Acc = 0.4236
2924 0.244
5680 0.452
2912 0.534
377 0.533
69 0.449
6 0.167
0 0.0
0 0.0
0.48186643078283947
0.48396946564885496
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.5370,	 Acc = 0.4273
380 0.339
1718 0.464
815 0.404
85 0.306
2 0.5
0 0.0
0 0.0
0 0.0
0.4400763358778626
0.48396946564885496
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5927,	 Acc1 = 0.3945,	 Acc2 = 0.4192

 ===== Epoch 84	 =====
[-0.36602148 -0.3783333   1.2740458   2.7829375  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.38981381e-01  2.71328032e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 6 6
train:	 Loss = 1.5316,	 Acc = 0.4207
2927 0.239
5677 0.45
2914 0.532
375 0.531
69 0.464
6 0.167
0 0.0
0 0.0
0.47948235814622275
0.48396946564885496
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.5176,	 Acc = 0.4113
380 0.189
1718 0.453
815 0.436
85 0.329
2 0.5
0 0.0
0 0.0
0 0.0
0.4435114503816794
0.48396946564885496
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5667,	 Acc1 = 0.3747,	 Acc2 = 0.4277

 ===== Epoch 85	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  1.1300849   1.54445    -0.40141198 -0.40778467  2.6660142   1.778258
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04  8.24797601e-02  3.33220333e-01
 -6.12232043e-10 -1.88931412e-10 -7.28978813e-01  2.05829382e-01
  5.05659230e-11 -3.35663330e-10] 2 6
train:	 Loss = 1.5285,	 Acc = 0.4236
2922 0.245
5681 0.449
2913 0.536
377 0.549
69 0.478
6 0.167
0 0.0
0 0.0
0.48142825558257796
0.48396946564885496
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 2
val:	 Loss = 1.5153,	 Acc = 0.4113
380 0.339
1718 0.442
815 0.393
85 0.282
2 0.5
0 0.0
0 0.0
0 0.0
0.4217557251908397
0.48396946564885496
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5712,	 Acc1 = 0.3833,	 Acc2 = 0.4058

 ===== Epoch 86	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 1 1
train:	 Loss = 1.5288,	 Acc = 0.4220
2926 0.243
5681 0.455
2910 0.525
376 0.511
69 0.449
6 0.333
0 0.0
0 0.0
0.47987170979871707
0.48396946564885496
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.5378,	 Acc = 0.4297
380 0.334
1718 0.459
815 0.431
85 0.247
2 0.5
0 0.0
0 0.0
0 0.0
0.4435114503816794
0.48396946564885496
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5944,	 Acc1 = 0.3930,	 Acc2 = 0.4175

 ===== Epoch 87	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.3279675   1.6747434
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10  2.18974829e-01 -1.40566409e-01
  5.05659230e-11 -3.35663330e-10] 3 2
train:	 Loss = 1.5288,	 Acc = 0.4210
2921 0.239
5684 0.448
2911 0.534
377 0.533
69 0.522
6 0.333
0 0.0
0 0.0
0.4797170332706975
0.48396946564885496
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 3
val:	 Loss = 1.5201,	 Acc = 0.4080
380 0.337
1718 0.431
815 0.405
85 0.294
2 0.5
0 0.0
0 0.0
0 0.0
0.4183206106870229
0.48396946564885496
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5847,	 Acc1 = 0.3875,	 Acc2 = 0.4108

 ===== Epoch 88	 =====
[ 0.7072193   3.4339478  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-2.32102107e-02  3.71510200e-02 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 3 1
train:	 Loss = 1.5309,	 Acc = 0.4197
2925 0.237
5682 0.452
2910 0.527
376 0.521
69 0.464
6 0.333
0 0.0
0 0.0
0.47893398208559107
0.48396946564885496
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.5233,	 Acc = 0.4450
380 0.337
1718 0.485
815 0.418
85 0.365
2 0.5
0 0.0
0 0.0
0 0.0
0.46068702290076335
0.48396946564885496
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5677,	 Acc1 = 0.3984,	 Acc2 = 0.4240

 ===== Epoch 89	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.5427674   1.9359941
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10  3.12551737e-01 -4.76921707e-01
  5.05659230e-11 -3.35663330e-10] 5 5
train:	 Loss = 1.5294,	 Acc = 0.4230
2923 0.252
5680 0.449
2914 0.53
376 0.524
69 0.478
6 0.333
0 0.0
0 0.0
0.47805417357656166
0.48396946564885496
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.5029,	 Acc = 0.4557
380 0.339
1718 0.497
815 0.439
85 0.306
2 0.5
0 0.0
0 0.0
0 0.0
0.4725190839694656
0.48396946564885496
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5687,	 Acc1 = 0.3926,	 Acc2 = 0.4170

 ===== Epoch 90	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   1.5982968   1.9314079  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -1.45985559e-01 -2.04407543e-01 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 2 5
train:	 Loss = 1.5286,	 Acc = 0.4240
2926 0.249
5675 0.452
2915 0.528
377 0.552
69 0.507
6 0.167
0 0.0
0 0.0
0.48064587480645876
0.48396946564885496
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.5188,	 Acc = 0.4517
380 0.337
1718 0.495
815 0.426
85 0.341
2 0.5
0 0.0
0 0.0
0 0.0
0.4683206106870229
0.48396946564885496
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5790,	 Acc1 = 0.3930,	 Acc2 = 0.4175

 ===== Epoch 91	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   0.69458294  2.0147097
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.2713953e-11 -3.1991790e-10 -2.5436093e-04 -1.9714674e-04
 -6.8311602e-02  2.1934786e-01 -1.5722629e-10 -9.7300190e-10
 -6.1223204e-10 -1.8893141e-10 -4.9447713e-10 -1.9952585e-10
  5.0565923e-11 -3.3566333e-10] 4 6
train:	 Loss = 1.5260,	 Acc = 0.4247
2926 0.247
5680 0.454
2910 0.532
377 0.517
69 0.522
6 0.333
0 0.0
0 0.0
0.48219420482194203
0.48396946564885496
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.5091,	 Acc = 0.4420
380 0.339
1718 0.476
815 0.431
85 0.318
2 0.5
0 0.0
0 0.0
0 0.0
0.4568702290076336
0.48396946564885496
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5664,	 Acc1 = 0.3988,	 Acc2 = 0.4245

 ===== Epoch 92	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   1.5520637   2.494801
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.2713953e-11 -3.1991790e-10 -2.5436093e-04 -1.9714674e-04
  3.4869757e-01  4.0431853e-02 -1.5722629e-10 -9.7300190e-10
 -6.1223204e-10 -1.8893141e-10 -4.9447713e-10 -1.9952585e-10
  5.0565923e-11 -3.3566333e-10] 1 4
train:	 Loss = 1.5246,	 Acc = 0.4226
2924 0.247
5676 0.449
2916 0.53
377 0.546
69 0.478
6 0.167
0 0.0
0 0.0
0.47954444935869084
0.48396946564885496
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 3
val:	 Loss = 1.5087,	 Acc = 0.4583
380 0.339
1718 0.485
815 0.469
85 0.341
2 0.5
0 0.0
0 0.0
0 0.0
0.47557251908396947
0.48396946564885496
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5606,	 Acc1 = 0.3998,	 Acc2 = 0.4257

 ===== Epoch 93	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.2034609   0.98217595] [ 1.58004391e+00  1.66560400e+00 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  1.80871040e-02  4.43391621e-01] 6 6
train:	 Loss = 1.5281,	 Acc = 0.4238
2923 0.243
5682 0.453
2912 0.53
377 0.549
68 0.515
6 0.333
0 0.0
0 0.0
0.48214483139856273
0.48396946564885496
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.5105,	 Acc = 0.4520
380 0.339
1718 0.499
815 0.416
85 0.341
2 0.5
0 0.0
0 0.0
0 0.0
0.4683206106870229
0.48396946564885496
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5751,	 Acc1 = 0.3951,	 Acc2 = 0.4200

 ===== Epoch 94	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   1.2361269   3.2254295  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.33738711e-02 -5.22903018e-02 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 2 0
train:	 Loss = 1.5315,	 Acc = 0.4221
2925 0.244
5683 0.452
2912 0.528
373 0.539
69 0.435
6 0.333
0 0.0
0 0.0
0.47959747871281655
0.48396946564885496
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.5181,	 Acc = 0.4250
380 0.339
1718 0.443
815 0.439
85 0.306
2 0.5
0 0.0
0 0.0
0 0.0
0.43740458015267175
0.48396946564885496
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5743,	 Acc1 = 0.3943,	 Acc2 = 0.4190

 ===== Epoch 95	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   0.53454614  1.194554
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.2713953e-11 -3.1991790e-10 -2.5436093e-04 -1.9714674e-04
 -9.9241018e-02 -1.1328474e-01 -1.5722629e-10 -9.7300190e-10
 -6.1223204e-10 -1.8893141e-10 -4.9447713e-10 -1.9952585e-10
  5.0565923e-11 -3.3566333e-10] 5 5
train:	 Loss = 1.5249,	 Acc = 0.4258
2925 0.25
5677 0.453
2914 0.538
377 0.515
69 0.435
6 0.333
0 0.0
0 0.0
0.4826937963065354
0.48396946564885496
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 3
val:	 Loss = 1.4912,	 Acc = 0.4707
380 0.334
1718 0.492
815 0.498
85 0.376
2 0.5
0 0.0
0 0.0
0 0.0
0.4904580152671756
0.4904580152671756
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5704,	 Acc1 = 0.3908,	 Acc2 = 0.4148

 ===== Epoch 96	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   1.5817047   3.2328951  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
  1.64096862e-01  4.75366414e-03 -4.94477126e-10 -1.99525854e-10
  8.07564163e+00  5.83798933e+00] 2 0
train:	 Loss = 1.5243,	 Acc = 0.4232
2924 0.247
5676 0.455
2916 0.523
377 0.552
69 0.391
6 0.333
0 0.0
0 0.0
0.48009730207872625
0.4904580152671756
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 3
val:	 Loss = 1.5161,	 Acc = 0.4423
380 0.339
1718 0.485
815 0.42
85 0.259
2 0.5
0 0.0
0 0.0
0 0.0
0.45725190839694657
0.4904580152671756
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5631,	 Acc1 = 0.4044,	 Acc2 = 0.4312

 ===== Epoch 97	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  1.0368425   2.5182924  -0.40141198 -0.40778467  2.154586    1.9926807
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -3.81156951e-01 -1.76810801e-01
 -6.12232043e-10 -1.88931412e-10 -3.02259088e-01 -2.00809136e-01
  5.05659230e-11 -3.35663330e-10] 1 5
train:	 Loss = 1.5301,	 Acc = 0.4247
2920 0.249
5684 0.455
2912 0.522
377 0.56
69 0.493
6 0.167
0 0.0
0 0.0
0.48143236074270557
0.4904580152671756
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 3
val:	 Loss = 1.5153,	 Acc = 0.4527
380 0.337
1718 0.477
815 0.472
85 0.294
2 0.5
0 0.0
0 0.0
0 0.0
0.46946564885496184
0.4904580152671756
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5932,	 Acc1 = 0.3866,	 Acc2 = 0.4098

 ===== Epoch 98	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.5785024   1.1522417
  3.0340457   3.4411876 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -9.82574895e-02  6.02427423e-02
 -8.03840235e-02  7.91770741e-02] 4 3
train:	 Loss = 1.5268,	 Acc = 0.4231
2922 0.249
5680 0.448
2914 0.534
377 0.525
69 0.522
6 0.333
0 0.0
0 0.0
0.47943842582356844
0.4904580152671756
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4933,	 Acc = 0.4633
380 0.337
1718 0.499
815 0.459
85 0.353
2 0.5
0 0.0
0 0.0
0 0.0
0.4816793893129771
0.4904580152671756
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5600,	 Acc1 = 0.3967,	 Acc2 = 0.4220

 ===== Epoch 99	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  2.82256675e+00  2.67618537e+00] 1 6
train:	 Loss = 1.5214,	 Acc = 0.4247
2927 0.249
5675 0.451
2915 0.53
376 0.566
69 0.478
6 0.167
0 0.0
0 0.0
0.4815838955867714
0.4904580152671756
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.5250,	 Acc = 0.4373
380 0.337
1718 0.477
815 0.415
85 0.306
2 0.5
0 0.0
0 0.0
0 0.0
0.45190839694656487
0.4904580152671756
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5721,	 Acc1 = 0.3996,	 Acc2 = 0.4254

 ===== Epoch 100	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  1.3014041   1.4412057  -0.40141198 -0.40778467  2.01669     3.3087935
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04  1.39964476e-01 -4.35226589e-01
 -6.12232043e-10 -1.88931412e-10 -5.16553342e-01 -1.75707996e-01
  5.05659230e-11 -3.35663330e-10] 5 5
train:	 Loss = 1.5243,	 Acc = 0.4256
2927 0.249
5679 0.457
2913 0.526
375 0.544
68 0.471
6 0.333
0 0.0
0 0.0
0.48268996792390223
0.4904580152671756
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 3
val:	 Loss = 1.5045,	 Acc = 0.4367
380 0.339
1718 0.473
815 0.42
85 0.306
2 0.5
0 0.0
0 0.0
0 0.0
0.45076335877862594
0.4904580152671756
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5706,	 Acc1 = 0.3920,	 Acc2 = 0.4163

 ===== Epoch 101	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   1.2369542   2.383669
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.2713953e-11 -3.1991790e-10 -2.5436093e-04 -1.9714674e-04
 -4.8361208e-02 -1.3092434e-01 -1.5722629e-10 -9.7300190e-10
 -6.1223204e-10 -1.8893141e-10 -4.9447713e-10 -1.9952585e-10
  5.0565923e-11 -3.3566333e-10] 5 5
train:	 Loss = 1.5256,	 Acc = 0.4230
2924 0.247
5679 0.453
2915 0.527
375 0.52
69 0.464
6 0.333
0 0.0
0 0.0
0.47998673153471916
0.4904580152671756
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.5110,	 Acc = 0.4540
380 0.339
1718 0.492
815 0.436
85 0.365
2 0.5
0 0.0
0 0.0
0 0.0
0.47061068702290076
0.4904580152671756
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5643,	 Acc1 = 0.4052,	 Acc2 = 0.4322

 ===== Epoch 102	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   2.9228194   2.8073611  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
  2.71617305e-02  3.32756490e-02 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 4
train:	 Loss = 1.5269,	 Acc = 0.4225
2922 0.248
5682 0.45
2912 0.529
377 0.528
69 0.464
6 0.333
0 0.0
0 0.0
0.4788856953349547
0.4904580152671756
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 3
val:	 Loss = 1.5302,	 Acc = 0.4367
380 0.337
1718 0.477
815 0.416
85 0.271
2 0.5
0 0.0
0 0.0
0 0.0
0.45114503816793894
0.4904580152671756
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5796,	 Acc1 = 0.3924,	 Acc2 = 0.4167

 ===== Epoch 103	 =====
[ 1.9423608   2.5403655  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  1.3109525   2.6243267  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-2.56179309e+00 -3.56959367e+00 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.12496987e-02 -1.42808720e-01
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 5
train:	 Loss = 1.5243,	 Acc = 0.4245
2926 0.245
5680 0.454
2911 0.531
376 0.551
69 0.449
6 0.167
0 0.0
0 0.0
0.48241539482415396
0.4904580152671756
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 3
val:	 Loss = 1.4973,	 Acc = 0.4437
380 0.334
1718 0.481
815 0.429
85 0.306
2 0.5
0 0.0
0 0.0
0 0.0
0.4595419847328244
0.4904580152671756
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5692,	 Acc1 = 0.3926,	 Acc2 = 0.4170

 ===== Epoch 104	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   1.0242072   0.7989234
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.2713953e-11 -3.1991790e-10 -2.5436093e-04 -1.9714674e-04
  1.4418215e-01 -1.5006916e-02 -1.5722629e-10 -9.7300190e-10
 -6.1223204e-10 -1.8893141e-10 -4.9447713e-10 -1.9952585e-10
  5.0565923e-11 -3.3566333e-10] 2 0
train:	 Loss = 1.5236,	 Acc = 0.4263
2926 0.243
5675 0.456
2916 0.538
376 0.553
69 0.435
6 0.167
0 0.0
0 0.0
0.48573324485733244
0.4904580152671756
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 3
val:	 Loss = 1.5049,	 Acc = 0.4667
380 0.334
1718 0.494
815 0.482
85 0.365
2 0.5
0 0.0
0 0.0
0 0.0
0.4858778625954199
0.4904580152671756
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5716,	 Acc1 = 0.4013,	 Acc2 = 0.4274

 ===== Epoch 105	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  1.3643135   1.2626215  -0.40141198 -0.40778467  1.8075736   3.2225313
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -9.12299529e-02 -4.62428272e-01
 -6.12232043e-10 -1.88931412e-10 -2.29270369e-01 -4.91982371e-01
  5.05659230e-11 -3.35663330e-10] 5 5
train:	 Loss = 1.5275,	 Acc = 0.4226
2925 0.248
5680 0.454
2911 0.527
377 0.501
69 0.464
6 0.333
0 0.0
0 0.0
0.4790445648567953
0.4904580152671756
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 3
val:	 Loss = 1.5118,	 Acc = 0.4513
380 0.337
1718 0.464
815 0.492
85 0.318
2 0.5
0 0.0
0 0.0
0 0.0
0.4679389312977099
0.4904580152671756
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5657,	 Acc1 = 0.4060,	 Acc2 = 0.4332

 ===== Epoch 106	 =====
[ 2.4427173   2.583399   -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.0596738   2.8402126 ] [ 1.75727010e-01  7.43020400e-02 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  1.47703588e-01  1.84746519e-01] 3 4
train:	 Loss = 1.5250,	 Acc = 0.4211
2927 0.242
5677 0.449
2913 0.531
376 0.529
69 0.478
6 0.333
0 0.0
0 0.0
0.4792611436787966
0.4904580152671756
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 3
val:	 Loss = 1.5076,	 Acc = 0.4460
380 0.339
1718 0.474
815 0.454
85 0.282
2 0.5
0 0.0
0 0.0
0 0.0
0.4614503816793893
0.4904580152671756
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5574,	 Acc1 = 0.4038,	 Acc2 = 0.4304

 ===== Epoch 107	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 3 1
train:	 Loss = 1.5236,	 Acc = 0.4283
2923 0.251
5682 0.46
2912 0.529
376 0.548
69 0.449
6 0.167
0 0.0
0 0.0
0.4855721393034826
0.4904580152671756
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.5464,	 Acc = 0.4443
380 0.337
1718 0.494
815 0.409
85 0.271
2 0.5
0 0.0
0 0.0
0 0.0
0.4599236641221374
0.4904580152671756
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5974,	 Acc1 = 0.3947,	 Acc2 = 0.4195

 ===== Epoch 108	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  3.2078085   1.2933158  -0.40141198 -0.40778467  3.3607972   1.9458528
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04  5.13626814e-01  1.97212040e-01
 -6.12232043e-10 -1.88931412e-10  2.24598479e-02  9.03641135e-02
  5.05659230e-11 -3.35663330e-10] 2 2
train:	 Loss = 1.5268,	 Acc = 0.4257
2923 0.252
5685 0.45
2909 0.536
377 0.531
68 0.5
6 0.5
0 0.0
0 0.0
0.4817025981205086
0.4904580152671756
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 3
val:	 Loss = 1.5141,	 Acc = 0.4537
380 0.342
1718 0.485
815 0.454
85 0.318
2 0.0
0 0.0
0 0.0
0 0.0
0.46984732824427483
0.4904580152671756
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5602,	 Acc1 = 0.4118,	 Acc2 = 0.4401

 ===== Epoch 109	 =====
[-0.36602148 -0.3783333   2.9632385   1.8972     -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -6.65855646e-01  2.10989088e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 2 2
train:	 Loss = 1.5275,	 Acc = 0.4224
2927 0.253
5678 0.447
2912 0.53
376 0.532
69 0.449
6 0.333
0 0.0
0 0.0
0.477159606238248
0.4904580152671756
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 3
val:	 Loss = 1.5262,	 Acc = 0.4467
380 0.332
1718 0.484
815 0.437
85 0.294
2 0.5
0 0.0
0 0.0
0 0.0
0.4633587786259542
0.4904580152671756
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.6023,	 Acc1 = 0.3870,	 Acc2 = 0.4103

 ===== Epoch 110	 =====
[ 0.5414023   2.0290234  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  0.5672535   2.3468356 ] [ 1.43820226e-01  2.84824491e-01 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
 -1.56745598e-01  6.49252057e-01] 6 6
train:	 Loss = 1.5255,	 Acc = 0.4277
2924 0.25
5678 0.454
2914 0.538
377 0.544
69 0.507
6 0.333
0 0.0
0 0.0
0.4852941176470588
0.4904580152671756
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.5054,	 Acc = 0.4647
380 0.337
1718 0.504
815 0.453
85 0.353
2 0.5
0 0.0
0 0.0
0 0.0
0.483206106870229
0.4904580152671756
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5834,	 Acc1 = 0.3939,	 Acc2 = 0.4185

 ===== Epoch 111	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.228592    1.4151931 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
 -3.47658813e-01 -1.90024972e-01] 2 5
train:	 Loss = 1.5241,	 Acc = 0.4266
2925 0.246
5682 0.455
2911 0.536
375 0.549
69 0.435
6 0.333
0 0.0
0 0.0
0.4850160345018246
0.4904580152671756
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 3
val:	 Loss = 1.5417,	 Acc = 0.3823
380 0.195
1718 0.419
815 0.404
85 0.282
2 0.5
0 0.0
0 0.0
0 0.0
0.40954198473282444
0.4904580152671756
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.6027,	 Acc1 = 0.3507,	 Acc2 = 0.3989

 ===== Epoch 112	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 0 0
train:	 Loss = 1.5295,	 Acc = 0.4214
2921 0.249
5682 0.452
2915 0.522
375 0.528
69 0.435
6 0.333
0 0.0
0 0.0
0.47717475406212007
0.4904580152671756
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 3
val:	 Loss = 1.5272,	 Acc = 0.4277
380 0.339
1718 0.464
815 0.404
85 0.318
2 0.5
0 0.0
0 0.0
0 0.0
0.4404580152671756
0.4904580152671756
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5719,	 Acc1 = 0.3988,	 Acc2 = 0.4245

 ===== Epoch 113	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   2.3778431   1.1589917
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.2713953e-11 -3.1991790e-10 -2.5436093e-04 -1.9714674e-04
 -1.5311234e-01  1.1351023e-01 -1.5722629e-10 -9.7300190e-10
 -6.1223204e-10 -1.8893141e-10 -4.9447713e-10 -1.9952585e-10
  5.0565923e-11 -3.3566333e-10] 2 2
train:	 Loss = 1.5254,	 Acc = 0.4237
2925 0.25
5676 0.452
2917 0.529
375 0.509
69 0.565
6 0.5
0 0.0
0 0.0
0.48003980979763355
0.4904580152671756
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 3
val:	 Loss = 1.4944,	 Acc = 0.4607
380 0.339
1718 0.492
815 0.463
85 0.353
2 0.5
0 0.0
0 0.0
0 0.0
0.4782442748091603
0.4904580152671756
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5562,	 Acc1 = 0.4056,	 Acc2 = 0.4327

 ===== Epoch 114	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.4128268   2.791221
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -3.74330766e-02 -1.75707996e-01
  5.05659230e-11 -3.35663330e-10] 2 5
train:	 Loss = 1.5282,	 Acc = 0.4231
2925 0.246
5680 0.447
2911 0.539
377 0.544
69 0.406
6 0.333
0 0.0
0 0.0
0.4804821408824505
0.4904580152671756
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.5425,	 Acc = 0.4447
380 0.339
1718 0.489
815 0.411
85 0.341
2 0.5
0 0.0
0 0.0
0 0.0
0.4599236641221374
0.4904580152671756
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5817,	 Acc1 = 0.4033,	 Acc2 = 0.4299

 ===== Epoch 115	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   1.1743139   0.8767159
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.2713953e-11 -3.1991790e-10 -2.5436093e-04 -1.9714674e-04
 -5.2351903e-02 -7.8005515e-02 -1.5722629e-10 -9.7300190e-10
 -6.1223204e-10 -1.8893141e-10 -4.9447713e-10 -1.9952585e-10
  5.0565923e-11 -3.3566333e-10] 2 2
train:	 Loss = 1.5240,	 Acc = 0.4291
2925 0.252
5675 0.463
2916 0.526
377 0.538
69 0.464
6 0.333
0 0.0
0 0.0
0.48623244498507134
0.4904580152671756
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 3
val:	 Loss = 1.5256,	 Acc = 0.4293
380 0.339
1718 0.459
815 0.425
85 0.271
2 0.5
0 0.0
0 0.0
0 0.0
0.44236641221374046
0.4904580152671756
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5726,	 Acc1 = 0.3974,	 Acc2 = 0.4227

 ===== Epoch 116	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   1.8981123   2.288095
 -0.3640846  -0.3725409   3.412006    0.8688173  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.2713953e-11 -3.1991790e-10 -2.5436093e-04 -1.9714674e-04
  2.2472093e-02 -9.8165073e-02 -1.5722629e-10 -9.7300190e-10
  1.2674949e+00 -5.2290302e-02 -4.9447713e-10 -1.9952585e-10
  5.0565923e-11 -3.3566333e-10] 5 2
train:	 Loss = 1.5243,	 Acc = 0.4243
2928 0.25
5677 0.453
2913 0.529
375 0.539
69 0.435
6 0.0
0 0.0
0 0.0
0.48064159292035397
0.4904580152671756
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 3
val:	 Loss = 1.5084,	 Acc = 0.4567
380 0.342
1718 0.494
815 0.444
85 0.353
2 0.0
0 0.0
0 0.0
0 0.0
0.4732824427480916
0.4904580152671756
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5642,	 Acc1 = 0.4038,	 Acc2 = 0.4304

 ===== Epoch 117	 =====
[-0.36602148 -0.3783333   0.71802175  0.88460493 -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -6.39801472e-02 -1.77960023e-02
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 0
train:	 Loss = 1.5243,	 Acc = 0.4227
2923 0.25
5683 0.446
2911 0.537
376 0.521
69 0.406
6 0.333
0 0.0
0 0.0
0.47838584853510224
0.4904580152671756
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.5150,	 Acc = 0.4410
380 0.339
1718 0.479
815 0.421
85 0.318
2 0.5
0 0.0
0 0.0
0 0.0
0.45572519083969465
0.4904580152671756
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5834,	 Acc1 = 0.3984,	 Acc2 = 0.4240

 ===== Epoch 118	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.772341    1.1966051
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10  4.17361438e-01  1.50606856e-02
  5.05659230e-11 -3.35663330e-10] 1 2
train:	 Loss = 1.5220,	 Acc = 0.4251
2925 0.251
5682 0.455
2911 0.528
376 0.537
68 0.426
6 0.333
0 0.0
0 0.0
0.48136680305208446
0.4904580152671756
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 3
val:	 Loss = 1.5018,	 Acc = 0.4483
380 0.339
1718 0.466
815 0.476
85 0.306
2 0.5
0 0.0
0 0.0
0 0.0
0.46412213740458014
0.4904580152671756
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5632,	 Acc1 = 0.4025,	 Acc2 = 0.4289

 ===== Epoch 119	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  1.2677008   2.0411375  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.74906792e-02  1.83611214e-01
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 6 6
train:	 Loss = 1.5214,	 Acc = 0.4275
2924 0.25
5684 0.461
2911 0.529
374 0.524
69 0.449
6 0.167
0 0.0
0 0.0
0.4849624060150376
0.4904580152671756
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 3
val:	 Loss = 1.4849,	 Acc = 0.4717
380 0.339
1718 0.495
815 0.498
85 0.329
2 0.5
0 0.0
0 0.0
0 0.0
0.4908396946564885
0.4908396946564885
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5497,	 Acc1 = 0.4060,	 Acc2 = 0.4332

 ===== Epoch 120	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  0.83851266  2.0690842
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10  2.33942270e-02  4.26719427e-01
  5.05659230e-11 -3.35663330e-10] 6 6
train:	 Loss = 1.5202,	 Acc = 0.4280
2922 0.251
5681 0.459
2914 0.532
376 0.532
69 0.406
6 0.333
0 0.0
0 0.0
0.4852973690028742
0.4908396946564885
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.5377,	 Acc = 0.4460
380 0.339
1718 0.482
815 0.431
85 0.341
2 0.5
0 0.0
0 0.0
0 0.0
0.4614503816793893
0.4908396946564885
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.6092,	 Acc1 = 0.3897,	 Acc2 = 0.4135

 ===== Epoch 121	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 2 1
train:	 Loss = 1.5197,	 Acc = 0.4279
2920 0.252
5685 0.459
2911 0.535
377 0.509
69 0.391
6 0.167
0 0.0
0 0.0
0.48474801061007955
0.4908396946564885
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.5030,	 Acc = 0.4260
380 0.342
1718 0.455
815 0.415
85 0.318
2 0.5
0 0.0
0 0.0
0 0.0
0.4381679389312977
0.4908396946564885
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5533,	 Acc1 = 0.4089,	 Acc2 = 0.4366

 ===== Epoch 122	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.0151439   2.6460109 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
 -1.33638948e-01  1.63632616e-01] 4 4
train:	 Loss = 1.5194,	 Acc = 0.4291
2929 0.25
5676 0.458
2913 0.542
376 0.503
68 0.456
6 0.5
0 0.0
0 0.0
0.48711140612899656
0.4908396946564885
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.5097,	 Acc = 0.4637
380 0.339
1718 0.497
815 0.463
85 0.376
2 0.0
0 0.0
0 0.0
0 0.0
0.4816793893129771
0.4908396946564885
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5570,	 Acc1 = 0.4161,	 Acc2 = 0.4453

 ===== Epoch 123	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.048387    2.4165974
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10  2.29267478e-01  3.61456454e-01
  5.05659230e-11 -3.35663330e-10] 6 6
train:	 Loss = 1.5155,	 Acc = 0.4285
2924 0.253
5680 0.46
2913 0.527
376 0.553
69 0.493
6 0.167
0 0.0
0 0.0
0.4852941176470588
0.4908396946564885
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.5003,	 Acc = 0.4463
380 0.337
1718 0.483
815 0.431
85 0.353
2 0.5
0 0.0
0 0.0
0 0.0
0.4622137404580153
0.4908396946564885
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5565,	 Acc1 = 0.4021,	 Acc2 = 0.4284

 ===== Epoch 124	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 0 1
train:	 Loss = 1.5200,	 Acc = 0.4287
2918 0.251
5686 0.457
2912 0.538
377 0.538
69 0.464
6 0.0
0 0.0
0 0.0
0.48596685082872926
0.4908396946564885
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4880,	 Acc = 0.4690
380 0.337
1718 0.499
815 0.481
85 0.329
2 0.5
0 0.0
0 0.0
0 0.0
0.48816793893129773
0.4908396946564885
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5626,	 Acc1 = 0.4000,	 Acc2 = 0.4259

 ===== Epoch 125	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.0487653   2.204639
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -9.34381096e-04  4.31739658e-01
  5.05659230e-11 -3.35663330e-10] 6 6
train:	 Loss = 1.5144,	 Acc = 0.4296
2922 0.253
5686 0.455
2909 0.543
376 0.535
69 0.478
6 0.333
0 0.0
0 0.0
0.48673446827326994
0.4908396946564885
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.5152,	 Acc = 0.4610
380 0.334
1718 0.498
815 0.453
85 0.365
2 0.5
0 0.0
0 0.0
0 0.0
0.47938931297709925
0.4908396946564885
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5695,	 Acc1 = 0.4062,	 Acc2 = 0.4334

 ===== Epoch 126	 =====
[ 0.90651864  1.4746479  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  0.7608057   1.8455856 ] [-9.04303417e-02 -2.22906098e-01 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  1.08519524e-01 -4.43391621e-01] 5 5
train:	 Loss = 1.5167,	 Acc = 0.4285
2924 0.252
5681 0.459
2913 0.534
375 0.528
69 0.449
6 0.167
0 0.0
0 0.0
0.48551525873507295
0.4908396946564885
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4969,	 Acc = 0.4527
380 0.339
1718 0.481
815 0.45
85 0.4
2 0.5
0 0.0
0 0.0
0 0.0
0.46908396946564884
0.4908396946564885
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5569,	 Acc1 = 0.4102,	 Acc2 = 0.4381

 ===== Epoch 127	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  1.295787    1.8206975  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -6.74865991e-02 -2.99218267e-01
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 5 5
train:	 Loss = 1.5182,	 Acc = 0.4275
2924 0.251
5682 0.454
2912 0.535
375 0.563
69 0.478
6 0.167
0 0.0
0 0.0
0.48440955329500224
0.4908396946564885
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.5055,	 Acc = 0.4593
380 0.339
1718 0.495
815 0.456
85 0.294
2 0.5
0 0.0
0 0.0
0 0.0
0.4767175572519084
0.4908396946564885
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5495,	 Acc1 = 0.4104,	 Acc2 = 0.4384

 ===== Epoch 128	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 6 1
train:	 Loss = 1.5169,	 Acc = 0.4267
2923 0.25
5681 0.457
2913 0.535
376 0.527
69 0.391
6 0.167
0 0.0
0 0.0
0.48391376451077944
0.4908396946564885
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 3
val:	 Loss = 1.4880,	 Acc = 0.4417
380 0.339
1718 0.471
815 0.438
85 0.353
2 0.0
0 0.0
0 0.0
0 0.0
0.45648854961832064
0.4908396946564885
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5525,	 Acc1 = 0.4029,	 Acc2 = 0.4294

 ===== Epoch 129	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 3 1
train:	 Loss = 1.5160,	 Acc = 0.4284
2922 0.251
5684 0.456
2910 0.539
377 0.523
69 0.464
6 0.333
0 0.0
0 0.0
0.48562900729604247
0.4908396946564885
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.5216,	 Acc = 0.4343
380 0.345
1718 0.467
815 0.42
85 0.306
2 0.5
0 0.0
0 0.0
0 0.0
0.44732824427480916
0.4908396946564885
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5663,	 Acc1 = 0.4038,	 Acc2 = 0.4304

 ===== Epoch 130	 =====
[-0.36602148 -0.3783333   1.4669688   1.8383018  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10  2.78933287e-01  1.99158303e-02
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 2 4
train:	 Loss = 1.5127,	 Acc = 0.4311
2928 0.252
5678 0.459
2911 0.54
376 0.564
69 0.406
6 0.167
0 0.0
0 0.0
0.4889380530973451
0.4908396946564885
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4856,	 Acc = 0.4790
380 0.337
1718 0.506
815 0.502
85 0.353
2 0.5
0 0.0
0 0.0
0 0.0
0.499618320610687
0.499618320610687
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5465,	 Acc1 = 0.4159,	 Acc2 = 0.4451

 ===== Epoch 131	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.066696    0.8564861
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -2.95709759e-01  1.20485485e-01
  5.05659230e-11 -3.35663330e-10] 4 4
train:	 Loss = 1.5132,	 Acc = 0.4311
2925 0.253
5677 0.461
2914 0.538
377 0.541
69 0.42
6 0.333
0 0.0
0 0.0
0.48866526595156473
0.499618320610687
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 3
val:	 Loss = 1.4908,	 Acc = 0.4500
380 0.342
1718 0.441
815 0.526
85 0.376
2 0.5
0 0.0
0 0.0
0 0.0
0.46564885496183206
0.499618320610687
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 4
Testing:	 Loss = 1.5469,	 Acc1 = 0.4124,	 Acc2 = 0.4409

 ===== Epoch 132	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   2.3123372   0.81158173 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -5.36771250e+00 -2.32929540e+00 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 5 1
train:	 Loss = 1.5141,	 Acc = 0.4276
2923 0.252
5681 0.457
2913 0.532
376 0.535
69 0.493
6 0.167
0 0.0
0 0.0
0.4842454394693201
0.499618320610687
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4852,	 Acc = 0.4333
380 0.339
1718 0.471
815 0.407
85 0.329
2 0.5
0 0.0
0 0.0
0 0.0
0.44694656488549617
0.499618320610687
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5474,	 Acc1 = 0.3949,	 Acc2 = 0.4197

 ===== Epoch 133	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.7068026   1.6032691
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -5.33406883e-02  4.51820567e-02
  5.05659230e-11 -3.35663330e-10] 3 4
train:	 Loss = 1.5153,	 Acc = 0.4261
2924 0.252
5683 0.453
2910 0.533
376 0.537
69 0.435
6 0.333
0 0.0
0 0.0
0.48241928350287483
0.499618320610687
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4894,	 Acc = 0.4727
380 0.342
1718 0.506
815 0.475
85 0.353
2 0.5
0 0.0
0 0.0
0 0.0
0.4916030534351145
0.499618320610687
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5463,	 Acc1 = 0.4137,	 Acc2 = 0.4423

 ===== Epoch 134	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.0451815   3.0737793 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  2.11000703e-02 -5.22568703e-01] 5 5
train:	 Loss = 1.5106,	 Acc = 0.4280
2925 0.248
5678 0.454
2913 0.545
377 0.536
69 0.42
6 0.333
0 0.0
0 0.0
0.4863430277562756
0.499618320610687
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4928,	 Acc = 0.4653
380 0.339
1718 0.505
815 0.452
85 0.376
2 0.0
0 0.0
0 0.0
0 0.0
0.48358778625954196
0.499618320610687
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5570,	 Acc1 = 0.4058,	 Acc2 = 0.4329

 ===== Epoch 135	 =====
[-0.36602148 -0.3783333   1.9957801   2.896203   -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10  5.13613939e-01  1.73277259e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 6 6
train:	 Loss = 1.5125,	 Acc = 0.4297
2925 0.25
5681 0.456
2913 0.543
374 0.548
69 0.493
6 0.333
0 0.0
0 0.0
0.487891186553135
0.499618320610687
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 3
val:	 Loss = 1.5042,	 Acc = 0.4363
380 0.345
1718 0.463
815 0.433
85 0.341
2 0.5
0 0.0
0 0.0
0 0.0
0.449618320610687
0.499618320610687
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5677,	 Acc1 = 0.3827,	 Acc2 = 0.4051

 ===== Epoch 136	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.9413016   1.203999
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -7.49564111e-01 -3.51416022e-02
  5.05659230e-11 -3.35663330e-10] 1 1
train:	 Loss = 1.5107,	 Acc = 0.4270
2923 0.249
5681 0.456
2912 0.535
377 0.552
69 0.391
6 0.167
0 0.0
0 0.0
0.4843559977888336
0.499618320610687
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.5097,	 Acc = 0.4610
380 0.337
1718 0.494
815 0.461
85 0.341
2 0.5
0 0.0
0 0.0
0 0.0
0.47900763358778625
0.499618320610687
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5583,	 Acc1 = 0.4126,	 Acc2 = 0.4411

 ===== Epoch 137	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.4437469   1.074028  ] [ 2.33032513e+00  1.56963062e+00 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
 -1.54739022e-01  1.74189568e-01] 3 6
train:	 Loss = 1.5072,	 Acc = 0.4311
2923 0.252
5678 0.459
2916 0.538
377 0.576
68 0.426
6 0.333
0 0.0
0 0.0
0.4888888888888889
0.499618320610687
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.5755,	 Acc = 0.4523
380 0.339
1718 0.499
815 0.426
85 0.271
2 0.5
0 0.0
0 0.0
0 0.0
0.4687022900763359
0.499618320610687
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.6349,	 Acc1 = 0.3914,	 Acc2 = 0.4155

 ===== Epoch 138	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   1.6930046   0.8767159
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.2713953e-11 -3.1991790e-10 -2.5436093e-04 -1.9714674e-04
  5.6717736e-01 -2.0046804e-02 -1.5722629e-10 -9.7300190e-10
 -6.1223204e-10 -1.8893141e-10 -4.9447713e-10 -1.9952585e-10
  5.0565923e-11 -3.3566333e-10] 3 2
train:	 Loss = 1.5147,	 Acc = 0.4296
2924 0.251
5679 0.46
2914 0.538
376 0.524
69 0.42
6 0.167
0 0.0
0 0.0
0.48728438743918623
0.499618320610687
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 3
val:	 Loss = 1.4629,	 Acc = 0.4753
380 0.337
1718 0.492
815 0.515
85 0.365
2 0.5
0 0.0
0 0.0
0 0.0
0.4954198473282443
0.499618320610687
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5364,	 Acc1 = 0.4168,	 Acc2 = 0.4461

 ===== Epoch 139	 =====
[ 3.6560874   3.1023347  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.0583506   3.4726796 ] [-4.46364975e+00 -4.25688744e+00 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  4.57184702e-01  2.58645117e-01] 6 6
train:	 Loss = 1.5073,	 Acc = 0.4299
2927 0.251
5675 0.458
2915 0.534
377 0.592
68 0.412
6 0.167
0 0.0
0 0.0
0.4878885079084172
0.499618320610687
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4881,	 Acc = 0.4737
380 0.339
1718 0.509
815 0.471
85 0.388
2 0.5
0 0.0
0 0.0
0 0.0
0.49312977099236643
0.499618320610687
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5532,	 Acc1 = 0.4114,	 Acc2 = 0.4396

 ===== Epoch 140	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.4038596   1.8669845
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.69764888e-01 -2.86153018e-01
  5.05659230e-11 -3.35663330e-10] 2 5
train:	 Loss = 1.5070,	 Acc = 0.4308
2925 0.251
5679 0.457
2913 0.544
376 0.553
69 0.449
6 0.333
0 0.0
0 0.0
0.4889970142651775
0.499618320610687
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4987,	 Acc = 0.4413
380 0.337
1718 0.474
815 0.429
85 0.365
2 0.5
0 0.0
0 0.0
0 0.0
0.45648854961832064
0.499618320610687
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5567,	 Acc1 = 0.4009,	 Acc2 = 0.4269

 ===== Epoch 141	 =====
[-0.36602148 -0.3783333   1.534796    1.0001359  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.87096930e+00 -1.48101497e+00
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 1 1
train:	 Loss = 1.5086,	 Acc = 0.4290
2930 0.252
5680 0.456
2909 0.537
375 0.579
68 0.397
6 0.167
0 0.0
0 0.0
0.48639079442354505
0.499618320610687
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4962,	 Acc = 0.4407
380 0.345
1718 0.487
815 0.4
85 0.318
2 0.5
0 0.0
0 0.0
0 0.0
0.4545801526717557
0.499618320610687
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5509,	 Acc1 = 0.4056,	 Acc2 = 0.4327

 ===== Epoch 142	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  3.9169276   2.3993447
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04  8.76925278e+00  5.42673111e+00
 -6.12232043e-10 -1.88931412e-10 -2.90091902e-01 -7.53034279e-02
  5.05659230e-11 -3.35663330e-10] 3 2
train:	 Loss = 1.5128,	 Acc = 0.4300
2923 0.252
5678 0.458
2915 0.54
377 0.538
69 0.435
6 0.167
0 0.0
0 0.0
0.4874516307352128
0.499618320610687
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.5262,	 Acc = 0.4413
380 0.345
1718 0.484
815 0.41
85 0.318
2 0.5
0 0.0
0 0.0
0 0.0
0.45534351145038165
0.499618320610687
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5653,	 Acc1 = 0.4108,	 Acc2 = 0.4389

 ===== Epoch 143	 =====
[-0.36602148 -0.3783333   0.6221692   1.7114443  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10  1.23157091e-01  1.00367725e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 6 4
train:	 Loss = 1.5106,	 Acc = 0.4295
2924 0.251
5681 0.455
2911 0.541
377 0.562
69 0.464
6 0.333
0 0.0
0 0.0
0.48728438743918623
0.499618320610687
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4876,	 Acc = 0.4427
380 0.339
1718 0.47
815 0.437
85 0.412
2 0.5
0 0.0
0 0.0
0 0.0
0.45763358778625957
0.499618320610687
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5459,	 Acc1 = 0.4108,	 Acc2 = 0.4389

 ===== Epoch 144	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.3033428   2.8355844
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10  2.02129945e-01 -8.03236589e-02
  5.05659230e-11 -3.35663330e-10] 4 2
train:	 Loss = 1.5073,	 Acc = 0.4314
2922 0.253
5681 0.46
2913 0.539
377 0.554
69 0.449
6 0.333
0 0.0
0 0.0
0.4890559363254477
0.499618320610687
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.5023,	 Acc = 0.4623
380 0.337
1718 0.5
815 0.452
85 0.365
2 0.5
0 0.0
0 0.0
0 0.0
0.4805343511450382
0.499618320610687
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5567,	 Acc1 = 0.4178,	 Acc2 = 0.4473

 ===== Epoch 145	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 1
train:	 Loss = 1.5091,	 Acc = 0.4320
2929 0.253
5678 0.461
2912 0.543
374 0.527
69 0.435
6 0.333
0 0.0
0 0.0
0.49009846221927206
0.499618320610687
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.5364,	 Acc = 0.4357
380 0.339
1718 0.475
815 0.407
85 0.341
2 0.5
0 0.0
0 0.0
0 0.0
0.449618320610687
0.499618320610687
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5784,	 Acc1 = 0.4002,	 Acc2 = 0.4262

 ===== Epoch 146	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 1 1
train:	 Loss = 1.5126,	 Acc = 0.4288
2924 0.251
5677 0.457
2915 0.535
377 0.554
69 0.478
6 0.333
0 0.0
0 0.0
0.4863998230871296
0.499618320610687
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.5155,	 Acc = 0.4307
380 0.332
1718 0.453
815 0.434
85 0.388
2 0.5
0 0.0
0 0.0
0 0.0
0.4450381679389313
0.499618320610687
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5687,	 Acc1 = 0.3897,	 Acc2 = 0.4135

 ===== Epoch 147	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.5905023   1.4942231  -0.40141198 -0.40778467  2.765647    2.5151825
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -5.24895787e-02 -4.35226589e-01
 -6.12232043e-10 -1.88931412e-10  5.79252303e-01 -3.36355329e-01
  5.05659230e-11 -3.35663330e-10] 5 5
train:	 Loss = 1.5080,	 Acc = 0.4321
2924 0.25
5681 0.46
2913 0.543
375 0.565
69 0.42
6 0.333
0 0.0
0 0.0
0.4910437859354268
0.499618320610687
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 3
val:	 Loss = 1.4703,	 Acc = 0.4717
380 0.339
1718 0.494
815 0.501
85 0.329
2 0.5
0 0.0
0 0.0
0 0.0
0.4908396946564885
0.499618320610687
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5472,	 Acc1 = 0.4104,	 Acc2 = 0.4384

 ===== Epoch 148	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 1 1
train:	 Loss = 1.5085,	 Acc = 0.4306
2918 0.253
5684 0.459
2915 0.538
376 0.553
69 0.406
6 0.5
0 0.0
0 0.0
0.48773480662983426
0.499618320610687
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 3
val:	 Loss = 1.5054,	 Acc = 0.4300
380 0.339
1718 0.444
815 0.455
85 0.318
2 0.5
0 0.0
0 0.0
0 0.0
0.4431297709923664
0.499618320610687
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5594,	 Acc1 = 0.4046,	 Acc2 = 0.4314

 ===== Epoch 149	 =====
[ 1.580703    2.3935442  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.7545761   2.66963   ] [-1.66489974e-01 -4.33428586e-02 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
 -5.40578604e-01 -7.91770741e-02] 1 3
train:	 Loss = 1.5050,	 Acc = 0.4352
2925 0.252
5684 0.463
2909 0.55
376 0.553
68 0.412
6 0.5
0 0.0
0 0.0
0.49441557005418557
0.499618320610687
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4938,	 Acc = 0.4613
380 0.334
1718 0.491
815 0.467
85 0.365
2 0.5
0 0.0
0 0.0
0 0.0
0.47977099236641224
0.499618320610687
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5641,	 Acc1 = 0.4040,	 Acc2 = 0.4307

 ===== Epoch 150	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.2682366   1.4751083
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -9.82574895e-02 -1.20485485e-01
  5.05659230e-11 -3.35663330e-10] 1 5
train:	 Loss = 1.5095,	 Acc = 0.4285
2925 0.248
5679 0.458
2913 0.538
376 0.553
69 0.406
6 0.333
0 0.0
0 0.0
0.48700652438350106
0.499618320610687
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4780,	 Acc = 0.4620
380 0.339
1718 0.485
815 0.479
85 0.376
2 0.5
0 0.0
0 0.0
0 0.0
0.47977099236641224
0.499618320610687
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5408,	 Acc1 = 0.4122,	 Acc2 = 0.4406

 ===== Epoch 151	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 6 1
train:	 Loss = 1.5007,	 Acc = 0.4317
2924 0.251
5681 0.46
2914 0.545
375 0.547
68 0.397
6 0.167
0 0.0
0 0.0
0.49015922158337016
0.499618320610687
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.5101,	 Acc = 0.4473
380 0.342
1718 0.491
815 0.415
85 0.353
2 0.5
0 0.0
0 0.0
0 0.0
0.46259541984732827
0.499618320610687
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5677,	 Acc1 = 0.3994,	 Acc2 = 0.4252

 ===== Epoch 152	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  0.6767949   2.7722168  -0.40141198 -0.40778467  1.9689571   2.1307
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.31580758e+00 -7.66406775e+00
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02 -1.00404574e-02
  5.05659230e-11 -3.35663330e-10] 1 0
train:	 Loss = 1.5070,	 Acc = 0.4316
2918 0.251
5685 0.463
2913 0.54
377 0.525
69 0.406
6 0.333
0 0.0
0 0.0
0.4898342541436464
0.499618320610687
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.5166,	 Acc = 0.4357
380 0.342
1718 0.468
815 0.42
85 0.353
2 0.5
0 0.0
0 0.0
0 0.0
0.449236641221374
0.499618320610687
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5586,	 Acc1 = 0.4025,	 Acc2 = 0.4289

 ===== Epoch 153	 =====
[ 2.6214783   2.3631675  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.1434429   2.585651  ] [-3.07799935e-01 -1.85755100e-02 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
 -2.25074649e-01  5.27847148e-02] 0 2
train:	 Loss = 1.5082,	 Acc = 0.4271
2926 0.251
5678 0.453
2913 0.538
376 0.545
69 0.42
6 0.167
0 0.0
0 0.0
0.48396372483963723
0.499618320610687
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4594,	 Acc = 0.4700
380 0.345
1718 0.49
815 0.498
85 0.353
2 0.5
0 0.0
0 0.0
0 0.0
0.48816793893129773
0.499618320610687
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5445,	 Acc1 = 0.4128,	 Acc2 = 0.4414

 ===== Epoch 154	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10  6.35440969e+00  1.32223105e+00
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 1 6
train:	 Loss = 1.5059,	 Acc = 0.4300
2926 0.251
5678 0.457
2912 0.546
377 0.523
69 0.435
6 0.167
0 0.0
0 0.0
0.4880557398805574
0.499618320610687
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4971,	 Acc = 0.4383
380 0.347
1718 0.48
815 0.404
85 0.341
2 0.5
0 0.0
0 0.0
0 0.0
0.45152671755725193
0.499618320610687
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5664,	 Acc1 = 0.4062,	 Acc2 = 0.4334

 ===== Epoch 155	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   2.8946233   2.9660015
 -0.3640846  -0.3725409   1.9284266   2.3146374  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 0 0
train:	 Loss = 1.5112,	 Acc = 0.4325
2926 0.251
5679 0.46
2913 0.543
376 0.566
68 0.426
6 0.167
0 0.0
0 0.0
0.491152399911524
0.499618320610687
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4787,	 Acc = 0.4627
380 0.342
1718 0.488
815 0.476
85 0.365
2 0.5
0 0.0
0 0.0
0 0.0
0.4801526717557252
0.499618320610687
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5519,	 Acc1 = 0.4118,	 Acc2 = 0.4401

 ===== Epoch 156	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.8382587   2.0296502
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -6.73766509e-02  3.76517147e-01
  5.05659230e-11 -3.35663330e-10] 6 6
train:	 Loss = 1.5071,	 Acc = 0.4274
2928 0.252
5678 0.456
2911 0.535
376 0.543
69 0.362
6 0.167
0 0.0
0 0.0
0.484070796460177
0.499618320610687
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4970,	 Acc = 0.4383
380 0.339
1718 0.47
815 0.428
85 0.329
2 0.5
0 0.0
0 0.0
0 0.0
0.45267175572519086
0.499618320610687
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5567,	 Acc1 = 0.4031,	 Acc2 = 0.4297

 ===== Epoch 157	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 0 1
train:	 Loss = 1.5039,	 Acc = 0.4341
2922 0.252
5681 0.459
2914 0.555
376 0.529
69 0.493
6 0.167
0 0.0
0 0.0
0.4928145036480212
0.499618320610687
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.5103,	 Acc = 0.4460
380 0.339
1718 0.48
815 0.436
85 0.341
2 0.5
0 0.0
0 0.0
0 0.0
0.4614503816793893
0.499618320610687
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5790,	 Acc1 = 0.3945,	 Acc2 = 0.4192

 ===== Epoch 158	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   1.8904729   0.9945161
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.2713953e-11 -3.1991790e-10 -2.5436093e-04 -1.9714674e-04
  1.8807979e-01  4.0431853e-02 -1.5722629e-10 -9.7300190e-10
 -6.1223204e-10 -1.8893141e-10 -4.9447713e-10 -1.9952585e-10
  5.0565923e-11 -3.3566333e-10] 4 2
train:	 Loss = 1.5077,	 Acc = 0.4297
2925 0.251
5679 0.46
2912 0.535
377 0.538
69 0.435
6 0.333
0 0.0
0 0.0
0.487448855468318
0.499618320610687
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4670,	 Acc = 0.4623
380 0.342
1718 0.483
815 0.487
85 0.353
2 0.5
0 0.0
0 0.0
0 0.0
0.47977099236641224
0.499618320610687
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5429,	 Acc1 = 0.4027,	 Acc2 = 0.4292

 ===== Epoch 159	 =====
[ 2.8019683   2.846664   -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.3418443   3.0685306 ] [ 1.00826478e+00  7.43020400e-02 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  9.64614525e-02  1.47797212e-01] 4 4
train:	 Loss = 1.5009,	 Acc = 0.4327
2921 0.251
5678 0.459
2918 0.547
376 0.564
69 0.406
6 0.333
0 0.0
0 0.0
0.4914336244058804
0.499618320610687
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4733,	 Acc = 0.4637
380 0.337
1718 0.495
815 0.47
85 0.341
2 0.5
0 0.0
0 0.0
0 0.0
0.4820610687022901
0.499618320610687
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5496,	 Acc1 = 0.4068,	 Acc2 = 0.4341

 ===== Epoch 160	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 1 1
train:	 Loss = 1.5059,	 Acc = 0.4298
2924 0.25
5680 0.457
2913 0.543
376 0.556
69 0.406
6 0.333
0 0.0
0 0.0
0.4879478107032287
0.499618320610687
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4958,	 Acc = 0.4640
380 0.332
1718 0.5
815 0.458
85 0.388
2 0.5
0 0.0
0 0.0
0 0.0
0.483206106870229
0.499618320610687
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5557,	 Acc1 = 0.4085,	 Acc2 = 0.4361

 ===== Epoch 161	 =====
[ 1.4267327   2.5859303  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.2224194   2.937313  ] [-1.98955977e+00 -3.62532020e+00 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
 -8.33969936e-02 -2.53366649e-01] 4 5
train:	 Loss = 1.5110,	 Acc = 0.4317
2920 0.25
5682 0.461
2914 0.545
377 0.536
69 0.391
6 0.167
0 0.0
0 0.0
0.4901635720601238
0.499618320610687
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 3
val:	 Loss = 1.4790,	 Acc = 0.4473
380 0.334
1718 0.463
815 0.474
85 0.376
2 0.5
0 0.0
0 0.0
0 0.0
0.4637404580152672
0.499618320610687
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5554,	 Acc1 = 0.4089,	 Acc2 = 0.4366

 ===== Epoch 162	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 1 1
train:	 Loss = 1.5017,	 Acc = 0.4313
2920 0.252
5682 0.456
2914 0.547
377 0.557
69 0.42
6 0.167
0 0.0
0 0.0
0.4891688770999116
0.499618320610687
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4731,	 Acc = 0.4730
380 0.334
1718 0.498
815 0.497
85 0.365
2 0.5
0 0.0
0 0.0
0 0.0
0.49312977099236643
0.499618320610687
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5446,	 Acc1 = 0.4104,	 Acc2 = 0.4384

 ===== Epoch 163	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 1 1
train:	 Loss = 1.5034,	 Acc = 0.4358
2921 0.25
5682 0.467
2914 0.547
376 0.559
69 0.377
6 0.167
0 0.0
0 0.0
0.4957444456725987
0.499618320610687
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4740,	 Acc = 0.4573
380 0.345
1718 0.487
815 0.456
85 0.376
2 0.5
0 0.0
0 0.0
0 0.0
0.47366412213740455
0.499618320610687
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5498,	 Acc1 = 0.4052,	 Acc2 = 0.4322

 ===== Epoch 164	 =====
[ 2.8078353   3.2845953  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.6090252   3.5382886 ] [-2.67135538e-02 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
 -4.51155692e-01 -1.58354156e-02] 0 0
train:	 Loss = 1.5050,	 Acc = 0.4311
2926 0.252
5678 0.462
2916 0.542
373 0.509
69 0.42
6 0.333
0 0.0
0 0.0
0.48927228489272284
0.499618320610687
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4732,	 Acc = 0.4557
380 0.339
1718 0.488
815 0.45
85 0.376
2 0.5
0 0.0
0 0.0
0 0.0
0.4725190839694656
0.499618320610687
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5434,	 Acc1 = 0.4085,	 Acc2 = 0.4361

 ===== Epoch 165	 =====
[-0.36602148 -0.3783333   2.8080876   2.520161   -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10  1.10004790e-01 -7.81349242e-02
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 5 2
train:	 Loss = 1.5069,	 Acc = 0.4304
2922 0.252
5680 0.458
2914 0.54
377 0.541
69 0.435
6 0.167
0 0.0
0 0.0
0.4879504753482202
0.499618320610687
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4696,	 Acc = 0.4743
380 0.337
1718 0.507
815 0.483
85 0.341
2 0.5
0 0.0
0 0.0
0 0.0
0.49427480916030536
0.499618320610687
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5443,	 Acc1 = 0.4054,	 Acc2 = 0.4324

 ===== Epoch 166	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 5 1
train:	 Loss = 1.5026,	 Acc = 0.4326
2925 0.251
5684 0.461
2908 0.545
376 0.553
69 0.42
6 0.333
0 0.0
0 0.0
0.4914298352316709
0.499618320610687
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 3
val:	 Loss = 1.4806,	 Acc = 0.4540
380 0.345
1718 0.471
815 0.477
85 0.365
2 0.5
0 0.0
0 0.0
0 0.0
0.46984732824427483
0.499618320610687
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5614,	 Acc1 = 0.3972,	 Acc2 = 0.4225

 ===== Epoch 167	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   4.5098963   2.8593147
 -0.3640846  -0.3725409   4.793742    2.3992465   0.43164262  1.3814524
  3.0203762   1.3075949 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
 -1.29308720e+01 -3.65380621e+00 -1.57226288e-10 -9.73001901e-10
 -1.02758551e+01 -5.36213303e+00 -9.07708779e-02  2.51011431e-01
  1.67803466e-01  1.10847905e-01] 2 2
train:	 Loss = 1.5042,	 Acc = 0.4345
2927 0.251
5676 0.464
2914 0.545
376 0.561
69 0.449
6 0.167
0 0.0
0 0.0
0.4939719057626369
0.499618320610687
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4642,	 Acc = 0.4617
380 0.342
1718 0.486
815 0.475
85 0.376
2 0.5
0 0.0
0 0.0
0 0.0
0.47900763358778625
0.499618320610687
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5452,	 Acc1 = 0.4143,	 Acc2 = 0.4431

 ===== Epoch 168	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   1.3379699   2.9989755  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
  1.65226862e-01  1.85392886e-01 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 6 4
train:	 Loss = 1.5006,	 Acc = 0.4308
2923 0.247
5682 0.46
2913 0.542
376 0.564
68 0.368
6 0.333
0 0.0
0 0.0
0.49021558872305143
0.499618320610687
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4970,	 Acc = 0.4283
380 0.339
1718 0.459
815 0.41
85 0.388
2 0.5
0 0.0
0 0.0
0 0.0
0.44122137404580153
0.499618320610687
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 4
Testing:	 Loss = 1.5470,	 Acc1 = 0.4091,	 Acc2 = 0.4369

 ===== Epoch 169	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   3.6707485   2.5325859
 -0.3640846  -0.3725409   4.244482    1.9488276  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  1.41815770e+00 -7.89487123e+00 -1.57226288e-10 -9.73001901e-10
  5.65834381e-02  1.42609915e-02 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 2 0
train:	 Loss = 1.5050,	 Acc = 0.4301
2929 0.254
5681 0.459
2908 0.54
376 0.521
68 0.426
6 0.333
0 0.0
0 0.0
0.48733266954309107
0.499618320610687
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4629,	 Acc = 0.4793
380 0.339
1718 0.497
815 0.52
85 0.365
2 0.5
0 0.0
0 0.0
0 0.0
0.499618320610687
0.499618320610687
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5400,	 Acc1 = 0.4064,	 Acc2 = 0.4336

 ===== Epoch 170	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   0.37603614  2.3414385
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.2713953e-11 -3.1991790e-10 -2.5436093e-04 -1.9714674e-04
  5.3398442e-02  3.2518551e-01 -1.5722629e-10 -9.7300190e-10
 -6.1223204e-10 -1.8893141e-10 -4.9447713e-10 -1.9952585e-10
  5.0565923e-11 -3.3566333e-10] 6 6
train:	 Loss = 1.5032,	 Acc = 0.4306
2923 0.249
5681 0.461
2913 0.538
376 0.553
69 0.449
6 0.167
0 0.0
0 0.0
0.4894416804864566
0.499618320610687
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4717,	 Acc = 0.4533
380 0.342
1718 0.488
815 0.444
85 0.341
2 0.5
0 0.0
0 0.0
0 0.0
0.46946564885496184
0.499618320610687
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5406,	 Acc1 = 0.4132,	 Acc2 = 0.4418

 ===== Epoch 171	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.3965367   2.852837
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10  8.04782286e-02  3.21294636e-01
  5.05659230e-11 -3.35663330e-10] 1 6
train:	 Loss = 1.5043,	 Acc = 0.4302
2926 0.249
5679 0.463
2913 0.537
375 0.541
69 0.377
6 0.167
0 0.0
0 0.0
0.488940499889405
0.499618320610687
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.5026,	 Acc = 0.4290
380 0.339
1718 0.454
815 0.427
85 0.341
2 0.5
0 0.0
0 0.0
0 0.0
0.44198473282442746
0.499618320610687
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5430,	 Acc1 = 0.4075,	 Acc2 = 0.4349

 ===== Epoch 172	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 0 1
train:	 Loss = 1.5036,	 Acc = 0.4309
2927 0.251
5678 0.462
2912 0.539
376 0.535
69 0.391
6 0.167
0 0.0
0 0.0
0.4891051874792611
0.499618320610687
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4976,	 Acc = 0.4313
380 0.339
1718 0.464
815 0.417
85 0.306
2 0.5
0 0.0
0 0.0
0 0.0
0.4446564885496183
0.499618320610687
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5511,	 Acc1 = 0.4054,	 Acc2 = 0.4324

 ===== Epoch 173	 =====
[ 2.7756312   1.8087921  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   1.9375814   3.3473663  -0.42342317 -0.42019257
  2.8748827   1.9951732 ] [ 7.82747984e-01 -1.54795907e-02 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
  2.08229437e-01 -9.50732827e-03 -4.94477126e-10 -1.99525854e-10
 -5.78762531e-01 -1.05569437e-02] 6 0
train:	 Loss = 1.5026,	 Acc = 0.4311
2923 0.252
5683 0.459
2913 0.542
375 0.541
68 0.441
6 0.167
0 0.0
0 0.0
0.4888888888888889
0.499618320610687
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4899,	 Acc = 0.4617
380 0.345
1718 0.487
815 0.475
85 0.341
2 0.5
0 0.0
0 0.0
0 0.0
0.47862595419847326
0.499618320610687
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5633,	 Acc1 = 0.4066,	 Acc2 = 0.4339

 ===== Epoch 174	 =====
[ 2.768918    2.9757652  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  1.1233442   3.0038185  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-3.47908831e+00 -4.10209179e+00 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04  7.99802542e-02  4.48827416e-01
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
train:	 Loss = 1.5011,	 Acc = 0.4344
2926 0.253
5680 0.462
2911 0.548
377 0.557
68 0.382
6 0.333
0 0.0
0 0.0
0.49325370493253706
0.499618320610687
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 3
val:	 Loss = 1.4929,	 Acc = 0.4627
380 0.339
1718 0.501
815 0.452
85 0.353
2 0.5
0 0.0
0 0.0
0 0.0
0.4805343511450382
0.499618320610687
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5583,	 Acc1 = 0.3990,	 Acc2 = 0.4247

 ===== Epoch 175	 =====
[-0.36602148 -0.3783333   1.665578    1.4237496  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.07623512e-01 -2.28982240e-01
  5.23268769e-04  1.12748065e-04  7.45956612e+00  1.01394196e+01
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 5 5
train:	 Loss = 1.5027,	 Acc = 0.4318
2924 0.249
5679 0.462
2914 0.54
376 0.553
69 0.449
6 0.333
0 0.0
0 0.0
0.49082264484741267
0.499618320610687
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4903,	 Acc = 0.4387
380 0.334
1718 0.477
815 0.417
85 0.329
2 0.5
0 0.0
0 0.0
0 0.0
0.4538167938931298
0.499618320610687
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5552,	 Acc1 = 0.4089,	 Acc2 = 0.4366

 ===== Epoch 176	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.4212856   1.8324797
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10  7.29916096e-02 -5.52225150e-02
  5.05659230e-11 -3.35663330e-10] 2 3
train:	 Loss = 1.5066,	 Acc = 0.4331
2923 0.251
5679 0.463
2914 0.541
377 0.552
69 0.435
6 0.333
0 0.0
0 0.0
0.4918739635157546
0.499618320610687
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 3
val:	 Loss = 1.4600,	 Acc = 0.4490
380 0.337
1718 0.471
815 0.465
85 0.353
2 0.5
0 0.0
0 0.0
0 0.0
0.46526717557251906
0.499618320610687
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5314,	 Acc1 = 0.4077,	 Acc2 = 0.4351

 ===== Epoch 177	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   1.8583888   1.7302114
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.2713953e-11 -3.1991790e-10 -2.5436093e-04 -1.9714674e-04
  1.5615423e-01  9.5870614e-02 -1.5722629e-10 -9.7300190e-10
 -6.1223204e-10 -1.8893141e-10 -4.9447713e-10 -1.9952585e-10
  5.0565923e-11 -3.3566333e-10] 3 4
train:	 Loss = 1.5031,	 Acc = 0.4317
2921 0.25
5685 0.462
2910 0.538
377 0.565
69 0.391
6 0.167
0 0.0
0 0.0
0.4903282856195424
0.499618320610687
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 3
val:	 Loss = 1.4609,	 Acc = 0.4623
380 0.337
1718 0.468
815 0.519
85 0.365
2 0.5
0 0.0
0 0.0
0 0.0
0.4805343511450382
0.499618320610687
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5396,	 Acc1 = 0.4122,	 Acc2 = 0.4406

 ===== Epoch 178	 =====
[-0.36602148 -0.3783333   2.255718    1.029585   -0.4409929  -0.3635196
  3.389237    3.2912273  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.81751651e-01  7.01982677e-02
  5.23268769e-04  1.12748065e-04  1.62487049e-02  7.48045668e-02
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 5 3
train:	 Loss = 1.5009,	 Acc = 0.4317
2925 0.251
5678 0.462
2914 0.539
377 0.541
68 0.5
6 0.167
0 0.0
0 0.0
0.4899922592060157
0.499618320610687
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4842,	 Acc = 0.4423
380 0.339
1718 0.484
815 0.413
85 0.329
2 0.5
0 0.0
0 0.0
0 0.0
0.45725190839694657
0.499618320610687
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5508,	 Acc1 = 0.4085,	 Acc2 = 0.4361

 ===== Epoch 179	 =====
[-0.36602148 -0.3783333   0.7801638   0.8257068  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10  2.80671678e-02  5.00852913e-02
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 4
train:	 Loss = 1.5041,	 Acc = 0.4337
2925 0.249
5679 0.462
2913 0.549
376 0.559
69 0.391
6 0.167
0 0.0
0 0.0
0.4934203251133473
0.499618320610687
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4723,	 Acc = 0.4637
380 0.339
1718 0.496
815 0.467
85 0.329
2 0.5
0 0.0
0 0.0
0 0.0
0.4816793893129771
0.499618320610687
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5419,	 Acc1 = 0.4035,	 Acc2 = 0.4302

 ===== Epoch 180	 =====
[ 1.6333194   3.2263732  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.4053898   3.527791  ] [ 3.51681828e-01  8.66857171e-02 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  4.42098156e-02  8.97340253e-02] 4 4
train:	 Loss = 1.4999,	 Acc = 0.4337
2925 0.249
5676 0.461
2915 0.549
377 0.568
69 0.406
6 0.333
0 0.0
0 0.0
0.4934203251133473
0.499618320610687
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.5078,	 Acc = 0.4517
380 0.337
1718 0.493
815 0.432
85 0.318
2 0.5
0 0.0
0 0.0
0 0.0
0.4683206106870229
0.499618320610687
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5610,	 Acc1 = 0.4060,	 Acc2 = 0.4332

 ===== Epoch 181	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10  5.53808546e+00  3.76092935e+00
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 5 1
train:	 Loss = 1.5026,	 Acc = 0.4316
2921 0.251
5680 0.46
2916 0.542
376 0.545
69 0.435
6 0.167
0 0.0
0 0.0
0.4898861501050072
0.499618320610687
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4677,	 Acc = 0.4710
380 0.337
1718 0.497
815 0.491
85 0.353
2 0.5
0 0.0
0 0.0
0 0.0
0.4904580152671756
0.499618320610687
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5256,	 Acc1 = 0.4186,	 Acc2 = 0.4483

 ===== Epoch 182	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 3 1
train:	 Loss = 1.4991,	 Acc = 0.4317
2921 0.25
5679 0.46
2917 0.543
376 0.551
69 0.377
6 0.5
0 0.0
0 0.0
0.4902177517409086
0.499618320610687
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4702,	 Acc = 0.4613
380 0.337
1718 0.492
815 0.465
85 0.365
2 0.5
0 0.0
0 0.0
0 0.0
0.47938931297709925
0.499618320610687
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5332,	 Acc1 = 0.4155,	 Acc2 = 0.4446

 ===== Epoch 183	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 5 1
train:	 Loss = 1.5021,	 Acc = 0.4332
2923 0.252
5680 0.463
2915 0.54
377 0.57
67 0.418
6 0.333
0 0.0
0 0.0
0.491763405196241
0.499618320610687
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 3
val:	 Loss = 1.4837,	 Acc = 0.4480
380 0.342
1718 0.466
815 0.47
85 0.341
2 0.5
0 0.0
0 0.0
0 0.0
0.4633587786259542
0.499618320610687
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5454,	 Acc1 = 0.4093,	 Acc2 = 0.4371

 ===== Epoch 184	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 5 1
train:	 Loss = 1.5001,	 Acc = 0.4299
2924 0.252
5685 0.457
2910 0.542
375 0.541
68 0.441
6 0.167
0 0.0
0 0.0
0.48739495798319327
0.499618320610687
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 3
val:	 Loss = 1.4619,	 Acc = 0.4590
380 0.334
1718 0.481
815 0.483
85 0.329
2 0.5
0 0.0
0 0.0
0 0.0
0.4770992366412214
0.499618320610687
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5279,	 Acc1 = 0.4196,	 Acc2 = 0.4496

 ===== Epoch 185	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.7171562   1.5884813
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10  1.12299230e-02 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 0 0
train:	 Loss = 1.5050,	 Acc = 0.4324
2923 0.245
5682 0.462
2911 0.543
377 0.584
69 0.449
6 0.167
0 0.0
0 0.0
0.49297954671089
0.499618320610687
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4994,	 Acc = 0.4487
380 0.337
1718 0.486
815 0.432
85 0.353
2 0.5
0 0.0
0 0.0
0 0.0
0.4648854961832061
0.499618320610687
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5522,	 Acc1 = 0.4120,	 Acc2 = 0.4404

 ===== Epoch 186	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   0.81986356  1.0678633
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.2713953e-11 -3.1991790e-10 -2.5436093e-04 -1.9714674e-04
  1.7482182e-02 -1.4100412e-01 -1.5722629e-10 -9.7300190e-10
 -6.1223204e-10 -1.8893141e-10 -4.9447713e-10 -1.9952585e-10
  5.0565923e-11 -3.3566333e-10] 5 5
train:	 Loss = 1.5054,	 Acc = 0.4318
2925 0.251
5677 0.462
2914 0.54
377 0.546
69 0.464
6 0.333
0 0.0
0 0.0
0.49032400751962846
0.499618320610687
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4842,	 Acc = 0.4553
380 0.342
1718 0.488
815 0.454
85 0.318
2 0.5
0 0.0
0 0.0
0 0.0
0.4717557251908397
0.499618320610687
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5662,	 Acc1 = 0.3949,	 Acc2 = 0.4197

 ===== Epoch 187	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 5 1
train:	 Loss = 1.5058,	 Acc = 0.4296
2922 0.252
5681 0.456
2914 0.542
376 0.553
69 0.406
6 0.167
0 0.0
0 0.0
0.48717665266416094
0.499618320610687
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4897,	 Acc = 0.4523
380 0.339
1718 0.48
815 0.456
85 0.365
2 0.5
0 0.0
0 0.0
0 0.0
0.4687022900763359
0.499618320610687
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 4
Testing:	 Loss = 1.5374,	 Acc1 = 0.4217,	 Acc2 = 0.4520

 ===== Epoch 188	 =====
[ 2.9694476   1.7961352  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   1.9381528   3.3448777  -0.42342317 -0.42019257
  2.8748827   1.992549  ] [ 2.65825335e-02  6.19183667e-03 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -1.01839006e-02 -4.75366414e-03 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11  5.27847139e-03] 0 0
train:	 Loss = 1.5026,	 Acc = 0.4266
2923 0.245
5681 0.456
2913 0.536
376 0.545
69 0.435
6 0.5
0 0.0
0 0.0
0.4853510226644555
0.499618320610687
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4726,	 Acc = 0.4763
380 0.334
1718 0.501
815 0.503
85 0.365
2 0.5
0 0.0
0 0.0
0 0.0
0.49694656488549616
0.499618320610687
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5414,	 Acc1 = 0.4116,	 Acc2 = 0.4399

 ===== Epoch 189	 =====
[ 2.324227    2.1252165  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.1879728   2.357333  ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 0 0
train:	 Loss = 1.5040,	 Acc = 0.4309
2926 0.251
5681 0.459
2912 0.543
375 0.549
68 0.397
6 0.333
0 0.0
0 0.0
0.4891616898916169
0.499618320610687
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4881,	 Acc = 0.4297
380 0.339
1718 0.467
815 0.405
85 0.306
2 0.5
0 0.0
0 0.0
0 0.0
0.44274809160305345
0.499618320610687
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5561,	 Acc1 = 0.4044,	 Acc2 = 0.4312

 ===== Epoch 190	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 1
train:	 Loss = 1.4999,	 Acc = 0.4327
2924 0.25
5678 0.464
2916 0.539
375 0.555
69 0.478
6 0.167
0 0.0
0 0.0
0.49181777974347635
0.499618320610687
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4775,	 Acc = 0.4530
380 0.339
1718 0.479
815 0.463
85 0.341
2 0.5
0 0.0
0 0.0
0 0.0
0.46946564885496184
0.499618320610687
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5494,	 Acc1 = 0.4087,	 Acc2 = 0.4364

 ===== Epoch 191	 =====
[ 2.2703938   2.5631478  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-7.43030488e-01  1.79563254e-01 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 6 6
train:	 Loss = 1.5040,	 Acc = 0.4285
2925 0.252
5676 0.453
2917 0.544
375 0.536
69 0.42
6 0.333
0 0.0
0 0.0
0.4856795311290501
0.499618320610687
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.5037,	 Acc = 0.4410
380 0.339
1718 0.482
815 0.416
85 0.306
2 0.5
0 0.0
0 0.0
0 0.0
0.45572519083969465
0.499618320610687
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5611,	 Acc1 = 0.4027,	 Acc2 = 0.4292

 ===== Epoch 192	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.3951468   1.0240809
  2.8973684   3.3572087 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10  5.31526566e-01  1.10445030e-01
  3.72778237e-01  1.21404849e-01] 2 2
train:	 Loss = 1.5024,	 Acc = 0.4299
2921 0.251
5680 0.457
2915 0.542
377 0.557
69 0.42
6 0.167
0 0.0
0 0.0
0.4876754725323312
0.499618320610687
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4696,	 Acc = 0.4547
380 0.337
1718 0.49
815 0.448
85 0.329
2 0.5
0 0.0
0 0.0
0 0.0
0.4717557251908397
0.499618320610687
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5454,	 Acc1 = 0.4137,	 Acc2 = 0.4423

 ===== Epoch 193	 =====
[-0.36602148 -0.3783333   3.092801    1.2991571  -0.4409929  -0.3635196
  2.0754247   2.749894   -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10  1.44395888e-01 -1.46016225e-01
  5.23268769e-04  1.12748065e-04  2.36193448e-01 -4.62428272e-01
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 5 5
train:	 Loss = 1.5036,	 Acc = 0.4317
2927 0.25
5678 0.461
2914 0.544
374 0.532
69 0.435
6 0.167
0 0.0
0 0.0
0.49043247428381814
0.499618320610687
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4514,	 Acc = 0.4743
380 0.337
1718 0.494
815 0.508
85 0.376
2 0.5
0 0.0
0 0.0
0 0.0
0.49427480916030536
0.499618320610687
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5355,	 Acc1 = 0.4087,	 Acc2 = 0.4364

 ===== Epoch 194	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  3.3431776   1.7537283  -0.40141198 -0.40778467  3.1088722   2.670454
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04  2.94929862e-01  9.52058136e-02
 -6.12232043e-10 -1.88931412e-10  9.63858366e-02  3.51416022e-02
  5.05659230e-11 -3.35663330e-10] 4 3
train:	 Loss = 1.5007,	 Acc = 0.4304
2927 0.249
5680 0.457
2912 0.545
375 0.552
68 0.426
6 0.167
0 0.0
0 0.0
0.4891051874792611
0.499618320610687
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4679,	 Acc = 0.4473
380 0.339
1718 0.469
815 0.461
85 0.353
2 0.5
0 0.0
0 0.0
0 0.0
0.4629770992366412
0.499618320610687
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5358,	 Acc1 = 0.4081,	 Acc2 = 0.4356

 ===== Epoch 195	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 2 1
train:	 Loss = 1.5048,	 Acc = 0.4306
2927 0.251
5678 0.46
2913 0.538
375 0.56
69 0.435
6 0.333
0 0.0
0 0.0
0.48888397301183495
0.499618320610687
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.5013,	 Acc = 0.4390
380 0.342
1718 0.475
815 0.417
85 0.353
2 0.5
0 0.0
0 0.0
0 0.0
0.4530534351145038
0.499618320610687
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5676,	 Acc1 = 0.3957,	 Acc2 = 0.4207

 ===== Epoch 196	 =====
[-0.36602148 -0.3783333   1.1733193   1.5483416  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -8.82584229e-02 -2.44066983e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 5 5
train:	 Loss = 1.5002,	 Acc = 0.4296
2922 0.249
5680 0.461
2915 0.535
376 0.553
69 0.391
6 0.5
0 0.0
0 0.0
0.4879504753482202
0.499618320610687
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4898,	 Acc = 0.4553
380 0.342
1718 0.481
815 0.465
85 0.353
2 0.5
0 0.0
0 0.0
0 0.0
0.4717557251908397
0.499618320610687
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5373,	 Acc1 = 0.4147,	 Acc2 = 0.4436

 ===== Epoch 197	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.2253029   2.5151825
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10  4.58540730e-02 -5.02022868e-03
  5.05659230e-11 -3.35663330e-10] 2 0
train:	 Loss = 1.5069,	 Acc = 0.4279
2925 0.251
5678 0.455
2914 0.539
376 0.535
69 0.435
6 0.333
0 0.0
0 0.0
0.4852372000442331
0.499618320610687
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4659,	 Acc = 0.4507
380 0.339
1718 0.48
815 0.452
85 0.353
2 0.5
0 0.0
0 0.0
0 0.0
0.466793893129771
0.499618320610687
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5333,	 Acc1 = 0.4052,	 Acc2 = 0.4322

 ===== Epoch 198	 =====
[ 2.1998484   1.1000015  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-2.84754705e+00 -1.80801618e+00 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 1 1
train:	 Loss = 1.5009,	 Acc = 0.4313
2922 0.251
5684 0.461
2911 0.536
376 0.572
69 0.435
6 0.167
0 0.0
0 0.0
0.4894981207163387
0.499618320610687
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4736,	 Acc = 0.4717
380 0.339
1718 0.494
815 0.494
85 0.388
2 0.5
0 0.0
0 0.0
0 0.0
0.4908396946564885
0.499618320610687
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5318,	 Acc1 = 0.4151,	 Acc2 = 0.4441

 ===== Epoch 199	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 6 1
train:	 Loss = 1.5034,	 Acc = 0.4337
2922 0.251
5680 0.459
2916 0.551
375 0.557
69 0.449
6 0.333
0 0.0
0 0.0
0.4927039575502985
0.499618320610687
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4655,	 Acc = 0.4460
380 0.339
1718 0.473
815 0.448
85 0.365
2 0.5
0 0.0
0 0.0
0 0.0
0.4614503816793893
0.499618320610687
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5428,	 Acc1 = 0.4000,	 Acc2 = 0.4259

 ===== Epoch 200	 =====
[-0.36602148 -0.3783333   0.69690204  1.3467288  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -1.24675840e-01 -7.81349242e-02
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 5 5
train:	 Loss = 1.5017,	 Acc = 0.4362
2927 0.252
5677 0.465
2912 0.548
377 0.576
69 0.449
6 0.333
0 0.0
0 0.0
0.4958522287357593
0.499618320610687
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4875,	 Acc = 0.4553
380 0.342
1718 0.493
815 0.439
85 0.353
2 0.5
0 0.0
0 0.0
0 0.0
0.4717557251908397
0.499618320610687
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5430,	 Acc1 = 0.4106,	 Acc2 = 0.4386

 ===== Epoch 201	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   1.5954367   2.2325168  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -1.69743337e-02 -2.18668535e-01 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 3 5
train:	 Loss = 1.5034,	 Acc = 0.4345
2927 0.252
5681 0.463
2909 0.546
376 0.566
69 0.406
6 0.333
0 0.0
0 0.0
0.49352947682778453
0.499618320610687
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4586,	 Acc = 0.4733
380 0.337
1718 0.495
815 0.499
85 0.4
2 0.5
0 0.0
0 0.0
0 0.0
0.49312977099236643
0.499618320610687
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5223,	 Acc1 = 0.4176,	 Acc2 = 0.4471

 ===== Epoch 202	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  0.3797429   2.4954653
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10  2.23652527e-01  2.00809136e-01
  5.05659230e-11 -3.35663330e-10] 1 6
train:	 Loss = 1.4998,	 Acc = 0.4316
2922 0.251
5682 0.458
2913 0.546
376 0.553
69 0.42
6 0.333
0 0.0
0 0.0
0.4899403051072297
0.499618320610687
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4755,	 Acc = 0.4387
380 0.339
1718 0.471
815 0.428
85 0.318
2 0.5
0 0.0
0 0.0
0 0.0
0.4530534351145038
0.499618320610687
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5496,	 Acc1 = 0.4044,	 Acc2 = 0.4312

 ===== Epoch 203	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   0.35884818  1.3056862
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.2713953e-11 -3.1991790e-10 -2.5436093e-04 -1.9714674e-04
  1.4418215e-01  5.1526362e-03 -1.5722629e-10 -9.7300190e-10
 -6.1223204e-10 -1.8893141e-10 -4.9447713e-10 -1.9952585e-10
  5.0565923e-11 -3.3566333e-10] 6 0
train:	 Loss = 1.5037,	 Acc = 0.4321
2926 0.252
5678 0.464
2913 0.537
376 0.537
69 0.42
6 0.333
0 0.0
0 0.0
0.4904888299048883
0.499618320610687
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4967,	 Acc = 0.4433
380 0.337
1718 0.46
815 0.466
85 0.353
2 0.5
0 0.0
0 0.0
0 0.0
0.4587786259541985
0.499618320610687
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5536,	 Acc1 = 0.3998,	 Acc2 = 0.4257

 ===== Epoch 204	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 1
train:	 Loss = 1.5045,	 Acc = 0.4305
2926 0.245
5679 0.459
2911 0.545
377 0.568
69 0.42
6 0.167
0 0.0
0 0.0
0.49059942490599423
0.499618320610687
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4628,	 Acc = 0.4640
380 0.339
1718 0.497
815 0.463
85 0.365
2 0.5
0 0.0
0 0.0
0 0.0
0.4820610687022901
0.499618320610687
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5276,	 Acc1 = 0.4180,	 Acc2 = 0.4476

 ===== Epoch 205	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.7217026   1.5934107
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.00404574e-02
  5.05659230e-11 -3.35663330e-10] 0 0
train:	 Loss = 1.4994,	 Acc = 0.4344
2922 0.253
5682 0.465
2914 0.542
375 0.549
69 0.464
6 0.167
0 0.0
0 0.0
0.49303559584346673
0.499618320610687
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4682,	 Acc = 0.4567
380 0.334
1718 0.482
815 0.469
85 0.376
2 0.5
0 0.0
0 0.0
0 0.0
0.47442748091603054
0.499618320610687
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5357,	 Acc1 = 0.4128,	 Acc2 = 0.4414

 ===== Epoch 206	 =====
[ 2.3047001   2.1252165  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.1879728   2.357333  ] [ 2.02919990e-02 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 0 0
train:	 Loss = 1.5000,	 Acc = 0.4322
2926 0.251
5680 0.46
2912 0.546
376 0.551
68 0.412
6 0.333
0 0.0
0 0.0
0.4909312099093121
0.499618320610687
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4905,	 Acc = 0.4440
380 0.337
1718 0.477
815 0.439
85 0.294
2 0.5
0 0.0
0 0.0
0 0.0
0.4595419847328244
0.499618320610687
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5748,	 Acc1 = 0.3844,	 Acc2 = 0.4071

 ===== Epoch 207	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.0848036   2.7955987 ] [ 3.62983632e+00  3.59436107e+00 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
 -1.00328599e-03  4.22277749e-02] 3 4
train:	 Loss = 1.5031,	 Acc = 0.4307
2922 0.253
5683 0.457
2913 0.542
375 0.565
69 0.391
6 0.167
0 0.0
0 0.0
0.4881715675436657
0.499618320610687
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4646,	 Acc = 0.4613
380 0.345
1718 0.485
815 0.477
85 0.353
2 0.5
0 0.0
0 0.0
0 0.0
0.4782442748091603
0.499618320610687
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5290,	 Acc1 = 0.4149,	 Acc2 = 0.4438

 ===== Epoch 208	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.2713953e-11 -3.1991790e-10 -2.5436093e-04 -1.9714674e-04
  2.1614847e+00  1.5171191e+00 -1.5722629e-10 -9.7300190e-10
 -6.1223204e-10 -1.8893141e-10 -4.9447713e-10 -1.9952585e-10
  5.0565923e-11 -3.3566333e-10] 6 6
train:	 Loss = 1.5007,	 Acc = 0.4317
2921 0.253
5683 0.462
2915 0.538
374 0.529
69 0.435
6 0.333
0 0.0
0 0.0
0.4892229468332044
0.499618320610687
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4700,	 Acc = 0.4567
380 0.334
1718 0.487
815 0.461
85 0.353
2 0.5
0 0.0
0 0.0
0 0.0
0.47442748091603054
0.499618320610687
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5356,	 Acc1 = 0.4149,	 Acc2 = 0.4438

 ===== Epoch 209	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   2.7796555   2.1725175
 -0.3640846  -0.3725409   2.4010212   1.1500181  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.2713953e-11 -3.1991790e-10 -2.5436093e-04 -1.9714674e-04
 -5.9007496e-01  5.0511628e-02 -1.5722629e-10 -9.7300190e-10
  3.6212143e-02  2.8521983e-02 -4.9447713e-10 -1.9952585e-10
  5.0565923e-11 -3.3566333e-10] 3 4
train:	 Loss = 1.5037,	 Acc = 0.4309
2919 0.251
5682 0.46
2915 0.541
377 0.549
69 0.42
6 0.333
0 0.0
0 0.0
0.48900430986849375
0.499618320610687
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.5059,	 Acc = 0.4423
380 0.342
1718 0.477
815 0.432
85 0.294
2 0.5
0 0.0
0 0.0
0 0.0
0.4568702290076336
0.499618320610687
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5763,	 Acc1 = 0.4021,	 Acc2 = 0.4284

 ===== Epoch 210	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   1.7056087   0.9211688
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.2713953e-11 -3.1991790e-10 -2.5436093e-04 -1.9714674e-04
  2.9681849e-01  2.1934786e-01 -1.5722629e-10 -9.7300190e-10
 -6.1223204e-10 -1.8893141e-10 -4.9447713e-10 -1.9952585e-10
  5.0565923e-11 -3.3566333e-10] 6 6
train:	 Loss = 1.5011,	 Acc = 0.4301
2924 0.25
5683 0.458
2910 0.543
377 0.549
69 0.42
5 0.2
0 0.0
0 0.0
0.48850066342326404
0.499618320610687
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4993,	 Acc = 0.4403
380 0.342
1718 0.465
815 0.44
85 0.376
2 0.5
0 0.0
0 0.0
0 0.0
0.4545801526717557
0.499618320610687
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5456,	 Acc1 = 0.4124,	 Acc2 = 0.4409

 ===== Epoch 211	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.9867626   1.8595908
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -8.60931873e-02  2.15869829e-01
  5.05659230e-11 -3.35663330e-10] 2 6
train:	 Loss = 1.5065,	 Acc = 0.4278
2923 0.252
5680 0.453
2914 0.538
376 0.556
69 0.449
6 0.333
0 0.0
0 0.0
0.48468767274737423
0.499618320610687
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4960,	 Acc = 0.4267
380 0.339
1718 0.454
815 0.417
85 0.353
2 0.5
0 0.0
0 0.0
0 0.0
0.43931297709923667
0.499618320610687
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5409,	 Acc1 = 0.4044,	 Acc2 = 0.4312

 ===== Epoch 212	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   0.8762474   3.0661654  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -3.50856520e-02 -1.37856245e-01 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 1 5
train:	 Loss = 1.5036,	 Acc = 0.4286
2925 0.251
5682 0.455
2911 0.538
375 0.557
69 0.42
6 0.333
0 0.0
0 0.0
0.4860112794426628
0.499618320610687
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4944,	 Acc = 0.4170
380 0.342
1718 0.449
815 0.396
85 0.294
2 0.5
0 0.0
0 0.0
0 0.0
0.42786259541984734
0.499618320610687
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5603,	 Acc1 = 0.3974,	 Acc2 = 0.4227

 ===== Epoch 213	 =====
[-0.36602148 -0.3783333   2.9384625   2.2573848  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10  2.47575417e-01  4.75711711e-02
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 2
train:	 Loss = 1.5010,	 Acc = 0.4322
2923 0.251
5679 0.462
2914 0.543
377 0.533
69 0.435
6 0.167
0 0.0
0 0.0
0.490547263681592
0.499618320610687
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4823,	 Acc = 0.4557
380 0.347
1718 0.481
815 0.469
85 0.306
2 0.5
0 0.0
0 0.0
0 0.0
0.4713740458015267
0.499618320610687
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5440,	 Acc1 = 0.4075,	 Acc2 = 0.4349

 ===== Epoch 214	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.4137096   1.8891662
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10  6.50372267e-01 -2.00809147e-02
  5.05659230e-11 -3.35663330e-10] 3 3
train:	 Loss = 1.4992,	 Acc = 0.4345
2923 0.25
5679 0.466
2914 0.546
377 0.533
69 0.464
6 0.333
0 0.0
0 0.0
0.494195688225539
0.499618320610687
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4627,	 Acc = 0.4670
380 0.345
1718 0.486
815 0.494
85 0.365
2 0.5
0 0.0
0 0.0
0 0.0
0.4847328244274809
0.499618320610687
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5306,	 Acc1 = 0.4188,	 Acc2 = 0.4486

 ===== Epoch 215	 =====
[-0.36602148 -0.3783333   0.13722287  1.2040142  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 4.05211973e+00  3.64699173e+00 -1.39022005e+00 -1.70728612e+00
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 0 1
train:	 Loss = 1.4995,	 Acc = 0.4306
2926 0.251
5674 0.458
2917 0.541
376 0.564
69 0.449
6 0.333
0 0.0
0 0.0
0.488940499889405
0.499618320610687
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 3
val:	 Loss = 1.4861,	 Acc = 0.4523
380 0.345
1718 0.449
815 0.518
85 0.365
2 0.5
0 0.0
0 0.0
0 0.0
0.4679389312977099
0.499618320610687
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5727,	 Acc1 = 0.3924,	 Acc2 = 0.4167

 ===== Epoch 216	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.878092    2.0578797  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -3.73662293e-01  5.10031164e-01
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 6 6
train:	 Loss = 1.5014,	 Acc = 0.4315
2924 0.248
5685 0.462
2910 0.546
374 0.527
69 0.42
6 0.0
0 0.0
0 0.0
0.4907120743034056
0.499618320610687
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 3
val:	 Loss = 1.4704,	 Acc = 0.4807
380 0.339
1718 0.506
815 0.507
85 0.353
2 0.5
0 0.0
0 0.0
0 0.0
0.501145038167939
0.501145038167939
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5431,	 Acc1 = 0.4106,	 Acc2 = 0.4386

 ===== Epoch 217	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   2.8946233   2.963779
 -0.3640846  -0.3725409   1.8117089   2.3096604  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.2713953e-11 -3.1991790e-10 -2.5436093e-04 -1.9714674e-04
 -6.7587185e-01  2.6326920e-03 -1.5722629e-10 -9.7300190e-10
  2.3086420e-01  9.5073283e-03 -4.9447713e-10 -1.9952585e-10
  5.0565923e-11 -3.3566333e-10] 0 0
train:	 Loss = 1.5030,	 Acc = 0.4297
2918 0.25
5684 0.462
2916 0.531
375 0.56
69 0.377
6 0.167
0 0.0
0 0.0
0.4876243093922652
0.501145038167939
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4673,	 Acc = 0.4577
380 0.337
1718 0.489
815 0.461
85 0.329
2 0.5
0 0.0
0 0.0
0 0.0
0.4751908396946565
0.501145038167939
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5345,	 Acc1 = 0.4118,	 Acc2 = 0.4401

 ===== Epoch 218	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   0.46426654  2.6170464
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.2713953e-11 -3.1991790e-10 -2.5436093e-04 -1.9714674e-04
  5.5393785e-02 -2.7456114e-01 -1.5722629e-10 -9.7300190e-10
 -6.1223204e-10 -1.8893141e-10 -4.9447713e-10 -1.9952585e-10
  5.0565923e-11 -3.3566333e-10] 5 5
train:	 Loss = 1.5045,	 Acc = 0.4333
2921 0.252
5679 0.462
2917 0.548
377 0.523
68 0.456
6 0.333
0 0.0
0 0.0
0.4919862937990494
0.501145038167939
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4809,	 Acc = 0.4660
380 0.337
1718 0.491
815 0.481
85 0.388
2 0.5
0 0.0
0 0.0
0 0.0
0.4847328244274809
0.501145038167939
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5484,	 Acc1 = 0.4130,	 Acc2 = 0.4416

 ===== Epoch 219	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.105526    2.8848264 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
 -8.13873112e-02 -1.74189568e-01] 3 5
train:	 Loss = 1.5050,	 Acc = 0.4283
2924 0.251
5682 0.458
2910 0.535
377 0.546
69 0.362
6 0.333
0 0.0
0 0.0
0.48551525873507295
0.501145038167939
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4703,	 Acc = 0.4620
380 0.337
1718 0.486
815 0.486
85 0.306
2 0.5
0 0.0
0 0.0
0 0.0
0.4801526717557252
0.501145038167939
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5518,	 Acc1 = 0.4011,	 Acc2 = 0.4272

 ===== Epoch 220	 =====
[ 1.6107283   2.0366175  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.0892698   2.357333  ] [-2.19375420e+00 -2.95350599e+00 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
 -3.51678170e-02  1.31961793e-01] 4 4
train:	 Loss = 1.5032,	 Acc = 0.4310
2924 0.249
5679 0.46
2914 0.541
376 0.548
69 0.464
6 0.167
0 0.0
0 0.0
0.4897169394073419
0.501145038167939
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 3
val:	 Loss = 1.4555,	 Acc = 0.4630
380 0.339
1718 0.479
815 0.498
85 0.353
2 0.5
0 0.0
0 0.0
0 0.0
0.48091603053435117
0.501145038167939
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5208,	 Acc1 = 0.4178,	 Acc2 = 0.4473

 ===== Epoch 221	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 0 1
train:	 Loss = 1.5040,	 Acc = 0.4289
2925 0.252
5681 0.457
2910 0.539
377 0.517
69 0.449
6 0.5
0 0.0
0 0.0
0.4861218622138671
0.501145038167939
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4641,	 Acc = 0.4680
380 0.339
1718 0.495
815 0.485
85 0.329
2 0.5
0 0.0
0 0.0
0 0.0
0.4866412213740458
0.501145038167939
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5372,	 Acc1 = 0.4174,	 Acc2 = 0.4468

 ===== Epoch 222	 =====
[-0.36602148 -0.3783333   1.858501    1.9606287  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -3.56586166e-02  7.01982677e-02
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 2 4
train:	 Loss = 1.5009,	 Acc = 0.4322
2920 0.251
5683 0.464
2913 0.539
377 0.531
69 0.406
6 0.333
0 0.0
0 0.0
0.490605658709107
0.501145038167939
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 3
val:	 Loss = 1.4639,	 Acc = 0.4717
380 0.334
1718 0.483
815 0.525
85 0.353
2 0.5
0 0.0
0 0.0
0 0.0
0.4916030534351145
0.501145038167939
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5402,	 Acc1 = 0.4071,	 Acc2 = 0.4344

 ===== Epoch 223	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.6988472   1.4849669
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10  3.46241534e-01  3.71496916e-01
  5.05659230e-11 -3.35663330e-10] 2 6
train:	 Loss = 1.4999,	 Acc = 0.4326
2927 0.25
5678 0.463
2912 0.543
377 0.544
68 0.426
6 0.333
0 0.0
0 0.0
0.49175976108837516
0.501145038167939
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4600,	 Acc = 0.4743
380 0.337
1718 0.504
815 0.492
85 0.318
2 0.5
0 0.0
0 0.0
0 0.0
0.49427480916030536
0.501145038167939
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5351,	 Acc1 = 0.4044,	 Acc2 = 0.4312

 ===== Epoch 224	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.7659011   1.4997545
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -2.38625750e-01  3.71496916e-01
  5.05659230e-11 -3.35663330e-10] 6 6
train:	 Loss = 1.4988,	 Acc = 0.4306
2925 0.253
5683 0.459
2910 0.537
376 0.559
68 0.426
6 0.667
0 0.0
0 0.0
0.487891186553135
0.501145038167939
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 3
val:	 Loss = 1.4781,	 Acc = 0.4513
380 0.184
1718 0.49
815 0.506
85 0.365
2 0.0
0 0.0
0 0.0
0 0.0
0.4900763358778626
0.501145038167939
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5609,	 Acc1 = 0.3699,	 Acc2 = 0.4235

 ===== Epoch 225	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.2227771   2.2909012
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -5.75511932e-01 -4.61861014e-01
  5.05659230e-11 -3.35663330e-10] 5 5
train:	 Loss = 1.5041,	 Acc = 0.4248
2924 0.247
5681 0.45
2911 0.539
377 0.541
69 0.406
6 0.5
0 0.0
0 0.0
0.48230871295886774
0.501145038167939
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 2
val:	 Loss = 1.4745,	 Acc = 0.4463
380 0.342
1718 0.482
815 0.433
85 0.318
2 0.5
0 0.0
0 0.0
0 0.0
0.4614503816793893
0.501145038167939
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5490,	 Acc1 = 0.4089,	 Acc2 = 0.4366

 ===== Epoch 226	 =====
[ 1.4319123   1.449334   -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-2.63070911e-01  1.20740816e-01 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 2 4
train:	 Loss = 1.5028,	 Acc = 0.4297
2923 0.25
5679 0.462
2915 0.534
376 0.545
69 0.406
6 0.333
0 0.0
0 0.0
0.487893864013267
0.501145038167939
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4735,	 Acc = 0.4460
380 0.337
1718 0.482
815 0.433
85 0.329
2 0.5
0 0.0
0 0.0
0 0.0
0.4618320610687023
0.501145038167939
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5433,	 Acc1 = 0.4137,	 Acc2 = 0.4423

 ===== Epoch 227	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.0477548   2.0961952
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10  2.62019962e-01  1.05424799e-01
  5.05659230e-11 -3.35663330e-10] 2 4
train:	 Loss = 1.5001,	 Acc = 0.4302
2924 0.251
5677 0.458
2916 0.541
376 0.543
69 0.449
6 0.333
0 0.0
0 0.0
0.4880583812472357
0.501145038167939
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 3
val:	 Loss = 1.4482,	 Acc = 0.4647
380 0.339
1718 0.483
815 0.493
85 0.376
2 0.5
0 0.0
0 0.0
0 0.0
0.48282442748091603
0.501145038167939
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5205,	 Acc1 = 0.4145,	 Acc2 = 0.4433

 ===== Epoch 228	 =====
[-0.36602148 -0.3783333   1.3134426   1.7363627  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10  1.58561349e-01 -1.00762025e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 2 2
train:	 Loss = 1.4998,	 Acc = 0.4322
2925 0.251
5676 0.462
2915 0.542
377 0.549
69 0.435
6 0.167
0 0.0
0 0.0
0.4908769213756497
0.501145038167939
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4649,	 Acc = 0.4767
380 0.337
1718 0.509
815 0.483
85 0.376
2 0.5
0 0.0
0 0.0
0 0.0
0.49694656488549616
0.501145038167939
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5360,	 Acc1 = 0.4147,	 Acc2 = 0.4436

 ===== Epoch 229	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 6 1
train:	 Loss = 1.4988,	 Acc = 0.4302
2920 0.25
5685 0.457
2912 0.544
377 0.552
69 0.435
5 0.2
0 0.0
0 0.0
0.48828470380194516
0.501145038167939
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 2
val:	 Loss = 1.4921,	 Acc = 0.4223
380 0.345
1718 0.451
815 0.411
85 0.306
2 0.5
0 0.0
0 0.0
0 0.0
0.433587786259542
0.501145038167939
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5647,	 Acc1 = 0.3813,	 Acc2 = 0.4033

 ===== Epoch 230	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.4745766   3.2989352
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10  1.87165348e-03 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 0 0
train:	 Loss = 1.4990,	 Acc = 0.4332
2921 0.252
5682 0.463
2914 0.544
376 0.54
69 0.391
6 0.333
0 0.0
0 0.0
0.4918757599204156
0.501145038167939
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4831,	 Acc = 0.4507
380 0.337
1718 0.488
815 0.429
85 0.4
2 0.5
0 0.0
0 0.0
0 0.0
0.467175572519084
0.501145038167939
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5485,	 Acc1 = 0.4081,	 Acc2 = 0.4356

 ===== Epoch 231	 =====
[-0.36602148 -0.3783333   1.5579473   2.7602844  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -5.89237325e-02 -6.05360754e-02
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 5 5
train:	 Loss = 1.5011,	 Acc = 0.4317
2922 0.25
5683 0.464
2912 0.534
376 0.559
69 0.464
6 0.167
0 0.0
0 0.0
0.49049303559584345
0.501145038167939
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4856,	 Acc = 0.4600
380 0.339
1718 0.497
815 0.45
85 0.353
2 0.5
0 0.0
0 0.0
0 0.0
0.47748091603053433
0.501145038167939
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5427,	 Acc1 = 0.4122,	 Acc2 = 0.4406

 ===== Epoch 232	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   3.4988701   1.9902607
 -0.3640846  -0.3725409   2.7586138   1.7671669  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.2713953e-11 -3.1991790e-10 -2.5436093e-04 -1.9714674e-04
 -5.7338726e-02 -2.4071960e-03 -1.5722629e-10 -9.7300190e-10
 -3.9383107e-01 -3.3275649e-02 -4.9447713e-10 -1.9952585e-10
  5.0565923e-11 -3.3566333e-10] 3 3
train:	 Loss = 1.5009,	 Acc = 0.4322
2925 0.252
5679 0.462
2913 0.539
376 0.561
69 0.435
6 0.167
0 0.0
0 0.0
0.49065575583324117
0.501145038167939
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4925,	 Acc = 0.4417
380 0.342
1718 0.468
815 0.443
85 0.341
2 0.5
0 0.0
0 0.0
0 0.0
0.45610687022900764
0.501145038167939
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5415,	 Acc1 = 0.4118,	 Acc2 = 0.4401

 ===== Epoch 233	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.577999    1.5268656
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -1.44111574e-01 -3.31335068e-01
  5.05659230e-11 -3.35663330e-10] 6 5
train:	 Loss = 1.5038,	 Acc = 0.4308
2923 0.252
5682 0.46
2911 0.539
377 0.544
69 0.406
6 0.5
0 0.0
0 0.0
0.4884466556108347
0.501145038167939
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4851,	 Acc = 0.4440
380 0.337
1718 0.484
815 0.418
85 0.353
2 0.5
0 0.0
0 0.0
0 0.0
0.4595419847328244
0.501145038167939
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5434,	 Acc1 = 0.4099,	 Acc2 = 0.4379

 ===== Epoch 234	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   1.5314387   1.1878861
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.2713953e-11 -3.1991790e-10 -2.5436093e-04 -1.9714674e-04
  1.0726974e-01 -4.7766186e-02 -1.5722629e-10 -9.7300190e-10
 -6.1223204e-10 -1.8893141e-10 -4.9447713e-10 -1.9952585e-10
  5.0565923e-11 -3.3566333e-10] 2 2
train:	 Loss = 1.5018,	 Acc = 0.4330
2927 0.251
5679 0.46
2911 0.547
376 0.551
69 0.464
6 0.167
0 0.0
0 0.0
0.49198097555580134
0.501145038167939
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4622,	 Acc = 0.4660
380 0.337
1718 0.492
815 0.485
85 0.329
2 0.5
0 0.0
0 0.0
0 0.0
0.4847328244274809
0.501145038167939
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5224,	 Acc1 = 0.4194,	 Acc2 = 0.4493

 ===== Epoch 235	 =====
[ 0.17484504  0.7937026  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-7.23910183e-02  2.84824491e-01 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
train:	 Loss = 1.5062,	 Acc = 0.4295
2926 0.252
5679 0.458
2912 0.538
376 0.551
69 0.406
6 0.167
0 0.0
0 0.0
0.4869497898694979
0.501145038167939
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.5092,	 Acc = 0.4360
380 0.342
1718 0.472
815 0.418
85 0.294
2 0.5
0 0.0
0 0.0
0 0.0
0.449618320610687
0.501145038167939
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5560,	 Acc1 = 0.4104,	 Acc2 = 0.4384

 ===== Epoch 236	 =====
[-0.36602148 -0.3783333   1.8398169   1.6684033  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10  5.21706700e-01 -9.57337841e-02
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 2 2
train:	 Loss = 1.5026,	 Acc = 0.4305
2922 0.25
5686 0.459
2908 0.542
377 0.557
69 0.406
6 0.0
0 0.0
0 0.0
0.4888348441300022
0.501145038167939
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4712,	 Acc = 0.4747
380 0.339
1718 0.495
815 0.508
85 0.353
2 0.5
0 0.0
0 0.0
0 0.0
0.49427480916030536
0.501145038167939
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5371,	 Acc1 = 0.4104,	 Acc2 = 0.4384

 ===== Epoch 237	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.4056292   1.7289653
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -1.61893725e-01 -3.01213712e-02
  5.05659230e-11 -3.35663330e-10] 1 3
train:	 Loss = 1.5003,	 Acc = 0.4301
2924 0.251
5680 0.455
2912 0.543
377 0.568
69 0.493
6 0.333
0 0.0
0 0.0
0.4880583812472357
0.501145038167939
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 3
val:	 Loss = 1.4528,	 Acc = 0.4577
380 0.337
1718 0.474
815 0.487
85 0.376
2 0.5
0 0.0
0 0.0
0 0.0
0.4751908396946565
0.501145038167939
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5342,	 Acc1 = 0.4116,	 Acc2 = 0.4399

 ===== Epoch 238	 =====
[-0.36602148 -0.3783333   1.1944402   1.06583    -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10  1.99018896e-01 -1.20875008e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 6 5
train:	 Loss = 1.5026,	 Acc = 0.4329
2921 0.249
5681 0.461
2914 0.543
377 0.584
69 0.42
6 0.333
0 0.0
0 0.0
0.492207361556317
0.501145038167939
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 2
val:	 Loss = 1.4908,	 Acc = 0.4437
380 0.337
1718 0.481
815 0.425
85 0.353
2 0.5
0 0.0
0 0.0
0 0.0
0.45916030534351143
0.501145038167939
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5597,	 Acc1 = 0.3957,	 Acc2 = 0.4207

 ===== Epoch 239	 =====
[-0.36602148 -0.3783333   2.2260695   0.99333996 -0.4409929  -0.3635196
  2.693854    3.3581967  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10  3.61599289e-02  1.58192530e-01
  5.23268769e-04  1.12748065e-04  4.52390850e-01  6.52839839e-01
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 6 6
train:	 Loss = 1.5019,	 Acc = 0.4292
2927 0.251
5675 0.457
2915 0.537
376 0.561
69 0.435
6 0.333
0 0.0
0 0.0
0.48689304280499945
0.501145038167939
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4563,	 Acc = 0.4687
380 0.337
1718 0.501
815 0.477
85 0.318
2 0.5
0 0.0
0 0.0
0 0.0
0.48778625954198473
0.501145038167939
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5278,	 Acc1 = 0.4132,	 Acc2 = 0.4418

 ===== Epoch 240	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   0.76124525  1.6302992  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -1.27881229e-01 -3.37510139e-01 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 5 5
train:	 Loss = 1.4988,	 Acc = 0.4318
2923 0.252
5683 0.466
2912 0.533
376 0.545
68 0.412
6 0.167
0 0.0
0 0.0
0.4899944720840243
0.501145038167939
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.5076,	 Acc = 0.4497
380 0.334
1718 0.489
815 0.431
85 0.353
2 0.5
0 0.0
0 0.0
0 0.0
0.466412213740458
0.501145038167939
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5491,	 Acc1 = 0.4151,	 Acc2 = 0.4441

 ===== Epoch 241	 =====
[ 2.7904587   2.0543375  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.2647454   2.5908997 ] [-1.05382013e+00  2.69344866e-01 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  4.03926700e-01  4.27556217e-01] 6 6
train:	 Loss = 1.5032,	 Acc = 0.4286
2924 0.25
5679 0.454
2914 0.54
376 0.582
69 0.42
6 0.333
0 0.0
0 0.0
0.4863998230871296
0.501145038167939
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 3
val:	 Loss = 1.4759,	 Acc = 0.4577
380 0.339
1718 0.482
815 0.476
85 0.318
2 0.5
0 0.0
0 0.0
0 0.0
0.47480916030534354
0.501145038167939
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5462,	 Acc1 = 0.4031,	 Acc2 = 0.4297

 ===== Epoch 242	 =====
[ 1.6980505   1.7936037  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.3842275   2.2891002 ] [ 1.21233118e+00  3.18879575e-01 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
 -2.72300541e-01  6.07024252e-01] 6 6
train:	 Loss = 1.5019,	 Acc = 0.4271
2925 0.249
5675 0.456
2916 0.535
377 0.538
69 0.464
6 0.333
0 0.0
0 0.0
0.48479486895941615
0.501145038167939
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4810,	 Acc = 0.4520
380 0.339
1718 0.486
815 0.445
85 0.329
2 0.5
0 0.0
0 0.0
0 0.0
0.4683206106870229
0.501145038167939
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5443,	 Acc1 = 0.4102,	 Acc2 = 0.4381

 ===== Epoch 243	 =====
[ 2.185234    1.2898561  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 5.09856343e-01  2.81728566e-01 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04  2.56199908e+00  3.59061933e+00
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 6 6
train:	 Loss = 1.5042,	 Acc = 0.4291
2919 0.25
5685 0.459
2912 0.534
377 0.568
69 0.406
6 0.167
0 0.0
0 0.0
0.4869046303458946
0.501145038167939
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 3
val:	 Loss = 1.4660,	 Acc = 0.4607
380 0.342
1718 0.483
815 0.485
85 0.318
2 0.5
0 0.0
0 0.0
0 0.0
0.4778625954198473
0.501145038167939
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5487,	 Acc1 = 0.4091,	 Acc2 = 0.4369

 ===== Epoch 244	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   1.5954367   2.01104    -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
  3.28186713e-02 -4.89627361e-01 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 5 5
train:	 Loss = 1.5014,	 Acc = 0.4315
2924 0.25
5681 0.46
2912 0.543
376 0.551
69 0.391
6 0.167
0 0.0
0 0.0
0.4900486510393631
0.501145038167939
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4885,	 Acc = 0.4300
380 0.342
1718 0.455
815 0.427
85 0.353
2 0.5
0 0.0
0 0.0
0 0.0
0.44274809160305345
0.501145038167939
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5407,	 Acc1 = 0.4021,	 Acc2 = 0.4284

 ===== Epoch 245	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 3 1
train:	 Loss = 1.5028,	 Acc = 0.4285
2927 0.249
5677 0.456
2913 0.54
376 0.551
69 0.406
6 0.333
0 0.0
0 0.0
0.4865612211038602
0.501145038167939
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4719,	 Acc = 0.4440
380 0.339
1718 0.471
815 0.449
85 0.318
2 0.5
0 0.0
0 0.0
0 0.0
0.45916030534351143
0.501145038167939
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5305,	 Acc1 = 0.4139,	 Acc2 = 0.4426

 ===== Epoch 246	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 6 1
train:	 Loss = 1.5021,	 Acc = 0.4327
2926 0.249
5680 0.462
2910 0.544
377 0.57
69 0.391
6 0.167
0 0.0
0 0.0
0.4920371599203716
0.501145038167939
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4746,	 Acc = 0.4700
380 0.334
1718 0.495
815 0.497
85 0.318
2 0.5
0 0.0
0 0.0
0 0.0
0.4896946564885496
0.501145038167939
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5283,	 Acc1 = 0.4182,	 Acc2 = 0.4478

 ===== Epoch 247	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  1.9299445   2.479227   -0.40141198 -0.40778467  2.6519973   2.852837
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -9.99801513e-03  5.03230751e-01
 -6.12232043e-10 -1.88931412e-10  3.74041474e-03  3.01213712e-01
  5.05659230e-11 -3.35663330e-10] 6 6
train:	 Loss = 1.5021,	 Acc = 0.4304
2925 0.249
5682 0.457
2910 0.548
376 0.545
69 0.377
6 0.333
0 0.0
0 0.0
0.4889970142651775
0.501145038167939
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4760,	 Acc = 0.4520
380 0.334
1718 0.48
815 0.463
85 0.306
2 0.5
0 0.0
0 0.0
0 0.0
0.46908396946564884
0.501145038167939
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5401,	 Acc1 = 0.4044,	 Acc2 = 0.4312

 ===== Epoch 248	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.3003119   2.0148623
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10  1.12299230e-02 -9.53843370e-02
  5.05659230e-11 -3.35663330e-10] 3 2
train:	 Loss = 1.4982,	 Acc = 0.4293
2926 0.247
5676 0.457
2914 0.538
377 0.584
69 0.406
6 0.333
0 0.0
0 0.0
0.4882769298827693
0.501145038167939
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4780,	 Acc = 0.4543
380 0.337
1718 0.498
815 0.429
85 0.329
2 0.5
0 0.0
0 0.0
0 0.0
0.4713740458015267
0.501145038167939
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5516,	 Acc1 = 0.4126,	 Acc2 = 0.4411

 ===== Epoch 249	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   2.307946    1.4434901
 -0.3640846  -0.3725409   0.8785344   1.7472588  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
 -1.01236366e-01  4.29517999e-02 -1.57226288e-10 -9.73001901e-10
  1.13348733e-03  1.85392886e-01 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 4
train:	 Loss = 1.5000,	 Acc = 0.4287
2924 0.252
5682 0.458
2911 0.532
377 0.573
69 0.391
5 0.0
0 0.0
0 0.0
0.48595754091110127
0.501145038167939
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.5003,	 Acc = 0.4437
380 0.342
1718 0.484
815 0.42
85 0.306
2 0.5
0 0.0
0 0.0
0 0.0
0.4583969465648855
0.501145038167939
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5536,	 Acc1 = 0.4075,	 Acc2 = 0.4349

 ===== Epoch 250	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 1
train:	 Loss = 1.4992,	 Acc = 0.4317
2924 0.251
5681 0.459
2911 0.541
377 0.568
69 0.464
6 0.167
0 0.0
0 0.0
0.4900486510393631
0.501145038167939
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4692,	 Acc = 0.4660
380 0.337
1718 0.492
815 0.485
85 0.329
2 0.5
0 0.0
0 0.0
0 0.0
0.4847328244274809
0.501145038167939
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5389,	 Acc1 = 0.4143,	 Acc2 = 0.4431

 ===== Epoch 251	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 6 1
train:	 Loss = 1.5021,	 Acc = 0.4295
2923 0.252
5679 0.457
2914 0.54
377 0.549
69 0.42
6 0.167
0 0.0
0 0.0
0.4868988391376451
0.501145038167939
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4788,	 Acc = 0.4633
380 0.339
1718 0.489
815 0.477
85 0.365
2 0.5
0 0.0
0 0.0
0 0.0
0.4812977099236641
0.501145038167939
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5350,	 Acc1 = 0.4182,	 Acc2 = 0.4478

 ===== Epoch 252	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 1 1
train:	 Loss = 1.4993,	 Acc = 0.4282
2924 0.25
5684 0.453
2909 0.542
376 0.574
69 0.377
6 0.0
0 0.0
0 0.0
0.48584697036709423
0.501145038167939
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4985,	 Acc = 0.4390
380 0.337
1718 0.478
815 0.417
85 0.306
2 0.5
0 0.0
0 0.0
0 0.0
0.4538167938931298
0.501145038167939
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5390,	 Acc1 = 0.4093,	 Acc2 = 0.4371

 ===== Epoch 253	 =====
[ 3.5794175   1.679691   -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.9436624   1.8639561 ] [-1.30871844e+00  6.19183630e-02 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
 -6.35033488e-01  1.21404849e-01] 3 4
train:	 Loss = 1.4991,	 Acc = 0.4292
2925 0.251
5679 0.455
2912 0.54
377 0.57
69 0.435
6 0.333
0 0.0
0 0.0
0.48700652438350106
0.501145038167939
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.5010,	 Acc = 0.4437
380 0.345
1718 0.484
815 0.421
85 0.294
2 0.5
0 0.0
0 0.0
0 0.0
0.4580152671755725
0.501145038167939
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5508,	 Acc1 = 0.4104,	 Acc2 = 0.4384

 ===== Epoch 254	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   2.3614192   2.3970046
 -0.3640846  -0.3725409   1.6211835   1.6203451  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.2713953e-11 -3.1991790e-10 -2.5436093e-04 -1.9714674e-04
  1.0427518e-01  1.3366978e-01 -1.5722629e-10 -9.7300190e-10
 -1.3580866e-02  3.1374180e-01 -4.9447713e-10 -1.9952585e-10
  5.0565923e-11 -3.3566333e-10] 4 4
train:	 Loss = 1.5029,	 Acc = 0.4301
2924 0.251
5681 0.461
2912 0.533
376 0.572
69 0.406
6 0.333
0 0.0
0 0.0
0.4880583812472357
0.501145038167939
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 3
val:	 Loss = 1.4758,	 Acc = 0.4470
380 0.342
1718 0.47
815 0.456
85 0.353
2 0.5
0 0.0
0 0.0
0 0.0
0.4622137404580153
0.501145038167939
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5331,	 Acc1 = 0.4135,	 Acc2 = 0.4421

 ===== Epoch 255	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   1.2527207   3.006441   -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
  1.49382517e-01 -1.90146565e-02 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 0 0
train:	 Loss = 1.4989,	 Acc = 0.4338
2924 0.25
5680 0.464
2914 0.543
375 0.576
69 0.435
6 0.167
0 0.0
0 0.0
0.49336576735957544
0.501145038167939
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 2
val:	 Loss = 1.4920,	 Acc = 0.4363
380 0.342
1718 0.477
815 0.406
85 0.318
2 0.5
0 0.0
0 0.0
0 0.0
0.45
0.501145038167939
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5528,	 Acc1 = 0.3961,	 Acc2 = 0.4212

 ===== Epoch 256	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   1.0887568   1.7035396
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.2713953e-11 -3.1991790e-10 -2.5436093e-04 -1.9714674e-04
  1.9606118e-01  2.7832132e-02 -1.5722629e-10 -9.7300190e-10
 -6.1223204e-10 -1.8893141e-10 -4.9447713e-10 -1.9952585e-10
  5.0565923e-11 -3.3566333e-10] 2 4
train:	 Loss = 1.4972,	 Acc = 0.4324
2927 0.251
5680 0.464
2910 0.537
376 0.569
69 0.406
6 0.333
0 0.0
0 0.0
0.49120672491980977
0.501145038167939
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4788,	 Acc = 0.4673
380 0.337
1718 0.491
815 0.493
85 0.318
2 0.5
0 0.0
0 0.0
0 0.0
0.4862595419847328
0.501145038167939
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5645,	 Acc1 = 0.4025,	 Acc2 = 0.4289

 ===== Epoch 257	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 5 1
train:	 Loss = 1.5045,	 Acc = 0.4280
2924 0.249
5681 0.456
2912 0.537
376 0.566
69 0.406
6 0.167
0 0.0
0 0.0
0.48584697036709423
0.501145038167939
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4765,	 Acc = 0.4597
380 0.345
1718 0.477
815 0.49
85 0.341
2 0.5
0 0.0
0 0.0
0 0.0
0.4763358778625954
0.501145038167939
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5383,	 Acc1 = 0.4178,	 Acc2 = 0.4473

 ===== Epoch 258	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6185868   1.4719001  -0.40141198 -0.40778467  1.9337245   3.26443
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04  6.72339439e-01  2.44814932e-01
 -6.12232043e-10 -1.88931412e-10  5.71768582e-01  8.03236589e-02
  5.05659230e-11 -3.35663330e-10] 2 2
train:	 Loss = 1.5025,	 Acc = 0.4307
2925 0.251
5680 0.463
2912 0.532
376 0.569
69 0.42
6 0.167
0 0.0
0 0.0
0.4889970142651775
0.501145038167939
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 2
val:	 Loss = 1.5155,	 Acc = 0.4297
380 0.339
1718 0.46
815 0.417
85 0.341
2 0.5
0 0.0
0 0.0
0 0.0
0.44274809160305345
0.501145038167939
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5781,	 Acc1 = 0.3961,	 Acc2 = 0.4212

 ===== Epoch 259	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 1 1
train:	 Loss = 1.5002,	 Acc = 0.4296
2925 0.25
5681 0.456
2912 0.541
375 0.571
69 0.449
6 0.167
0 0.0
0 0.0
0.4875594382395223
0.501145038167939
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4938,	 Acc = 0.4463
380 0.337
1718 0.489
815 0.423
85 0.294
2 0.5
0 0.0
0 0.0
0 0.0
0.4622137404580153
0.501145038167939
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5632,	 Acc1 = 0.3982,	 Acc2 = 0.4237

 ===== Epoch 260	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  3.2656631   1.6504843  -0.40141198 -0.40778467  3.5214243   2.5915859
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04  1.72458023e-01  2.51615375e-01
 -6.12232043e-10 -1.88931412e-10 -1.01907361e+00  1.60647318e-01
  5.05659230e-11 -3.35663330e-10] 4 2
train:	 Loss = 1.5008,	 Acc = 0.4322
2923 0.251
5681 0.459
2913 0.544
377 0.578
68 0.397
6 0.167
0 0.0
0 0.0
0.4906578220011056
0.501145038167939
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4803,	 Acc = 0.4620
380 0.339
1718 0.492
815 0.469
85 0.329
2 0.5
0 0.0
0 0.0
0 0.0
0.47977099236641224
0.501145038167939
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5467,	 Acc1 = 0.4174,	 Acc2 = 0.4468

 ===== Epoch 261	 =====
[-0.36602148 -0.3783333   2.9928868  -4.2395334   2.932054    1.0856445
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.2713953e-11 -3.1991790e-10  9.8878808e-02  7.9167728e+00
 -1.4829544e+00  5.0511628e-02 -1.5722629e-10 -9.7300190e-10
 -6.1223204e-10 -1.8893141e-10 -4.9447713e-10 -1.9952585e-10
  5.0565923e-11 -3.3566333e-10] 4 2
train:	 Loss = 1.5009,	 Acc = 0.4307
2920 0.25
5683 0.463
2914 0.532
377 0.57
68 0.426
6 0.167
0 0.0
0 0.0
0.4891688770999116
0.501145038167939
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 2
val:	 Loss = 1.5051,	 Acc = 0.4227
380 0.342
1718 0.446
815 0.422
85 0.318
2 0.5
0 0.0
0 0.0
0 0.0
0.43435114503816796
0.501145038167939
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5712,	 Acc1 = 0.3955,	 Acc2 = 0.4205

 ===== Epoch 262	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 3 1
train:	 Loss = 1.5008,	 Acc = 0.4308
2922 0.252
5681 0.459
2914 0.541
376 0.553
69 0.435
6 0.167
0 0.0
0 0.0
0.4886137519345567
0.501145038167939
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 3
val:	 Loss = 1.4580,	 Acc = 0.4717
380 0.337
1718 0.492
815 0.504
85 0.341
2 0.5
0 0.0
0 0.0
0 0.0
0.4912213740458015
0.501145038167939
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5362,	 Acc1 = 0.4042,	 Acc2 = 0.4309

 ===== Epoch 263	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409   1.2876205   2.3021948  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -4.59464967e-01  7.60586262e-02 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 1 1
train:	 Loss = 1.5026,	 Acc = 0.4309
2921 0.253
5682 0.457
2915 0.544
375 0.549
69 0.391
6 0.333
0 0.0
0 0.0
0.4884492096827678
0.501145038167939
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4922,	 Acc = 0.4523
380 0.337
1718 0.489
815 0.445
85 0.294
2 0.5
0 0.0
0 0.0
0 0.0
0.46908396946564884
0.501145038167939
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5544,	 Acc1 = 0.3963,	 Acc2 = 0.4215

 ===== Epoch 264	 =====
[-0.36602148 -0.3783333   2.755288    2.3547933  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.75685209e-01 -7.95236540e+00
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 2 1
train:	 Loss = 1.5015,	 Acc = 0.4324
2919 0.25
5681 0.46
2916 0.541
377 0.576
69 0.449
6 0.333
0 0.0
0 0.0
0.49110398939109295
0.501145038167939
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.5308,	 Acc = 0.4443
380 0.337
1718 0.485
815 0.422
85 0.306
2 0.5
0 0.0
0 0.0
0 0.0
0.4599236641221374
0.501145038167939
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5828,	 Acc1 = 0.3988,	 Acc2 = 0.4245

 ===== Epoch 265	 =====
[ 1.6319873   1.4189571  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-2.21734715e+00 -2.19810200e+00 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 5 1
train:	 Loss = 1.5006,	 Acc = 0.4286
2924 0.252
5681 0.453
2913 0.541
376 0.564
68 0.441
6 0.333
0 0.0
0 0.0
0.48573639982308714
0.501145038167939
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 2
val:	 Loss = 1.5047,	 Acc = 0.4517
380 0.337
1718 0.486
815 0.448
85 0.306
2 0.5
0 0.0
0 0.0
0 0.0
0.4683206106870229
0.501145038167939
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5744,	 Acc1 = 0.4071,	 Acc2 = 0.4344

 ===== Epoch 266	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 0 1
train:	 Loss = 1.5052,	 Acc = 0.4239
2922 0.248
5680 0.448
2914 0.538
377 0.546
69 0.362
6 0.333
0 0.0
0 0.0
0.4805438868007959
0.501145038167939
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4969,	 Acc = 0.4707
380 0.342
1718 0.512
815 0.458
85 0.329
2 0.5
0 0.0
0 0.0
0 0.0
0.48931297709923666
0.501145038167939
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5622,	 Acc1 = 0.4087,	 Acc2 = 0.4364

 ===== Epoch 267	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   3.6042883   2.3947818
 -0.3640846  -0.3725409   4.2856765   1.7447703  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
 -1.34441648e-02  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -7.95579076e-01  4.75366414e-03 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 0 0
train:	 Loss = 1.4986,	 Acc = 0.4279
2925 0.25
5681 0.455
2912 0.54
375 0.539
69 0.406
6 0.333
0 0.0
0 0.0
0.48534778281543733
0.501145038167939
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 2
val:	 Loss = 1.5199,	 Acc = 0.4200
380 0.339
1718 0.448
815 0.411
85 0.294
2 0.5
0 0.0
0 0.0
0 0.0
0.4316793893129771
0.501145038167939
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5670,	 Acc1 = 0.3978,	 Acc2 = 0.4232

 ===== Epoch 268	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.4084755   0.9349377 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
 -4.08860826e+00 -2.64451432e+00] 1 1
train:	 Loss = 1.5043,	 Acc = 0.4219
2922 0.239
5685 0.449
2911 0.538
375 0.552
69 0.391
6 0.167
0 0.0
0 0.0
0.48098607119168696
0.501145038167939
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 3
val:	 Loss = 1.4891,	 Acc = 0.4433
380 0.342
1718 0.464
815 0.459
85 0.329
2 0.5
0 0.0
0 0.0
0 0.0
0.4580152671755725
0.501145038167939
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5618,	 Acc1 = 0.4083,	 Acc2 = 0.4359

 ===== Epoch 269	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   2.012315    1.9858154
 -0.3640846  -0.3725409   1.3436917   0.8439322  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.2713953e-11 -3.1991790e-10 -2.5436093e-04 -1.9714674e-04
  2.2997899e-01 -7.8797522e+00 -1.5722629e-10 -9.7300190e-10
 -1.3580516e-01 -3.8029313e-02 -4.9447713e-10 -1.9952585e-10
  5.0565923e-11 -3.3566333e-10] 1 3
train:	 Loss = 1.5010,	 Acc = 0.4276
2922 0.251
5679 0.454
2916 0.536
376 0.561
69 0.42
6 0.333
0 0.0
0 0.0
0.4846340924165377
0.501145038167939
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4864,	 Acc = 0.4420
380 0.342
1718 0.476
815 0.433
85 0.282
2 0.5
0 0.0
0 0.0
0 0.0
0.45648854961832064
0.501145038167939
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5325,	 Acc1 = 0.4174,	 Acc2 = 0.4468

 ===== Epoch 270	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.5133436   1.26315
  0.47731173  3.572405  ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -8.01036060e-01 -2.35950738e-01
 -1.96647787e+00 -7.94937849e+00] 4 5
train:	 Loss = 1.4987,	 Acc = 0.4301
2922 0.251
5682 0.458
2912 0.538
377 0.56
69 0.42
6 0.167
0 0.0
0 0.0
0.4879504753482202
0.501145038167939
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4813,	 Acc = 0.4447
380 0.345
1718 0.478
815 0.433
85 0.318
2 0.5
0 0.0
0 0.0
0 0.0
0.45916030534351143
0.501145038167939
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5339,	 Acc1 = 0.4102,	 Acc2 = 0.4381

 ===== Epoch 271	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.0341163   1.4258157
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.29525763e-01  9.53843370e-02
  5.05659230e-11 -3.35663330e-10] 2 4
train:	 Loss = 1.4984,	 Acc = 0.4306
2920 0.252
5683 0.46
2914 0.538
377 0.549
69 0.406
5 0.2
0 0.0
0 0.0
0.4880636604774536
0.501145038167939
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.5496,	 Acc = 0.4307
380 0.342
1718 0.464
815 0.417
85 0.282
2 0.5
0 0.0
0 0.0
0 0.0
0.4435114503816794
0.501145038167939
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 4
Testing:	 Loss = 1.5735,	 Acc1 = 0.4095,	 Acc2 = 0.4374

 ===== Epoch 272	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   2.720835    1.9435852
 -0.3640846  -0.3725409   2.021115    1.0927825  -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.2713953e-11 -3.1991790e-10 -2.5436093e-04 -1.9714674e-04
 -1.1021390e-01 -4.7766186e-02 -1.5722629e-10 -9.7300190e-10
  4.8775664e-01 -1.2834893e-01 -4.9447713e-10 -1.9952585e-10
  5.0565923e-11 -3.3566333e-10] 3 2
train:	 Loss = 1.5016,	 Acc = 0.4296
2926 0.252
5680 0.458
2913 0.535
374 0.578
69 0.42
6 0.167
0 0.0
0 0.0
0.48728157487281576
0.501145038167939
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.5093,	 Acc = 0.4463
380 0.342
1718 0.473
815 0.45
85 0.341
2 0.5
0 0.0
0 0.0
0 0.0
0.4614503816793893
0.501145038167939
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5577,	 Acc1 = 0.4093,	 Acc2 = 0.4371

 ===== Epoch 273	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.3530962   2.1282353
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -3.82737309e-01  2.15869829e-01
  5.05659230e-11 -3.35663330e-10] 2 6
train:	 Loss = 1.4990,	 Acc = 0.4301
2927 0.251
5681 0.459
2910 0.537
375 0.56
69 0.42
6 0.167
0 0.0
0 0.0
0.4879991151421303
0.501145038167939
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4725,	 Acc = 0.4647
380 0.339
1718 0.491
815 0.479
85 0.353
2 0.5
0 0.0
0 0.0
0 0.0
0.48282442748091603
0.501145038167939
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5374,	 Acc1 = 0.4108,	 Acc2 = 0.4389

 ===== Epoch 274	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  2.425074    2.09866
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -9.63858366e-02 -1.25505716e-01
  5.05659230e-11 -3.35663330e-10] 3 3
train:	 Loss = 1.4983,	 Acc = 0.4322
2927 0.251
5679 0.46
2910 0.543
377 0.573
69 0.377
6 0.167
0 0.0
0 0.0
0.4907642959849574
0.501145038167939
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 3
val:	 Loss = 1.4896,	 Acc = 0.4383
380 0.339
1718 0.455
815 0.456
85 0.376
2 0.5
0 0.0
0 0.0
0 0.0
0.45267175572519086
0.501145038167939
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5337,	 Acc1 = 0.4089,	 Acc2 = 0.4366

 ===== Epoch 275	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.169993    1.1152723
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10  1.17908411e-01 -4.36759859e-01
  5.05659230e-11 -3.35663330e-10] 5 5
train:	 Loss = 1.4986,	 Acc = 0.4306
2923 0.251
5680 0.46
2915 0.539
375 0.549
69 0.42
6 0.333
0 0.0
0 0.0
0.4886677722498618
0.501145038167939
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.5212,	 Acc = 0.4447
380 0.342
1718 0.483
815 0.427
85 0.294
2 0.5
0 0.0
0 0.0
0 0.0
0.4595419847328244
0.501145038167939
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5733,	 Acc1 = 0.3998,	 Acc2 = 0.4257

 ===== Epoch 276	 =====
[-0.36602148 -0.3783333   1.8028573   3.0547748  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -1.58056900e-01 -1.51044458e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 5 5
train:	 Loss = 1.5013,	 Acc = 0.4302
2924 0.251
5681 0.461
2913 0.532
375 0.565
69 0.478
6 0.333
0 0.0
0 0.0
0.4882795223352499
0.501145038167939
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4916,	 Acc = 0.4610
380 0.339
1718 0.478
815 0.494
85 0.341
2 0.5
0 0.0
0 0.0
0 0.0
0.47862595419847326
0.501145038167939
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5498,	 Acc1 = 0.4112,	 Acc2 = 0.4394

 ===== Epoch 277	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 6 1
train:	 Loss = 1.4998,	 Acc = 0.4325
2925 0.252
5682 0.458
2909 0.545
377 0.586
69 0.435
6 0.333
0 0.0
0 0.0
0.49098750414685394
0.501145038167939
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.5112,	 Acc = 0.4350
380 0.342
1718 0.46
815 0.439
85 0.306
2 0.5
0 0.0
0 0.0
0 0.0
0.4484732824427481
0.501145038167939
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5610,	 Acc1 = 0.4122,	 Acc2 = 0.4406

 ===== Epoch 278	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 5 1
train:	 Loss = 1.5004,	 Acc = 0.4317
2925 0.25
5682 0.461
2913 0.539
374 0.58
68 0.426
6 0.167
0 0.0
0 0.0
0.4904345902908327
0.501145038167939
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4809,	 Acc = 0.4483
380 0.339
1718 0.477
815 0.449
85 0.341
2 0.5
0 0.0
0 0.0
0 0.0
0.46412213740458014
0.501145038167939
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5381,	 Acc1 = 0.4137,	 Acc2 = 0.4423

 ===== Epoch 279	 =====
[ 2.091876    2.089777   -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.6007043   2.3415868 ] [-6.92457557e-01  2.78632641e-01 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
 -1.82871401e-01  4.64505523e-01] 6 6
train:	 Loss = 1.5019,	 Acc = 0.4286
2919 0.251
5682 0.455
2916 0.538
376 0.566
69 0.391
6 0.167
0 0.0
0 0.0
0.4856890264117582
0.501145038167939
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4705,	 Acc = 0.4633
380 0.332
1718 0.493
815 0.477
85 0.318
2 0.5
0 0.0
0 0.0
0 0.0
0.48244274809160304
0.501145038167939
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5351,	 Acc1 = 0.4126,	 Acc2 = 0.4411

 ===== Epoch 280	 =====
[-0.36602148 -0.3783333   0.68431145  1.1179322  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10  2.90803257e-02  8.77971202e-02
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 4
train:	 Loss = 1.5023,	 Acc = 0.4270
2925 0.249
5685 0.454
2907 0.539
377 0.538
68 0.412
6 0.167
0 0.0
0 0.0
0.4844631206458034
0.501145038167939
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 3
val:	 Loss = 1.4671,	 Acc = 0.4557
380 0.337
1718 0.472
815 0.487
85 0.353
2 0.5
0 0.0
0 0.0
0 0.0
0.4729007633587786
0.501145038167939
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5348,	 Acc1 = 0.4048,	 Acc2 = 0.4317

 ===== Epoch 281	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.5567844   2.4067388
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10  1.87165348e-03 -1.70687780e-01
  5.05659230e-11 -3.35663330e-10] 5 5
train:	 Loss = 1.4999,	 Acc = 0.4309
2921 0.252
5685 0.457
2911 0.543
377 0.576
69 0.348
5 0.4
0 0.0
0 0.0
0.4886702774400354
0.501145038167939
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4986,	 Acc = 0.4697
380 0.342
1718 0.507
815 0.463
85 0.353
2 0.5
0 0.0
0 0.0
0 0.0
0.48816793893129773
0.501145038167939
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5525,	 Acc1 = 0.4155,	 Acc2 = 0.4446

 ===== Epoch 282	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  3.454395    1.3240099  -0.40141198 -0.40778467  2.979311    1.9729637
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -5.48619866e-01 -7.48045668e-02
 -6.12232043e-10 -1.88931412e-10  9.42335784e-01 -5.52225150e-02
  5.05659230e-11 -3.35663330e-10] 2 3
train:	 Loss = 1.5065,	 Acc = 0.4251
2920 0.246
5682 0.457
2914 0.53
377 0.536
69 0.362
6 0.333
0 0.0
0 0.0
0.48297966401414677
0.501145038167939
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4725,	 Acc = 0.4787
380 0.339
1718 0.51
815 0.494
85 0.318
2 0.5
0 0.0
0 0.0
0 0.0
0.4988549618320611
0.501145038167939
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5542,	 Acc1 = 0.3967,	 Acc2 = 0.4220

 ===== Epoch 283	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   1.8610632   1.9169135
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.2713953e-11 -3.1991790e-10 -2.5436093e-04 -1.9714674e-04
  6.4371303e-02  2.0170826e-01 -1.5722629e-10 -9.7300190e-10
 -6.1223204e-10 -1.8893141e-10 -4.9447713e-10 -1.9952585e-10
  5.0565923e-11 -3.3566333e-10] 6 6
train:	 Loss = 1.5001,	 Acc = 0.4317
2924 0.249
5686 0.462
2908 0.537
375 0.579
69 0.435
6 0.333
0 0.0
0 0.0
0.4907120743034056
0.501145038167939
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 3
val:	 Loss = 1.4692,	 Acc = 0.4413
380 0.179
1718 0.483
815 0.486
85 0.365
2 0.0
0 0.0
0 0.0
0 0.0
0.47938931297709925
0.501145038167939
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5489,	 Acc1 = 0.3724,	 Acc2 = 0.4264

 ===== Epoch 284	 =====
[-0.36602148 -0.3783333   3.735742    2.1826293   3.551579    0.75447047
  2.9202187   3.3777292  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.2713953e-11 -3.1991790e-10  9.6852496e-02  1.2373462e-02
 -1.2006235e+00  2.5312187e-02  4.3739382e-02  6.8004145e-03
 -6.1223204e-10 -1.8893141e-10 -4.9447713e-10 -1.9952585e-10
  5.0565923e-11 -3.3566333e-10] 3 3
train:	 Loss = 1.4995,	 Acc = 0.4298
2925 0.249
5677 0.459
2916 0.54
375 0.552
69 0.362
6 0.333
0 0.0
0 0.0
0.488333517637952
0.501145038167939
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.5025,	 Acc = 0.4407
380 0.339
1718 0.482
815 0.417
85 0.282
2 0.5
0 0.0
0 0.0
0 0.0
0.45534351145038165
0.501145038167939
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5685,	 Acc1 = 0.4007,	 Acc2 = 0.4267

 ===== Epoch 285	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.5245826   2.0395088
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -3.74330836e-03 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 0 0
train:	 Loss = 1.4971,	 Acc = 0.4311
2919 0.251
5684 0.463
2914 0.532
376 0.58
69 0.362
6 0.167
0 0.0
0 0.0
0.4891148193170516
0.501145038167939
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4854,	 Acc = 0.4553
380 0.334
1718 0.485
815 0.464
85 0.318
2 0.5
0 0.0
0 0.0
0 0.0
0.4729007633587786
0.501145038167939
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5388,	 Acc1 = 0.4186,	 Acc2 = 0.4483

 ===== Epoch 286	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.671387    2.20577    -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10  8.18430519e+00  1.25937784e+00
  5.23268769e-04  1.12748065e-04  5.24857193e-02  6.80041552e-01
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 6 6
train:	 Loss = 1.4995,	 Acc = 0.4290
2923 0.251
5679 0.456
2914 0.536
377 0.578
69 0.406
6 0.167
0 0.0
0 0.0
0.48656716417910445
0.501145038167939
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 3
val:	 Loss = 1.4922,	 Acc = 0.4577
380 0.339
1718 0.486
815 0.465
85 0.341
2 0.5
0 0.0
0 0.0
0 0.0
0.47480916030534354
0.501145038167939
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5525,	 Acc1 = 0.4110,	 Acc2 = 0.4391

 ===== Epoch 287	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  3.099963    1.0366008  -0.40141198 -0.40778467  2.6607103   2.2539315
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04  1.92454070e-01  3.87623668e-01
 -6.12232043e-10 -1.88931412e-10  2.59211063e-01  3.76517147e-01
  5.05659230e-11 -3.35663330e-10] 6 6
train:	 Loss = 1.5029,	 Acc = 0.4306
2926 0.251
5676 0.459
2915 0.54
376 0.561
69 0.406
6 0.167
0 0.0
0 0.0
0.488829904888299
0.501145038167939
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4838,	 Acc = 0.4620
380 0.339
1718 0.491
815 0.472
85 0.329
2 0.5
0 0.0
0 0.0
0 0.0
0.47977099236641224
0.501145038167939
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5381,	 Acc1 = 0.4176,	 Acc2 = 0.4471

 ===== Epoch 288	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467  1.9928231   2.1479526
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10  8.60931873e-02 -3.51416022e-02
  5.05659230e-11 -3.35663330e-10] 2 3
train:	 Loss = 1.5002,	 Acc = 0.4305
2920 0.251
5684 0.458
2913 0.542
376 0.564
69 0.362
6 0.333
0 0.0
0 0.0
0.4885057471264368
0.501145038167939
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 2
val:	 Loss = 1.5283,	 Acc = 0.4297
380 0.337
1718 0.466
815 0.412
85 0.282
2 0.5
0 0.0
0 0.0
0 0.0
0.4431297709923664
0.501145038167939
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5852,	 Acc1 = 0.3932,	 Acc2 = 0.4177

 ===== Epoch 289	 =====
[-0.36602148 -0.3783333   3.2910037   2.8327742   2.6646886   1.1056483
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.2713953e-11 -3.1991790e-10  4.1954029e-01 -7.7964897e+00
 -3.2398425e-02  7.5711064e-02 -1.5722629e-10 -9.7300190e-10
  4.0006318e+00  2.3673246e+00 -4.9447713e-10 -1.9952585e-10
  5.0565923e-11 -3.3566333e-10] 2 2
train:	 Loss = 1.5009,	 Acc = 0.4274
2922 0.25
5680 0.455
2914 0.534
377 0.565
69 0.377
6 0.167
0 0.0
0 0.0
0.4846340924165377
0.501145038167939
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4985,	 Acc = 0.4390
380 0.337
1718 0.463
815 0.442
85 0.376
2 0.5
0 0.0
0 0.0
0 0.0
0.4538167938931298
0.501145038167939
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5575,	 Acc1 = 0.4056,	 Acc2 = 0.4327

 ===== Epoch 290	 =====
[-0.36602148 -0.3783333   2.2086048   1.731832   -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10  5.49018204e-01  1.30537197e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 0 2
train:	 Loss = 1.4999,	 Acc = 0.4306
2922 0.252
5682 0.461
2913 0.533
377 0.576
68 0.382
6 0.333
0 0.0
0 0.0
0.488503205836834
0.501145038167939
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 3
val:	 Loss = 1.4907,	 Acc = 0.4507
380 0.337
1718 0.477
815 0.455
85 0.376
2 0.5
0 0.0
0 0.0
0 0.0
0.467175572519084
0.501145038167939
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5599,	 Acc1 = 0.4079,	 Acc2 = 0.4354

 ===== Epoch 291	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335   1.4416807   0.8478215
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.2713953e-11 -3.1991790e-10 -2.5436093e-04 -1.9714674e-04
  1.2622401e-01  1.2610994e-01 -1.5722629e-10 -9.7300190e-10
 -6.1223204e-10 -1.8893141e-10 -4.9447713e-10 -1.9952585e-10
  5.0565923e-11 -3.3566333e-10] 3 2
train:	 Loss = 1.4993,	 Acc = 0.4334
2925 0.251
5679 0.464
2912 0.538
377 0.589
69 0.377
6 0.167
0 0.0
0 0.0
0.49242508017250913
0.501145038167939
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4702,	 Acc = 0.4647
380 0.334
1718 0.494
815 0.476
85 0.353
2 0.5
0 0.0
0 0.0
0 0.0
0.48358778625954196
0.501145038167939
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5533,	 Acc1 = 0.4009,	 Acc2 = 0.4269

 ===== Epoch 292	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 0 1
train:	 Loss = 1.4990,	 Acc = 0.4298
2921 0.248
5683 0.458
2912 0.542
377 0.549
69 0.377
6 0.5
0 0.0
0 0.0
0.4884492096827678
0.501145038167939
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4636,	 Acc = 0.4707
380 0.342
1718 0.494
815 0.494
85 0.353
2 0.5
0 0.0
0 0.0
0 0.0
0.48931297709923666
0.501145038167939
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5277,	 Acc1 = 0.4143,	 Acc2 = 0.4431

 ===== Epoch 293	 =====
[-0.36602148 -0.3783333   2.438894    2.193956   -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10  6.33989036e-01 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 0 0
train:	 Loss = 1.4985,	 Acc = 0.4317
2921 0.251
5684 0.461
2913 0.541
375 0.544
69 0.435
6 0.167
0 0.0
0 0.0
0.489996683983641
0.501145038167939
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.5300,	 Acc = 0.4423
380 0.339
1718 0.483
815 0.422
85 0.271
2 0.5
0 0.0
0 0.0
0 0.0
0.45725190839694657
0.501145038167939
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5853,	 Acc1 = 0.4000,	 Acc2 = 0.4259

 ===== Epoch 294	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 1 1
train:	 Loss = 1.5016,	 Acc = 0.4296
2923 0.251
5681 0.461
2914 0.534
376 0.559
68 0.338
6 0.167
0 0.0
0 0.0
0.4874516307352128
0.501145038167939
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 2
val:	 Loss = 1.5094,	 Acc = 0.4327
380 0.334
1718 0.457
815 0.433
85 0.376
2 0.5
0 0.0
0 0.0
0 0.0
0.44694656488549617
0.501145038167939
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5666,	 Acc1 = 0.4000,	 Acc2 = 0.4259

 ===== Epoch 295	 =====
[-0.36602148 -0.3783333   2.4510777   3.0728977   1.9099528   0.9345046
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.2713953e-11 -3.1991790e-10 -7.1530299e+00 -3.7814364e+00
  4.8836574e-01  7.6725800e-03 -1.5722629e-10 -9.7300190e-10
 -6.1223204e-10 -1.8893141e-10 -4.9447713e-10 -1.9952585e-10
  5.0565923e-11 -3.3566333e-10] 4 0
train:	 Loss = 1.4976,	 Acc = 0.4340
2921 0.251
5683 0.463
2912 0.544
377 0.578
69 0.377
6 0.167
0 0.0
0 0.0
0.4929810987067536
0.501145038167939
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.5192,	 Acc = 0.4457
380 0.342
1718 0.483
815 0.431
85 0.306
2 0.5
0 0.0
0 0.0
0 0.0
0.46068702290076335
0.501145038167939
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5806,	 Acc1 = 0.3986,	 Acc2 = 0.4242

 ===== Epoch 296	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  2.03807     2.569905  ] [ 2.93632174e+00  3.21356320e+00 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
 -1.04500167e-01 -1.74189568e-01] 1 5
train:	 Loss = 1.4995,	 Acc = 0.4286
2926 0.251
5683 0.457
2909 0.538
376 0.556
68 0.382
6 0.167
0 0.0
0 0.0
0.48617562486175625
0.501145038167939
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4834,	 Acc = 0.4613
380 0.334
1718 0.488
815 0.479
85 0.318
2 0.5
0 0.0
0 0.0
0 0.0
0.47977099236641224
0.501145038167939
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5379,	 Acc1 = 0.4178,	 Acc2 = 0.4473

 ===== Epoch 297	 =====
[ 2.4108732   2.8998234  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-6.86734319e-01  2.97208160e-01 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 6 6
train:	 Loss = 1.4993,	 Acc = 0.4300
2924 0.25
5681 0.458
2911 0.54
377 0.565
69 0.406
6 0.167
0 0.0
0 0.0
0.4882795223352499
0.501145038167939
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4978,	 Acc = 0.4333
380 0.337
1718 0.46
815 0.433
85 0.318
2 0.5
0 0.0
0 0.0
0 0.0
0.44732824427480916
0.501145038167939
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5503,	 Acc1 = 0.4007,	 Acc2 = 0.4267

 ===== Epoch 298	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 1 1
train:	 Loss = 1.4999,	 Acc = 0.4317
2926 0.251
5677 0.459
2915 0.545
375 0.563
69 0.391
6 0.333
0 0.0
0 0.0
0.49015704490157047
0.501145038167939
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 2
val:	 Loss = 1.5183,	 Acc = 0.4337
380 0.339
1718 0.469
815 0.415
85 0.318
2 0.5
0 0.0
0 0.0
0 0.0
0.44732824427480916
0.501145038167939
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5764,	 Acc1 = 0.3885,	 Acc2 = 0.4120

 ===== Epoch 299	 =====
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
  1.3167702   1.4204416 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
 -3.87961102e+00 -3.62103152e+00] 2 1
train:	 Loss = 1.4992,	 Acc = 0.4297
2924 0.251
5680 0.459
2913 0.538
376 0.545
69 0.406
6 0.333
0 0.0
0 0.0
0.48761609907120745
0.501145038167939
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.4877,	 Acc = 0.4493
380 0.332
1718 0.49
815 0.429
85 0.341
2 0.5
0 0.0
0 0.0
0 0.0
0.466412213740458
0.501145038167939
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5432,	 Acc1 = 0.4027,	 Acc2 = 0.4292

 ===== Epoch 300	 =====
[ 4.9140444  -5.7879252  -0.4208693  -0.3341335   0.89358026  2.032491
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [-5.85970354e+00  6.61597776e+00 -2.54360930e-04 -1.97146743e-04
  1.11260444e-01 -1.23364508e-01 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 1 5
train:	 Loss = 1.4979,	 Acc = 0.4319
2920 0.251
5681 0.462
2917 0.537
375 0.579
69 0.406
6 0.167
0 0.0
0 0.0
0.49038461538461536
0.501145038167939
[-0.36602148 -0.3783333  -0.4208693  -0.3341335  -0.4409929  -0.3635196
  2.6213946   1.6951303  -0.40141198 -0.40778467  2.5944145   2.9218464
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -2.54360930e-04 -1.97146743e-04
  5.23268769e-04  1.12748065e-04 -2.49911733e-02  8.16049799e-02
 -6.12232043e-10 -1.88931412e-10 -3.55614237e-02  5.02022840e-02
  5.05659230e-11 -3.35663330e-10] 3 4
val:	 Loss = 1.5097,	 Acc = 0.4653
380 0.334
1718 0.499
815 0.469
85 0.329
2 0.5
0 0.0
0 0.0
0 0.0
0.48435114503816795
0.501145038167939
[-0.36602148 -0.3783333   1.0742188   2.3366706  -0.4409929  -0.3635196
 -0.3640846  -0.3725409  -0.40141198 -0.40778467 -0.42342317 -0.42019257
 -0.38555372 -0.3798593 ] [ 7.27139529e-11 -3.19917898e-10 -4.98209447e-02  2.28587940e-01
  5.23268769e-04  1.12748065e-04 -1.57226288e-10 -9.73001901e-10
 -6.12232043e-10 -1.88931412e-10 -4.94477126e-10 -1.99525854e-10
  5.05659230e-11 -3.35663330e-10] 4 6
Testing:	 Loss = 1.5732,	 Acc1 = 0.4011,	 Acc2 = 0.4272
